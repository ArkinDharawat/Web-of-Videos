,end_time,lecture,lecture_name,lecture_no,sentence,start_time,sub_lecture,tokenized_sentence
0,00:00:01,2,TF Transformation,1.8,[SOUND],00:00:00,8,SOUND
1,00:00:15,2,TF Transformation,1.8,"In this lecture, we continue the discussion of Vector Space Model.",00:00:10,8,In lecture continue discussion Vector Space Model
2,00:00:18,2,TF Transformation,1.8,"In particular, we are going to talk about the TF transformation.",00:00:15,8,In particular going talk TF transformation
3,00:00:20,2,TF Transformation,1.8,"In the previous lecture,",00:00:18,8,In previous lecture
4,00:00:25,2,TF Transformation,1.8,we have derived a TF-IDF weighting formula using the vector space model.,00:00:20,8,derived TF IDF weighting formula using vector space model
5,00:00:32,2,TF Transformation,1.8,And we have shown that this model actually works pretty well for,00:00:27,8,And shown model actually works pretty well
6,00:00:37,2,TF Transformation,1.8,"these examples as shown on this slide except for d5,",00:00:32,8,examples shown slide except d5
7,00:00:41,2,TF Transformation,1.8,which has received a very high score.,00:00:37,8,received high score
8,00:00:46,2,TF Transformation,1.8,"Indeed, it has received the highest score among all these documents.",00:00:41,8,Indeed received highest score among documents
9,00:00:53,2,TF Transformation,1.8,"But this document is intuitively non-relevant, so this is not desirable.",00:00:46,8,But document intuitively non relevant desirable
10,00:00:57,2,TF Transformation,1.8,"In this lecture, we're going to talk about how would you use TF",00:00:53,8,In lecture going talk would use TF
11,00:01:00,2,TF Transformation,1.8,transformation to solve this problem.,00:00:57,8,transformation solve problem
12,00:01:05,2,TF Transformation,1.8,"Before we discuss the details, let's take a look at the formula for",00:01:00,8,Before discuss details let take look formula
13,00:01:09,2,TF Transformation,1.8,this symbol here for IDF weighting ranking function and,00:01:05,8,symbol IDF weighting ranking function
14,00:01:13,2,TF Transformation,1.8,see why this document has received such a high score.,00:01:09,8,see document received high score
15,00:01:17,2,TF Transformation,1.8,"So this is the formula, and if you look at the formula carefully,",00:01:13,8,So formula look formula carefully
16,00:01:23,2,TF Transformation,1.8,then you will see it involves a sum over all the matched query terms.,00:01:17,8,see involves sum matched query terms
17,00:01:28,2,TF Transformation,1.8,"And inside the sum, each matched query sum has a particular weight.",00:01:23,8,And inside sum matched query sum particular weight
18,00:01:30,2,TF Transformation,1.8,And this weight is TF-IDF weighting.,00:01:28,8,And weight TF IDF weighting
19,00:01:36,2,TF Transformation,1.8,So it has an IDF component where we see 2 variables.,00:01:31,8,So IDF component see 2 variables
20,00:01:41,2,TF Transformation,1.8,"One is the total number of documents in the collection, and that is m.",00:01:36,8,One total number documents collection
21,00:01:45,2,TF Transformation,1.8,The other is the documentive frequency.,00:01:41,8,The documentive frequency
22,00:01:52,2,TF Transformation,1.8,This is the number of documents that contain this word w.,00:01:45,8,This number documents contain word w
23,00:01:54,2,TF Transformation,1.8,"The other variables in, involving the formula,",00:01:52,8,The variables involving formula
24,00:01:58,2,TF Transformation,1.8,include the count of the query term.,00:01:54,8,include count query term
25,00:02:06,2,TF Transformation,1.8,"W in the query, and the count of the word in the document.",00:02:01,8,W query count word document
26,00:02:11,2,TF Transformation,1.8,"If you look at this document again, now it's not hard to",00:02:07,8,If look document hard
27,00:02:16,2,TF Transformation,1.8,realize that the reason why it has received a high score is because,00:02:11,8,realize reason received high score
28,00:02:21,2,TF Transformation,1.8,it has a very high count of campaign.,00:02:16,8,high count campaign
29,00:02:27,2,TF Transformation,1.8,"So the count of campaign in this document is a four, which is much higher than",00:02:21,8,So count campaign document four much higher
30,00:02:31,2,TF Transformation,1.8,"the other documents, and has contributed to the high score of this document.",00:02:27,8,documents contributed high score document
31,00:02:37,2,TF Transformation,1.8,"So intriguingly, in order to lower the score for this document, we need",00:02:31,8,So intriguingly order lower score document need
32,00:02:43,2,TF Transformation,1.8,"to somehow restrict the contribution of, the matching of this term in the document.",00:02:37,8,somehow restrict contribution matching term document
33,00:02:48,2,TF Transformation,1.8,And if you think about the matching of terms in the document carefully you,00:02:44,8,And think matching terms document carefully
34,00:02:53,2,TF Transformation,1.8,actually would realize we probably shouldn't reward,00:02:48,8,actually would realize probably reward
35,00:02:58,2,TF Transformation,1.8,multiple occurrences so generously.,00:02:53,8,multiple occurrences generously
36,00:03:04,2,TF Transformation,1.8,"And by that I mean the first occurrence of a term says a lot about the,",00:02:58,8,And I mean first occurrence term says lot
37,00:03:09,2,TF Transformation,1.8,"the matching of this term, because it goes from zero count",00:03:04,8,matching term goes zero count
38,00:03:15,2,TF Transformation,1.8,"to a count of one, and that increase means a lot.",00:03:09,8,count one increase means lot
39,00:03:19,2,TF Transformation,1.8,"Once we see a word in the document,",00:03:17,8,Once see word document
40,00:03:21,2,TF Transformation,1.8,it's very likely that the document is talking about this word.,00:03:19,8,likely document talking word
41,00:03:26,2,TF Transformation,1.8,"If we see an extra occurrence on top of the first occurrence,",00:03:23,8,If see extra occurrence top first occurrence
42,00:03:34,2,TF Transformation,1.8,"that is to go from one to two, then we also can say that well, the second",00:03:28,8,go one two also say well second
43,00:03:38,2,TF Transformation,1.8,occurrence kind of confirmed that it's not a accidental mention of the word.,00:03:34,8,occurrence kind confirmed accidental mention word
44,00:03:44,2,TF Transformation,1.8,"Now, we are more sure that this document is talking about this word.",00:03:39,8,Now sure document talking word
45,00:03:50,2,TF Transformation,1.8,"But imagine we have seen, let's say, 50 times of the word in the document.",00:03:44,8,But imagine seen let say 50 times word document
46,00:03:54,2,TF Transformation,1.8,"Then, adding one extra occurrence is not going to test more about",00:03:50,8,Then adding one extra occurrence going test
47,00:03:59,2,TF Transformation,1.8,evidence because we are already sure that this document is about this word.,00:03:54,8,evidence already sure document word
48,00:04:04,2,TF Transformation,1.8,So if you're thinking this way it seems that,00:04:01,8,So thinking way seems
49,00:04:08,2,TF Transformation,1.8,we should restrict the contributing of a high account of term.,00:04:04,8,restrict contributing high account term
50,00:04:12,2,TF Transformation,1.8,And that is the idea of TF Transformation.,00:04:09,8,And idea TF Transformation
51,00:04:17,2,TF Transformation,1.8,So this transformation function is going to turn the raw count of word,00:04:12,8,So transformation function going turn raw count word
52,00:04:22,2,TF Transformation,1.8,"into a Term Frequency Weight, for the word in the document.",00:04:17,8,Term Frequency Weight word document
53,00:04:27,2,TF Transformation,1.8,"So here I show in x-axis, that raw count,",00:04:22,8,So I show x axis raw count
54,00:04:31,2,TF Transformation,1.8,and in y-axis I show the Term Frequency Weight.,00:04:27,8,axis I show Term Frequency Weight
55,00:04:37,2,TF Transformation,1.8,"So, in the previous ranking functions we actually have increasingly,",00:04:33,8,So previous ranking functions actually increasingly
56,00:04:40,2,TF Transformation,1.8,used some kind of transformation.,00:04:37,8,used kind transformation
57,00:04:44,2,TF Transformation,1.8,So for example in the zero-one bit vector retentation we actually use,00:04:40,8,So example zero one bit vector retentation actually use
58,00:04:49,2,TF Transformation,1.8,the Suchier transformation function as shown here.,00:04:44,8,Suchier transformation function shown
59,00:04:53,2,TF Transformation,1.8,Basically if the count is zero then it has zero weight.,00:04:49,8,Basically count zero zero weight
60,00:04:55,2,TF Transformation,1.8,Otherwise it would have a weight of one.,00:04:53,8,Otherwise would weight one
61,00:04:58,2,TF Transformation,1.8,It's flat.,00:04:57,8,It flat
62,00:05:04,2,TF Transformation,1.8,Now what about using Term Count as a TF weight.,00:04:59,8,Now using Term Count TF weight
63,00:05:06,2,TF Transformation,1.8,"Well that's a linear function, right?",00:05:04,8,Well linear function right
64,00:05:10,2,TF Transformation,1.8,So it has just exactly the same weight as the count.,00:05:06,8,So exactly weight count
65,00:05:16,2,TF Transformation,1.8,Now we have just seen that this is not desirable.,00:05:11,8,Now seen desirable
66,00:05:20,2,TF Transformation,1.8,So what we want is something like this.,00:05:18,8,So want something like
67,00:05:22,2,TF Transformation,1.8,"So for example with a logarithm function,",00:05:20,8,So example logarithm function
68,00:05:26,2,TF Transformation,1.8,we can have a sub-linear transformation that looks like this.,00:05:22,8,sub linear transformation looks like
69,00:05:30,2,TF Transformation,1.8,And this will control the influence of really high weight because it's going to,00:05:26,8,And control influence really high weight going
70,00:05:35,2,TF Transformation,1.8,"lower its inference, yet it will retain the inference of small count.",00:05:30,8,lower inference yet retain inference small count
71,00:05:41,2,TF Transformation,1.8,Or we might want to even bend the curve more by applying logarithm twice.,00:05:36,8,Or might want even bend curve applying logarithm twice
72,00:05:46,2,TF Transformation,1.8,Now people have tried all these methods and they are indeed working better than,00:05:42,8,Now people tried methods indeed working better
73,00:05:52,2,TF Transformation,1.8,"the linear form of the transformation, but so far what works the best",00:05:46,8,linear form transformation far works best
74,00:05:56,2,TF Transformation,1.8,seems to be this special transformation called a BM25 transformation.,00:05:52,8,seems special transformation called BM25 transformation
75,00:05:59,2,TF Transformation,1.8,BM stands for best matching.,00:05:58,8,BM stands best matching
76,00:06:05,2,TF Transformation,1.8,"Now in this transformation, you can see there's a parameter k here.",00:06:01,8,Now transformation see parameter k
77,00:06:10,2,TF Transformation,1.8,And this k controls the upper bound of this function.,00:06:06,8,And k controls upper bound function
78,00:06:15,2,TF Transformation,1.8,It's easy to see this function has a upper bound because if you look at,00:06:10,8,It easy see function upper bound look
79,00:06:22,2,TF Transformation,1.8,"the x divided by x plus k where k is not an active number,",00:06:15,8,x divided x plus k k active number
80,00:06:28,2,TF Transformation,1.8,"then the numerator will never be able to exceed the denominator, right?",00:06:22,8,numerator never able exceed denominator right
81,00:06:29,2,TF Transformation,1.8,"So, it's upper bounded by k plus 1.",00:06:28,8,So upper bounded k plus 1
82,00:06:34,2,TF Transformation,1.8,"Now, this is also difference between this transformation function and",00:06:29,8,Now also difference transformation function
83,00:06:35,2,TF Transformation,1.8,the logarithm transformation.,00:06:34,8,logarithm transformation
84,00:06:39,2,TF Transformation,1.8,Which it doesn't have upperbound.,00:06:37,8,Which upperbound
85,00:06:44,2,TF Transformation,1.8,"Now furthermore, one interesting property of this function is that as we vary K,",00:06:39,8,Now furthermore one interesting property function vary K
86,00:06:50,2,TF Transformation,1.8,"we can actually simulate different transformation functions,",00:06:45,8,actually simulate different transformation functions
87,00:06:52,2,TF Transformation,1.8,including the two extremes that are shown here.,00:06:50,8,including two extremes shown
88,00:06:57,2,TF Transformation,1.8,"That is a zero one bit transformation, and the unit transformation.",00:06:52,8,That zero one bit transformation unit transformation
89,00:07:01,2,TF Transformation,1.8,"So for example, if we set k to zero, now you can see",00:06:57,8,So example set k zero see
90,00:07:05,2,TF Transformation,1.8,the function value would be one.,00:07:03,8,function value would one
91,00:07:13,2,TF Transformation,1.8,"So we precisely, recover the zero one bit transformation.",00:07:07,8,So precisely recover zero one bit transformation
92,00:07:19,2,TF Transformation,1.8,"If you set k to a very large number, on the other hand,",00:07:15,8,If set k large number hand
93,00:07:22,2,TF Transformation,1.8,"other hand, it's going to look more like the linear transformation function.",00:07:19,8,hand going look like linear transformation function
94,00:07:29,2,TF Transformation,1.8,"So in this sense, this transformation is very flexible,",00:07:24,8,So sense transformation flexible
95,00:07:34,2,TF Transformation,1.8,it allows us to control the shape of the transformation.,00:07:29,8,allows us control shape transformation
96,00:07:37,2,TF Transformation,1.8,It also has a nice property of the upper bound.,00:07:34,8,It also nice property upper bound
97,00:07:43,2,TF Transformation,1.8,And this upper bound is useful to control the inference of a particular term.,00:07:37,8,And upper bound useful control inference particular term
98,00:07:49,2,TF Transformation,1.8,"And so that we can prevent a, a spammer from just increasing the count of",00:07:43,8,And prevent spammer increasing count
99,00:07:54,2,TF Transformation,1.8,1 term to spam all queries that might match this term.,00:07:49,8,1 term spam queries might match term
100,00:08:02,2,TF Transformation,1.8,In other words this upper bound might also ensure that all terms,00:07:57,8,In words upper bound might also ensure terms
101,00:08:06,2,TF Transformation,1.8,"will be counted when we aggregate the, the weights, to compute a score.",00:08:02,8,counted aggregate weights compute score
102,00:08:10,2,TF Transformation,1.8,"As I said, this transformation function has worked well, so far.",00:08:06,8,As I said transformation function worked well far
103,00:08:14,2,TF Transformation,1.8,"So to summarise this lecture,",00:08:12,8,So summarise lecture
104,00:08:19,2,TF Transformation,1.8,the main point is that we need to do some sub linearity of TF Transformation.,00:08:14,8,main point need sub linearity TF Transformation
105,00:08:24,2,TF Transformation,1.8,And this is needed to capture the intuition of diminishing return from,00:08:19,8,And needed capture intuition diminishing return
106,00:08:25,2,TF Transformation,1.8,high Term Counts.,00:08:24,8,high Term Counts
107,00:08:31,2,TF Transformation,1.8,It's also to avoid a dominance by one single term over all others.,00:08:26,8,It also avoid dominance one single term others
108,00:08:37,2,TF Transformation,1.8,"This BM25 Transformation, Transformation that we talked about is very interesting.",00:08:31,8,This BM25 Transformation Transformation talked interesting
109,00:08:43,2,TF Transformation,1.8,It's so far one of the best performing TF Transforming formation formulas.,00:08:37,8,It far one best performing TF Transforming formation formulas
110,00:08:47,2,TF Transformation,1.8,"It has upper bound, and it's also robust and effective.",00:08:43,8,It upper bound also robust effective
111,00:08:52,2,TF Transformation,1.8,"Now, if we're plug in this function into our TF-IDF weighting",00:08:47,8,Now plug function TF IDF weighting
112,00:08:57,2,TF Transformation,1.8,vector space model then we would end up having the following,00:08:52,8,vector space model would end following
113,00:09:01,2,TF Transformation,1.8,"ranking function, which has a BM25 TF component.",00:08:57,8,ranking function BM25 TF component
114,00:09:06,2,TF Transformation,1.8,Now this is already very close to a state,00:09:01,8,Now already close state
115,00:09:11,2,TF Transformation,1.8,of the art ranking function called a BM25.,00:09:06,8,art ranking function called BM25
116,00:09:19,2,TF Transformation,1.8,And we will discuss how we can further improve this formula in the next lecture.,00:09:11,8,And discuss improve formula next lecture
117,00:00:05,4,Feedback in Text Retrieval,3.6,[SOUND] This lecture is about,00:00:00,8,SOUND This lecture
118,00:00:12,4,Feedback in Text Retrieval,3.6,the Feedback in Text Retrieval.,00:00:05,8,Feedback Text Retrieval
119,00:00:14,4,Feedback in Text Retrieval,3.6,"So, in this lecture,",00:00:12,8,So lecture
120,00:00:18,4,Feedback in Text Retrieval,3.6,we're going to continue the discussion on text retrieval methods.,00:00:14,8,going continue discussion text retrieval methods
121,00:00:24,4,Feedback in Text Retrieval,3.6,"In particular, we're going to talk about Feedback in Text Retrieval.",00:00:18,8,In particular going talk Feedback Text Retrieval
122,00:00:30,4,Feedback in Text Retrieval,3.6,This is a diagram that shows the retrieval process.,00:00:24,8,This diagram shows retrieval process
123,00:00:34,4,Feedback in Text Retrieval,3.6,We can see the user would typed in a query and,00:00:30,8,We see user would typed query
124,00:00:40,4,Feedback in Text Retrieval,3.6,then the query would be sent to a Retrieval Engine or,00:00:34,8,query would sent Retrieval Engine
125,00:00:45,4,Feedback in Text Retrieval,3.6,search engine and the engine would return results.,00:00:40,8,search engine engine would return results
126,00:00:49,4,Feedback in Text Retrieval,3.6,These results would be shown to the user.,00:00:45,8,These results would shown user
127,00:00:55,4,Feedback in Text Retrieval,3.6,"After the user has seen these results, the user can actually make judgments.",00:00:49,8,After user seen results user actually make judgments
128,00:00:58,4,Feedback in Text Retrieval,3.6,"So for example, the user has say, well, this is good and",00:00:55,8,So example user say well good
129,00:01:00,4,Feedback in Text Retrieval,3.6,this document is not very useful.,00:00:58,8,document useful
130,00:01:03,4,Feedback in Text Retrieval,3.6,"This is good again, et cetera.",00:01:00,8,This good et cetera
131,00:01:07,4,Feedback in Text Retrieval,3.6,"Now this is called a relevance judgment or Relevance Feedback, because we've",00:01:03,8,Now called relevance judgment Relevance Feedback
132,00:01:12,4,Feedback in Text Retrieval,3.6,got some feedback information from the user based on the judgments.,00:01:07,8,got feedback information user based judgments
133,00:01:14,4,Feedback in Text Retrieval,3.6,This can be very useful to the system.,00:01:12,8,This useful system
134,00:01:18,4,Feedback in Text Retrieval,3.6,Learn what exactly is interesting to the user.,00:01:14,8,Learn exactly interesting user
135,00:01:22,4,Feedback in Text Retrieval,3.6,So the feedback module would then take this as input and,00:01:18,8,So feedback module would take input
136,00:01:26,4,Feedback in Text Retrieval,3.6,also use the document collection to try to improve ranking.,00:01:22,8,also use document collection try improve ranking
137,00:01:30,4,Feedback in Text Retrieval,3.6,"Typically, it would involve updating the query.",00:01:26,8,Typically would involve updating query
138,00:01:34,4,Feedback in Text Retrieval,3.6,So the system can now rank the results more accurately for the user.,00:01:30,8,So system rank results accurately user
139,00:01:36,4,Feedback in Text Retrieval,3.6,So this is called Relevance Feedback.,00:01:34,8,So called Relevance Feedback
140,00:01:42,4,Feedback in Text Retrieval,3.6,The feedback is based on relevance judgements made by the users.,00:01:36,8,The feedback based relevance judgements made users
141,00:01:44,4,Feedback in Text Retrieval,3.6,"Now these judgements are reliable, but",00:01:42,8,Now judgements reliable
142,00:01:50,4,Feedback in Text Retrieval,3.6,"the users generally don't want to make extra effort, unless they have to.",00:01:44,8,users generally want make extra effort unless
143,00:01:57,4,Feedback in Text Retrieval,3.6,So the downside's that involves some extra effort by the user.,00:01:50,8,So downside involves extra effort user
144,00:02:00,4,Feedback in Text Retrieval,3.6,"There is another form of feedback called a Pseudo Relevance Feedback,",00:01:57,8,There another form feedback called Pseudo Relevance Feedback
145,00:02:03,4,Feedback in Text Retrieval,3.6,or a blind feedback also called an automatic feedback.,00:02:00,8,blind feedback also called automatic feedback
146,00:02:08,4,Feedback in Text Retrieval,3.6,"In this case, you can see once the user has got without an effect,",00:02:03,8,In case see user got without effect
147,00:02:11,4,Feedback in Text Retrieval,3.6,we don't have to involve users.,00:02:08,8,involve users
148,00:02:14,4,Feedback in Text Retrieval,3.6,So you can see there's no user involved here.,00:02:11,8,So see user involved
149,00:02:19,4,Feedback in Text Retrieval,3.6,And we simply assume that the top ranked documents to be relevant.,00:02:14,8,And simply assume top ranked documents relevant
150,00:02:24,4,Feedback in Text Retrieval,3.6,"Let's say, we have assumed the top ten is relevant.",00:02:19,8,Let say assumed top ten relevant
151,00:02:28,4,Feedback in Text Retrieval,3.6,And then we will then use these assumed,00:02:24,8,And use assumed
152,00:02:33,4,Feedback in Text Retrieval,3.6,documents to learn and to improve the query.,00:02:28,8,documents learn improve query
153,00:02:35,4,Feedback in Text Retrieval,3.6,"Now you might wonder, you know,",00:02:33,8,Now might wonder know
154,00:02:40,4,Feedback in Text Retrieval,3.6,how could this help if we simply assume the top rank documents would be random.,00:02:35,8,could help simply assume top rank documents would random
155,00:02:47,4,Feedback in Text Retrieval,3.6,Well you can imagine these top rank documents are actually similar to relevant,00:02:40,8,Well imagine top rank documents actually similar relevant
156,00:02:53,4,Feedback in Text Retrieval,3.6,"documents, even if they are not relevant, they look like relevant documents.",00:02:47,8,documents even relevant look like relevant documents
157,00:02:59,4,Feedback in Text Retrieval,3.6,"So, it's possible to learn some related terms to the query from this set.",00:02:53,8,So possible learn related terms query set
158,00:03:03,4,Feedback in Text Retrieval,3.6,"In fact, you may recall that we talked about using language model to",00:02:59,8,In fact may recall talked using language model
159,00:03:08,4,Feedback in Text Retrieval,3.6,analyze word association to learn related words to the word computer.,00:03:03,8,analyze word association learn related words word computer
160,00:03:09,4,Feedback in Text Retrieval,3.6,Right?,00:03:08,8,Right
161,00:03:11,4,Feedback in Text Retrieval,3.6,"And then what we did is first,",00:03:09,8,And first
162,00:03:15,4,Feedback in Text Retrieval,3.6,use computer to retrieve all the documents that contain computer.,00:03:11,8,use computer retrieve documents contain computer
163,00:03:18,4,Feedback in Text Retrieval,3.6,"So, imagine now the query here is a computer.",00:03:15,8,So imagine query computer
164,00:03:19,4,Feedback in Text Retrieval,3.6,Right?,00:03:18,8,Right
165,00:03:23,4,Feedback in Text Retrieval,3.6,And then the results will be those documents that contain computer.,00:03:19,8,And results documents contain computer
166,00:03:28,4,Feedback in Text Retrieval,3.6,And what we can do then is to take the top end results.,00:03:23,8,And take top end results
167,00:03:33,4,Feedback in Text Retrieval,3.6,They can match computer very well and we're going to count,00:03:28,8,They match computer well going count
168,00:03:38,4,Feedback in Text Retrieval,3.6,the terms in this set and then we're going to then use the background,00:03:33,8,terms set going use background
169,00:03:44,4,Feedback in Text Retrieval,3.6,"language model to choose the terms that are frequent the in this set,",00:03:38,8,language model choose terms frequent set
170,00:03:47,4,Feedback in Text Retrieval,3.6,but not frequent the in the whole collection.,00:03:44,8,frequent whole collection
171,00:03:52,4,Feedback in Text Retrieval,3.6,"So, if we make a contrast between these two, what we can find is that",00:03:47,8,So make contrast two find
172,00:03:58,4,Feedback in Text Retrieval,3.6,"we'll learn some related terms too, the work computer as what I've seen before.",00:03:52,8,learn related terms work computer I seen
173,00:04:04,4,Feedback in Text Retrieval,3.6,And these related words can then be added to the original query to expand the query.,00:03:58,8,And related words added original query expand query
174,00:04:08,4,Feedback in Text Retrieval,3.6,"And this would help us free documents that don't necessarily match computer,",00:04:04,8,And would help us free documents necessarily match computer
175,00:04:11,4,Feedback in Text Retrieval,3.6,but match other words like program and software.,00:04:08,8,match words like program software
176,00:04:18,4,Feedback in Text Retrieval,3.6,So this is factored for improving the search doubt.,00:04:11,8,So factored improving search doubt
177,00:04:21,4,Feedback in Text Retrieval,3.6,"But of course, pseudo relevancy feedback is completely unreliable.",00:04:18,8,But course pseudo relevancy feedback completely unreliable
178,00:04:23,4,Feedback in Text Retrieval,3.6,We have to arbitrarily set a cutoff.,00:04:21,8,We arbitrarily set cutoff
179,00:04:26,4,Feedback in Text Retrieval,3.6,So there is also something in between called Implicit Feedback.,00:04:23,8,So also something called Implicit Feedback
180,00:04:30,4,Feedback in Text Retrieval,3.6,"In this case, what we do, we do involve users, but",00:04:26,8,In case involve users
181,00:04:33,4,Feedback in Text Retrieval,3.6,we don't have to ask users to make judgements.,00:04:30,8,ask users make judgements
182,00:04:38,4,Feedback in Text Retrieval,3.6,"Instead, we are going to observe how the user interacts with the search results.",00:04:33,8,Instead going observe user interacts search results
183,00:04:41,4,Feedback in Text Retrieval,3.6,"So, in this case, we're going to look at the clickthroughs.",00:04:38,8,So case going look clickthroughs
184,00:04:45,4,Feedback in Text Retrieval,3.6,"So the user clicked on this one and the, the user viewed this one.",00:04:41,8,So user clicked one user viewed one
185,00:04:50,4,Feedback in Text Retrieval,3.6,And the user skipped this one and the user viewed this one again.,00:04:45,8,And user skipped one user viewed one
186,00:04:56,4,Feedback in Text Retrieval,3.6,Now this also is a clue about whether a document is useful to the user and,00:04:50,8,Now also clue whether document useful user
187,00:05:03,4,Feedback in Text Retrieval,3.6,we can even assume that we're going to use only the snippet here in this document.,00:04:56,8,even assume going use snippet document
188,00:05:06,4,Feedback in Text Retrieval,3.6,"The text that's actually seen by the user,",00:05:03,8,The text actually seen user
189,00:05:11,4,Feedback in Text Retrieval,3.6,instead of the actual document of this entry in the link.,00:05:06,8,instead actual document entry link
190,00:05:15,4,Feedback in Text Retrieval,3.6,"There that same web search may be broken, but that, it doesn't matter.",00:05:11,8,There web search may broken matter
191,00:05:20,4,Feedback in Text Retrieval,3.6,"If the user tries to fetch this document that because of the displayed text,",00:05:15,8,If user tries fetch document displayed text
192,00:05:26,4,Feedback in Text Retrieval,3.6,"we can assume this displayed text is probably relevant is interesting to user,",00:05:20,8,assume displayed text probably relevant interesting user
193,00:05:29,4,Feedback in Text Retrieval,3.6,so we can learn from such information.,00:05:26,8,learn information
194,00:05:32,4,Feedback in Text Retrieval,3.6,"And this is called Implicit Feedback and we can again,",00:05:29,8,And called Implicit Feedback
195,00:05:35,4,Feedback in Text Retrieval,3.6,use the information to update the query.,00:05:32,8,use information update query
196,00:05:39,4,Feedback in Text Retrieval,3.6,This is a very important technique used in modern search engines.,00:05:35,8,This important technique used modern search engines
197,00:05:45,4,Feedback in Text Retrieval,3.6,"You know, think about Google and Bing and they can collect a lot of user activities.",00:05:39,8,You know think Google Bing collect lot user activities
198,00:05:46,4,Feedback in Text Retrieval,3.6,Why they are serverless?,00:05:45,8,Why serverless
199,00:05:49,4,Feedback in Text Retrieval,3.6,Right. So they would observe what documents we,00:05:46,8,Right So would observe documents
200,00:05:51,4,Feedback in Text Retrieval,3.6,"click on, what documents we skip.",00:05:49,8,click documents skip
201,00:05:54,4,Feedback in Text Retrieval,3.6,And this information is very valuable and,00:05:51,8,And information valuable
202,00:05:58,4,Feedback in Text Retrieval,3.6,they can use this to encode the search engine.,00:05:54,8,use encode search engine
203,00:05:59,4,Feedback in Text Retrieval,3.6,"So to summarize,",00:05:58,8,So summarize
204,00:06:04,4,Feedback in Text Retrieval,3.6,we would talk about the three kinds of feedback here rather than feedback.,00:05:59,8,would talk three kinds feedback rather feedback
205,00:06:08,4,Feedback in Text Retrieval,3.6,"Where the use exquisite judgement, it takes some used effort, but",00:06:04,8,Where use exquisite judgement takes used effort
206,00:06:10,4,Feedback in Text Retrieval,3.6,the judgement that information is reliable.,00:06:08,8,judgement information reliable
207,00:06:16,4,Feedback in Text Retrieval,3.6,"We talked about the Pseudo Feedback, where we simply assumed top random documents.",00:06:10,8,We talked Pseudo Feedback simply assumed top random documents
208,00:06:18,4,Feedback in Text Retrieval,3.6,"We get random, we don't have to involve the user.",00:06:16,8,We get random involve user
209,00:06:22,4,Feedback in Text Retrieval,3.6,"Therefore, we could do that actually before we,",00:06:18,8,Therefore could actually
210,00:06:24,4,Feedback in Text Retrieval,3.6,we return the results to the user.,00:06:22,8,return results user
211,00:06:29,4,Feedback in Text Retrieval,3.6,"And the third is Implicit Feedback, where we use clickthroughs.",00:06:24,8,And third Implicit Feedback use clickthroughs
212,00:06:32,4,Feedback in Text Retrieval,3.6,"Where we don't, we involve users, but",00:06:29,8,Where involve users
213,00:06:36,4,Feedback in Text Retrieval,3.6,the user doesn't have to make explicit effort to make judgement.,00:06:32,8,user make explicit effort make judgement
214,00:00:05,4,Smoothing Methods Part - 1,3.5,[SOUND],00:00:00,6,SOUND
215,00:00:09,4,Smoothing Methods Part - 1,3.5,This lecture is about the specific,00:00:07,6,This lecture specific
216,00:00:14,4,Smoothing Methods Part - 1,3.5,smoothing methods for language models used in Probabilistic Retrieval Model.,00:00:09,6,smoothing methods language models used Probabilistic Retrieval Model
217,00:00:21,4,Smoothing Methods Part - 1,3.5,In this lecture we will continue the discussion of language models for,00:00:16,6,In lecture continue discussion language models
218,00:00:26,4,Smoothing Methods Part - 1,3.5,"information retrieval, particularly the query likelihood retrieval method.",00:00:21,6,information retrieval particularly query likelihood retrieval method
219,00:00:29,4,Smoothing Methods Part - 1,3.5,And we're going to talk about the specific smoothing methods used for,00:00:26,6,And going talk specific smoothing methods used
220,00:00:31,4,Smoothing Methods Part - 1,3.5,such a retrieval function.,00:00:29,6,retrieval function
221,00:00:37,4,Smoothing Methods Part - 1,3.5,"So, this is a slide from a previous lecture where we show that with",00:00:33,6,So slide previous lecture show
222,00:00:42,4,Smoothing Methods Part - 1,3.5,query likelihood ranking and the smoothing with the collection language model.,00:00:37,6,query likelihood ranking smoothing collection language model
223,00:00:48,4,Smoothing Methods Part - 1,3.5,We end up having a retrieval function that looks like the following.,00:00:42,6,We end retrieval function looks like following
224,00:00:53,4,Smoothing Methods Part - 1,3.5,"So, this is the retrieval function,",00:00:49,6,So retrieval function
225,00:00:57,4,Smoothing Methods Part - 1,3.5,based on these assumptions that we have discussed.,00:00:53,6,based assumptions discussed
226,00:01:00,4,Smoothing Methods Part - 1,3.5,You can see it's a sum of all the matched query terms here.,00:00:57,6,You see sum matched query terms
227,00:01:06,4,Smoothing Methods Part - 1,3.5,"And inside the sum it's a count of term in the query,",00:01:00,6,And inside sum count term query
228,00:01:11,4,Smoothing Methods Part - 1,3.5,and some weight for the term in the document.,00:01:06,6,weight term document
229,00:01:13,4,Smoothing Methods Part - 1,3.5,"We have TFI, TF weight here.",00:01:11,6,We TFI TF weight
230,00:01:18,4,Smoothing Methods Part - 1,3.5,"And then we have another constant here, in n.",00:01:13,6,And another constant n
231,00:01:25,4,Smoothing Methods Part - 1,3.5,"So clearly, if we want to implement this function using a programming language,",00:01:20,6,So clearly want implement function using programming language
232,00:01:27,4,Smoothing Methods Part - 1,3.5,we'll still need to figure out a few variables.,00:01:25,6,still need figure variables
233,00:01:32,4,Smoothing Methods Part - 1,3.5,"In particular, we're going to need to know how to estimate the,",00:01:27,6,In particular going need know estimate
234,00:01:36,4,Smoothing Methods Part - 1,3.5,probability of would exactly.,00:01:32,6,probability would exactly
235,00:01:39,4,Smoothing Methods Part - 1,3.5,And how do we set alpha?,00:01:36,6,And set alpha
236,00:01:44,4,Smoothing Methods Part - 1,3.5,"So in order to answer these questions, we have to think about this very specific",00:01:40,6,So order answer questions think specific
237,00:01:47,4,Smoothing Methods Part - 1,3.5,"smoothing methods, and that is the main topic of this lecture.",00:01:44,6,smoothing methods main topic lecture
238,00:01:50,4,Smoothing Methods Part - 1,3.5,We're going to talk about two smoothing methods.,00:01:48,6,We going talk two smoothing methods
239,00:01:57,4,Smoothing Methods Part - 1,3.5,"The first is the simple linear interpolation, with a fixed coefficient.",00:01:50,6,The first simple linear interpolation fixed coefficient
240,00:01:59,4,Smoothing Methods Part - 1,3.5,And this is also called a Jelinek and Mercer smoothing.,00:01:57,6,And also called Jelinek Mercer smoothing
241,00:02:04,4,Smoothing Methods Part - 1,3.5,So the idea is actually very simple.,00:02:01,6,So idea actually simple
242,00:02:06,4,Smoothing Methods Part - 1,3.5,This picture shows how we estimate,00:02:04,6,This picture shows estimate
243,00:02:12,4,Smoothing Methods Part - 1,3.5,"document language model by using maximum [INAUDIBLE] method,",00:02:08,6,document language model using maximum INAUDIBLE method
244,00:02:17,4,Smoothing Methods Part - 1,3.5,that gives us word counts normalized by the total number of words in the text.,00:02:12,6,gives us word counts normalized total number words text
245,00:02:22,4,Smoothing Methods Part - 1,3.5,The idea of using this method is to,00:02:17,6,The idea using method
246,00:02:26,4,Smoothing Methods Part - 1,3.5,maximize the probability of the observed text.,00:02:22,6,maximize probability observed text
247,00:02:33,4,Smoothing Methods Part - 1,3.5,"As a result, if a word like network, is not observed in the text.",00:02:26,6,As result word like network observed text
248,00:02:36,4,Smoothing Methods Part - 1,3.5,"It's going to get zero probability, as shown here.",00:02:33,6,It going get zero probability shown
249,00:02:42,4,Smoothing Methods Part - 1,3.5,"So the idea of smoothing, then, is to rely on collection average model,",00:02:37,6,So idea smoothing rely collection average model
250,00:02:47,4,Smoothing Methods Part - 1,3.5,where this word is not going to have a zero probability to help us decide,00:02:42,6,word going zero probability help us decide
251,00:02:50,4,Smoothing Methods Part - 1,3.5,what non-zero probability should be assigned to such a word.,00:02:47,6,non zero probability assigned word
252,00:02:55,4,Smoothing Methods Part - 1,3.5,"So, we can know that network as a non-zero probability here.",00:02:50,6,So know network non zero probability
253,00:03:01,4,Smoothing Methods Part - 1,3.5,"So, in this approach what we do is, we do a linear interpolation between",00:02:55,6,So approach linear interpolation
254,00:03:05,4,Smoothing Methods Part - 1,3.5,the maximum likelihood or estimate here and the collection language model.,00:03:01,6,maximum likelihood estimate collection language model
255,00:03:07,4,Smoothing Methods Part - 1,3.5,"And this controlled by the smoothing parameter, lambda.",00:03:05,6,And controlled smoothing parameter lambda
256,00:03:12,4,Smoothing Methods Part - 1,3.5,Which is between 0 and 1.,00:03:07,6,Which 0 1
257,00:03:14,4,Smoothing Methods Part - 1,3.5,So this is a smoothing parameter.,00:03:12,6,So smoothing parameter
258,00:03:19,4,Smoothing Methods Part - 1,3.5,"The larger lambda is the two the more smoothing we have, we will have.",00:03:14,6,The larger lambda two smoothing
259,00:03:27,4,Smoothing Methods Part - 1,3.5,"So by mixing them together, we achieve the goal of assigning non-zero probability.",00:03:21,6,So mixing together achieve goal assigning non zero probability
260,00:03:29,4,Smoothing Methods Part - 1,3.5,And these two are word in our network.,00:03:27,6,And two word network
261,00:03:31,4,Smoothing Methods Part - 1,3.5,So let's see how it works for some of the words here.,00:03:29,6,So let see works words
262,00:03:36,4,Smoothing Methods Part - 1,3.5,For example if we compute to the smallest probability for text.,00:03:32,6,For example compute smallest probability text
263,00:03:41,4,Smoothing Methods Part - 1,3.5,"Now, the next one right here is made give us 10 over 100,",00:03:37,6,Now next one right made give us 10 100
264,00:03:43,4,Smoothing Methods Part - 1,3.5,and that's going to be here.,00:03:41,6,going
265,00:03:47,4,Smoothing Methods Part - 1,3.5,"But the connection probability is this, so",00:03:44,6,But connection probability
266,00:03:51,4,Smoothing Methods Part - 1,3.5,we just combine them together with this simple formula.,00:03:47,6,combine together simple formula
267,00:03:58,4,Smoothing Methods Part - 1,3.5,"We can also see a, the word network.",00:03:53,6,We also see word network
268,00:04:02,4,Smoothing Methods Part - 1,3.5,Which used to have zero probability now is getting a non-zero,00:03:58,6,Which used zero probability getting non zero
269,00:04:05,4,Smoothing Methods Part - 1,3.5,probability of this value.,00:04:02,6,probability value
270,00:04:12,4,Smoothing Methods Part - 1,3.5,"And that's because the count is going to be zero for network here, but",00:04:05,6,And count going zero network
271,00:04:18,4,Smoothing Methods Part - 1,3.5,this part is non zero and that's basically how this method works.,00:04:12,6,part non zero basically method works
272,00:04:26,4,Smoothing Methods Part - 1,3.5,If you think about this and you can easily see now the alpha sub d,00:04:19,6,If think easily see alpha sub
273,00:04:31,4,Smoothing Methods Part - 1,3.5,"in this smoothing method is basically lambda because that's, remember,",00:04:26,6,smoothing method basically lambda remember
274,00:04:36,4,Smoothing Methods Part - 1,3.5,the coefficient in front of the probability of the word given by,00:04:31,6,coefficient front probability word given
275,00:04:39,4,Smoothing Methods Part - 1,3.5,"the collection language model here, right?",00:04:36,6,collection language model right
276,00:04:42,4,Smoothing Methods Part - 1,3.5,"Okay, so this is the first smoothing method.",00:04:40,6,Okay first smoothing method
277,00:04:49,4,Smoothing Methods Part - 1,3.5,"The second one is similar, but it has a find end for manual interpretation.",00:04:43,6,The second one similar find end manual interpretation
278,00:04:54,4,Smoothing Methods Part - 1,3.5,It's often called a duration of the ply or Bayesian smoothing.,00:04:49,6,It often called duration ply Bayesian smoothing
279,00:05:00,4,Smoothing Methods Part - 1,3.5,"So again here, we face the problem of zero probability for like network.",00:04:54,6,So face problem zero probability like network
280,00:05:05,4,Smoothing Methods Part - 1,3.5,"Again we'll use the collection language model, but",00:05:00,6,Again use collection language model
281,00:05:08,4,Smoothing Methods Part - 1,3.5,in this case we're going to combine them in a somewhat different ways.,00:05:05,6,case going combine somewhat different ways
282,00:05:15,4,Smoothing Methods Part - 1,3.5,The formula first can be seen as a interpolation of the maximum,00:05:08,6,The formula first seen interpolation maximum
283,00:05:20,4,Smoothing Methods Part - 1,3.5,and the collection language model as before.,00:05:15,6,collection language model
284,00:05:23,4,Smoothing Methods Part - 1,3.5,As in the J M's [INAUDIBLE].,00:05:20,6,As J M INAUDIBLE
285,00:05:28,4,Smoothing Methods Part - 1,3.5,"Only and after the coefficient [INAUDIBLE] is not the lambda, a fixed lambda, but",00:05:23,6,Only coefficient INAUDIBLE lambda fixed lambda
286,00:05:31,4,Smoothing Methods Part - 1,3.5,"a dynamic coefficient in this form,",00:05:28,6,dynamic coefficient form
287,00:05:36,4,Smoothing Methods Part - 1,3.5,"when mu is a parameter, it's a non, negative value.",00:05:31,6,mu parameter non negative value
288,00:05:40,4,Smoothing Methods Part - 1,3.5,"And you can see if we set mu to a constant,",00:05:36,6,And see set mu constant
289,00:05:44,4,Smoothing Methods Part - 1,3.5,the effect is that a long document would actually get smaller coefficient here.,00:05:40,6,effect long document would actually get smaller coefficient
290,00:05:47,4,Smoothing Methods Part - 1,3.5,Right? Because a long document,00:05:46,6,Right Because long document
291,00:05:49,4,Smoothing Methods Part - 1,3.5,we have a longer length.,00:05:47,6,longer length
292,00:05:53,4,Smoothing Methods Part - 1,3.5,"Therefore, the coefficient is actually smaller.",00:05:49,6,Therefore coefficient actually smaller
293,00:05:57,4,Smoothing Methods Part - 1,3.5,And so a long document would have less smoothing as we would expect.,00:05:53,6,And long document would less smoothing would expect
294,00:06:05,4,Smoothing Methods Part - 1,3.5,So this seems to make more sense than a fixed coefficient smoothing.,00:05:59,6,So seems make sense fixed coefficient smoothing
295,00:06:10,4,Smoothing Methods Part - 1,3.5,"Of course, this part would be of this form, so",00:06:05,6,Of course part would form
296,00:06:12,4,Smoothing Methods Part - 1,3.5,that the two coefficients would sum to 1.,00:06:10,6,two coefficients would sum 1
297,00:06:16,4,Smoothing Methods Part - 1,3.5,"Now, this is one way to understand that this is smoothing.",00:06:12,6,Now one way understand smoothing
298,00:06:21,4,Smoothing Methods Part - 1,3.5,"Basically, it means that it's a dynamic coefficient interpolation.",00:06:16,6,Basically means dynamic coefficient interpolation
299,00:06:25,4,Smoothing Methods Part - 1,3.5,There is another way to understand this formula.,00:06:22,6,There another way understand formula
300,00:06:31,4,Smoothing Methods Part - 1,3.5,Which is even easier to remember and that's this side.,00:06:27,6,Which even easier remember side
301,00:06:38,4,Smoothing Methods Part - 1,3.5,So it's easy to see we can rewrite this modern method in this form.,00:06:33,6,So easy see rewrite modern method form
302,00:06:43,4,Smoothing Methods Part - 1,3.5,"Now, in this form, we can easily see what change we have made to the maximum",00:06:38,6,Now form easily see change made maximum
303,00:06:45,4,Smoothing Methods Part - 1,3.5,"estimator, which would be this part, right?",00:06:43,6,estimator would part right
304,00:06:50,4,Smoothing Methods Part - 1,3.5,So it normalizes the count by the top elements.,00:06:47,6,So normalizes count top elements
305,00:06:56,4,Smoothing Methods Part - 1,3.5,"So, in this form, we can see what we did,",00:06:52,6,So form see
306,00:07:00,4,Smoothing Methods Part - 1,3.5,is we add this to the count of every word.,00:06:56,6,add count every word
307,00:07:03,4,Smoothing Methods Part - 1,3.5,"So, what does this mean?",00:07:01,6,So mean
308,00:07:04,4,Smoothing Methods Part - 1,3.5,"Well, this is basically",00:07:03,6,Well basically
309,00:07:10,4,Smoothing Methods Part - 1,3.5,something relative to the probability of the word in the collection..,00:07:06,6,something relative probability word collection
310,00:07:13,4,Smoothing Methods Part - 1,3.5,And we multiply that by the parameter mu.,00:07:10,6,And multiply parameter mu
311,00:07:18,4,Smoothing Methods Part - 1,3.5,"And when we combine this with the count here,",00:07:14,6,And combine count
312,00:07:23,4,Smoothing Methods Part - 1,3.5,essentially we are adding pseudo counts to the observed text.,00:07:18,6,essentially adding pseudo counts observed text
313,00:07:30,4,Smoothing Methods Part - 1,3.5,"We pretend every word, has got this many pseudocount.",00:07:23,6,We pretend every word got many pseudocount
314,00:07:35,4,Smoothing Methods Part - 1,3.5,So the total count would be the sum of these pseudocount and,00:07:31,6,So total count would sum pseudocount
315,00:07:38,4,Smoothing Methods Part - 1,3.5,the actual count of the word in the document.,00:07:35,6,actual count word document
316,00:07:46,4,Smoothing Methods Part - 1,3.5,"As a result, in total, we would have added this minute pseudocount.",00:07:39,6,As result total would added minute pseudocount
317,00:07:48,4,Smoothing Methods Part - 1,3.5,"Why? Because if you take a sum of this,",00:07:46,6,Why Because take sum
318,00:07:52,4,Smoothing Methods Part - 1,3.5,"this one, move over all the words and",00:07:48,6,one move words
319,00:07:57,4,Smoothing Methods Part - 1,3.5,"we'll see the probability of the words would sum to 1, and that gives us just mu.",00:07:52,6,see probability words would sum 1 gives us mu
320,00:08:00,4,Smoothing Methods Part - 1,3.5,So this is the total number of pseudo counters that we added.,00:07:57,6,So total number pseudo counters added
321,00:08:05,4,Smoothing Methods Part - 1,3.5,"And, and so these probabilities would still sum to 1.",00:08:01,6,And probabilities would still sum 1
322,00:08:12,4,Smoothing Methods Part - 1,3.5,"So in this case, we can easily see the method is essentially to",00:08:05,6,So case easily see method essentially
323,00:08:18,4,Smoothing Methods Part - 1,3.5,add these as a pseudocount to this data.,00:08:13,6,add pseudocount data
324,00:08:21,4,Smoothing Methods Part - 1,3.5,Pretend we actually augment the data,00:08:18,6,Pretend actually augment data
325,00:08:26,4,Smoothing Methods Part - 1,3.5,by including by some pseudo data defined by the collection language model.,00:08:21,6,including pseudo data defined collection language model
326,00:08:28,4,Smoothing Methods Part - 1,3.5,"As a result, we have more counts.",00:08:26,6,As result counts
327,00:08:35,4,Smoothing Methods Part - 1,3.5,"It's the, the total counts for, for word, a word that would be like this.",00:08:29,6,It total counts word word would like
328,00:08:39,4,Smoothing Methods Part - 1,3.5,"And, as a result, even if a word has zero counts here.",00:08:35,6,And result even word zero counts
329,00:08:43,4,Smoothing Methods Part - 1,3.5,"And say if we have zero come here and that it would still have none,",00:08:39,6,And say zero come would still none
330,00:08:47,4,Smoothing Methods Part - 1,3.5,"zero count because of this part, right?",00:08:43,6,zero count part right
331,00:08:50,4,Smoothing Methods Part - 1,3.5,And so this is how this method works.,00:08:47,6,And method works
332,00:08:52,4,Smoothing Methods Part - 1,3.5,Let's also take a look at this specific example here.,00:08:50,6,Let also take look specific example
333,00:08:58,4,Smoothing Methods Part - 1,3.5,"All right, so for text again, we will have 10 as original count.",00:08:52,6,All right text 10 original count
334,00:09:01,4,Smoothing Methods Part - 1,3.5,That we actually observe but we also added some pseudocount.,00:08:58,6,That actually observe also added pseudocount
335,00:09:05,4,Smoothing Methods Part - 1,3.5,"And so, the probability of text would be of this form.",00:09:03,6,And probability text would form
336,00:09:10,4,Smoothing Methods Part - 1,3.5,Naturally the probability of network would be just this part.,00:09:05,6,Naturally probability network would part
337,00:09:14,4,Smoothing Methods Part - 1,3.5,"And so, here you can also see what's alpha sub d here.",00:09:11,6,And also see alpha sub
338,00:09:16,4,Smoothing Methods Part - 1,3.5,Can you see it?,00:09:15,6,Can see
339,00:09:19,4,Smoothing Methods Part - 1,3.5,If you want to think about you can pause the video.,00:09:16,6,If want think pause video
340,00:09:25,4,Smoothing Methods Part - 1,3.5,Have you noticed that this part is basically of a sub t?,00:09:20,6,Have noticed part basically sub
341,00:09:32,4,Smoothing Methods Part - 1,3.5,"So we can see this case of our sub t does depend on the document, right?",00:09:25,6,So see case sub depend document right
342,00:09:39,4,Smoothing Methods Part - 1,3.5,Because this lens depends on the document whereas in the linear interpolation.,00:09:32,6,Because lens depends document whereas linear interpolation
343,00:09:42,4,Smoothing Methods Part - 1,3.5,The James move method this is the constant.,00:09:39,6,The James move method constant
344,00:00:10,4,Smoothing of Language Model - Part 1,3.4,This lecture is about smoothing of language models.,00:00:07,4,This lecture smoothing language models
345,00:00:15,4,Smoothing of Language Model - Part 1,3.4,In this lecture we're going to continue talking about the probabilistic,00:00:11,4,In lecture going continue talking probabilistic
346,00:00:16,4,Smoothing of Language Model - Part 1,3.4,retrieval model.,00:00:15,4,retrieval model
347,00:00:20,4,Smoothing of Language Model - Part 1,3.4,"In particular, we're going to talk about smoothing of language model and",00:00:16,4,In particular going talk smoothing language model
348,00:00:22,4,Smoothing of Language Model - Part 1,3.4,"the query likelihood of it, which will method.",00:00:20,4,query likelihood method
349,00:00:26,4,Smoothing of Language Model - Part 1,3.4,So you have seen this slide from a previous lecture.,00:00:23,4,So seen slide previous lecture
350,00:00:30,4,Smoothing of Language Model - Part 1,3.4,This is the ranking function based on the query likelihood.,00:00:26,4,This ranking function based query likelihood
351,00:00:35,4,Smoothing of Language Model - Part 1,3.4,Here we assume that the independence of generating each query word,00:00:32,4,Here assume independence generating query word
352,00:00:42,4,Smoothing of Language Model - Part 1,3.4,and the formula would look like the following.,00:00:38,4,formula would look like following
353,00:00:48,4,Smoothing of Language Model - Part 1,3.4,Where we take a sum over all of the query words and inside is the sum there is,00:00:42,4,Where take sum query words inside sum
354,00:00:52,4,Smoothing of Language Model - Part 1,3.4,"a log of probability of a word given by the document, or document language model.",00:00:48,4,log probability word given document document language model
355,00:00:58,4,Smoothing of Language Model - Part 1,3.4,So the main task now is to estimate this document language model.,00:00:53,4,So main task estimate document language model
356,00:01:02,4,Smoothing of Language Model - Part 1,3.4,As we said before different methods for,00:00:59,4,As said different methods
357,00:01:06,4,Smoothing of Language Model - Part 1,3.4,estimating this model would lead to different retrieval functions.,00:01:02,4,estimating model would lead different retrieval functions
358,00:01:10,4,Smoothing of Language Model - Part 1,3.4,"So, in this lecture we're going to look into this in more detail.",00:01:06,4,So lecture going look detail
359,00:01:13,4,Smoothing of Language Model - Part 1,3.4,"So, how do I estimate this language model?",00:01:10,4,So I estimate language model
360,00:01:16,4,Smoothing of Language Model - Part 1,3.4,"Well, the obvious choice would be the Maximum Likelihood Estimate",00:01:13,4,Well obvious choice would Maximum Likelihood Estimate
361,00:01:17,4,Smoothing of Language Model - Part 1,3.4,that we have seen before.,00:01:16,4,seen
362,00:01:22,4,Smoothing of Language Model - Part 1,3.4,And that is we're going to normalize the word frequencies in the document.,00:01:17,4,And going normalize word frequencies document
363,00:01:26,4,Smoothing of Language Model - Part 1,3.4,And the estimated probability would look like this.,00:01:24,4,And estimated probability would look like
364,00:01:32,4,Smoothing of Language Model - Part 1,3.4,This is a step function here.,00:01:30,4,This step function
365,00:01:39,4,Smoothing of Language Model - Part 1,3.4,Which means all the words that have the same frequency,00:01:36,4,Which means words frequency
366,00:01:41,4,Smoothing of Language Model - Part 1,3.4,count will have an equal probability.,00:01:39,4,count equal probability
367,00:01:48,4,Smoothing of Language Model - Part 1,3.4,This is another frequency in the count that has a different probability.,00:01:43,4,This another frequency count different probability
368,00:01:51,4,Smoothing of Language Model - Part 1,3.4,"Note that for words that have not occurred in the document here,",00:01:48,4,Note words occurred document
369,00:01:55,4,Smoothing of Language Model - Part 1,3.4,they all have zero probability.,00:01:51,4,zero probability
370,00:02:01,4,Smoothing of Language Model - Part 1,3.4,"So we know this is just like a model that we assume earlier in the lecture, where",00:01:55,4,So know like model assume earlier lecture
371,00:02:07,4,Smoothing of Language Model - Part 1,3.4,we assume the user with the sample word from the document to formulate the query.,00:02:01,4,assume user sample word document formulate query
372,00:02:13,4,Smoothing of Language Model - Part 1,3.4,And there is no chance of sampling any word that is not in the document.,00:02:09,4,And chance sampling word document
373,00:02:15,4,Smoothing of Language Model - Part 1,3.4,And we know that's not good.,00:02:13,4,And know good
374,00:02:17,4,Smoothing of Language Model - Part 1,3.4,So how would we improve this?,00:02:15,4,So would improve
375,00:02:23,4,Smoothing of Language Model - Part 1,3.4,"Well, in order to assign a non-zero probability",00:02:17,4,Well order assign non zero probability
376,00:02:28,4,Smoothing of Language Model - Part 1,3.4,"to words that have not been observed in the document, we would have to take",00:02:23,4,words observed document would take
377,00:02:35,4,Smoothing of Language Model - Part 1,3.4,away some probability to mass from the words that are observing the document.,00:02:28,4,away probability mass words observing document
378,00:02:38,4,Smoothing of Language Model - Part 1,3.4,"So for example here, we have to take away some [INAUDIBLE] mass,",00:02:35,4,So example take away INAUDIBLE mass
379,00:02:43,4,Smoothing of Language Model - Part 1,3.4,because we need some extra problem in the mass for the unseen words.,00:02:38,4,need extra problem mass unseen words
380,00:02:45,4,Smoothing of Language Model - Part 1,3.4,"Otherwise, they won't sum to 1.",00:02:43,4,Otherwise sum 1
381,00:02:48,4,Smoothing of Language Model - Part 1,3.4,So all these probabilities must be sum to 1.,00:02:45,4,So probabilities must sum 1
382,00:02:53,4,Smoothing of Language Model - Part 1,3.4,"So to make this transformation, and to improve the maximum [INAUDIBLE].",00:02:48,4,So make transformation improve maximum INAUDIBLE
383,00:03:00,4,Smoothing of Language Model - Part 1,3.4,By assigning nonzero probabilities to words that are not observed in the data.,00:02:53,4,By assigning nonzero probabilities words observed data
384,00:03:07,4,Smoothing of Language Model - Part 1,3.4,"We have to do smoothing, and smoothing has to do with improving the estimate",00:03:01,4,We smoothing smoothing improving estimate
385,00:03:12,4,Smoothing of Language Model - Part 1,3.4,"by considering the possibility that, if the author had been written.",00:03:07,4,considering possibility author written
386,00:03:19,4,Smoothing of Language Model - Part 1,3.4,"Helping, asking to write more words for the document.",00:03:13,4,Helping asking write words document
387,00:03:22,4,Smoothing of Language Model - Part 1,3.4,"The user, the author might have rethink other words.",00:03:19,4,The user author might rethink words
388,00:03:27,4,Smoothing of Language Model - Part 1,3.4,If you think about this factor then a smoothed LM model,00:03:22,4,If think factor smoothed LM model
389,00:03:30,4,Smoothing of Language Model - Part 1,3.4,would be a more accurate representation of the actual topic.,00:03:27,4,would accurate representation actual topic
390,00:03:35,4,Smoothing of Language Model - Part 1,3.4,Imagine you have seen abstract of such article.,00:03:30,4,Imagine seen abstract article
391,00:03:37,4,Smoothing of Language Model - Part 1,3.4,Let's say this document is abstract.,00:03:35,4,Let say document abstract
392,00:03:39,4,Smoothing of Language Model - Part 1,3.4,Right.,00:03:38,4,Right
393,00:03:45,4,Smoothing of Language Model - Part 1,3.4,"If we assume and see words in this abstract we have or,",00:03:39,4,If assume see words abstract
394,00:03:51,4,Smoothing of Language Model - Part 1,3.4,or probability of 0 that would mean it's no chance,00:03:45,4,probability 0 would mean chance
395,00:03:57,4,Smoothing of Language Model - Part 1,3.4,of sampling a word outside the abstract that the formula to query.,00:03:51,4,sampling word outside abstract formula query
396,00:04:03,4,Smoothing of Language Model - Part 1,3.4,"But imagine the user who is interested in the topic of this abstract, the user might",00:03:57,4,But imagine user interested topic abstract user might
397,00:04:08,4,Smoothing of Language Model - Part 1,3.4,actually choose a word that is not in the abstractor to to use as query.,00:04:03,4,actually choose word abstractor use query
398,00:04:14,4,Smoothing of Language Model - Part 1,3.4,"So obviously if we had asked this author to write more,",00:04:10,4,So obviously asked author write
399,00:04:18,4,Smoothing of Language Model - Part 1,3.4,the author would have written a full text of that article.,00:04:14,4,author would written full text article
400,00:04:23,4,Smoothing of Language Model - Part 1,3.4,So smoothing of the language model is attempted to,00:04:18,4,So smoothing language model attempted
401,00:04:28,4,Smoothing of Language Model - Part 1,3.4,"to try to recover the model for the whole, whole article.",00:04:23,4,try recover model whole whole article
402,00:04:34,4,Smoothing of Language Model - Part 1,3.4,And then of course we don't have written knowledge about any words are not observed,00:04:28,4,And course written knowledge words observed
403,00:04:39,4,Smoothing of Language Model - Part 1,3.4,"in the abstract there, so that's why smoothing is actually a tricky problem.",00:04:34,4,abstract smoothing actually tricky problem
404,00:04:43,4,Smoothing of Language Model - Part 1,3.4,So let's talk a little more about how to smooth a LM word.,00:04:39,4,So let talk little smooth LM word
405,00:04:48,4,Smoothing of Language Model - Part 1,3.4,The key question here is what probability should be assigned to those unseen words.,00:04:43,4,The key question probability assigned unseen words
406,00:04:50,4,Smoothing of Language Model - Part 1,3.4,Right. And,00:04:50,4,Right And
407,00:04:53,4,Smoothing of Language Model - Part 1,3.4,there are many different ways of doing that.,00:04:50,4,many different ways
408,00:04:56,4,Smoothing of Language Model - Part 1,3.4,"One idea here, that's very useful for",00:04:53,4,One idea useful
409,00:05:00,4,Smoothing of Language Model - Part 1,3.4,retrieval is let the probability of an unseen word be proportional,00:04:56,4,retrieval let probability unseen word proportional
410,00:05:03,4,Smoothing of Language Model - Part 1,3.4,to its probability given by a reference language model.,00:05:00,4,probability given reference language model
411,00:05:07,4,Smoothing of Language Model - Part 1,3.4,"That means if you don't observe the word in the data set,",00:05:03,4,That means observe word data set
412,00:05:11,4,Smoothing of Language Model - Part 1,3.4,we're going to assume that its probability is kind of,00:05:07,4,going assume probability kind
413,00:05:16,4,Smoothing of Language Model - Part 1,3.4,governed by another reference language model that we were constructing.,00:05:12,4,governed another reference language model constructing
414,00:05:20,4,Smoothing of Language Model - Part 1,3.4,It will tell us which unseen words we have likely a higher probability.,00:05:16,4,It tell us unseen words likely higher probability
415,00:05:26,4,Smoothing of Language Model - Part 1,3.4,In the case of retrieval a natural choice would be to,00:05:22,4,In case retrieval natural choice would
416,00:05:30,4,Smoothing of Language Model - Part 1,3.4,take the Collection Language Model as a Reference Language Model.,00:05:26,4,take Collection Language Model Reference Language Model
417,00:05:33,4,Smoothing of Language Model - Part 1,3.4,That is to say if you don't observe a word in the document,00:05:30,4,That say observe word document
418,00:05:35,4,Smoothing of Language Model - Part 1,3.4,we're going to assume that.,00:05:33,4,going assume
419,00:05:37,4,Smoothing of Language Model - Part 1,3.4,The probability of this word,00:05:35,4,The probability word
420,00:05:41,4,Smoothing of Language Model - Part 1,3.4,would be proportional to the probability of the word in the whole collection.,00:05:37,4,would proportional probability word whole collection
421,00:05:42,4,Smoothing of Language Model - Part 1,3.4,"So, more formally,",00:05:41,4,So formally
422,00:05:46,4,Smoothing of Language Model - Part 1,3.4,we'll be estimating the probability of a word getting a document as follows.,00:05:42,4,estimating probability word getting document follows
423,00:05:54,4,Smoothing of Language Model - Part 1,3.4,"If the word is seen in the document, then the probability",00:05:48,4,If word seen document probability
424,00:06:00,4,Smoothing of Language Model - Part 1,3.4,would be a discounted the maximum [INAUDIBLE] estimated p sub c here.,00:05:54,4,would discounted maximum INAUDIBLE estimated p sub c
425,00:06:07,4,Smoothing of Language Model - Part 1,3.4,"Otherwise, if the word is not seen in the document, we'll then let",00:06:02,4,Otherwise word seen document let
426,00:06:12,4,Smoothing of Language Model - Part 1,3.4,"probability be proportional to the probability of the word in the collection,",00:06:07,4,probability proportional probability word collection
427,00:06:17,4,Smoothing of Language Model - Part 1,3.4,and here the coefficient of is to,00:06:12,4,coefficient
428,00:06:21,4,Smoothing of Language Model - Part 1,3.4,control the amount of probability mass that we assign to unseen words.,00:06:17,4,control amount probability mass assign unseen words
429,00:06:25,4,Smoothing of Language Model - Part 1,3.4,Obviously all these probabilities must sum to 1.,00:06:22,4,Obviously probabilities must sum 1
430,00:06:28,4,Smoothing of Language Model - Part 1,3.4,"So, alpha sub d is constrained in some way.",00:06:25,4,So alpha sub constrained way
431,00:06:33,4,Smoothing of Language Model - Part 1,3.4,"So, what if we plug in this smoothing formula into our",00:06:29,4,So plug smoothing formula
432,00:06:35,4,Smoothing of Language Model - Part 1,3.4,query likelihood Ranking Function?,00:06:33,4,query likelihood Ranking Function
433,00:06:36,4,Smoothing of Language Model - Part 1,3.4,This is what we would get.,00:06:35,4,This would get
434,00:06:42,4,Smoothing of Language Model - Part 1,3.4,"In this formula, you can see, right, we have",00:06:37,4,In formula see right
435,00:06:48,4,Smoothing of Language Model - Part 1,3.4,this as a sum over all the query words.,00:06:43,4,sum query words
436,00:06:52,4,Smoothing of Language Model - Part 1,3.4,And note that we have written in the form of a sum over all the vocabulary.,00:06:48,4,And note written form sum vocabulary
437,00:06:56,4,Smoothing of Language Model - Part 1,3.4,"You see here this is a sum of all the words in the vocabulary,",00:06:52,4,You see sum words vocabulary
438,00:07:00,4,Smoothing of Language Model - Part 1,3.4,but note that we have a count of the word in the query.,00:06:56,4,note count word query
439,00:07:04,4,Smoothing of Language Model - Part 1,3.4,"So, in effect we are just taking a sum of query words, right.",00:07:00,4,So effect taking sum query words right
440,00:07:10,4,Smoothing of Language Model - Part 1,3.4,This is in now a common way that,00:07:04,4,This common way
441,00:07:16,4,Smoothing of Language Model - Part 1,3.4,we will use because of its convenience in some transformations.,00:07:10,4,use convenience transformations
442,00:07:21,4,Smoothing of Language Model - Part 1,3.4,"So, this is as I said, this is sum of all the query words.",00:07:18,4,So I said sum query words
443,00:07:27,4,Smoothing of Language Model - Part 1,3.4,"In our smoothing method, we're assuming the words that are not",00:07:23,4,In smoothing method assuming words
444,00:07:31,4,Smoothing of Language Model - Part 1,3.4,"observed in the document, that we have a somewhat different form of probability.",00:07:27,4,observed document somewhat different form probability
445,00:07:32,4,Smoothing of Language Model - Part 1,3.4,And then it's for this form.,00:07:31,4,And form
446,00:07:37,4,Smoothing of Language Model - Part 1,3.4,So we're going to then decompose this sum into two parts.,00:07:34,4,So going decompose sum two parts
447,00:07:44,4,Smoothing of Language Model - Part 1,3.4,One sum is over all the query words that are matched in the document.,00:07:38,4,One sum query words matched document
448,00:07:49,4,Smoothing of Language Model - Part 1,3.4,"That means in this sum, all the words have a non",00:07:44,4,That means sum words non
449,00:07:55,4,Smoothing of Language Model - Part 1,3.4,"zero probability, in the document, sorry.",00:07:49,4,zero probability document sorry
450,00:07:59,4,Smoothing of Language Model - Part 1,3.4,"It's, the non zero count of the word in the document.",00:07:55,4,It non zero count word document
451,00:08:01,4,Smoothing of Language Model - Part 1,3.4,They all occur in the document.,00:07:59,4,They occur document
452,00:08:08,4,Smoothing of Language Model - Part 1,3.4,"And they also have to, of course, have a non-zero count in the query.",00:08:02,4,And also course non zero count query
453,00:08:09,4,Smoothing of Language Model - Part 1,3.4,"So, these are the words that are matched.",00:08:08,4,So words matched
454,00:08:13,4,Smoothing of Language Model - Part 1,3.4,These are the query words that are matched in the document.,00:08:11,4,These query words matched document
455,00:08:19,4,Smoothing of Language Model - Part 1,3.4,"On the other hand in this sum we are s, taking the sum over all the words that",00:08:15,4,On hand sum taking sum words
456,00:08:24,4,Smoothing of Language Model - Part 1,3.4,are note our query was not matched in the document.,00:08:19,4,note query matched document
457,00:08:33,4,Smoothing of Language Model - Part 1,3.4,So they occur in the query due to this term but they don't occur in the document.,00:08:25,4,So occur query due term occur document
458,00:08:33,4,Smoothing of Language Model - Part 1,3.4,"In this case,",00:08:33,4,In case
459,00:08:39,4,Smoothing of Language Model - Part 1,3.4,these words have this probability because of our assumption about the smoothing.,00:08:33,4,words probability assumption smoothing
460,00:08:44,4,Smoothing of Language Model - Part 1,3.4,"But that here, these c words have a different probability.",00:08:40,4,But c words different probability
461,00:08:51,4,Smoothing of Language Model - Part 1,3.4,Now we can go further by rewriting the second sum,00:08:47,4,Now go rewriting second sum
462,00:08:54,4,Smoothing of Language Model - Part 1,3.4,as a difference of two other sums.,00:08:52,4,difference two sums
463,00:09:00,4,Smoothing of Language Model - Part 1,3.4,Basically the first sum is actually the sum over all the query words.,00:08:54,4,Basically first sum actually sum query words
464,00:09:05,4,Smoothing of Language Model - Part 1,3.4,Now we know that the original sum is not over the query words.,00:09:00,4,Now know original sum query words
465,00:09:10,4,Smoothing of Language Model - Part 1,3.4,This is over all the query words that are not matched in the document.,00:09:05,4,This query words matched document
466,00:09:19,4,Smoothing of Language Model - Part 1,3.4,So here we pretend that they are actually over all the query words.,00:09:12,4,So pretend actually query words
467,00:09:21,4,Smoothing of Language Model - Part 1,3.4,"So, we take a sum over all the query words.",00:09:19,4,So take sum query words
468,00:09:25,4,Smoothing of Language Model - Part 1,3.4,"Obviously this sum has extra terms that are,",00:09:21,4,Obviously sum extra terms
469,00:09:28,4,Smoothing of Language Model - Part 1,3.4,this sum has extra terms that are not in this sum.,00:09:25,4,sum extra terms sum
470,00:09:33,4,Smoothing of Language Model - Part 1,3.4,Because here we're taking sum over all the query words.,00:09:30,4,Because taking sum query words
471,00:09:37,4,Smoothing of Language Model - Part 1,3.4,There it's not matched in the document.,00:09:33,4,There matched document
472,00:09:44,4,Smoothing of Language Model - Part 1,3.4,"So in order to make them equal, we have to then subtract another sum here.",00:09:37,4,So order make equal subtract another sum
473,00:09:48,4,Smoothing of Language Model - Part 1,3.4,And this is a sum over all the query words that are mentioned in the document.,00:09:44,4,And sum query words mentioned document
474,00:09:55,4,Smoothing of Language Model - Part 1,3.4,And this makes sense because here we're considering all query words.,00:09:51,4,And makes sense considering query words
475,00:09:59,4,Smoothing of Language Model - Part 1,3.4,And then we subtract the query that was matched in the document.,00:09:55,4,And subtract query matched document
476,00:10:04,4,Smoothing of Language Model - Part 1,3.4,That will give us the query rules that not matched in the document.,00:09:59,4,That give us query rules matched document
477,00:10:11,4,Smoothing of Language Model - Part 1,3.4,And this is almost a reverse process of the first step here.,00:10:05,4,And almost reverse process first step
478,00:10:14,4,Smoothing of Language Model - Part 1,3.4,And you might wonder why we want to do that.,00:10:12,4,And might wonder want
479,00:10:20,4,Smoothing of Language Model - Part 1,3.4,"Well, that's because if we do this then",00:10:14,4,Well
480,00:10:25,4,Smoothing of Language Model - Part 1,3.4,we'll have different forms of terms inside these sums.,00:10:20,4,different forms terms inside sums
481,00:10:30,4,Smoothing of Language Model - Part 1,3.4,"So, now we can see in the sum we have, all the words,",00:10:25,4,So see sum words
482,00:10:36,4,Smoothing of Language Model - Part 1,3.4,"matched query words, matched in the document with this kind of terms.",00:10:30,4,matched query words matched document kind terms
483,00:10:42,4,Smoothing of Language Model - Part 1,3.4,Here we have another sum over the same set of terms.,00:10:36,4,Here another sum set terms
484,00:10:45,4,Smoothing of Language Model - Part 1,3.4,Matched query terms in document.,00:10:43,4,Matched query terms document
485,00:10:47,4,Smoothing of Language Model - Part 1,3.4,But inside the sum it's different.,00:10:45,4,But inside sum different
486,00:10:52,4,Smoothing of Language Model - Part 1,3.4,But these two sums can clearly be merged.,00:10:49,4,But two sums clearly merged
487,00:10:57,4,Smoothing of Language Model - Part 1,3.4,"So, if we do that we'll get another form",00:10:54,4,So get another form
488,00:11:02,4,Smoothing of Language Model - Part 1,3.4,of the formula that looks like the following at the bottom here.,00:10:57,4,formula looks like following bottom
489,00:11:10,4,Smoothing of Language Model - Part 1,3.4,"And note that this is a very interesting, because here we combine the, these two,",00:11:04,4,And note interesting combine two
490,00:11:16,4,Smoothing of Language Model - Part 1,3.4,that are a sum of the query words matched in the document in the one sum here.,00:11:10,4,sum query words matched document one sum
491,00:11:23,4,Smoothing of Language Model - Part 1,3.4,"And the other sum, now is the compose [INAUDIBLE] to two parts, and,",00:11:18,4,And sum compose INAUDIBLE two parts
492,00:11:26,4,Smoothing of Language Model - Part 1,3.4,and these two parts look much simpler.,00:11:23,4,two parts look much simpler
493,00:11:30,4,Smoothing of Language Model - Part 1,3.4,Just because these are the probabilities of unseen words.,00:11:26,4,Just probabilities unseen words
494,00:11:37,4,Smoothing of Language Model - Part 1,3.4,"But this formula is very interesting, because you can see the sum is now over",00:11:31,4,But formula interesting see sum
495,00:11:40,4,Smoothing of Language Model - Part 1,3.4,all the matched query terms.,00:11:37,4,matched query terms
496,00:11:47,4,Smoothing of Language Model - Part 1,3.4,"And just like in the vector space model, we take a sum of terms,",00:11:41,4,And like vector space model take sum terms
497,00:11:50,4,Smoothing of Language Model - Part 1,3.4,that intersection of query vector and the document vector.,00:11:47,4,intersection query vector document vector
498,00:11:55,4,Smoothing of Language Model - Part 1,3.4,So it all already looks a little bit like the vector space model.,00:11:51,4,So already looks little bit like vector space model
499,00:11:59,4,Smoothing of Language Model - Part 1,3.4,In fact there is even more severity here.,00:11:55,4,In fact even severity
500,00:12:01,4,Smoothing of Language Model - Part 1,3.4,"As we, we explain on this slide.",00:11:59,4,As explain slide
501,00:00:08,1,Course Welcome,,[SOUND] Hello,00:00:00,1,SOUND Hello
502,00:00:12,1,Course Welcome,,welcome to the course in Text Retrieval and Search Engines.,00:00:08,1,welcome course Text Retrieval Search Engines
503,00:00:13,1,Course Welcome,,I'm Cheng Xiang Zhai.,00:00:12,1,I Cheng Xiang Zhai
504,00:00:16,1,Course Welcome,,I have a nickname Cheng.,00:00:13,1,I nickname Cheng
505,00:00:19,1,Course Welcome,,I'm a professor of the Department of Computer Science at,00:00:16,1,I professor Department Computer Science
506,00:00:21,1,Course Welcome,,the University of Illinois at Urbana-Champaign.,00:00:19,1,University Illinois Urbana Champaign
507,00:00:27,1,Course Welcome,,this first lecture is a basic introduction to the course.,00:00:23,1,first lecture basic introduction course
508,00:00:30,1,Course Welcome,,A brief introduction to what we we'll cover in the course.,00:00:27,1,A brief introduction cover course
509,00:00:34,1,Course Welcome,,We're going to first talk about the data mining specialization since this course is,00:00:30,1,We going first talk data mining specialization since course
510,00:00:36,1,Course Welcome,,part of that specialization.,00:00:34,1,part specialization
511,00:00:41,1,Course Welcome,,And then we'll cover motivation objectives of the course.,00:00:36,1,And cover motivation objectives course
512,00:00:47,1,Course Welcome,,This will be followed by pre-requisites and course format and reference books.,00:00:41,1,This followed pre requisites course format reference books
513,00:00:51,1,Course Welcome,,"And then finally we'll talk about the course schedule,",00:00:48,1,And finally talk course schedule
514,00:00:56,1,Course Welcome,,which has number of topics to be covered in the rest of this course.,00:00:51,1,number topics covered rest course
515,00:01:01,1,Course Welcome,,So the data mining specialization offered by the University of Illinois,00:00:57,1,So data mining specialization offered University Illinois
516,00:01:06,1,Course Welcome,,at Urbana-Champaign is really to address the need for data mining techniques to,00:01:01,1,Urbana Champaign really address need data mining techniques
517,00:01:10,1,Course Welcome,,"handle a lot of big data, to turn the big data into knowledge.",00:01:06,1,handle lot big data turn big data knowledge
518,00:01:15,1,Course Welcome,,"There are five lecture-based courses, as you see on the slide.",00:01:10,1,There five lecture based courses see slide
519,00:01:19,1,Course Welcome,,"Plus one capstone, project course in the end.",00:01:15,1,Plus one capstone project course end
520,00:01:23,1,Course Welcome,,"I'm teaching two of them which is this course, Text Retrieval and",00:01:19,1,I teaching two course Text Retrieval
521,00:01:25,1,Course Welcome,,Search Engines and this one.,00:01:23,1,Search Engines one
522,00:01:29,1,Course Welcome,,So the two courses that I cover here are all about the text data.,00:01:25,1,So two courses I cover text data
523,00:01:35,1,Course Welcome,,"In contrast, the other courses are covering more general techniques that can",00:01:31,1,In contrast courses covering general techniques
524,00:01:38,1,Course Welcome,,be applied to all kinds of data.,00:01:35,1,applied kinds data
525,00:01:43,1,Course Welcome,,So Patent Discovery taught by the Professor Jowi Han and Cluster Analysis,00:01:38,1,So Patent Discovery taught Professor Jowi Han Cluster Analysis
526,00:01:49,1,Course Welcome,,again taught by him about the general data mining techniques to handle structure.,00:01:43,1,taught general data mining techniques handle structure
527,00:01:52,1,Course Welcome,,The end and structure text data.,00:01:49,1,The end structure text data
528,00:01:57,1,Course Welcome,,"And data mine, data visualization covered by professor Jung Hart is about",00:01:52,1,And data mine data visualization covered professor Jung Hart
529,00:02:00,1,Course Welcome,,the general visualization techniques.,00:01:57,1,general visualization techniques
530,00:02:03,1,Course Welcome,,Again applicable to all kinds of data.,00:02:00,1,Again applicable kinds data
531,00:02:05,1,Course Welcome,,So the motivation for this course.,00:02:03,1,So motivation course
532,00:02:06,1,Course Welcome,,In fact also for,00:02:05,1,In fact also
533,00:02:11,1,Course Welcome,,the other courses that I'm teaching is that we have a lot of text data.,00:02:06,1,courses I teaching lot text data
534,00:02:15,1,Course Welcome,,"And the data is everywhere, is growing rapidly, so",00:02:11,1,And data everywhere growing rapidly
535,00:02:19,1,Course Welcome,,you must have been experiencing this growth.,00:02:15,1,must experiencing growth
536,00:02:23,1,Course Welcome,,Just think about how much text data you're dealing with every day.,00:02:19,1,Just think much text data dealing every day
537,00:02:26,1,Course Welcome,,"I listed some data types here, for",00:02:23,1,I listed data types
538,00:02:30,1,Course Welcome,,"example, on the internet we see a lot of web pages, news articles etcetera.",00:02:26,1,example internet see lot web pages news articles etcetera
539,00:02:33,1,Course Welcome,,"And then we have block articles,",00:02:30,1,And block articles
540,00:02:37,1,Course Welcome,,"emails, scientific literature, tweets, as well speaking,",00:02:33,1,emails scientific literature tweets well speaking
541,00:02:43,1,Course Welcome,,"maybe a lot of tweets are being written, and a lot of emails are, are being sent.",00:02:37,1,maybe lot tweets written lot emails sent
542,00:02:48,1,Course Welcome,,"So, the amount of text data is beyond our capacity to understand them.",00:02:43,1,So amount text data beyond capacity understand
543,00:02:52,1,Course Welcome,,"Also, the amount of data makes it possible to actually analyze the data to discover",00:02:48,1,Also amount data makes possible actually analyze data discover
544,00:02:57,1,Course Welcome,,"interesting knowledge and that's what we meant by, harnessing big text data.",00:02:52,1,interesting knowledge meant harnessing big text data
545,00:00:04,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,[SOUND].,00:00:00,10,SOUND
546,00:00:09,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,This lecture is about recommender systems.,00:00:07,10,This lecture recommender systems
547,00:00:19,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So, so far we have talked about a lot of aspects of search engines.",00:00:12,10,So far talked lot aspects search engines
548,00:00:24,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And we have talked about the problem of search and the ranking problem,",00:00:19,10,And talked problem search ranking problem
549,00:00:29,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"different methods for ranking, implementation of search engine and",00:00:24,10,different methods ranking implementation search engine
550,00:00:33,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"how to evaluate the search engine, et cetera.",00:00:29,10,evaluate search engine et cetera
551,00:00:41,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"This is partly because we know that web search engines are,",00:00:36,10,This partly know web search engines
552,00:00:44,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"by far, the most important applications of text retrieval.",00:00:41,10,far important applications text retrieval
553,00:00:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And they are the most useful tools to help people convert big raw text,00:00:44,10,And useful tools help people convert big raw text
554,00:00:53,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,data into a small set of relevant documents.,00:00:49,10,data small set relevant documents
555,00:01:01,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Another reason why we spend so many lectures on search engines is because,00:00:56,10,Another reason spend many lectures search engines
556,00:01:07,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,many techniques used in search engines are actually also very useful for,00:01:01,10,many techniques used search engines actually also useful
557,00:01:10,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"recommender systems, which is the topic of this lecture.",00:01:07,10,recommender systems topic lecture
558,00:01:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And so overall the two systems are actually well connected,",00:01:11,10,And overall two systems actually well connected
559,00:01:19,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,and there are many techniques that are shared by them.,00:01:16,10,many techniques shared
560,00:01:26,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So this is a slide that you have seen before when we talked about,00:01:22,10,So slide seen talked
561,00:01:30,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,the two different modes of text access pull and push.,00:01:26,10,two different modes text access pull push
562,00:01:36,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And, we mentioned that recommender systems are the main systems to serve",00:01:31,10,And mentioned recommender systems main systems serve
563,00:01:42,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"users in the push mode, where the systems will take the initiative to",00:01:36,10,users push mode systems take initiative
564,00:01:46,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"recommend the information to user, or to push the relevant information to the user.",00:01:42,10,recommend information user push relevant information user
565,00:01:51,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And this often works well when the user has a relatively stable information need,",00:01:46,10,And often works well user relatively stable information need
566,00:01:55,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,when the system has good knowledge about what a user wants.,00:01:51,10,system good knowledge user wants
567,00:02:00,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So a recommender system is sometimes called a filtering system.,00:01:56,10,So recommender system sometimes called filtering system
568,00:02:05,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And it's because recommending useful items to people is like,00:02:00,10,And recommending useful items people like
569,00:02:10,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,discarding or filtering out the useless articles.,00:02:05,10,discarding filtering useless articles
570,00:02:14,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So in this sense, they are kind of similar.",00:02:10,10,So sense kind similar
571,00:02:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And in all the cases, the system must make a binary decision.",00:02:16,10,And cases system must make binary decision
572,00:02:25,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And usually, there is a dynamic source of information items,",00:02:20,10,And usually dynamic source information items
573,00:02:30,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"and you have some knowledge about the user's interest, and then the system would",00:02:25,10,knowledge user interest system would
574,00:02:34,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"make a delivery decision, whether this item is interesting to the user.",00:02:30,10,make delivery decision whether item interesting user
575,00:02:39,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And then if it is interesting then the system would recommend the article to,00:02:34,10,And interesting system would recommend article
576,00:02:39,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,the user.,00:02:39,10,user
577,00:02:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So the basic filtering question here is really, will this user like, this item?",00:02:43,10,So basic filtering question really user like item
578,00:02:50,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Will U like item X?,00:02:49,10,Will U like item X
579,00:02:56,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And there are two ways to answer this question if you think about it, right?",00:02:50,10,And two ways answer question think right
580,00:03:00,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"One is look at what items U likes, and",00:02:56,10,One look items U likes
581,00:03:03,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,then we can see if X is actually like those items.,00:03:00,10,see X actually like items
582,00:03:11,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"The other is to look at who likes X ,and we can see if this user looks like a,",00:03:05,10,The look likes X see user looks like
583,00:03:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"one of those users, or like most of those users.",00:03:11,10,one users like users
584,00:03:18,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And these strategies can be combined.,00:03:16,10,And strategies combined
585,00:03:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,If we follow the first strategy and,00:03:18,10,If follow first strategy
586,00:03:26,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"look at item similarity in the case of recommended text objects,",00:03:20,10,look item similarity case recommended text objects
587,00:03:31,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,then we are talking about a content-based filtering or content-based recommendation.,00:03:26,10,talking content based filtering content based recommendation
588,00:03:37,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If we look at the second strategy then, this will compare users.",00:03:31,10,If look second strategy compare users
589,00:03:40,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And in this case, we're exploiting user similarity,",00:03:37,10,And case exploiting user similarity
590,00:03:43,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,and the technique is often called a collaborative filtering.,00:03:40,10,technique often called collaborative filtering
591,00:03:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So let's first look at the content-based filtering system.,00:03:46,10,So let first look content based filtering system
592,00:03:51,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,This is what a system would look like.,00:03:49,10,This system would look like
593,00:03:56,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Inside the system, there would be a binary classifier that would have some",00:03:52,10,Inside system would binary classifier would
594,00:04:00,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"knowledge about the user's interests, and it's called the user interest profile.",00:03:56,10,knowledge user interests called user interest profile
595,00:04:05,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,It maintains the profile to keep track of the user's interest.,00:04:02,10,It maintains profile keep track user interest
596,00:04:10,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And then there is a utility function to guide the user to make decisions, and",00:04:06,10,And utility function guide user make decisions
597,00:04:13,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,I'll explain the utility of the function in a moment.,00:04:10,10,I explain utility function moment
598,00:04:17,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,It helps the system decide where to set the threshold.,00:04:13,10,It helps system decide set threshold
599,00:04:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And then the accepted documents will be those that have passed,00:04:17,10,And accepted documents passed
600,00:04:23,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,the threshold according to the classifier.,00:04:20,10,threshold according classifier
601,00:04:28,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"There should be also an initialization module that would take a user's input,",00:04:23,10,There also initialization module would take user input
602,00:04:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"maybe from a user's, specified keywords, or a chosen category, et cetera.",00:04:28,10,maybe user specified keywords chosen category et cetera
603,00:04:38,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And this will be, to feed the system with a initial user profile.",00:04:35,10,And feed system initial user profile
604,00:04:43,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,There is also typically a learning module that will learn from,00:04:39,10,There also typically learning module learn
605,00:04:45,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,users' feedback over time.,00:04:43,10,users feedback time
606,00:04:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Now note that in this case, typically users' information need is stable so",00:04:45,10,Now note case typically users information need stable
607,00:04:53,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"the system would have a lot of opportunities to observe the users,",00:04:49,10,system would lot opportunities observe users
608,00:04:58,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"you know, if the user has taken a recommended item as viewed that, and",00:04:53,10,know user taken recommended item viewed
609,00:05:04,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"this is a cu, a signal to indicate that the recommended item may be relevant.",00:04:58,10,cu signal indicate recommended item may relevant
610,00:05:07,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If the user discarded it, no, it's not relevant.",00:05:04,10,If user discarded relevant
611,00:05:11,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And so, such feedback can be a long-term feedback and can last for a long time and",00:05:07,10,And feedback long term feedback last long time
612,00:05:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"the system can clock, collect a lot of information about this user's interests.",00:05:11,10,system clock collect lot information user interests
613,00:05:19,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And this can then be used to improve the classifier.,00:05:16,10,And used improve classifier
614,00:05:24,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Now whats the criteria for evaluating such a system?,00:05:19,10,Now whats criteria evaluating system
615,00:05:30,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,How do we know this filtering system actually performs well?,00:05:24,10,How know filtering system actually performs well
616,00:05:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Now in this case we cannot use the ranking evaluation measures, like a map,",00:05:30,10,Now case cannot use ranking evaluation measures like map
617,00:05:38,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"because we can't afford waiting for a lot of documents,",00:05:35,10,afford waiting lot documents
618,00:05:42,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,and then rank the documents to make a decision for the user.,00:05:38,10,rank documents make decision user
619,00:05:47,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And so, the system must make a decision, in real time,",00:05:42,10,And system must make decision real time
620,00:05:51,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,in general to decide whether the item is above the threshold or not.,00:05:47,10,general decide whether item threshold
621,00:05:55,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So in other words, we're trying to decide absolute relevance.",00:05:51,10,So words trying decide absolute relevance
622,00:06:01,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So in this case one common use strategy is to use a utility function,00:05:56,10,So case one common use strategy use utility function
623,00:06:03,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,through a valid system.,00:06:01,10,valid system
624,00:06:08,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So here I show a linear utility function that's defined as, for example,",00:06:03,10,So I show linear utility function defined example
625,00:06:12,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"3 multiplied by the number of good items that you delivered,",00:06:08,10,3 multiplied number good items delivered
626,00:06:17,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"minus 2 multiplied by the number of bad items you delete, that you delivered.",00:06:12,10,minus 2 multiplied number bad items delete delivered
627,00:06:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So in other words, we, we could kind of just",00:06:17,10,So words could kind
628,00:06:26,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"treat this as almost a, in a gambling game.",00:06:22,10,treat almost gambling game
629,00:06:30,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If you delete, if you deliver one good item,",00:06:26,10,If delete deliver one good item
630,00:06:34,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"let's say you win $3, you gain $3.",00:06:30,10,let say win 3 gain 3
631,00:06:37,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"But if you deliver a bad one, you would lose $2.",00:06:34,10,But deliver bad one would lose 2
632,00:06:41,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And this utility function basically kind of measures,",00:06:37,10,And utility function basically kind measures
633,00:06:45,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"how much money you would, get by doing this kind of game, right.",00:06:41,10,much money would get kind game right
634,00:06:50,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And so it's clear that if you want to maximize this utility function,",00:06:45,10,And clear want maximize utility function
635,00:06:56,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"your strategy should be to deliver as many good articles as possible,",00:06:50,10,strategy deliver many good articles possible
636,00:06:59,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,and minimize the delivery of bad articles.,00:06:56,10,minimize delivery bad articles
637,00:07:02,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"That, that's obvious, right.",00:06:59,10,That obvious right
638,00:07:07,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Now one interesting question here is, how should we set these coefficients?",00:07:03,10,Now one interesting question set coefficients
639,00:07:10,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Now I just showed a 3 and a negative 2,",00:07:07,10,Now I showed 3 negative 2
640,00:07:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"as the possible coefficients, but one can ask the question, are they reasonable?",00:07:10,10,possible coefficients one ask question reasonable
641,00:07:19,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So what do you think?,00:07:17,10,So think
642,00:07:23,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Do you think that's a reasonable choice?,00:07:20,10,Do think reasonable choice
643,00:07:24,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,What about other choices?,00:07:23,10,What choices
644,00:07:32,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So for example, we can have 10 and minus 1, or 1 minus 10.",00:07:26,10,So example 10 minus 1 1 minus 10
645,00:07:34,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,What's the difference?,00:07:32,10,What difference
646,00:07:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,What do you think?,00:07:34,10,What think
647,00:07:43,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,How would this utility function affect the system's threshold of this issue?,00:07:36,10,How would utility function affect system threshold issue
648,00:07:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Right, you can think of these two extreme cases, 10 minus 1 versus 1 minus 10.",00:07:43,10,Right think two extreme cases 10 minus 1 versus 1 minus 10
649,00:07:54,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Which one do we think it would encourage the system to over-deliver?,00:07:49,10,Which one think would encourage system deliver
650,00:07:57,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And which one would encourage the system to be conservative?,00:07:54,10,And one would encourage system conservative
651,00:07:58,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Yeah?,00:07:57,10,Yeah
652,00:08:02,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If you think about it, you will see that when we get a big award for",00:07:58,10,If think see get big award
653,00:08:08,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"delivering a good document, you incur only a small penalty for delivering a bad one.",00:08:02,10,delivering good document incur small penalty delivering bad one
654,00:08:11,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Intuitively, you would be encouraging to deliver more, right?",00:08:08,10,Intuitively would encouraging deliver right
655,00:08:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And you can try to deliver more in hope of getting a good one delivered, and",00:08:11,10,And try deliver hope getting good one delivered
656,00:08:17,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,then you'll get a big award.,00:08:16,10,get big award
657,00:08:22,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Right, so on the other hand, if you choose 1 minus 10,",00:08:17,10,Right hand choose 1 minus 10
658,00:08:28,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,you don't really get such a big prize if you deliver a good document.,00:08:22,10,really get big prize deliver good document
659,00:08:31,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"On the other hand, you will have a big loss if you deliver bad one.",00:08:28,10,On hand big loss deliver bad one
660,00:08:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,You can imagine that the system would be very reluctant to,00:08:31,10,You imagine system would reluctant
661,00:08:36,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,deliver lot of documents.,00:08:35,10,deliver lot documents
662,00:08:41,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,It has to be absolutely sure that it's a non-relevant one.,00:08:36,10,It absolutely sure non relevant one
663,00:08:46,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So this utility function has to be designed based on a specific application.,00:08:41,10,So utility function designed based specific application
664,00:08:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,The three basic problems in content-based filtering are the following.,00:08:46,10,The three basic problems content based filtering following
665,00:08:53,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,First has to make a filtering decision.,00:08:49,10,First make filtering decision
666,00:08:58,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So it has to be a binary decision maker, a binary classifier.",00:08:53,10,So binary decision maker binary classifier
667,00:09:03,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Given a text, a text document, and a profile description of the user,",00:08:58,10,Given text text document profile description user
668,00:09:08,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"it has to say yes or no, whether this document should be delivered or not.",00:09:03,10,say yes whether document delivered
669,00:09:11,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So that's a decision module, and",00:09:08,10,So decision module
670,00:09:14,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,there should be a initialization module as you have seen earlier.,00:09:11,10,initialization module seen earlier
671,00:09:17,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And this is to get the system started.,00:09:14,10,And get system started
672,00:09:23,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And we have to initialize the system based on only very limited text description,",00:09:17,10,And initialize system based limited text description
673,00:09:25,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,or very few examples from the user.,00:09:23,10,examples user
674,00:09:30,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And the third component is a learning module which ha,",00:09:26,10,And third component learning module ha
675,00:09:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,has to be able to learn from limited relevance judgments because we,00:09:30,10,able learn limited relevance judgments
676,00:09:41,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,can only learn from the user about their preferences on the delivery documents.,00:09:35,10,learn user preferences delivery documents
677,00:09:44,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If we don't deliver a document to the user, we'd never know",00:09:41,10,If deliver document user never know
678,00:09:48,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"we would never be able to know whether the user likes it or not, right.",00:09:44,10,would never able know whether user likes right
679,00:09:55,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And we can accumulate a lot of documents, we can learn from the entire history.",00:09:50,10,And accumulate lot documents learn entire history
680,00:10:01,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Now, all these models would have to be optimized to maximize the utility.",00:09:55,10,Now models would optimized maximize utility
681,00:10:03,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So how can we build a such a system?,00:10:01,10,So build system
682,00:10:05,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And there are many different approaches.,00:10:03,10,And many different approaches
683,00:10:09,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Here we are going to talk about how to extend a retrieval system,",00:10:05,10,Here going talk extend retrieval system
684,00:10:12,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,a search engine for information filtering.,00:10:09,10,search engine information filtering
685,00:10:15,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"Again, here's why we've spent a lot of times talk about the search engines.",00:10:12,10,Again spent lot times talk search engines
686,00:10:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,Because it's actually not very hard to extend the search engine for,00:10:15,10,Because actually hard extend search engine
687,00:10:22,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,information filtering.,00:10:20,10,information filtering
688,00:10:26,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So, here is the basic idea for extending a retrieval system for",00:10:22,10,So basic idea extending retrieval system
689,00:10:27,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,information filtering.,00:10:26,10,information filtering
690,00:10:31,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"First, we can reuse a lot of retrieval techniques to do scoring.",00:10:27,10,First reuse lot retrieval techniques scoring
691,00:10:35,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"All right, so we know how to score documents against queries et cetera.",00:10:31,10,All right know score documents queries et cetera
692,00:10:40,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,We can measure the similarity between a profile text description and a document.,00:10:35,10,We measure similarity profile text description document
693,00:10:44,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And then we can use a score threshold for the filtering decision.,00:10:40,10,And use score threshold filtering decision
694,00:10:48,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"We, we do retrieval and then we kind of find the scores of documents, and",00:10:44,10,We retrieval kind find scores documents
695,00:10:52,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"then we apply a threshold to, to say,",00:10:48,10,apply threshold say
696,00:10:56,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,to see whether a document is passing this threshold or not.,00:10:52,10,see whether document passing threshold
697,00:10:59,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And if it's passing the threshold, we are going to say it's relevant and",00:10:56,10,And passing threshold going say relevant
698,00:11:02,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,we are going to deliver it to the user.,00:10:59,10,going deliver user
699,00:11:07,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And another component that we have to add is, of course, to learn from the history.",00:11:02,10,And another component add course learn history
700,00:11:11,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And here we can use the traditional feedback techniques,00:11:07,10,And use traditional feedback techniques
701,00:11:13,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,to learn to improve scoring.,00:11:11,10,learn improve scoring
702,00:11:20,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And we know Rocchio can be used for scoring improvement, right?",00:11:13,10,And know Rocchio used scoring improvement right
703,00:11:25,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And, but we have to develop new approaches to learn how to set the threshold.",00:11:20,10,And develop new approaches learn set threshold
704,00:11:27,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And you know, we need to set it initially, and",00:11:25,10,And know need set initially
705,00:11:32,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,then we have to learn how to update the threshold over time.,00:11:27,10,learn update threshold time
706,00:11:38,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So here's what the system might look like if we just,00:11:32,10,So system might look like
707,00:11:45,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"generalized a vector-space model for filtering problems, right?",00:11:38,10,generalized vector space model filtering problems right
708,00:11:49,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"So you can see the document vector could be fed into a scoring module, which it",00:11:45,10,So see document vector could fed scoring module
709,00:11:53,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,already exists in in a search engine that implements a vector-space model.,00:11:49,10,already exists search engine implements vector space model
710,00:11:57,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And the profile will be treated as a query essentially.,00:11:53,10,And profile treated query essentially
711,00:12:01,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And then the profile vector can be matched with the document vector,",00:11:57,10,And profile vector matched document vector
712,00:12:02,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,to generate the score.,00:12:01,10,generate score
713,00:12:06,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And then this score will be fed into a thresholding module that would,00:12:03,10,And score fed thresholding module would
714,00:12:07,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,say yes or no.,00:12:06,10,say yes
715,00:12:13,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,And then the evaluation would be based on the utility for the filtering results.,00:12:07,10,And evaluation would based utility filtering results
716,00:12:16,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"If it says yes, and then the document will be sent to the user, and",00:12:13,10,If says yes document sent user
717,00:12:19,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,then the user could give some feedback.,00:12:16,10,user could give feedback
718,00:12:24,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"And the feedback information would have been use, would be used to both",00:12:19,10,And feedback information would use would used
719,00:12:28,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,adjust to the threshold and adjust the vector representation.,00:12:24,10,adjust threshold adjust vector representation
720,00:12:33,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,So the vector learning is essentially the same as query modification or,00:12:28,10,So vector learning essentially query modification
721,00:12:36,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,feedback in the case of search.,00:12:33,10,feedback case search
722,00:12:38,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,"The threshold learning is a no,",00:12:36,10,The threshold learning
723,00:12:42,5,Recommender Systems- Content-based Filtering - Part 1 ,4.6,new component in that we need to talk a little bit more about.,00:12:38,10,new component need talk little bit
724,00:00:03,2,Vector Space Model- Basic Idea,1.5,[SOUND].,00:00:00,5,SOUND
725,00:00:11,2,Vector Space Model- Basic Idea,1.5,This lecture is about the vector space retrieval model.,00:00:07,5,This lecture vector space retrieval model
726,00:00:14,2,Vector Space Model- Basic Idea,1.5,We're going to give an introduction to its basic idea.,00:00:11,5,We going give introduction basic idea
727,00:00:23,2,Vector Space Model- Basic Idea,1.5,In the last lecture we talked about the different ways of designing,00:00:18,5,In last lecture talked different ways designing
728,00:00:28,2,Vector Space Model- Basic Idea,1.5,a retrieval model which would give us a different the ranking function.,00:00:23,5,retrieval model would give us different ranking function
729,00:00:33,2,Vector Space Model- Basic Idea,1.5,"In this lecture, we're going to talk about the, the specific way of",00:00:30,5,In lecture going talk specific way
730,00:00:36,2,Vector Space Model- Basic Idea,1.5,design the ramping function called a vector space mutual model.,00:00:33,5,design ramping function called vector space mutual model
731,00:00:41,2,Vector Space Model- Basic Idea,1.5,And we're going to give a brief introduction to the basic idea.,00:00:37,5,And going give brief introduction basic idea
732,00:00:47,2,Vector Space Model- Basic Idea,1.5,Vector space model is a special case of,00:00:44,5,Vector space model special case
733,00:00:50,2,Vector Space Model- Basic Idea,1.5,similarity based models as we discussed before.,00:00:47,5,similarity based models discussed
734,00:00:55,2,Vector Space Model- Basic Idea,1.5,"Which means, we assume relevance is roughly",00:00:50,5,Which means assume relevance roughly
735,00:00:59,2,Vector Space Model- Basic Idea,1.5,similarity between a document and a query.,00:00:55,5,similarity document query
736,00:01:06,2,Vector Space Model- Basic Idea,1.5,"Now whether this assumption is true, is actually a question.",00:01:02,5,Now whether assumption true actually question
737,00:01:10,2,Vector Space Model- Basic Idea,1.5,But in order to solve our search problem we have to,00:01:06,5,But order solve search problem
738,00:01:15,2,Vector Space Model- Basic Idea,1.5,convert the vague notion of relevance into a more precise,00:01:10,5,convert vague notion relevance precise
739,00:01:21,2,Vector Space Model- Basic Idea,1.5,definition that can be implemented with the programming language.,00:01:15,5,definition implemented programming language
740,00:01:26,2,Vector Space Model- Basic Idea,1.5,So in this process we have to make a number of assumptions.,00:01:21,5,So process make number assumptions
741,00:01:31,2,Vector Space Model- Basic Idea,1.5,This is the first assumption that we make here.,00:01:26,5,This first assumption make
742,00:01:34,2,Vector Space Model- Basic Idea,1.5,Basically we assume that if a document is more,00:01:31,5,Basically assume document
743,00:01:37,2,Vector Space Model- Basic Idea,1.5,"similar to a query than another document,",00:01:34,5,similar query another document
744,00:01:41,2,Vector Space Model- Basic Idea,1.5,then the first document would be assumed to be more relevant than the second one.,00:01:37,5,first document would assumed relevant second one
745,00:01:45,2,Vector Space Model- Basic Idea,1.5,And this is the basis for ranking documents in this approach.,00:01:41,5,And basis ranking documents approach
746,00:01:52,2,Vector Space Model- Basic Idea,1.5,"Again, it's questionable whether this is really the best definition for relevance.",00:01:46,5,Again questionable whether really best definition relevance
747,00:01:55,2,Vector Space Model- Basic Idea,1.5,As we will see later there are other ways to model relevance.,00:01:52,5,As see later ways model relevance
748,00:02:03,2,Vector Space Model- Basic Idea,1.5,The first idea of vector space retrieval model is actually very easy to understand.,00:01:55,5,The first idea vector space retrieval model actually easy understand
749,00:02:10,2,Vector Space Model- Basic Idea,1.5,"Imagine a high dimensional space, where each dimension corresponds to a term.",00:02:03,5,Imagine high dimensional space dimension corresponds term
750,00:02:16,2,Vector Space Model- Basic Idea,1.5,"So, here, I show a three dimensional space with three words,",00:02:11,5,So I show three dimensional space three words
751,00:02:19,2,Vector Space Model- Basic Idea,1.5,"programming, library, and presidential.",00:02:16,5,programming library presidential
752,00:02:23,2,Vector Space Model- Basic Idea,1.5,"So each term, here, defines one dimension.",00:02:21,5,So term defines one dimension
753,00:02:29,2,Vector Space Model- Basic Idea,1.5,Now we can consider vectors in this three dimensional space.,00:02:24,5,Now consider vectors three dimensional space
754,00:02:32,2,Vector Space Model- Basic Idea,1.5,And we're going to assume all our documents and,00:02:29,5,And going assume documents
755,00:02:35,2,Vector Space Model- Basic Idea,1.5,the query will be placed in this vector space.,00:02:32,5,query placed vector space
756,00:02:42,2,Vector Space Model- Basic Idea,1.5,"So, for example, one document that might be represented at by this vector, d1.",00:02:35,5,So example one document might represented vector d1
757,00:02:50,2,Vector Space Model- Basic Idea,1.5,Now this means this document probably covers library and presidential.,00:02:44,5,Now means document probably covers library presidential
758,00:02:52,2,Vector Space Model- Basic Idea,1.5,But it doesn't really talk about programming.,00:02:50,5,But really talk programming
759,00:03:00,2,Vector Space Model- Basic Idea,1.5,"All right, what does this mean in terms of presentation of document?",00:02:54,5,All right mean terms presentation document
760,00:03:01,2,Vector Space Model- Basic Idea,1.5,"That just means,",00:03:00,5,That means
761,00:03:05,2,Vector Space Model- Basic Idea,1.5,we're going to look at our document from the perspective of this vector.,00:03:01,5,going look document perspective vector
762,00:03:07,2,Vector Space Model- Basic Idea,1.5,We're going to ignore everything else.,00:03:05,5,We going ignore everything else
763,00:03:12,2,Vector Space Model- Basic Idea,1.5,Basically what we see here is only the vector of the document.,00:03:07,5,Basically see vector document
764,00:03:16,2,Vector Space Model- Basic Idea,1.5,Of course the document has other information.,00:03:14,5,Of course document information
765,00:03:19,2,Vector Space Model- Basic Idea,1.5,"For example, the orders of words are simply ignored and",00:03:16,5,For example orders words simply ignored
766,00:03:22,2,Vector Space Model- Basic Idea,1.5,that's because we're assume that the words.,00:03:19,5,assume words
767,00:03:28,2,Vector Space Model- Basic Idea,1.5,"So with this representation you have already seen, d1,",00:03:24,5,So representation already seen d1
768,00:03:31,2,Vector Space Model- Basic Idea,1.5,seems to suggest a topic in either presidential library.,00:03:28,5,seems suggest topic either presidential library
769,00:03:36,2,Vector Space Model- Basic Idea,1.5,Now this is different from another document.,00:03:33,5,Now different another document
770,00:03:39,2,Vector Space Model- Basic Idea,1.5,"Which might be represented as a different vector, d2 here.",00:03:36,5,Which might represented different vector d2
771,00:03:43,2,Vector Space Model- Basic Idea,1.5,"Now in this case, the document that covers programming and library, but",00:03:39,5,Now case document covers programming library
772,00:03:45,2,Vector Space Model- Basic Idea,1.5,it doesn't talk about presidential.,00:03:43,5,talk presidential
773,00:03:48,2,Vector Space Model- Basic Idea,1.5,So what does this remind you?,00:03:46,5,So remind
774,00:03:54,2,Vector Space Model- Basic Idea,1.5,"Well, you can probably guess, the topic is likely about program language and",00:03:48,5,Well probably guess topic likely program language
775,00:03:56,2,Vector Space Model- Basic Idea,1.5,"the library is software library, library.",00:03:54,5,library software library library
776,00:04:04,2,Vector Space Model- Basic Idea,1.5,"So this shows that by using this vector space representation,",00:03:58,5,So shows using vector space representation
777,00:04:08,2,Vector Space Model- Basic Idea,1.5,we can actually capture the differences between topics of documents.,00:04:04,5,actually capture differences topics documents
778,00:04:12,2,Vector Space Model- Basic Idea,1.5,Now you can also imagine there are other vectors.,00:04:09,5,Now also imagine vectors
779,00:04:12,2,Vector Space Model- Basic Idea,1.5,"For example,",00:04:12,5,For example
780,00:04:18,2,Vector Space Model- Basic Idea,1.5,"d3 is pointing in that direction, that might be about presidential programming.",00:04:12,5,d3 pointing direction might presidential programming
781,00:04:21,2,Vector Space Model- Basic Idea,1.5,And in fact we're going to place all the documents in this vector space.,00:04:18,5,And fact going place documents vector space
782,00:04:26,2,Vector Space Model- Basic Idea,1.5,And they will be pointing to all kinds of directions.,00:04:22,5,And pointing kinds directions
783,00:04:30,2,Vector Space Model- Basic Idea,1.5,"And similarly, we're going to place our query also in this space,",00:04:26,5,And similarly going place query also space
784,00:04:31,2,Vector Space Model- Basic Idea,1.5,as another vector.,00:04:30,5,another vector
785,00:04:37,2,Vector Space Model- Basic Idea,1.5,And then we're going to measure the similarity between the query vector and,00:04:32,5,And going measure similarity query vector
786,00:04:39,2,Vector Space Model- Basic Idea,1.5,every document vector.,00:04:37,5,every document vector
787,00:04:45,2,Vector Space Model- Basic Idea,1.5,"So, in this case for example, we can easily see d2 seems to be the closest of,",00:04:39,5,So case example easily see d2 seems closest
788,00:04:50,2,Vector Space Model- Basic Idea,1.5,to this query factor and therefore d2 will be ranked above others.,00:04:45,5,query factor therefore d2 ranked others
789,00:04:56,2,Vector Space Model- Basic Idea,1.5,"So this was a, basically the main idea of the, the vector space model.",00:04:51,5,So basically main idea vector space model
790,00:05:04,2,Vector Space Model- Basic Idea,1.5,"So to be more pri, precise, be more precise.",00:04:58,5,So pri precise precise
791,00:05:09,2,Vector Space Model- Basic Idea,1.5,Vector space model is a framework.,00:05:04,5,Vector space model framework
792,00:05:12,2,Vector Space Model- Basic Idea,1.5,"In this framework, we make the following assumptions.",00:05:09,5,In framework make following assumptions
793,00:05:17,2,Vector Space Model- Basic Idea,1.5,"First, we represent a document and query by a term vector.",00:05:12,5,First represent document query term vector
794,00:05:21,2,Vector Space Model- Basic Idea,1.5,So here a term can be any basic concept.,00:05:18,5,So term basic concept
795,00:05:28,2,Vector Space Model- Basic Idea,1.5,"For example, a word or a phrase, or even enneagram of characters.",00:05:21,5,For example word phrase even enneagram characters
796,00:05:32,2,Vector Space Model- Basic Idea,1.5,Those are a sequence of characters inside a word.,00:05:28,5,Those sequence characters inside word
797,00:05:37,2,Vector Space Model- Basic Idea,1.5,Each term is assumed to define one dimension.,00:05:34,5,Each term assumed define one dimension
798,00:05:38,2,Vector Space Model- Basic Idea,1.5,Therefore N terms.,00:05:37,5,Therefore N terms
799,00:05:42,2,Vector Space Model- Basic Idea,1.5,"In our vocabulary, we define N-dimensional space.",00:05:38,5,In vocabulary define N dimensional space
800,00:05:49,2,Vector Space Model- Basic Idea,1.5,A query vector would consist of a number of elements,00:05:44,5,A query vector would consist number elements
801,00:05:54,2,Vector Space Model- Basic Idea,1.5,corresponding to the weights of different terms.,00:05:49,5,corresponding weights different terms
802,00:05:59,2,Vector Space Model- Basic Idea,1.5,Each document vector is also similar.,00:05:54,5,Each document vector also similar
803,00:06:04,2,Vector Space Model- Basic Idea,1.5,It has a number of elements and each value of each element,00:05:59,5,It number elements value element
804,00:06:08,2,Vector Space Model- Basic Idea,1.5,is indicating that weight of the corresponding term.,00:06:05,5,indicating weight corresponding term
805,00:06:12,2,Vector Space Model- Basic Idea,1.5,"Here you can see, we have seen there are N dimensions.",00:06:09,5,Here see seen N dimensions
806,00:06:14,2,Vector Space Model- Basic Idea,1.5,"Therefore, there are N elements,",00:06:12,5,Therefore N elements
807,00:06:18,2,Vector Space Model- Basic Idea,1.5,each corresponding to the weight on the particular term.,00:06:15,5,corresponding weight particular term
808,00:06:26,2,Vector Space Model- Basic Idea,1.5,So the relevance in this case would be assume to be the similarity,00:06:21,5,So relevance case would assume similarity
809,00:06:31,2,Vector Space Model- Basic Idea,1.5,"between the two vectors, therefore our range in function is",00:06:26,5,two vectors therefore range function
810,00:06:35,2,Vector Space Model- Basic Idea,1.5,also defined as the similarity between the query vector and document vector.,00:06:31,5,also defined similarity query vector document vector
811,00:06:41,2,Vector Space Model- Basic Idea,1.5,"Now, if I ask you to write the program to the internet this approach",00:06:37,5,Now I ask write program internet approach
812,00:06:42,2,Vector Space Model- Basic Idea,1.5,in the search engine.,00:06:41,5,search engine
813,00:06:47,2,Vector Space Model- Basic Idea,1.5,"You would realize that this was far from clear, right?",00:06:44,5,You would realize far clear right
814,00:06:51,2,Vector Space Model- Basic Idea,1.5,We haven't seen a lot of things in detail,00:06:48,5,We seen lot things detail
815,00:06:56,2,Vector Space Model- Basic Idea,1.5,therefore it's impossible to actually write the program to implement this.,00:06:51,5,therefore impossible actually write program implement
816,00:06:58,2,Vector Space Model- Basic Idea,1.5,That's why I said this is a framework.,00:06:56,5,That I said framework
817,00:07:03,2,Vector Space Model- Basic Idea,1.5,And this has to be refined in order to actually,00:06:59,5,And refined order actually
818,00:07:08,2,Vector Space Model- Basic Idea,1.5,"suggest a particular function, that you can implement on the computer.",00:07:04,5,suggest particular function implement computer
819,00:07:13,2,Vector Space Model- Basic Idea,1.5,"So, what does this framework not serve?",00:07:10,5,So framework serve
820,00:07:17,2,Vector Space Model- Basic Idea,1.5,"Well, it actually hasn't set many things",00:07:13,5,Well actually set many things
821,00:07:22,2,Vector Space Model- Basic Idea,1.5,that would be required in order to implement this function.,00:07:17,5,would required order implement function
822,00:07:30,2,Vector Space Model- Basic Idea,1.5,"First, it did not say how we should define or select the basic concepts exactly.",00:07:24,5,First say define select basic concepts exactly
823,00:07:36,2,Vector Space Model- Basic Idea,1.5,"We clearly assume the concepts are orthogonal,",00:07:32,5,We clearly assume concepts orthogonal
824,00:07:38,2,Vector Space Model- Basic Idea,1.5,otherwise there will be redundancy.,00:07:36,5,otherwise redundancy
825,00:07:45,2,Vector Space Model- Basic Idea,1.5,"For example, if two synonyms are somehow distinguished as two different concepts.",00:07:38,5,For example two synonyms somehow distinguished two different concepts
826,00:07:48,2,Vector Space Model- Basic Idea,1.5,Then they would be defined in two different dimensions.,00:07:45,5,Then would defined two different dimensions
827,00:07:53,2,Vector Space Model- Basic Idea,1.5,And then that would clearly cause a redundancy here.,00:07:48,5,And would clearly cause redundancy
828,00:08:01,2,Vector Space Model- Basic Idea,1.5,Or overemphasizing of matching this concept.,00:07:54,5,Or overemphasizing matching concept
829,00:08:05,2,Vector Space Model- Basic Idea,1.5,Because it would be as if you matched the two dimensions,00:08:01,5,Because would matched two dimensions
830,00:08:08,2,Vector Space Model- Basic Idea,1.5,when you actually matched one semantic concept.,00:08:05,5,actually matched one semantic concept
831,00:08:16,2,Vector Space Model- Basic Idea,1.5,"Secondly, it did not say how we exactly should place documents and",00:08:11,5,Secondly say exactly place documents
832,00:08:18,2,Vector Space Model- Basic Idea,1.5,query in this space.,00:08:16,5,query space
833,00:08:22,2,Vector Space Model- Basic Idea,1.5,Basically I show you some examples of query and document vectors.,00:08:18,5,Basically I show examples query document vectors
834,00:08:27,2,Vector Space Model- Basic Idea,1.5,But where exactly should the vector for a particular document point to?,00:08:22,5,But exactly vector particular document point
835,00:08:33,2,Vector Space Model- Basic Idea,1.5,[INAUDIBLE] So this is equivalent to how to define the term weights.,00:08:27,5,INAUDIBLE So equivalent define term weights
836,00:08:37,2,Vector Space Model- Basic Idea,1.5,How do you computer use element values in those vectors?,00:08:33,5,How computer use element values vectors
837,00:08:44,2,Vector Space Model- Basic Idea,1.5,This is a very important question because term weight,00:08:39,5,This important question term weight
838,00:08:47,2,Vector Space Model- Basic Idea,1.5,in the query vector indicates the importance of term.,00:08:44,5,query vector indicates importance term
839,00:08:51,2,Vector Space Model- Basic Idea,1.5,"So depending on how you assign the weight,",00:08:48,5,So depending assign weight
840,00:08:56,2,Vector Space Model- Basic Idea,1.5,you might prefer some terms to be matched over others.,00:08:51,5,might prefer terms matched others
841,00:08:59,2,Vector Space Model- Basic Idea,1.5,"Similarly, term weight in the document is also very meaningful.",00:08:56,5,Similarly term weight document also meaningful
842,00:09:02,2,Vector Space Model- Basic Idea,1.5,It indicates how well the term characterizes the document.,00:08:59,5,It indicates well term characterizes document
843,00:09:08,2,Vector Space Model- Basic Idea,1.5,"If you got it wrong, then you clearly don't represent this document accurately.",00:09:03,5,If got wrong clearly represent document accurately
844,00:09:13,2,Vector Space Model- Basic Idea,1.5,"Finally, how we define the similarity measure is also not clear.",00:09:10,5,Finally define similarity measure also clear
845,00:09:20,2,Vector Space Model- Basic Idea,1.5,So these questions must be addressed before we can have an operational,00:09:15,5,So questions must addressed operational
846,00:09:24,2,Vector Space Model- Basic Idea,1.5,function that we can actually implement using a program language.,00:09:20,5,function actually implement using program language
847,00:09:32,2,Vector Space Model- Basic Idea,1.5,So how do we solve these problems,00:09:25,5,So solve problems
848,00:09:39,2,Vector Space Model- Basic Idea,1.5,is the main topic of the next lecture.,00:09:32,5,main topic next lecture
849,00:00:05,2,Text Retrieval Problem,1.3,[SOUND] This lecture is about,00:00:00,3,SOUND This lecture
850,00:00:11,2,Text Retrieval Problem,1.3,the text retrieval problem.,00:00:05,3,text retrieval problem
851,00:00:15,2,Text Retrieval Problem,1.3,This picture shows our overall plan for lectures.,00:00:11,3,This picture shows overall plan lectures
852,00:00:21,2,Text Retrieval Problem,1.3,"In the last lecture, we talked about the high level strategies for text access.",00:00:15,3,In last lecture talked high level strategies text access
853,00:00:24,2,Text Retrieval Problem,1.3,We talked about push versus pull.,00:00:21,3,We talked push versus pull
854,00:00:30,2,Text Retrieval Problem,1.3,Search engines are the main tools for supporting the pull mode.,00:00:25,3,Search engines main tools supporting pull mode
855,00:00:32,2,Text Retrieval Problem,1.3,"Starting from this lecture,",00:00:30,3,Starting lecture
856,00:00:36,2,Text Retrieval Problem,1.3,we're going to talk about the how search engines work in detail.,00:00:32,3,going talk search engines work detail
857,00:00:42,2,Text Retrieval Problem,1.3,"So first, it's about the text retrieval problem.",00:00:38,3,So first text retrieval problem
858,00:00:46,2,Text Retrieval Problem,1.3,We're going to talk about the three things in this lecture.,00:00:42,3,We going talk three things lecture
859,00:00:49,2,Text Retrieval Problem,1.3,"First, we'll define text retrieval.",00:00:46,3,First define text retrieval
860,00:00:54,2,Text Retrieval Problem,1.3,"Second, we're going to make a comparison between text retrieval and",00:00:49,3,Second going make comparison text retrieval
861,00:00:56,2,Text Retrieval Problem,1.3,"the related task, database retrieval.",00:00:54,3,related task database retrieval
862,00:01:02,2,Text Retrieval Problem,1.3,"Finally, we're going to talk about the document selection versus document ranking",00:00:58,3,Finally going talk document selection versus document ranking
863,00:01:06,2,Text Retrieval Problem,1.3,as two strategies for responding to a user's query.,00:01:02,3,two strategies responding user query
864,00:01:11,2,Text Retrieval Problem,1.3,So what is text retrieval?,00:01:09,3,So text retrieval
865,00:01:17,2,Text Retrieval Problem,1.3,It should be a task that's familiar to most of us because we're using web,00:01:12,3,It task familiar us using web
866,00:01:19,2,Text Retrieval Problem,1.3,search engines all the time.,00:01:17,3,search engines time
867,00:01:25,2,Text Retrieval Problem,1.3,So text retrieval is basically a task where the system would respond,00:01:19,3,So text retrieval basically task system would respond
868,00:01:30,2,Text Retrieval Problem,1.3,"to a user's query with relevant lock-ins, basically through",00:01:25,3,user query relevant lock ins basically
869,00:01:37,2,Text Retrieval Problem,1.3,supported querying as one way to implement the poor mold of information access.,00:01:30,3,supported querying one way implement poor mold information access
870,00:01:40,2,Text Retrieval Problem,1.3,So the scenario's the following.,00:01:39,3,So scenario following
871,00:01:43,2,Text Retrieval Problem,1.3,You have a collection of text documents.,00:01:40,3,You collection text documents
872,00:01:46,2,Text Retrieval Problem,1.3,These documents could be all the web pages on the web.,00:01:43,3,These documents could web pages web
873,00:01:51,2,Text Retrieval Problem,1.3,Or all the literature articles in the digital library or,00:01:47,3,Or literature articles digital library
874,00:01:56,2,Text Retrieval Problem,1.3,maybe all the text files in your computer.,00:01:51,3,maybe text files computer
875,00:02:04,2,Text Retrieval Problem,1.3,A user will typically give a query to the system to express the information need.,00:01:58,3,A user typically give query system express information need
876,00:02:09,2,Text Retrieval Problem,1.3,And then the system would return relevant documents to users.,00:02:04,3,And system would return relevant documents users
877,00:02:14,2,Text Retrieval Problem,1.3,Relevant documents refer to those documents that are useful to the user who,00:02:09,3,Relevant documents refer documents useful user
878,00:02:15,2,Text Retrieval Problem,1.3,typed in the query.,00:02:14,3,typed query
879,00:02:19,2,Text Retrieval Problem,1.3,Now this task is a often called information retrieval.,00:02:16,3,Now task often called information retrieval
880,00:02:25,2,Text Retrieval Problem,1.3,"But literally, information retrieval would broadly include the retrieval of",00:02:21,3,But literally information retrieval would broadly include retrieval
881,00:02:27,2,Text Retrieval Problem,1.3,other non-textual information as well.,00:02:25,3,non textual information well
882,00:02:31,2,Text Retrieval Problem,1.3,"For example, audio, video, et cetera.",00:02:27,3,For example audio video et cetera
883,00:02:37,2,Text Retrieval Problem,1.3,It's worth noting that text retrieval is at the core of,00:02:33,3,It worth noting text retrieval core
884,00:02:41,2,Text Retrieval Problem,1.3,information retrieval in the sense that other medias such as,00:02:37,3,information retrieval sense medias
885,00:02:47,2,Text Retrieval Problem,1.3,video can be retrieved by exploiting the companion text data.,00:02:41,3,video retrieved exploiting companion text data
886,00:02:52,2,Text Retrieval Problem,1.3,"So for example, can the image search engines actually",00:02:47,3,So example image search engines actually
887,00:02:57,2,Text Retrieval Problem,1.3,match a user's query with the companion text data of the image?,00:02:52,3,match user query companion text data image
888,00:03:03,2,Text Retrieval Problem,1.3,"This problem is also called the, the search problem,",00:02:59,3,This problem also called search problem
889,00:03:08,2,Text Retrieval Problem,1.3,and the technology is often called search technology in industry.,00:03:05,3,technology often called search technology industry
890,00:03:14,2,Text Retrieval Problem,1.3,"If you ever take on course in databases,",00:03:11,3,If ever take course databases
891,00:03:18,2,Text Retrieval Problem,1.3,it'll be useful to pause the lecture at this point and,00:03:14,3,useful pause lecture point
892,00:03:25,2,Text Retrieval Problem,1.3,think about the differences between text retrieval and database retrieval.,00:03:18,3,think differences text retrieval database retrieval
893,00:03:28,2,Text Retrieval Problem,1.3,Now these two tasks are similar in many ways.,00:03:25,3,Now two tasks similar many ways
894,00:03:31,2,Text Retrieval Problem,1.3,But there are some important differences.,00:03:29,3,But important differences
895,00:03:38,2,Text Retrieval Problem,1.3,"So, spend a moment to think about the differences between the two.",00:03:33,3,So spend moment think differences two
896,00:03:42,2,Text Retrieval Problem,1.3,Think about the data and information managed by a search engine versus,00:03:38,3,Think data information managed search engine versus
897,00:03:46,2,Text Retrieval Problem,1.3,"those that are man, managed by a database system.",00:03:42,3,man managed database system
898,00:03:51,2,Text Retrieval Problem,1.3,Think about the difference between the queries that you typically specify for,00:03:47,3,Think difference queries typically specify
899,00:03:57,2,Text Retrieval Problem,1.3,a database system versus the queries that typed in by users on the search engine.,00:03:51,3,database system versus queries typed users search engine
900,00:04:00,2,Text Retrieval Problem,1.3,And then finally think about the answers.,00:03:59,3,And finally think answers
901,00:04:04,2,Text Retrieval Problem,1.3,What's the difference between the two?,00:04:02,3,What difference two
902,00:04:07,2,Text Retrieval Problem,1.3,"Okay, so",00:04:06,3,Okay
903,00:04:11,2,Text Retrieval Problem,1.3,if we think probably the information out there are managed by the two systems.,00:04:07,3,think probably information managed two systems
904,00:04:18,2,Text Retrieval Problem,1.3,"We will see that in text retrieval, the data is unstructured, it's free text.",00:04:11,3,We see text retrieval data unstructured free text
905,00:04:23,2,Text Retrieval Problem,1.3,"But in databases, they are structured data, where there is a clear defined",00:04:18,3,But databases structured data clear defined
906,00:04:28,2,Text Retrieval Problem,1.3,schema to tell you this column is the names of people and,00:04:23,3,schema tell column names people
907,00:04:31,2,Text Retrieval Problem,1.3,"that column is ages, et cetera.",00:04:28,3,column ages et cetera
908,00:04:33,2,Text Retrieval Problem,1.3,"In unstructured text,",00:04:31,3,In unstructured text
909,00:04:38,2,Text Retrieval Problem,1.3,it's not obvious what are the names of people mentioned in the text.,00:04:33,3,obvious names people mentioned text
910,00:04:42,2,Text Retrieval Problem,1.3,"Because of this difference,",00:04:40,3,Because difference
911,00:04:46,2,Text Retrieval Problem,1.3,we can also see that text information tends to be more ambiguous.,00:04:42,3,also see text information tends ambiguous
912,00:04:51,2,Text Retrieval Problem,1.3,And we'll talk about that in the natural language processing lecture.,00:04:46,3,And talk natural language processing lecture
913,00:04:55,2,Text Retrieval Problem,1.3,"Whereas in databases, the data tend to have well-defined semantics.",00:04:51,3,Whereas databases data tend well defined semantics
914,00:05:01,2,Text Retrieval Problem,1.3,"There is also important difference in the queries, and",00:04:58,3,There also important difference queries
915,00:05:05,2,Text Retrieval Problem,1.3,"this is partly due to the difference in the information, or data.",00:05:01,3,partly due difference information data
916,00:05:12,2,Text Retrieval Problem,1.3,"So text queries tend to be ambiguous, whereas in their research,",00:05:07,3,So text queries tend ambiguous whereas research
917,00:05:16,2,Text Retrieval Problem,1.3,the queries are particularly well-defined.,00:05:12,3,queries particularly well defined
918,00:05:17,2,Text Retrieval Problem,1.3,"Think about the SQL query,",00:05:16,3,Think SQL query
919,00:05:22,2,Text Retrieval Problem,1.3,that would clear the specify what records to be returned.,00:05:17,3,would clear specify records returned
920,00:05:26,2,Text Retrieval Problem,1.3,So it has very well defined semantics.,00:05:22,3,So well defined semantics
921,00:05:31,2,Text Retrieval Problem,1.3,Queue all queries or naturally ending queries tend to be incomplete.,00:05:26,3,Queue queries naturally ending queries tend incomplete
922,00:05:34,2,Text Retrieval Problem,1.3,"Also in that it doesn't really,",00:05:31,3,Also really
923,00:05:37,2,Text Retrieval Problem,1.3,fully specify what documents should be retrieved.,00:05:34,3,fully specify documents retrieved
924,00:05:42,2,Text Retrieval Problem,1.3,"Whereas, in the database search, the SQL query",00:05:38,3,Whereas database search SQL query
925,00:05:47,2,Text Retrieval Problem,1.3,can be regarded as a computer specification for what should be returned.,00:05:42,3,regarded computer specification returned
926,00:05:51,2,Text Retrieval Problem,1.3,"And because of these differences, the answers would be also different.",00:05:47,3,And differences answers would also different
927,00:05:56,2,Text Retrieval Problem,1.3,"In the case of text retrieval, we're looking for relevant documents.",00:05:52,3,In case text retrieval looking relevant documents
928,00:06:02,2,Text Retrieval Problem,1.3,"In the database search, we are retrieving records or",00:05:58,3,In database search retrieving records
929,00:06:07,2,Text Retrieval Problem,1.3,"matched records with the SQL query, more precisely.",00:06:02,3,matched records SQL query precisely
930,00:06:15,2,Text Retrieval Problem,1.3,"Now in the case of text retrieval, what should be the right answers to",00:06:09,3,Now case text retrieval right answers
931,00:06:19,2,Text Retrieval Problem,1.3,"a query is not very well specified, as we just discussed.",00:06:15,3,query well specified discussed
932,00:06:25,2,Text Retrieval Problem,1.3,So it's unclear what should be the right answers to a query.,00:06:21,3,So unclear right answers query
933,00:06:29,2,Text Retrieval Problem,1.3,"And this has very important consequences, and",00:06:25,3,And important consequences
934,00:06:34,2,Text Retrieval Problem,1.3,that is text retrieval is an empirically defined problem.,00:06:29,3,text retrieval empirically defined problem
935,00:06:46,2,Text Retrieval Problem,1.3,"And so this a problem because if it's empirically defined,",00:06:38,3,And problem empirically defined
936,00:06:51,2,Text Retrieval Problem,1.3,then we cannot mathematically prove one method is better than another method.,00:06:46,3,cannot mathematically prove one method better another method
937,00:06:56,2,Text Retrieval Problem,1.3,That also means we must rely on emperical evaluation,00:06:52,3,That also means must rely emperical evaluation
938,00:07:01,2,Text Retrieval Problem,1.3,more than users to know which method works better.,00:06:56,3,users know method works better
939,00:07:05,2,Text Retrieval Problem,1.3,"And that's why we have one lecture,",00:07:02,3,And one lecture
940,00:07:09,2,Text Retrieval Problem,1.3,actually more than one lectures to cover the issue of evaluation.,00:07:05,3,actually one lectures cover issue evaluation
941,00:07:12,2,Text Retrieval Problem,1.3,Because this is a very important topic for search engines.,00:07:09,3,Because important topic search engines
942,00:07:17,2,Text Retrieval Problem,1.3,"Without knowing how to evaluate an algorithm appropriately,",00:07:13,3,Without knowing evaluate algorithm appropriately
943,00:07:21,2,Text Retrieval Problem,1.3,there's no way to tell whether we have got the better algorithm or,00:07:17,3,way tell whether got better algorithm
944,00:07:23,2,Text Retrieval Problem,1.3,whether one system is better than another.,00:07:21,3,whether one system better another
945,00:07:31,2,Text Retrieval Problem,1.3,So now let's look at the problem in a formal way.,00:07:28,3,So let look problem formal way
946,00:07:36,2,Text Retrieval Problem,1.3,So this slide shows a formal formulation of the text retrieval problem.,00:07:31,3,So slide shows formal formulation text retrieval problem
947,00:07:43,2,Text Retrieval Problem,1.3,"First, we have our vocabulary set which is just a set of words in a language.",00:07:37,3,First vocabulary set set words language
948,00:07:49,2,Text Retrieval Problem,1.3,"Now here, we're considering just one language, but",00:07:44,3,Now considering one language
949,00:07:53,2,Text Retrieval Problem,1.3,in reality on the web there might be multiple natural languages.,00:07:49,3,reality web might multiple natural languages
950,00:07:56,2,Text Retrieval Problem,1.3,We have text that are in all kinds of languages.,00:07:53,3,We text kinds languages
951,00:08:01,2,Text Retrieval Problem,1.3,"But here for simplicity, we just assume there is one kind of language.",00:07:57,3,But simplicity assume one kind language
952,00:08:06,2,Text Retrieval Problem,1.3,"As the techniques used for retrieving data from multiple languages,.",00:08:01,3,As techniques used retrieving data multiple languages
953,00:08:10,2,Text Retrieval Problem,1.3,Are more or less similar to the techniques used for,00:08:06,3,Are less similar techniques used
954,00:08:13,2,Text Retrieval Problem,1.3,retrieving documents in one language.,00:08:10,3,retrieving documents one language
955,00:08:15,2,Text Retrieval Problem,1.3,"Although there is important difference, the principles and",00:08:13,3,Although important difference principles
956,00:08:18,2,Text Retrieval Problem,1.3,methods are very similar.,00:08:15,3,methods similar
957,00:08:24,2,Text Retrieval Problem,1.3,"Next we have the query, which is a sequence of words.",00:08:21,3,Next query sequence words
958,00:08:31,2,Text Retrieval Problem,1.3,And so here you can see the query,00:08:26,3,And see query
959,00:08:36,2,Text Retrieval Problem,1.3,is defined as a sequence of words.,00:08:33,3,defined sequence words
960,00:08:41,2,Text Retrieval Problem,1.3,Each q sub i is a word in the vocabulary.,00:08:36,3,Each q sub word vocabulary
961,00:08:44,2,Text Retrieval Problem,1.3,A document is defined in the same way.,00:08:42,3,A document defined way
962,00:08:47,2,Text Retrieval Problem,1.3,So it's also a sequence of words.,00:08:44,3,So also sequence words
963,00:08:51,2,Text Retrieval Problem,1.3,"And here, d sub ij is also a word in the vocabulary.",00:08:47,3,And sub ij also word vocabulary
964,00:08:55,2,Text Retrieval Problem,1.3,"Now typically, the documents are much longer than queries.",00:08:52,3,Now typically documents much longer queries
965,00:09:01,2,Text Retrieval Problem,1.3,But there are also cases where the documents may be very short.,00:08:57,3,But also cases documents may short
966,00:09:08,2,Text Retrieval Problem,1.3,"So you can think about the, what might be a example of that case.",00:09:04,3,So think might example case
967,00:09:13,2,Text Retrieval Problem,1.3,"I hope you can think of, of twitter search, all right?",00:09:09,3,I hope think twitter search right
968,00:09:14,2,Text Retrieval Problem,1.3,Tweets are very short.,00:09:13,3,Tweets short
969,00:09:20,2,Text Retrieval Problem,1.3,"But in general, documents are longer then the queries.",00:09:16,3,But general documents longer queries
970,00:09:27,2,Text Retrieval Problem,1.3,"Now, then we have a collection of documents.",00:09:23,3,Now collection documents
971,00:09:31,2,Text Retrieval Problem,1.3,And this collection can be very large.,00:09:29,3,And collection large
972,00:09:32,2,Text Retrieval Problem,1.3,So think about the web.,00:09:31,3,So think web
973,00:09:35,2,Text Retrieval Problem,1.3,"It could, could be very large.",00:09:32,3,It could could large
974,00:09:41,2,Text Retrieval Problem,1.3,"And then the goal of text retrieval is you'll find the set of relevant documents,",00:09:35,3,And goal text retrieval find set relevant documents
975,00:09:46,2,Text Retrieval Problem,1.3,"which we denote by R of q, because it depends on the query.",00:09:41,3,denote R q depends query
976,00:09:50,2,Text Retrieval Problem,1.3,"And this is, in general, a subset of all the documents in the collection.",00:09:46,3,And general subset documents collection
977,00:09:57,2,Text Retrieval Problem,1.3,"Unfortunately, this set of random documents is generally unknown,",00:09:52,3,Unfortunately set random documents generally unknown
978,00:10:03,2,Text Retrieval Problem,1.3,and usually depend in the sense that for the same query typed,00:09:57,3,usually depend sense query typed
979,00:10:08,2,Text Retrieval Problem,1.3,"in by different users, the expected relevant documents may be different.",00:10:03,3,different users expected relevant documents may different
980,00:10:13,2,Text Retrieval Problem,1.3,The query given to us by the user is only a hint,00:10:09,3,The query given us user hint
981,00:10:15,2,Text Retrieval Problem,1.3,on which document should be in this set.,00:10:13,3,document set
982,00:10:25,2,Text Retrieval Problem,1.3,"And indeed, the user is generally unable to specify what exactly should be in",00:10:17,3,And indeed user generally unable specify exactly
983,00:10:29,2,Text Retrieval Problem,1.3,"the set, especially in the case of a web search where the collection is so large.",00:10:25,3,set especially case web search collection large
984,00:10:32,2,Text Retrieval Problem,1.3,The user doesn't have complete knowledge about the whole collection.,00:10:29,3,The user complete knowledge whole collection
985,00:10:39,2,Text Retrieval Problem,1.3,"So, the best a search system can do is",00:10:33,3,So best search system
986,00:10:45,2,Text Retrieval Problem,1.3,to compute an approximation of this relevent document set.,00:10:39,3,compute approximation relevent document set
987,00:10:50,2,Text Retrieval Problem,1.3,So we denote it by R prime of q.,00:10:45,3,So denote R prime q
988,00:10:55,2,Text Retrieval Problem,1.3,"So, formally, we can see the task is to compute this R prime of q,",00:10:50,3,So formally see task compute R prime q
989,00:11:00,2,Text Retrieval Problem,1.3,an approximation of the relevant documents.,00:10:55,3,approximation relevant documents
990,00:11:01,2,Text Retrieval Problem,1.3,So how can we do that?,00:11:00,3,So
991,00:11:07,2,Text Retrieval Problem,1.3,"Now, imagine if you are now asked to write a program to do this.",00:11:02,3,Now imagine asked write program
992,00:11:09,2,Text Retrieval Problem,1.3,What would you do?,00:11:08,3,What would
993,00:11:11,2,Text Retrieval Problem,1.3,Now think for a moment.,00:11:09,3,Now think moment
994,00:11:14,2,Text Retrieval Problem,1.3,"Right, so these are your input.",00:11:11,3,Right input
995,00:11:20,2,Text Retrieval Problem,1.3,"With the query, the documents and",00:11:15,3,With query documents
996,00:11:25,2,Text Retrieval Problem,1.3,"then you will have computed the answers to this query,",00:11:20,3,computed answers query
997,00:11:28,2,Text Retrieval Problem,1.3,which is set of documents that would be useful to the user.,00:11:25,3,set documents would useful user
998,00:11:33,2,Text Retrieval Problem,1.3,So how would you solve the problem?,00:11:29,3,So would solve problem
999,00:11:37,2,Text Retrieval Problem,1.3,In general there are two strategies that we can use.,00:11:33,3,In general two strategies use
1000,00:11:42,2,Text Retrieval Problem,1.3,"All right, the first strategy is to do document selection.",00:11:39,3,All right first strategy document selection
1001,00:11:46,2,Text Retrieval Problem,1.3,"And that is, we're going to have a binary classification function, or",00:11:42,3,And going binary classification function
1002,00:11:47,2,Text Retrieval Problem,1.3,binary classified.,00:11:46,3,binary classified
1003,00:11:52,2,Text Retrieval Problem,1.3,That's a function that will take a document and,00:11:49,3,That function take document
1004,00:11:57,2,Text Retrieval Problem,1.3,"query as input, and then give a zero or one as output,",00:11:52,3,query input give zero one output
1005,00:12:01,2,Text Retrieval Problem,1.3,"to indicate whether this document is relevant to the query, or not.",00:11:57,3,indicate whether document relevant query
1006,00:12:05,2,Text Retrieval Problem,1.3,"So in this case, you can see the document.",00:12:02,3,So case see document
1007,00:12:15,2,Text Retrieval Problem,1.3,"The, the relevant document set is defined as follows.",00:12:08,3,The relevant document set defined follows
1008,00:12:19,2,Text Retrieval Problem,1.3,"It basically, all the documents that",00:12:15,3,It basically documents
1009,00:12:24,2,Text Retrieval Problem,1.3,have a value of one by this function.,00:12:19,3,value one function
1010,00:12:25,2,Text Retrieval Problem,1.3,"And so in this case,",00:12:24,3,And case
1011,00:12:29,2,Text Retrieval Problem,1.3,you can see the system must have decided if a document is relevant or not.,00:12:25,3,see system must decided document relevant
1012,00:12:33,2,Text Retrieval Problem,1.3,"Basically, that has to say whether it's one or zero.",00:12:29,3,Basically say whether one zero
1013,00:12:36,2,Text Retrieval Problem,1.3,And this is called absolute relevance.,00:12:33,3,And called absolute relevance
1014,00:12:39,2,Text Retrieval Problem,1.3,"Basically, it needs to know exactly whether it's going to be useful",00:12:36,3,Basically needs know exactly whether going useful
1015,00:12:39,2,Text Retrieval Problem,1.3,to the user.,00:12:39,3,user
1016,00:12:44,2,Text Retrieval Problem,1.3,"Alternatively, there's another strategy called document ranking.",00:12:41,3,Alternatively another strategy called document ranking
1017,00:12:47,2,Text Retrieval Problem,1.3,"Now in this case,",00:12:46,3,Now case
1018,00:12:52,2,Text Retrieval Problem,1.3,the system is not going to make a call whether a document is relevant or not.,00:12:47,3,system going make call whether document relevant
1019,00:12:56,2,Text Retrieval Problem,1.3,"Rather, the system's going to use a real value function, f,",00:12:52,3,Rather system going use real value function f
1020,00:13:01,2,Text Retrieval Problem,1.3,here that would simply give us a value.,00:12:56,3,would simply give us value
1021,00:13:04,2,Text Retrieval Problem,1.3,That would indicate which document is more likely relevant.,00:13:01,3,That would indicate document likely relevant
1022,00:13:08,2,Text Retrieval Problem,1.3,"So it's not going to make a call whether this document is relevant or not,",00:13:05,3,So going make call whether document relevant
1023,00:13:12,2,Text Retrieval Problem,1.3,but rather it would say which document is more likely relevant.,00:13:08,3,rather would say document likely relevant
1024,00:13:18,2,Text Retrieval Problem,1.3,So this function then can be used to rank the documents.,00:13:12,3,So function used rank documents
1025,00:13:23,2,Text Retrieval Problem,1.3,And then we're going to let the user decide where to stop when the user looks,00:13:18,3,And going let user decide stop user looks
1026,00:13:24,2,Text Retrieval Problem,1.3,at the documents.,00:13:23,3,documents
1027,00:13:31,2,Text Retrieval Problem,1.3,"So we have a threshold, theta, here to determine",00:13:25,3,So threshold theta determine
1028,00:13:35,2,Text Retrieval Problem,1.3,what documents should be in this approximation set.,00:13:31,3,documents approximation set
1029,00:13:41,2,Text Retrieval Problem,1.3,And we're going to assume that all the documents that are ranked above this,00:13:37,3,And going assume documents ranked
1030,00:13:43,2,Text Retrieval Problem,1.3,threshold are in the set.,00:13:41,3,threshold set
1031,00:13:49,2,Text Retrieval Problem,1.3,"Because in effect, these are the documents that we delivered to the user.",00:13:45,3,Because effect documents delivered user
1032,00:13:54,2,Text Retrieval Problem,1.3,And theta is a cutoff determined by the user.,00:13:49,3,And theta cutoff determined user
1033,00:13:59,2,Text Retrieval Problem,1.3,So here we've got some collaboration from,00:13:54,3,So got collaboration
1034,00:14:03,2,Text Retrieval Problem,1.3,"the user in some sense because we don't really make a cutoff, and",00:13:59,3,user sense really make cutoff
1035,00:14:08,2,Text Retrieval Problem,1.3,the user kind of helped the system make a cutoff.,00:14:03,3,user kind helped system make cutoff
1036,00:14:12,2,Text Retrieval Problem,1.3,"So in this case, the system only needs to decide if one document is more likely",00:14:08,3,So case system needs decide one document likely
1037,00:14:14,2,Text Retrieval Problem,1.3,relevant than another.,00:14:12,3,relevant another
1038,00:14:16,2,Text Retrieval Problem,1.3,"And that is, it only needs for",00:14:14,3,And needs
1039,00:14:20,2,Text Retrieval Problem,1.3,determined relative relevance as opposed to absolute relevance.,00:14:16,3,determined relative relevance opposed absolute relevance
1040,00:14:26,2,Text Retrieval Problem,1.3,"Now you can probably already sense that relevant,",00:14:22,3,Now probably already sense relevant
1041,00:14:31,2,Text Retrieval Problem,1.3,relative relevance would be easier to determine the absolute relevance.,00:14:26,3,relative relevance would easier determine absolute relevance
1042,00:14:32,2,Text Retrieval Problem,1.3,"Because in the first case,",00:14:31,3,Because first case
1043,00:14:36,2,Text Retrieval Problem,1.3,"we have to say exactly whether a document is relevant or not, right?",00:14:32,3,say exactly whether document relevant right
1044,00:14:45,2,Text Retrieval Problem,1.3,And it turns out that ranking is indeed generally preferred to document selection.,00:14:37,3,And turns ranking indeed generally preferred document selection
1045,00:14:50,2,Text Retrieval Problem,1.3,So let's look this these two strategies in more detail.,00:14:46,3,So let look two strategies detail
1046,00:14:53,2,Text Retrieval Problem,1.3,So this pictures shows how it works.,00:14:50,3,So pictures shows works
1047,00:14:58,2,Text Retrieval Problem,1.3,"So on the left side, we see these documents.",00:14:53,3,So left side see documents
1048,00:15:02,2,Text Retrieval Problem,1.3,And we use the pluses to indicate the relevant documents.,00:14:58,3,And use pluses indicate relevant documents
1049,00:15:08,2,Text Retrieval Problem,1.3,So we can see the true relevant documents here consists,00:15:02,3,So see true relevant documents consists
1050,00:15:16,2,Text Retrieval Problem,1.3,"this set of true relevant documents consists of these pluses, these documents.",00:15:08,3,set true relevant documents consists pluses documents
1051,00:15:20,2,Text Retrieval Problem,1.3,"And with the document selection function,",00:15:16,3,And document selection function
1052,00:15:25,2,Text Retrieval Problem,1.3,"we can do, basically classify them into two groups,",00:15:20,3,basically classify two groups
1053,00:15:30,2,Text Retrieval Problem,1.3,relevant documents and non-relevant ones.,00:15:25,3,relevant documents non relevant ones
1054,00:15:34,2,Text Retrieval Problem,1.3,"Of course, the classifier will not be perfect, so it will make mistakes.",00:15:30,3,Of course classifier perfect make mistakes
1055,00:15:38,2,Text Retrieval Problem,1.3,So here we can see in the approximation of,00:15:34,3,So see approximation
1056,00:15:43,2,Text Retrieval Problem,1.3,the relevant documents we have got some non-relevant documents.,00:15:38,3,relevant documents got non relevant documents
1057,00:15:48,2,Text Retrieval Problem,1.3,"And similarly, there's a relevant document that that's misclassified as non-relevant.",00:15:43,3,And similarly relevant document misclassified non relevant
1058,00:15:54,2,Text Retrieval Problem,1.3,"In the case of document ranking, we can see the system seems like",00:15:48,3,In case document ranking see system seems like
1059,00:15:59,2,Text Retrieval Problem,1.3,simply ranks all the documents in the descending order of the scores.,00:15:54,3,simply ranks documents descending order scores
1060,00:16:04,2,Text Retrieval Problem,1.3,And we're going to let the user stop wherever the user wants to stop.,00:15:59,3,And going let user stop wherever user wants stop
1061,00:16:08,2,Text Retrieval Problem,1.3,"So if a user wants to examine more documents, then the user will",00:16:04,3,So user wants examine documents user
1062,00:16:13,2,Text Retrieval Problem,1.3,go down the list to examine more and stop at the lower position.,00:16:08,3,go list examine stop lower position
1063,00:16:17,2,Text Retrieval Problem,1.3,"But if the user only wants to read a few random documents,",00:16:13,3,But user wants read random documents
1064,00:16:20,2,Text Retrieval Problem,1.3,the user might stop at the top position.,00:16:17,3,user might stop top position
1065,00:16:26,2,Text Retrieval Problem,1.3,"So in this case, the user stops at d4, so the effect,",00:16:20,3,So case user stops d4 effect
1066,00:16:30,2,Text Retrieval Problem,1.3,we have delivered these four documents to our user.,00:16:26,3,delivered four documents user
1067,00:16:37,2,Text Retrieval Problem,1.3,"So as I said, ranking is generally preferred.",00:16:33,3,So I said ranking generally preferred
1068,00:16:42,2,Text Retrieval Problem,1.3,"And one of the reasons is because the classifier,",00:16:37,3,And one reasons classifier
1069,00:16:46,2,Text Retrieval Problem,1.3,"in the case of document selection, is unlikely accurate.",00:16:42,3,case document selection unlikely accurate
1070,00:16:49,2,Text Retrieval Problem,1.3,Why? Because the only clue is usually,00:16:46,3,Why Because clue usually
1071,00:16:51,2,Text Retrieval Problem,1.3,the query.,00:16:49,3,query
1072,00:16:56,2,Text Retrieval Problem,1.3,"But the query may not be accurate, in the sense that it could be overly constrained.",00:16:51,3,But query may accurate sense could overly constrained
1073,00:17:04,2,Text Retrieval Problem,1.3,"For example, you might expect the relevant documents to talk about all these",00:16:57,3,For example might expect relevant documents talk
1074,00:17:09,2,Text Retrieval Problem,1.3,"topics you, by using specific vocabulary, and as a result,",00:17:04,3,topics using specific vocabulary result
1075,00:17:14,2,Text Retrieval Problem,1.3,"you might match no random documents, because in the collection,",00:17:09,3,might match random documents collection
1076,00:17:19,2,Text Retrieval Problem,1.3,no others have discussed the topic using these vocabularies.,00:17:14,3,others discussed topic using vocabularies
1077,00:17:20,2,Text Retrieval Problem,1.3,All right.,00:17:19,3,All right
1078,00:17:25,2,Text Retrieval Problem,1.3,"So in this case, we'll see there is this problem of",00:17:20,3,So case see problem
1079,00:17:31,2,Text Retrieval Problem,1.3,no relevant documents to return in the case of overly constrained query.,00:17:25,3,relevant documents return case overly constrained query
1080,00:17:37,2,Text Retrieval Problem,1.3,"On the other hand, if the query is under constrained, for example,",00:17:33,3,On hand query constrained example
1081,00:17:44,2,Text Retrieval Problem,1.3,if the query does not have sufficient discriminating words you'll,00:17:39,3,query sufficient discriminating words
1082,00:17:46,2,Text Retrieval Problem,1.3,"find in relevant documents, you may actually end up having.",00:17:44,3,find relevant documents may actually end
1083,00:17:47,2,Text Retrieval Problem,1.3,over delivery.,00:17:46,3,delivery
1084,00:17:53,2,Text Retrieval Problem,1.3,And this is when you thought these words might be sufficient to,00:17:49,3,And thought words might sufficient
1085,00:17:58,2,Text Retrieval Problem,1.3,"help you find the relevant documents, but it turns out that they're not sufficient.",00:17:53,3,help find relevant documents turns sufficient
1086,00:18:04,2,Text Retrieval Problem,1.3,And there are many distraction documents using similar words.,00:17:58,3,And many distraction documents using similar words
1087,00:18:07,2,Text Retrieval Problem,1.3,And so this is the case of over delivery.,00:18:04,3,And case delivery
1088,00:18:13,2,Text Retrieval Problem,1.3,"Unfortunately, it's very hard to find the right position between these two extremes.",00:18:08,3,Unfortunately hard find right position two extremes
1089,00:18:15,2,Text Retrieval Problem,1.3,Why?,00:18:15,3,Why
1090,00:18:19,2,Text Retrieval Problem,1.3,"Because, when the users looking for the information in general,",00:18:15,3,Because users looking information general
1091,00:18:24,2,Text Retrieval Problem,1.3,the user does not have a good knowledge about the the information to be found.,00:18:19,3,user good knowledge information found
1092,00:18:28,2,Text Retrieval Problem,1.3,"And in that case, the user does not have a good knowledge about",00:18:24,3,And case user good knowledge
1093,00:18:33,2,Text Retrieval Problem,1.3,what vocabularies will be used in those random documents.,00:18:28,3,vocabularies used random documents
1094,00:18:38,2,Text Retrieval Problem,1.3,So it's very hard for a user to pre-specify,00:18:33,3,So hard user pre specify
1095,00:18:44,2,Text Retrieval Problem,1.3,the right level of of constraints.,00:18:38,3,right level constraints
1096,00:18:49,2,Text Retrieval Problem,1.3,"Even if the class file is accurate, we also still want to rank these",00:18:44,3,Even class file accurate also still want rank
1097,00:18:54,2,Text Retrieval Problem,1.3,relevant documents because they are generally not equally relevant.,00:18:49,3,relevant documents generally equally relevant
1098,00:18:58,2,Text Retrieval Problem,1.3,Relevance is often a matter of degree.,00:18:56,3,Relevance often matter degree
1099,00:19:05,2,Text Retrieval Problem,1.3,So we must prioritize these documents for user to exam.,00:18:59,3,So must prioritize documents user exam
1100,00:19:10,2,Text Retrieval Problem,1.3,"And this, note that this prioritization is very important,",00:19:06,3,And note prioritization important
1101,00:19:16,2,Text Retrieval Problem,1.3,because a user cannot digest all the contents at once.,00:19:12,3,user cannot digest contents
1102,00:19:20,2,Text Retrieval Problem,1.3,The user generally would have to look at each document sequentially.,00:19:16,3,The user generally would look document sequentially
1103,00:19:26,2,Text Retrieval Problem,1.3,"And therefore, it would make sense to feed users with the most",00:19:21,3,And therefore would make sense feed users
1104,00:19:32,2,Text Retrieval Problem,1.3,"relevant documents, and that's what ranking is doing.",00:19:26,3,relevant documents ranking
1105,00:19:35,2,Text Retrieval Problem,1.3,So for these reasons ranking is generally preferred.,00:19:32,3,So reasons ranking generally preferred
1106,00:19:39,2,Text Retrieval Problem,1.3,"Now, this preference also has a theoretical justification, and",00:19:36,3,Now preference also theoretical justification
1107,00:19:42,2,Text Retrieval Problem,1.3,this is given by the probability ranking principle.,00:19:39,3,given probability ranking principle
1108,00:19:47,2,Text Retrieval Problem,1.3,In the end of this lecture there is a reference for this.,00:19:44,3,In end lecture reference
1109,00:19:54,2,Text Retrieval Problem,1.3,"This principal says, returning a ranked list of documents in descending order of",00:19:49,3,This principal says returning ranked list documents descending order
1110,00:19:57,2,Text Retrieval Problem,1.3,"probability, that a document is relevant to the query,",00:19:54,3,probability document relevant query
1111,00:20:01,2,Text Retrieval Problem,1.3,is the optimal strategy under the following two assumptions.,00:19:57,3,optimal strategy following two assumptions
1112,00:20:05,2,Text Retrieval Problem,1.3,"First, the utility of a document to a user",00:20:02,3,First utility document user
1113,00:20:09,2,Text Retrieval Problem,1.3,Is independent of the utility of any other document.,00:20:05,3,Is independent utility document
1114,00:20:15,2,Text Retrieval Problem,1.3,"Second, a user would be assumed to browse the results sequentially.",00:20:10,3,Second user would assumed browse results sequentially
1115,00:20:20,2,Text Retrieval Problem,1.3,"Now it's easy to understand why these two assumptions are needed,",00:20:17,3,Now easy understand two assumptions needed
1116,00:20:25,2,Text Retrieval Problem,1.3,"in order to justify for the ranking, strategy.",00:20:20,3,order justify ranking strategy
1117,00:20:30,2,Text Retrieval Problem,1.3,"Because, if the documents are independent,",00:20:27,3,Because documents independent
1118,00:20:34,2,Text Retrieval Problem,1.3,then we can evaluate the utility of each document that's separate.,00:20:30,3,evaluate utility document separate
1119,00:20:40,2,Text Retrieval Problem,1.3,And this would allow us to compute a score for each document independently.,00:20:36,3,And would allow us compute score document independently
1120,00:20:43,2,Text Retrieval Problem,1.3,And then we're going to rank these documents based on those scores.,00:20:40,3,And going rank documents based scores
1121,00:20:51,2,Text Retrieval Problem,1.3,The second assumption is to say that the user would indeed follow the rank list.,00:20:45,3,The second assumption say user would indeed follow rank list
1122,00:20:55,2,Text Retrieval Problem,1.3,"If the user is not going to follow the ranked list, is not going to examine",00:20:51,3,If user going follow ranked list going examine
1123,00:20:59,2,Text Retrieval Problem,1.3,"the documents sequentially, then obviously the ordering would not be optimal.",00:20:55,3,documents sequentially obviously ordering would optimal
1124,00:21:08,2,Text Retrieval Problem,1.3,"So under these two assumptions, we can theoretically justify the ranking strategy",00:21:00,3,So two assumptions theoretically justify ranking strategy
1125,00:21:12,2,Text Retrieval Problem,1.3,is in fact the best that you could do.,00:21:08,3,fact best could
1126,00:21:14,2,Text Retrieval Problem,1.3,Now I've put one question here.,00:21:12,3,Now I put one question
1127,00:21:16,2,Text Retrieval Problem,1.3,Do these 2 assumptions hold?,00:21:14,3,Do 2 assumptions hold
1128,00:21:22,2,Text Retrieval Problem,1.3,Now I suggest you to pause the lecture for a moment to think about these.,00:21:18,3,Now I suggest pause lecture moment think
1129,00:21:33,2,Text Retrieval Problem,1.3,Now can you think of some examples that would suggest,00:21:27,3,Now think examples would suggest
1130,00:21:38,2,Text Retrieval Problem,1.3,these assumptions aren't necessarily true?,00:21:33,3,assumptions necessarily true
1131,00:21:46,2,Text Retrieval Problem,1.3,Now if you think for,00:21:44,3,Now think
1132,00:21:51,2,Text Retrieval Problem,1.3,a moment you may realize none of the assumptions is actually true.,00:21:46,3,moment may realize none assumptions actually true
1133,00:21:57,2,Text Retrieval Problem,1.3,"For example in the case of independence assumption, we might have",00:21:53,3,For example case independence assumption might
1134,00:22:02,2,Text Retrieval Problem,1.3,identical documents that have similar content or exactly the same content.,00:21:57,3,identical documents similar content exactly content
1135,00:22:07,2,Text Retrieval Problem,1.3,"If you look at each of them alone, each is relevant.",00:22:02,3,If look alone relevant
1136,00:22:12,2,Text Retrieval Problem,1.3,"But if the user has already seen one of them, we assume it's",00:22:07,3,But user already seen one assume
1137,00:22:17,2,Text Retrieval Problem,1.3,generally not very useful for the user to see another similar or duplicate one.,00:22:12,3,generally useful user see another similar duplicate one
1138,00:22:23,2,Text Retrieval Problem,1.3,So clearly the utility of a document is dependent,00:22:19,3,So clearly utility document dependent
1139,00:22:25,2,Text Retrieval Problem,1.3,on other documents that the user has seen.,00:22:23,3,documents user seen
1140,00:22:32,2,Text Retrieval Problem,1.3,"In some other cases, you might see a scenario where one document that may not",00:22:27,3,In cases might see scenario one document may
1141,00:22:38,2,Text Retrieval Problem,1.3,"be useful to the user, but when three particular documents are put together,",00:22:32,3,useful user three particular documents put together
1142,00:22:40,2,Text Retrieval Problem,1.3,they provide answer to the user's question.,00:22:38,3,provide answer user question
1143,00:22:44,2,Text Retrieval Problem,1.3,So this is collective relevance.,00:22:42,3,So collective relevance
1144,00:22:48,2,Text Retrieval Problem,1.3,And that also suggests that the value of the document,00:22:44,3,And also suggests value document
1145,00:22:51,2,Text Retrieval Problem,1.3,might depend on other documents.,00:22:48,3,might depend documents
1146,00:22:58,2,Text Retrieval Problem,1.3,Sequential browsing generally would make sense if you have a ranked list there.,00:22:51,3,Sequential browsing generally would make sense ranked list
1147,00:23:04,2,Text Retrieval Problem,1.3,"But even if you have a run list, there is evidence showing that",00:22:59,3,But even run list evidence showing
1148,00:23:10,2,Text Retrieval Problem,1.3,users don't always just go strictly sequentially through the entire list.,00:23:04,3,users always go strictly sequentially entire list
1149,00:23:14,2,Text Retrieval Problem,1.3,"They sometimes would look at the bottom for example, or skip some.",00:23:10,3,They sometimes would look bottom example skip
1150,00:23:19,2,Text Retrieval Problem,1.3,And if you think about the more complicated interfaces that would possibly,00:23:14,3,And think complicated interfaces would possibly
1151,00:23:24,2,Text Retrieval Problem,1.3,"use like, two dimensional interface where you can put additional information",00:23:19,3,use like two dimensional interface put additional information
1152,00:23:29,2,Text Retrieval Problem,1.3,"on the screen, then sequential browsing is a very restrictive assumption.",00:23:24,3,screen sequential browsing restrictive assumption
1153,00:23:34,2,Text Retrieval Problem,1.3,"So the point here is that,",00:23:31,3,So point
1154,00:23:39,2,Text Retrieval Problem,1.3,"none of these assumptions is really true, but nevertheless,",00:23:34,3,none assumptions really true nevertheless
1155,00:23:46,2,Text Retrieval Problem,1.3,the probability ranking principle establishes some solid foundation for,00:23:39,3,probability ranking principle establishes solid foundation
1156,00:23:51,2,Text Retrieval Problem,1.3,ranking as a primary task for search engines.,00:23:46,3,ranking primary task search engines
1157,00:23:53,2,Text Retrieval Problem,1.3,And this has actually been the basis for,00:23:51,3,And actually basis
1158,00:23:56,2,Text Retrieval Problem,1.3,a lot of research work in information retrieval.,00:23:53,3,lot research work information retrieval
1159,00:24:00,2,Text Retrieval Problem,1.3,And many algorithms have been designed based on this assumption.,00:23:56,3,And many algorithms designed based assumption
1160,00:24:06,2,Text Retrieval Problem,1.3,Despite that the assumptions aren't necessarily true.,00:24:01,3,Despite assumptions necessarily true
1161,00:24:12,2,Text Retrieval Problem,1.3,"And we can, address this problem by doing post processing of a ranked list.",00:24:06,3,And address problem post processing ranked list
1162,00:24:15,2,Text Retrieval Problem,1.3,"For example, to remove redundancy.",00:24:12,3,For example remove redundancy
1163,00:24:22,2,Text Retrieval Problem,1.3,"So to summarize this lecture,",00:24:20,3,So summarize lecture
1164,00:24:28,2,Text Retrieval Problem,1.3,the main points that you can take away are the following.,00:24:22,3,main points take away following
1165,00:24:30,2,Text Retrieval Problem,1.3,"First, text retrieval is an empirically defined problem.",00:24:28,3,First text retrieval empirically defined problem
1166,00:24:37,2,Text Retrieval Problem,1.3,And that means which algorithm is better must be judged by the users.,00:24:31,3,And means algorithm better must judged users
1167,00:24:41,2,Text Retrieval Problem,1.3,"Second, document ranking is generally prefer and",00:24:38,3,Second document ranking generally prefer
1168,00:24:46,2,Text Retrieval Problem,1.3,"this is, will help users prioritize examination of search results.",00:24:41,3,help users prioritize examination search results
1169,00:24:51,2,Text Retrieval Problem,1.3,"And this is also to bypass the difficulty in determining absolute relevance,",00:24:47,3,And also bypass difficulty determining absolute relevance
1170,00:24:58,2,Text Retrieval Problem,1.3,because we can get some help from users in determining where to make the cut off.,00:24:52,3,get help users determining make cut
1171,00:24:59,2,Text Retrieval Problem,1.3,It's more flexible.,00:24:58,3,It flexible
1172,00:25:05,2,Text Retrieval Problem,1.3,So this further suggests that the main technical challenge,00:25:02,3,So suggests main technical challenge
1173,00:25:10,2,Text Retrieval Problem,1.3,in designing the search engine is with designing effective ranking function.,00:25:05,3,designing search engine designing effective ranking function
1174,00:25:16,2,Text Retrieval Problem,1.3,"In other words, we need to define what is the value of this function f",00:25:10,3,In words need define value function f
1175,00:25:19,2,Text Retrieval Problem,1.3,on the query and document pair.,00:25:16,3,query document pair
1176,00:25:25,2,Text Retrieval Problem,1.3,Now how to design such a function is a main topic in the following lectures.,00:25:21,3,Now design function main topic following lectures
1177,00:25:31,2,Text Retrieval Problem,1.3,There are two suggested additional readings.,00:25:29,3,There two suggested additional readings
1178,00:25:36,2,Text Retrieval Problem,1.3,The first is the classic paper on probability ranking principle.,00:25:31,3,The first classic paper probability ranking principle
1179,00:25:42,2,Text Retrieval Problem,1.3,"The second, is a must read for anyone doing research information retrieval.",00:25:36,3,The second must read anyone research information retrieval
1180,00:25:48,2,Text Retrieval Problem,1.3,"It's classical IR book, which has excellent coverage of",00:25:42,3,It classical IR book excellent coverage
1181,00:25:55,2,Text Retrieval Problem,1.3,"the main research results in early days, up to the time when the book was written.",00:25:48,3,main research results early days time book written
1182,00:25:59,2,Text Retrieval Problem,1.3,Chapter six of this book has an in depth discussion of,00:25:55,3,Chapter six book depth discussion
1183,00:26:02,2,Text Retrieval Problem,1.3,"the probability of the ranking principal, and",00:25:59,3,probability ranking principal
1184,00:26:06,2,Text Retrieval Problem,1.3,"the probabilistic retrieval models, in general.",00:26:02,3,probabilistic retrieval models general
1185,00:00:09,5,Web Search- Introduction & Web Crawler,4.1,This lecture is about the web search.,00:00:07,1,This lecture web search
1186,00:00:14,5,Web Search- Introduction & Web Crawler,4.1,In this lecture we are going to talk about one of,00:00:11,1,In lecture going talk one
1187,00:00:19,5,Web Search- Introduction & Web Crawler,4.1,"the most important applications of text retrieval, web search engines.",00:00:14,1,important applications text retrieval web search engines
1188,00:00:21,5,Web Search- Introduction & Web Crawler,4.1,So let's first look at some general challenges and,00:00:19,1,So let first look general challenges
1189,00:00:23,5,Web Search- Introduction & Web Crawler,4.1,opportunities in web search.,00:00:21,1,opportunities web search
1190,00:00:27,5,Web Search- Introduction & Web Crawler,4.1,"Now, many information retrieval algorithms had been developed at the,",00:00:23,1,Now many information retrieval algorithms developed
1191,00:00:29,5,Web Search- Introduction & Web Crawler,4.1,before the web was born.,00:00:27,1,web born
1192,00:00:33,5,Web Search- Introduction & Web Crawler,4.1,"So, when the web was born, it created the best opportunity to apply",00:00:29,1,So web born created best opportunity apply
1193,00:00:39,5,Web Search- Introduction & Web Crawler,4.1,those algorithms to major application problem that everyone would care about.,00:00:33,1,algorithms major application problem everyone would care
1194,00:00:45,5,Web Search- Introduction & Web Crawler,4.1,"So naturally, there had to be some further extensions of the classical",00:00:39,1,So naturally extensions classical
1195,00:00:52,5,Web Search- Introduction & Web Crawler,4.1,search algorithms to address some new challenges encountered in web search.,00:00:45,1,search algorithms address new challenges encountered web search
1196,00:00:56,5,Web Search- Introduction & Web Crawler,4.1,So here are some general challenges.,00:00:53,1,So general challenges
1197,00:00:58,5,Web Search- Introduction & Web Crawler,4.1,"Firstly, this is a scalability challenge.",00:00:56,1,Firstly scalability challenge
1198,00:01:00,5,Web Search- Introduction & Web Crawler,4.1,"How we handle the size of the web,",00:00:58,1,How handle size web
1199,00:01:02,5,Web Search- Introduction & Web Crawler,4.1,and ensure completeness of coverage of all the information.,00:01:00,1,ensure completeness coverage information
1200,00:01:07,5,Web Search- Introduction & Web Crawler,4.1,"How to serve many users quickly, and by answering all their queries.",00:01:03,1,How serve many users quickly answering queries
1201,00:01:10,5,Web Search- Introduction & Web Crawler,4.1,"All right, so, that's one major challenge.",00:01:07,1,All right one major challenge
1202,00:01:15,5,Web Search- Introduction & Web Crawler,4.1,"And before the web was born, the scale of search was relatively small.",00:01:10,1,And web born scale search relatively small
1203,00:01:20,5,Web Search- Introduction & Web Crawler,4.1,The second problem is that there is low quality information.,00:01:15,1,The second problem low quality information
1204,00:01:22,5,Web Search- Introduction & Web Crawler,4.1,And there are often spams.,00:01:20,1,And often spams
1205,00:01:24,5,Web Search- Introduction & Web Crawler,4.1,The third challenge is dynamics of the web.,00:01:22,1,The third challenge dynamics web
1206,00:01:27,5,Web Search- Introduction & Web Crawler,4.1,The new pages are constantly created and,00:01:24,1,The new pages constantly created
1207,00:01:32,5,Web Search- Introduction & Web Crawler,4.1,"some pages may be updated, eve-, very quickly.",00:01:27,1,pages may updated eve quickly
1208,00:01:36,5,Web Search- Introduction & Web Crawler,4.1,"So it makes it harder to, keep the index fresh.",00:01:32,1,So makes harder keep index fresh
1209,00:01:38,5,Web Search- Introduction & Web Crawler,4.1,"So these are some of the challenges that the,",00:01:36,1,So challenges
1210,00:01:42,5,Web Search- Introduction & Web Crawler,4.1,"we have to solve in order to, build a high quality web search engine.",00:01:38,1,solve order build high quality web search engine
1211,00:01:46,5,Web Search- Introduction & Web Crawler,4.1,"On the other hand, there are also some interesting opportunities that we can",00:01:44,1,On hand also interesting opportunities
1212,00:01:49,5,Web Search- Introduction & Web Crawler,4.1,leverage to improve search results.,00:01:46,1,leverage improve search results
1213,00:01:52,5,Web Search- Introduction & Web Crawler,4.1,There are many additional heuristics.,00:01:49,1,There many additional heuristics
1214,00:02:00,5,Web Search- Introduction & Web Crawler,4.1,For example you know using links that we can leverage to improve scoring.,00:01:52,1,For example know using links leverage improve scoring
1215,00:02:03,5,Web Search- Introduction & Web Crawler,4.1,Now the errors that we talked about such as the vector space model are general,00:02:00,1,Now errors talked vector space model general
1216,00:02:04,5,Web Search- Introduction & Web Crawler,4.1,algorithms.,00:02:03,1,algorithms
1217,00:02:11,5,Web Search- Introduction & Web Crawler,4.1,"And they can be applied to any search applications, so that's, the advantage.",00:02:05,1,And applied search applications advantage
1218,00:02:15,5,Web Search- Introduction & Web Crawler,4.1,"On the other hand, they also don't take advantage of special characteristics",00:02:11,1,On hand also take advantage special characteristics
1219,00:02:21,5,Web Search- Introduction & Web Crawler,4.1,"of pages, or documents, in the specific applications such as web search.",00:02:15,1,pages documents specific applications web search
1220,00:02:23,5,Web Search- Introduction & Web Crawler,4.1,Web pages are linked with each other so,00:02:21,1,Web pages linked
1221,00:02:28,5,Web Search- Introduction & Web Crawler,4.1,obviously the linking is something that we can also leverage.,00:02:23,1,obviously linking something also leverage
1222,00:02:33,5,Web Search- Introduction & Web Crawler,4.1,So because of these challenges and opportunities there are new techniques,00:02:28,1,So challenges opportunities new techniques
1223,00:02:39,5,Web Search- Introduction & Web Crawler,4.1,"that have been developed for web search, or due to the need of a web search.",00:02:33,1,developed web search due need web search
1224,00:02:43,5,Web Search- Introduction & Web Crawler,4.1,"One is parallel indexing and searching, and this is to address the issue of",00:02:39,1,One parallel indexing searching address issue
1225,00:02:48,5,Web Search- Introduction & Web Crawler,4.1,"scalability, in particular Google's imaging of MapReduce",00:02:43,1,scalability particular Google imaging MapReduce
1226,00:02:53,5,Web Search- Introduction & Web Crawler,4.1,"is very inferential, and has been very helpful in that aspect.",00:02:48,1,inferential helpful aspect
1227,00:02:56,5,Web Search- Introduction & Web Crawler,4.1,"Second, there are techniques that are developed for,",00:02:53,1,Second techniques developed
1228,00:02:58,5,Web Search- Introduction & Web Crawler,4.1,addressing the problem of spams.,00:02:56,1,addressing problem spams
1229,00:03:00,5,Web Search- Introduction & Web Crawler,4.1,"So, spam detection.",00:02:58,1,So spam detection
1230,00:03:03,5,Web Search- Introduction & Web Crawler,4.1,"We'll have to prevent those, spam pages from being ranked high.",00:03:00,1,We prevent spam pages ranked high
1231,00:03:07,5,Web Search- Introduction & Web Crawler,4.1,And there are also techniques to achieve robust ranking.,00:03:04,1,And also techniques achieve robust ranking
1232,00:03:10,5,Web Search- Introduction & Web Crawler,4.1,And we're going to use a lot of signals to rank pages so,00:03:07,1,And going use lot signals rank pages
1233,00:03:15,5,Web Search- Introduction & Web Crawler,4.1,that it's not easy to spam the search engine with particular tricks.,00:03:10,1,easy spam search engine particular tricks
1234,00:03:20,5,Web Search- Introduction & Web Crawler,4.1,And the third line of techniques is link analysis.,00:03:15,1,And third line techniques link analysis
1235,00:03:25,5,Web Search- Introduction & Web Crawler,4.1,And these are techniques that can allow us to,00:03:21,1,And techniques allow us
1236,00:03:30,5,Web Search- Introduction & Web Crawler,4.1,to improve search results by leveraging extra information.,00:03:26,1,improve search results leveraging extra information
1237,00:03:34,5,Web Search- Introduction & Web Crawler,4.1,And in general in web search we're going to use,00:03:30,1,And general web search going use
1238,00:03:35,5,Web Search- Introduction & Web Crawler,4.1,multiple features for ranking.,00:03:34,1,multiple features ranking
1239,00:03:41,5,Web Search- Introduction & Web Crawler,4.1,Not just link analysis but also exploiting all kinds of crawls like,00:03:35,1,Not link analysis also exploiting kinds crawls like
1240,00:03:47,5,Web Search- Introduction & Web Crawler,4.1,the layout of web pages or anchor text that describes a link to another page.,00:03:41,1,layout web pages anchor text describes link another page
1241,00:03:51,5,Web Search- Introduction & Web Crawler,4.1,So here's a picture showing the basic search engine technologies.,00:03:47,1,So picture showing basic search engine technologies
1242,00:03:55,5,Web Search- Introduction & Web Crawler,4.1,"Basically, this is the web on the left and then user on the right side.",00:03:51,1,Basically web left user right side
1243,00:04:00,5,Web Search- Introduction & Web Crawler,4.1,"And we're going to help these, this user get access to the web information.",00:03:55,1,And going help user get access web information
1244,00:04:04,5,Web Search- Introduction & Web Crawler,4.1,And the first component is the crawler that with the crawl pages and,00:04:00,1,And first component crawler crawl pages
1245,00:04:07,5,Web Search- Introduction & Web Crawler,4.1,the second component is indexer.,00:04:04,1,second component indexer
1246,00:04:09,5,Web Search- Introduction & Web Crawler,4.1,That will take these pages create an invert index.,00:04:07,1,That take pages create invert index
1247,00:04:14,5,Web Search- Introduction & Web Crawler,4.1,"The third component that is a retrieval, not with the using,",00:04:10,1,The third component retrieval using
1248,00:04:19,5,Web Search- Introduction & Web Crawler,4.1,"but the index to answer user's query, by talking to the user's browser.",00:04:14,1,index answer user query talking user browser
1249,00:04:23,5,Web Search- Introduction & Web Crawler,4.1,"And then, the search results would be, given to the user.",00:04:19,1,And search results would given user
1250,00:04:26,5,Web Search- Introduction & Web Crawler,4.1,"And, and then the browser will show those results and,",00:04:23,1,And browser show results
1251,00:04:29,5,Web Search- Introduction & Web Crawler,4.1,to allow the user to interact with the web.,00:04:26,1,allow user interact web
1252,00:04:32,5,Web Search- Introduction & Web Crawler,4.1,So we're going to talk about each of these component.,00:04:29,1,So going talk component
1253,00:04:37,5,Web Search- Introduction & Web Crawler,4.1,First we're going to talk about the crawler also called a spider or,00:04:32,1,First going talk crawler also called spider
1254,00:04:43,5,Web Search- Introduction & Web Crawler,4.1,a software robot that would do something like a crawling pages on the web.,00:04:37,1,software robot would something like crawling pages web
1255,00:04:46,5,Web Search- Introduction & Web Crawler,4.1,To build a toy crawler is relatively easy because you just need to start with a set,00:04:43,1,To build toy crawler relatively easy need start set
1256,00:04:53,5,Web Search- Introduction & Web Crawler,4.1,of seed pages and then fetch pages from the web and parse these pages new links.,00:04:46,1,seed pages fetch pages web parse pages new links
1257,00:04:56,5,Web Search- Introduction & Web Crawler,4.1,And then add them to the priority of q and,00:04:53,1,And add priority q
1258,00:05:01,5,Web Search- Introduction & Web Crawler,4.1,"then just explore those additional links, right.",00:04:56,1,explore additional links right
1259,00:05:05,5,Web Search- Introduction & Web Crawler,4.1,But to build a real crawler actually is tricky and,00:05:01,1,But build real crawler actually tricky
1260,00:05:09,5,Web Search- Introduction & Web Crawler,4.1,there are some complicated issues that we have do deal with.,00:05:05,1,complicated issues deal
1261,00:05:13,5,Web Search- Introduction & Web Crawler,4.1,"For example robustness, what if the server doesn't respond.",00:05:09,1,For example robustness server respond
1262,00:05:19,5,Web Search- Introduction & Web Crawler,4.1,"What if there's a trap that generates dynamically generated webpages that might,",00:05:13,1,What trap generates dynamically generated webpages might
1263,00:05:23,5,Web Search- Introduction & Web Crawler,4.1,attract your crawler to keep crawling the same site and,00:05:19,1,attract crawler keep crawling site
1264,00:05:26,5,Web Search- Introduction & Web Crawler,4.1,to fetch dynamically generated pages.,00:05:23,1,fetch dynamically generated pages
1265,00:05:28,5,Web Search- Introduction & Web Crawler,4.1,The results of this issue of crawling and,00:05:26,1,The results issue crawling
1266,00:05:32,5,Web Search- Introduction & Web Crawler,4.1,you don't want to overload one particular server with many crawling requests.,00:05:28,1,want overload one particular server many crawling requests
1267,00:05:39,5,Web Search- Introduction & Web Crawler,4.1,"And you have to respect the, the robot exclusion protocol.",00:05:34,1,And respect robot exclusion protocol
1268,00:05:42,5,Web Search- Introduction & Web Crawler,4.1,You also need to handle different types of files.,00:05:39,1,You also need handle different types files
1269,00:05:46,5,Web Search- Introduction & Web Crawler,4.1,"There are images, PDF files, all kinds of formats on the web.",00:05:42,1,There images PDF files kinds formats web
1270,00:05:51,5,Web Search- Introduction & Web Crawler,4.1,And you have to also consider URL extensions.,00:05:47,1,And also consider URL extensions
1271,00:05:57,5,Web Search- Introduction & Web Crawler,4.1,"So, sometimes those are cgi scripts, and, you know, internal references, etc., and",00:05:51,1,So sometimes cgi scripts know internal references etc
1272,00:06:03,5,Web Search- Introduction & Web Crawler,4.1,"sometimes, you have JavaScripts on the page that, they also create challenges.",00:05:57,1,sometimes JavaScripts page also create challenges
1273,00:06:07,5,Web Search- Introduction & Web Crawler,4.1,And you ideally should also recognize [INAUDIBLE] the pages,00:06:03,1,And ideally also recognize INAUDIBLE pages
1274,00:06:11,5,Web Search- Introduction & Web Crawler,4.1,"because you don't have to duplicate to the, those pages.",00:06:07,1,duplicate pages
1275,00:06:15,5,Web Search- Introduction & Web Crawler,4.1,"And finally, you may be interesting to discover hidden URLs.",00:06:11,1,And finally may interesting discover hidden URLs
1276,00:06:19,5,Web Search- Introduction & Web Crawler,4.1,"Those are URLs that may not be linked, to any page.",00:06:15,1,Those URLs may linked page
1277,00:06:22,5,Web Search- Introduction & Web Crawler,4.1,"But if you truncate the URL to, shorter pass,",00:06:19,1,But truncate URL shorter pass
1278,00:06:24,5,Web Search- Introduction & Web Crawler,4.1,you might be able to get some additional pages.,00:06:22,1,might able get additional pages
1279,00:06:29,5,Web Search- Introduction & Web Crawler,4.1,"So, what are the major crawling strategies?",00:06:27,1,So major crawling strategies
1280,00:06:32,5,Web Search- Introduction & Web Crawler,4.1,"In general, Breadth-First, is most common,",00:06:29,1,In general Breadth First common
1281,00:06:36,5,Web Search- Introduction & Web Crawler,4.1,"because it naturally balance, balances server load.",00:06:32,1,naturally balance balances server load
1282,00:06:42,5,Web Search- Introduction & Web Crawler,4.1,"You would not, keep probing a particular server [INAUDIBLE].",00:06:36,1,You would keep probing particular server INAUDIBLE
1283,00:06:46,5,Web Search- Introduction & Web Crawler,4.1,"Also parallel crawling is very natural, because this task is very easy",00:06:42,1,Also parallel crawling natural task easy
1284,00:06:51,5,Web Search- Introduction & Web Crawler,4.1,to parallelise and there are some variations of the crawling task.,00:06:46,1,parallelise variations crawling task
1285,00:06:54,5,Web Search- Introduction & Web Crawler,4.1,One interesting variation is called focused crawling.,00:06:51,1,One interesting variation called focused crawling
1286,00:06:59,5,Web Search- Introduction & Web Crawler,4.1,In this kind we're going to crawl just some pages about a particular topic.,00:06:54,1,In kind going crawl pages particular topic
1287,00:07:02,5,Web Search- Introduction & Web Crawler,4.1,"For example, all pages about automobiles.",00:06:59,1,For example pages automobiles
1288,00:07:08,5,Web Search- Introduction & Web Crawler,4.1,"And, and, this is typically going to start with a query,",00:07:04,1,And typically going start query
1289,00:07:11,5,Web Search- Introduction & Web Crawler,4.1,and then you can use the query to get some results.,00:07:08,1,use query get results
1290,00:07:12,5,Web Search- Introduction & Web Crawler,4.1,From the major search engine.,00:07:11,1,From major search engine
1291,00:07:18,5,Web Search- Introduction & Web Crawler,4.1,And then you can start it with those results and gradually crawl more.,00:07:12,1,And start results gradually crawl
1292,00:07:25,5,Web Search- Introduction & Web Crawler,4.1,"So one challenge in crawling is to find the new pages that people have created,",00:07:18,1,So one challenge crawling find new pages people created
1293,00:07:30,5,Web Search- Introduction & Web Crawler,4.1,"and people probably are creating new pages all the time, and this is",00:07:25,1,people probably creating new pages time
1294,00:07:35,5,Web Search- Introduction & Web Crawler,4.1,very challenging if the new pages have not been actually linked to any old page.,00:07:30,1,challenging new pages actually linked old page
1295,00:07:40,5,Web Search- Introduction & Web Crawler,4.1,"If they are, then you can probably refine them by recrawling the older page.",00:07:35,1,If probably refine recrawling older page
1296,00:07:45,5,Web Search- Introduction & Web Crawler,4.1,"So these are also some um,interesting challenges that have to be solved.",00:07:41,1,So also um interesting challenges solved
1297,00:07:52,5,Web Search- Introduction & Web Crawler,4.1,And finally we might face the scenario of incremental crawling or repeated crawling.,00:07:47,1,And finally might face scenario incremental crawling repeated crawling
1298,00:07:53,5,Web Search- Introduction & Web Crawler,4.1,"Right? So your first,",00:07:52,1,Right So first
1299,00:07:55,5,Web Search- Introduction & Web Crawler,4.1,let's say if you want to be able to web search engine.,00:07:53,1,let say want able web search engine
1300,00:07:58,5,Web Search- Introduction & Web Crawler,4.1,And you were the first to crawl a lot of data from the web.,00:07:55,1,And first crawl lot data web
1301,00:08:03,5,Web Search- Introduction & Web Crawler,4.1,"And then, but then once you have collected all the data and",00:07:58,1,And collected data
1302,00:08:08,5,Web Search- Introduction & Web Crawler,4.1,"in future we just need to crawl the, the update pages.",00:08:03,1,future need crawl update pages
1303,00:08:13,5,Web Search- Introduction & Web Crawler,4.1,"You, you, in general you don't have to re-crawl everything, right?",00:08:08,1,You general crawl everything right
1304,00:08:14,5,Web Search- Introduction & Web Crawler,4.1,Or it's not necessary.,00:08:13,1,Or necessary
1305,00:08:21,5,Web Search- Introduction & Web Crawler,4.1,"So, in this case you, you go as you minimize a resource overhead",00:08:16,1,So case go minimize resource overhead
1306,00:08:26,5,Web Search- Introduction & Web Crawler,4.1,"by using minimum resource to, to just still crawl updated pages.",00:08:21,1,using minimum resource still crawl updated pages
1307,00:08:31,5,Web Search- Introduction & Web Crawler,4.1,So this is after a very interesting research question here.,00:08:27,1,So interesting research question
1308,00:08:38,5,Web Search- Introduction & Web Crawler,4.1,And [INAUDIBLE] research question is that there aren't,00:08:31,1,And INAUDIBLE research question
1309,00:08:46,5,Web Search- Introduction & Web Crawler,4.1,"many standard algorithms [INAUDIBLE] for doing this, this task.",00:08:38,1,many standard algorithms INAUDIBLE task
1310,00:08:48,5,Web Search- Introduction & Web Crawler,4.1,"Right? But in general, you can imagine,",00:08:46,1,Right But general imagine
1311,00:08:51,5,Web Search- Introduction & Web Crawler,4.1,you can learn from the past experience.,00:08:48,1,learn past experience
1312,00:08:53,5,Web Search- Introduction & Web Crawler,4.1,Right.,00:08:52,1,Right
1313,00:08:58,5,Web Search- Introduction & Web Crawler,4.1,"So the two major factors that you have to consider are first,",00:08:53,1,So two major factors consider first
1314,00:09:00,5,Web Search- Introduction & Web Crawler,4.1,will this page be updated frequently?,00:08:58,1,page updated frequently
1315,00:09:03,5,Web Search- Introduction & Web Crawler,4.1,And do I have to crawl this page again?,00:09:00,1,And I crawl page
1316,00:09:07,5,Web Search- Introduction & Web Crawler,4.1,If the page is a static page that hasn't been changed for,00:09:03,1,If page static page changed
1317,00:09:11,5,Web Search- Introduction & Web Crawler,4.1,"months you probably don't have to re-crawl it everyday, right?",00:09:07,1,months probably crawl everyday right
1318,00:09:14,5,Web Search- Introduction & Web Crawler,4.1,Because it's unlikely that it will be changed frequently.,00:09:11,1,Because unlikely changed frequently
1319,00:09:16,5,Web Search- Introduction & Web Crawler,4.1,"On the other hand if it's you know,",00:09:14,1,On hand know
1320,00:09:19,5,Web Search- Introduction & Web Crawler,4.1,sports score page that gets updated very frequently and,00:09:16,1,sports score page gets updated frequently
1321,00:09:25,5,Web Search- Introduction & Web Crawler,4.1,"you may need to re-crawl it maybe even multiple times, on the same day.",00:09:19,1,may need crawl maybe even multiple times day
1322,00:09:31,5,Web Search- Introduction & Web Crawler,4.1,"The other factor to consider is, is this page frequently accessed by users?",00:09:25,1,The factor consider page frequently accessed users
1323,00:09:35,5,Web Search- Introduction & Web Crawler,4.1,"If it, if it is, that means it's a high utility page, and",00:09:31,1,If means high utility page
1324,00:09:39,5,Web Search- Introduction & Web Crawler,4.1,then thus it's more important to ensure such a page to be fresh.,00:09:35,1,thus important ensure page fresh
1325,00:09:45,5,Web Search- Introduction & Web Crawler,4.1,Compare it with another page that has never been fetched by any users for,00:09:40,1,Compare another page never fetched users
1326,00:09:46,5,Web Search- Introduction & Web Crawler,4.1,a year.,00:09:45,1,year
1327,00:09:49,5,Web Search- Introduction & Web Crawler,4.1,"Than, even though that page has been changed a lot, then,",00:09:46,1,Than even though page changed lot
1328,00:09:55,5,Web Search- Introduction & Web Crawler,4.1,"it's probably not necessary to crawl that page or at least it's not as urgent as,",00:09:49,1,probably necessary crawl page least urgent
1329,00:10:01,5,Web Search- Introduction & Web Crawler,4.1,to maintain the freshness of frequently accessed page by users.,00:09:55,1,maintain freshness frequently accessed page users
1330,00:10:02,5,Web Search- Introduction & Web Crawler,4.1,"So to summarize,",00:10:01,1,So summarize
1331,00:10:06,5,Web Search- Introduction & Web Crawler,4.1,web search is one of the most important applications of text retrieval.,00:10:02,1,web search one important applications text retrieval
1332,00:10:08,5,Web Search- Introduction & Web Crawler,4.1,"And there are some new challenges particularly scalability,",00:10:06,1,And new challenges particularly scalability
1333,00:10:10,5,Web Search- Introduction & Web Crawler,4.1,"efficiency, quality information.",00:10:08,1,efficiency quality information
1334,00:10:15,5,Web Search- Introduction & Web Crawler,4.1,"There are also new opportunities particularly, rich link information and",00:10:10,1,There also new opportunities particularly rich link information
1335,00:10:16,5,Web Search- Introduction & Web Crawler,4.1,"layout, et cetera.",00:10:15,1,layout et cetera
1336,00:10:22,5,Web Search- Introduction & Web Crawler,4.1,Crawler is an essential component of web search applications.,00:10:18,1,Crawler essential component web search applications
1337,00:10:24,5,Web Search- Introduction & Web Crawler,4.1,"And, in general, we can classify two scenarios.",00:10:22,1,And general classify two scenarios
1338,00:10:28,5,Web Search- Introduction & Web Crawler,4.1,Once is initial crawling and here we want to have complete crawling,00:10:24,1,Once initial crawling want complete crawling
1339,00:10:33,5,Web Search- Introduction & Web Crawler,4.1,of the web if you are doing a general search engine or,00:10:30,1,web general search engine
1340,00:10:37,5,Web Search- Introduction & Web Crawler,4.1,focus crawling if you want to just target it at a certain type of pages.,00:10:33,1,focus crawling want target certain type pages
1341,00:10:43,5,Web Search- Introduction & Web Crawler,4.1,And then there is another scenario that's incremental updating of the crawl data or,00:10:38,1,And another scenario incremental updating crawl data
1342,00:10:44,5,Web Search- Introduction & Web Crawler,4.1,incremental crawling.,00:10:43,1,incremental crawling
1343,00:10:47,5,Web Search- Introduction & Web Crawler,4.1,In this case you need to optimize the resource.,00:10:44,1,In case need optimize resource
1344,00:10:52,5,Web Search- Introduction & Web Crawler,4.1,For to use minimum resource we get the [INAUDIBLE],00:10:47,1,For use minimum resource get INAUDIBLE
1345,00:00:11,4,Query Likelihood Retrieval Function,3.3,This lecture is about query likelihood and probabilistic retrieval model.,00:00:07,3,This lecture query likelihood probabilistic retrieval model
1346,00:00:15,4,Query Likelihood Retrieval Function,3.3,"In this lecture,",00:00:14,3,In lecture
1347,00:00:19,4,Query Likelihood Retrieval Function,3.3,we continue the discussion of probabilistic retrieval model.,00:00:15,3,continue discussion probabilistic retrieval model
1348,00:00:20,4,Query Likelihood Retrieval Function,3.3,"In particular,",00:00:19,3,In particular
1349,00:00:25,4,Query Likelihood Retrieval Function,3.3,we're going to talk about the query likelihood of the retrieval function.,00:00:20,3,going talk query likelihood retrieval function
1350,00:00:31,4,Query Likelihood Retrieval Function,3.3,In the query of likelihood retrieval model our idea is a model.,00:00:25,3,In query likelihood retrieval model idea model
1351,00:00:35,4,Query Likelihood Retrieval Function,3.3,"How a likely a user, who likes a document would pose a particular query.",00:00:31,3,How likely user likes document would pose particular query
1352,00:00:39,4,Query Likelihood Retrieval Function,3.3,"So in this case, you can imagine,",00:00:36,3,So case imagine
1353,00:00:45,4,Query Likelihood Retrieval Function,3.3,if a user likes this particular document about the presidential campaign news.,00:00:39,3,user likes particular document presidential campaign news
1354,00:00:51,4,Query Likelihood Retrieval Function,3.3,"Then we can assume, the user would use this",00:00:46,3,Then assume user would use
1355,00:00:54,4,Query Likelihood Retrieval Function,3.3,working as a basis to oppose a query to try and retrieve this doc.,00:00:51,3,working basis oppose query try retrieve doc
1356,00:01:04,4,Query Likelihood Retrieval Function,3.3,"So you can imagine the user, could use a process that works as follows, where",00:00:57,3,So imagine user could use process works follows
1357,00:01:08,4,Query Likelihood Retrieval Function,3.3,we assume that the query is generated by sampling words from the document.,00:01:04,3,assume query generated sampling words document
1358,00:01:15,4,Query Likelihood Retrieval Function,3.3,"So for example, a user might pick a word like",00:01:10,3,So example user might pick word like
1359,00:01:19,4,Query Likelihood Retrieval Function,3.3,"presidential from this document, and then use this as a query word.",00:01:15,3,presidential document use query word
1360,00:01:24,4,Query Likelihood Retrieval Function,3.3,"And then the user would pick another word, like campaign and",00:01:20,3,And user would pick another word like campaign
1361,00:01:25,4,Query Likelihood Retrieval Function,3.3,that would be the second query word.,00:01:24,3,would second query word
1362,00:01:28,4,Query Likelihood Retrieval Function,3.3,"Now this, of course,",00:01:27,3,Now course
1363,00:01:33,4,Query Likelihood Retrieval Function,3.3,"is assumption that we have made about, how a user would post a query.",00:01:28,3,assumption made user would post query
1364,00:01:38,4,Query Likelihood Retrieval Function,3.3,Whether a user actually followed this process.,00:01:35,3,Whether user actually followed process
1365,00:01:39,4,Query Likelihood Retrieval Function,3.3,Maybe a different question.,00:01:38,3,Maybe different question
1366,00:01:41,4,Query Likelihood Retrieval Function,3.3,"But this assumption,",00:01:39,3,But assumption
1367,00:01:45,4,Query Likelihood Retrieval Function,3.3,has allowed us to formally characterize this conditional probability.,00:01:41,3,allowed us formally characterize conditional probability
1368,00:01:50,4,Query Likelihood Retrieval Function,3.3,And this allows to also not rely on the big table that I showed you earlier,00:01:46,3,And allows also rely big table I showed earlier
1369,00:01:55,4,Query Likelihood Retrieval Function,3.3,to use imperative data to estimate this probability.,00:01:52,3,use imperative data estimate probability
1370,00:02:00,4,Query Likelihood Retrieval Function,3.3,And this is why we can use this idea to then further derive,00:01:56,3,And use idea derive
1371,00:02:03,4,Query Likelihood Retrieval Function,3.3,retrieval function that we can implement with the languages.,00:02:00,3,retrieval function implement languages
1372,00:02:09,4,Query Likelihood Retrieval Function,3.3,"So, as you see, the assumption that we've made here is, each query word,",00:02:04,3,So see assumption made query word
1373,00:02:12,4,Query Likelihood Retrieval Function,3.3,"is independent in this sample, and also,",00:02:09,3,independent sample also
1374,00:02:17,4,Query Likelihood Retrieval Function,3.3,each word is basically obtained from the document.,00:02:12,3,word basically obtained document
1375,00:02:24,4,Query Likelihood Retrieval Function,3.3,So now let's see how this works exactly.,00:02:20,3,So let see works exactly
1376,00:02:28,4,Query Likelihood Retrieval Function,3.3,"Well, since we are computing a query likelihood,",00:02:24,3,Well since computing query likelihood
1377,00:02:35,4,Query Likelihood Retrieval Function,3.3,"then the probability here is just the probability of this particular query,",00:02:29,3,probability probability particular query
1378,00:02:37,4,Query Likelihood Retrieval Function,3.3,which is a sequence of words.,00:02:35,3,sequence words
1379,00:02:42,4,Query Likelihood Retrieval Function,3.3,And we make the assumption that each word is generated independently.,00:02:37,3,And make assumption word generated independently
1380,00:02:46,4,Query Likelihood Retrieval Function,3.3,"So, as a result, the probability of the query is just a product",00:02:42,3,So result probability query product
1381,00:02:48,4,Query Likelihood Retrieval Function,3.3,of the probability of each query word.,00:02:46,3,probability query word
1382,00:02:52,4,Query Likelihood Retrieval Function,3.3,"Now, how do we compute the probability of each query word?",00:02:50,3,Now compute probability query word
1383,00:02:54,4,Query Likelihood Retrieval Function,3.3,"Well, based on the assumption,",00:02:52,3,Well based assumption
1384,00:03:00,4,Query Likelihood Retrieval Function,3.3,"that a word is picked from the document, that the user has in mind.",00:02:54,3,word picked document user mind
1385,00:03:03,4,Query Likelihood Retrieval Function,3.3,"Now we know the probability of each word is just the,",00:03:00,3,Now know probability word
1386,00:03:08,4,Query Likelihood Retrieval Function,3.3,the relative frequency of the word in the document.,00:03:03,3,relative frequency word document
1387,00:03:13,4,Query Likelihood Retrieval Function,3.3,"So, for example the probability of presidential given the document,",00:03:08,3,So example probability presidential given document
1388,00:03:17,4,Query Likelihood Retrieval Function,3.3,"would be just the count of presidential in the document,",00:03:13,3,would count presidential document
1389,00:03:20,4,Query Likelihood Retrieval Function,3.3,divided by the total number of words in the document or document length.,00:03:17,3,divided total number words document document length
1390,00:03:27,4,Query Likelihood Retrieval Function,3.3,"So with this these assumptions, we now have actual simple formula for",00:03:23,3,So assumptions actual simple formula
1391,00:03:28,4,Query Likelihood Retrieval Function,3.3,"retrieval, right?",00:03:27,3,retrieval right
1392,00:03:30,4,Query Likelihood Retrieval Function,3.3,We can use this to rank our document.,00:03:28,3,We use rank document
1393,00:03:34,4,Query Likelihood Retrieval Function,3.3,So does this model work?,00:03:30,3,So model work
1394,00:03:38,4,Query Likelihood Retrieval Function,3.3,"Let's take a look, here are some example documents that you have seen before.",00:03:34,3,Let take look example documents seen
1395,00:03:41,4,Query Likelihood Retrieval Function,3.3,Suppose now the query is presidential campaign.,00:03:38,3,Suppose query presidential campaign
1396,00:03:45,4,Query Likelihood Retrieval Function,3.3,And we see the formula here on the top.,00:03:41,3,And see formula top
1397,00:03:47,4,Query Likelihood Retrieval Function,3.3,So how do we score these documents?,00:03:45,3,So score documents
1398,00:03:50,4,Query Likelihood Retrieval Function,3.3,"Well it's very simple, right, we just count how many times we have seen",00:03:47,3,Well simple right count many times seen
1399,00:03:54,4,Query Likelihood Retrieval Function,3.3,"presidential, how many times we have seen campaign etc.",00:03:50,3,presidential many times seen campaign etc
1400,00:03:57,4,Query Likelihood Retrieval Function,3.3,"And see here 44 and we've seen president Jou Tai,",00:03:54,3,And see 44 seen president Jou Tai
1401,00:04:01,4,Query Likelihood Retrieval Function,3.3,so that's two over the lands of document the four.,00:03:57,3,two lands document four
1402,00:04:06,4,Query Likelihood Retrieval Function,3.3,Multiply by 1 over lands of document of 4 for the probability of,00:04:01,3,Multiply 1 lands document 4 probability
1403,00:04:11,4,Query Likelihood Retrieval Function,3.3,campaign and seeming we can probabilities for the other two documents.,00:04:06,3,campaign seeming probabilities two documents
1404,00:04:18,4,Query Likelihood Retrieval Function,3.3,"Now if you'll look at this, these numbers or these, this, these formulas for",00:04:13,3,Now look numbers formulas
1405,00:04:25,4,Query Likelihood Retrieval Function,3.3,"scoring all these documents, it seems to make sense because, if we assume d3 and",00:04:18,3,scoring documents seems make sense assume d3
1406,00:04:29,4,Query Likelihood Retrieval Function,3.3,"d4 have about the same length, then it looks like we will rank d4",00:04:25,3,d4 length looks like rank d4
1407,00:04:36,4,Query Likelihood Retrieval Function,3.3,"above d3 and which is above d2, right?",00:04:31,3,d3 d2 right
1408,00:04:42,4,Query Likelihood Retrieval Function,3.3,"And as we would expect, looks like it did capture the tf heuristic.",00:04:36,3,And would expect looks like capture tf heuristic
1409,00:04:44,4,Query Likelihood Retrieval Function,3.3,And so this seems to work well.,00:04:42,3,And seems work well
1410,00:04:50,4,Query Likelihood Retrieval Function,3.3,"However, if we try a different query like this one,",00:04:46,3,However try different query like one
1411,00:04:54,4,Query Likelihood Retrieval Function,3.3,"presidential campaign update, then we might see a problem.",00:04:50,3,presidential campaign update might see problem
1412,00:04:56,4,Query Likelihood Retrieval Function,3.3,But what problem?,00:04:55,3,But problem
1413,00:05:02,4,Query Likelihood Retrieval Function,3.3,"Well, think about update, now none of these documents has mentioned update.",00:04:56,3,Well think update none documents mentioned update
1414,00:05:07,4,Query Likelihood Retrieval Function,3.3,So according to our assumption that a user would pick a order from a document,00:05:02,3,So according assumption user would pick order document
1415,00:05:09,4,Query Likelihood Retrieval Function,3.3,"to generate a query,",00:05:07,3,generate query
1416,00:05:15,4,Query Likelihood Retrieval Function,3.3,then the probability of obtaining a word like update would be what.,00:05:09,3,probability obtaining word like update would
1417,00:05:16,4,Query Likelihood Retrieval Function,3.3,"Would be zero, right?",00:05:15,3,Would zero right
1418,00:05:21,4,Query Likelihood Retrieval Function,3.3,"So that cause a problem, because it would cause all these documents",00:05:17,3,So cause problem would cause documents
1419,00:05:23,4,Query Likelihood Retrieval Function,3.3,to have zero probability of generating this query.,00:05:21,3,zero probability generating query
1420,00:05:31,4,Query Likelihood Retrieval Function,3.3,"Now, while it's fine to have a zero probability for d2 which is not relevant.",00:05:25,3,Now fine zero probability d2 relevant
1421,00:05:34,4,Query Likelihood Retrieval Function,3.3,It's not okay to have zero for d3 and,00:05:31,3,It okay zero d3
1422,00:05:38,4,Query Likelihood Retrieval Function,3.3,"d4, because now we no longer can distinguish them.",00:05:34,3,d4 longer distinguish
1423,00:05:41,4,Query Likelihood Retrieval Function,3.3,"What's worse, we can't even distinguish them from d 2.",00:05:38,3,What worse even distinguish 2
1424,00:05:45,4,Query Likelihood Retrieval Function,3.3,"All right, so that's obviously not desirable.",00:05:41,3,All right obviously desirable
1425,00:05:50,4,Query Likelihood Retrieval Function,3.3,"Now when one has such result, we should think about what has caused this problem.",00:05:45,3,Now one result think caused problem
1426,00:05:55,4,Query Likelihood Retrieval Function,3.3,"So we have to examine what assumptions have been made,",00:05:52,3,So examine assumptions made
1427,00:05:58,4,Query Likelihood Retrieval Function,3.3,as we derive this ranking function.,00:05:56,3,derive ranking function
1428,00:06:03,4,Query Likelihood Retrieval Function,3.3,Now if you examine those assumptions carefully you would realize.,00:05:59,3,Now examine assumptions carefully would realize
1429,00:06:06,4,Query Likelihood Retrieval Function,3.3,"What has caused this problem, right?",00:06:03,3,What caused problem right
1430,00:06:11,4,Query Likelihood Retrieval Function,3.3,"So, take a moment to think about, what do you think is the reason why",00:06:06,3,So take moment think think reason
1431,00:06:14,4,Query Likelihood Retrieval Function,3.3,"update has zero probability, and how do we fix it?",00:06:11,3,update zero probability fix
1432,00:06:18,4,Query Likelihood Retrieval Function,3.3,Right?,00:06:17,3,Right
1433,00:06:21,4,Query Likelihood Retrieval Function,3.3,"So, if you think about this for the moment that you realize that.",00:06:18,3,So think moment realize
1434,00:06:23,4,Query Likelihood Retrieval Function,3.3,That's because we have made an assumption,00:06:21,3,That made assumption
1435,00:06:29,4,Query Likelihood Retrieval Function,3.3,that every query word must be drawn from the document in the user's mind.,00:06:23,3,every query word must drawn document user mind
1436,00:06:32,4,Query Likelihood Retrieval Function,3.3,"So, in order to fix this, we have to assume that,",00:06:29,3,So order fix assume
1437,00:06:36,4,Query Likelihood Retrieval Function,3.3,"the user could have drawn a word, not necessarily from the document.",00:06:32,3,user could drawn word necessarily document
1438,00:06:38,4,Query Likelihood Retrieval Function,3.3,So let's see improved model.,00:06:36,3,So let see improved model
1439,00:06:40,4,Query Likelihood Retrieval Function,3.3,"An improvement here is to say that,",00:06:38,3,An improvement say
1440,00:06:45,4,Query Likelihood Retrieval Function,3.3,"well, instead of drawing a word from the document, let's imagine that the user",00:06:40,3,well instead drawing word document let imagine user
1441,00:06:50,4,Query Likelihood Retrieval Function,3.3,would actually draw a word from a document model and so I show a model here.,00:06:45,3,would actually draw word document model I show model
1442,00:06:53,4,Query Likelihood Retrieval Function,3.3,"Here we assume that this document is generated,",00:06:50,3,Here assume document generated
1443,00:06:55,4,Query Likelihood Retrieval Function,3.3,by using this unigram image model.,00:06:53,3,using unigram image model
1444,00:07:01,4,Query Likelihood Retrieval Function,3.3,"Now, this model, doesn't necessarily assign zero probability for update.",00:06:55,3,Now model necessarily assign zero probability update
1445,00:07:05,4,Query Likelihood Retrieval Function,3.3,In fact we assume this model does not assign zero probability for any word.,00:07:01,3,In fact assume model assign zero probability word
1446,00:07:08,4,Query Likelihood Retrieval Function,3.3,Now if we're thinking this way then the generation,00:07:05,3,Now thinking way generation
1447,00:07:10,4,Query Likelihood Retrieval Function,3.3,process is a little bit different.,00:07:08,3,process little bit different
1448,00:07:14,4,Query Likelihood Retrieval Function,3.3,"Now the user has this model in mind, instead of this particular document.",00:07:10,3,Now user model mind instead particular document
1449,00:07:17,4,Query Likelihood Retrieval Function,3.3,Although the model has to be estimated based on the document.,00:07:14,3,Although model estimated based document
1450,00:07:22,4,Query Likelihood Retrieval Function,3.3,So the user can again generate the query using a similar process.,00:07:17,3,So user generate query using similar process
1451,00:07:27,4,Query Likelihood Retrieval Function,3.3,"They may pick a word, for example presidential and another word campaign.",00:07:22,3,They may pick word example presidential another word campaign
1452,00:07:32,4,Query Likelihood Retrieval Function,3.3,"Now the difference is that, this time we can also pick a word like update,",00:07:29,3,Now difference time also pick word like update
1453,00:07:34,4,Query Likelihood Retrieval Function,3.3,even though update it doesn't occur in the document,00:07:32,3,even though update occur document
1454,00:07:38,4,Query Likelihood Retrieval Function,3.3,to potentially generate a query word like update.,00:07:34,3,potentially generate query word like update
1455,00:07:43,4,Query Likelihood Retrieval Function,3.3,"So that, a query was updated we want to have zero probabilities.",00:07:38,3,So query updated want zero probabilities
1456,00:07:46,4,Query Likelihood Retrieval Function,3.3,"So this would fix our problem and it's also reasonable,",00:07:43,3,So would fix problem also reasonable
1457,00:07:51,4,Query Likelihood Retrieval Function,3.3,"because we're now thinking of what the user is looking for in a more general way,",00:07:46,3,thinking user looking general way
1458,00:07:55,4,Query Likelihood Retrieval Function,3.3,that is unique language model instead of a fixed document.,00:07:51,3,unique language model instead fixed document
1459,00:07:56,4,Query Likelihood Retrieval Function,3.3,"So how do we compute this query,",00:07:55,3,So compute query
1460,00:08:01,4,Query Likelihood Retrieval Function,3.3,"like if we make this sum where it involves two steps, right?",00:07:56,3,like make sum involves two steps right
1461,00:08:06,4,Query Likelihood Retrieval Function,3.3,"The first is the computer's model, and we call it talking the language model here.",00:08:01,3,The first computer model call talking language model
1462,00:08:12,4,Query Likelihood Retrieval Function,3.3,"For example, I have shown two possible energy models here.",00:08:07,3,For example I shown two possible energy models
1463,00:08:15,4,Query Likelihood Retrieval Function,3.3,This has been based on two documents.,00:08:12,3,This based two documents
1464,00:08:17,4,Query Likelihood Retrieval Function,3.3,And then given a query and I get a mining algorithms.,00:08:15,3,And given query I get mining algorithms
1465,00:08:22,4,Query Likelihood Retrieval Function,3.3,"The second step, is just to compute the likelihood of this query.",00:08:17,3,The second step compute likelihood query
1466,00:08:27,4,Query Likelihood Retrieval Function,3.3,"And by making independence assumptions, we could then have this probability as",00:08:22,3,And making independence assumptions could probability
1467,00:08:29,4,Query Likelihood Retrieval Function,3.3,"a product of the probability of each query word, all right?",00:08:27,3,product probability query word right
1468,00:08:32,4,Query Likelihood Retrieval Function,3.3,But we do this for both documents.,00:08:30,3,But documents
1469,00:08:35,4,Query Likelihood Retrieval Function,3.3,And then we're going to score these two documents and then rank them.,00:08:32,3,And going score two documents rank
1470,00:08:41,4,Query Likelihood Retrieval Function,3.3,So that's the basic idea of this query likelihood retrieval function.,00:08:37,3,So basic idea query likelihood retrieval function
1471,00:08:48,4,Query Likelihood Retrieval Function,3.3,So more generally than this ranking function would look like the following and,00:08:41,3,So generally ranking function would look like following
1472,00:08:53,4,Query Likelihood Retrieval Function,3.3,"here as, we assume that query has end words W1 through WN.",00:08:48,3,assume query end words W1 WN
1473,00:09:01,4,Query Likelihood Retrieval Function,3.3,"And then the scoring function, the ranking function is the probability",00:08:53,3,And scoring function ranking function probability
1474,00:09:06,4,Query Likelihood Retrieval Function,3.3,"that we observe this query, given that the user is thinking of this document.",00:09:01,3,observe query given user thinking document
1475,00:09:11,4,Query Likelihood Retrieval Function,3.3,And this assumed to be product of probabilities of all individual words and,00:09:06,3,And assumed product probabilities individual words
1476,00:09:13,4,Query Likelihood Retrieval Function,3.3,this is based on the independence assumption.,00:09:11,3,based independence assumption
1477,00:09:19,4,Query Likelihood Retrieval Function,3.3,"Now we actually often score the, document for",00:09:15,3,Now actually often score document
1478,00:09:25,4,Query Likelihood Retrieval Function,3.3,"this query by using log of the query likelihood, as shown on the sigma line.",00:09:19,3,query using log query likelihood shown sigma line
1479,00:09:32,4,Query Likelihood Retrieval Function,3.3,Now we do this to avoid having a lot of small probabilities.,00:09:26,3,Now avoid lot small probabilities
1480,00:09:35,4,Query Likelihood Retrieval Function,3.3,"M, multiplied together.",00:09:34,3,M multiplied together
1481,00:09:38,4,Query Likelihood Retrieval Function,3.3,And this could cause underflow and,00:09:35,3,And could cause underflow
1482,00:09:44,4,Query Likelihood Retrieval Function,3.3,we might lose precision by transforming the value as a logarithm function.,00:09:38,3,might lose precision transforming value logarithm function
1483,00:09:50,4,Query Likelihood Retrieval Function,3.3,"We maintain the order of these documents, yet we can avoid the end of flow problem.",00:09:44,3,We maintain order documents yet avoid end flow problem
1484,00:09:57,4,Query Likelihood Retrieval Function,3.3,So if we take longer than transformation of coarse the product that would become,00:09:52,3,So take longer transformation coarse product would become
1485,00:09:59,4,Query Likelihood Retrieval Function,3.3,"a sum, as you stake in the line here.",00:09:57,3,sum stake line
1486,00:10:03,4,Query Likelihood Retrieval Function,3.3,"So it's a sum of all of the query words, and inside the sum",00:09:59,3,So sum query words inside sum
1487,00:10:07,4,Query Likelihood Retrieval Function,3.3,that is log of the probability of this word given by the document.,00:10:03,3,log probability word given document
1488,00:10:13,4,Query Likelihood Retrieval Function,3.3,"And then we can further rewrite the sum, into a different form.",00:10:09,3,And rewrite sum different form
1489,00:10:19,4,Query Likelihood Retrieval Function,3.3,"So in the first of the sum here, in this sum,",00:10:14,3,So first sum sum
1490,00:10:25,4,Query Likelihood Retrieval Function,3.3,we have it over all the query words n query words.,00:10:21,3,query words n query words
1491,00:10:33,4,Query Likelihood Retrieval Function,3.3,"And in this sum, we have a sum of all the possible words but",00:10:28,3,And sum sum possible words
1492,00:10:37,4,Query Likelihood Retrieval Function,3.3,we put a counter here of each word in the query.,00:10:33,3,put counter word query
1493,00:10:39,4,Query Likelihood Retrieval Function,3.3,"Essentially we are only considering the words in the query,",00:10:37,3,Essentially considering words query
1494,00:10:43,4,Query Likelihood Retrieval Function,3.3,"because if a word is not in the query, it can would be zero.",00:10:39,3,word query would zero
1495,00:10:46,4,Query Likelihood Retrieval Function,3.3,So we're still considering only these end words.,00:10:43,3,So still considering end words
1496,00:10:50,4,Query Likelihood Retrieval Function,3.3,"But we're using a different form as if we were going to a sum of all the words,",00:10:46,3,But using different form going sum words
1497,00:10:51,4,Query Likelihood Retrieval Function,3.3,in the vocabulary.,00:10:50,3,vocabulary
1498,00:10:56,4,Query Likelihood Retrieval Function,3.3,And of course a word might occur multiple times in the query.,00:10:52,3,And course word might occur multiple times query
1499,00:10:58,4,Query Likelihood Retrieval Function,3.3,"That's wh, why we have a count here.",00:10:56,3,That wh count
1500,00:11:03,4,Query Likelihood Retrieval Function,3.3,And then this part is,00:11:01,3,And part
1501,00:11:06,4,Query Likelihood Retrieval Function,3.3,log of the probability of the word given by the document MG model.,00:11:03,3,log probability word given document MG model
1502,00:11:11,4,Query Likelihood Retrieval Function,3.3,"So you can see, in this material function,",00:11:08,3,So see material function
1503,00:11:13,4,Query Likelihood Retrieval Function,3.3,we actually know the count of the word in the query.,00:11:11,3,actually know count word query
1504,00:11:16,4,Query Likelihood Retrieval Function,3.3,"So, the only thing that we don't know is this document language model.",00:11:13,3,So thing know document language model
1505,00:11:21,4,Query Likelihood Retrieval Function,3.3,"Therefore, we can convert through the retrieval problem",00:11:17,3,Therefore convert retrieval problem
1506,00:11:24,4,Query Likelihood Retrieval Function,3.3,into the problem of estimating this document language model.,00:11:21,3,problem estimating document language model
1507,00:11:30,4,Query Likelihood Retrieval Function,3.3,"So that we can compute, the probability of each query we're given by this document.",00:11:24,3,So compute probability query given document
1508,00:11:36,4,Query Likelihood Retrieval Function,3.3,"At different estimation methods here, would lead to different ranking functions.",00:11:32,3,At different estimation methods would lead different ranking functions
1509,00:11:40,4,Query Likelihood Retrieval Function,3.3,"And this is just like a different a ways to place a doc in the vector,",00:11:36,3,And like different ways place doc vector
1510,00:11:42,4,Query Likelihood Retrieval Function,3.3,in the vector space.,00:11:40,3,vector space
1511,00:11:45,4,Query Likelihood Retrieval Function,3.3,Would lead it to a different ranking function in the vector space model.,00:11:42,3,Would lead different ranking function vector space model
1512,00:11:50,4,Query Likelihood Retrieval Function,3.3,"Here are different ways to estimate this stuff in the language model,",00:11:45,3,Here different ways estimate stuff language model
1513,00:11:55,4,Query Likelihood Retrieval Function,3.3,will lead you to a different ranking function for query likelihood.,00:11:50,3,lead different ranking function query likelihood
1514,00:00:05,4,Probabilistic Retrieval Model- Basic Idea,3.1,[SOUND] This lecture is about,00:00:00,1,SOUND This lecture
1515,00:00:12,4,Probabilistic Retrieval Model- Basic Idea,3.1,the probabilistic retrieval model.,00:00:05,1,probabilistic retrieval model
1516,00:00:18,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In this lecture, we're going to continue the discussion of text retrieval methods.",00:00:12,1,In lecture going continue discussion text retrieval methods
1517,00:00:23,4,Probabilistic Retrieval Model- Basic Idea,3.1,We're going to look at another kind of very different way to design ranking,00:00:18,1,We going look another kind different way design ranking
1518,00:00:27,4,Probabilistic Retrieval Model- Basic Idea,3.1,"functions, then the Vector Space Model that we discussed before.",00:00:23,1,functions Vector Space Model discussed
1519,00:00:37,4,Probabilistic Retrieval Model- Basic Idea,3.1,In probabilistic models we define the ranking function based,00:00:32,1,In probabilistic models define ranking function based
1520,00:00:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,on the probability that this document is random to this query.,00:00:37,1,probability document random query
1521,00:00:48,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In other words, we are, we introduced a binary random variable here.",00:00:42,1,In words introduced binary random variable
1522,00:00:53,4,Probabilistic Retrieval Model- Basic Idea,3.1,This is the variable R here.,00:00:48,1,This variable R
1523,00:00:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,And we also assume that the query and,00:00:53,1,And also assume query
1524,00:01:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,the documents are all observations from random variables.,00:00:56,1,documents observations random variables
1525,00:01:03,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Note that in the vector space model, we assume they are vectors.",00:01:00,1,Note vector space model assume vectors
1526,00:01:10,4,Probabilistic Retrieval Model- Basic Idea,3.1,But here we assumed we assumed they are the data observed from random variables.,00:01:03,1,But assumed assumed data observed random variables
1527,00:01:14,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And so the problem, model retrieval",00:01:10,1,And problem model retrieval
1528,00:01:19,4,Probabilistic Retrieval Model- Basic Idea,3.1,becomes to estimate the probability of relevance.,00:01:14,1,becomes estimate probability relevance
1529,00:01:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In this category of models, there are different variants.",00:01:19,1,In category models different variants
1530,00:01:27,4,Probabilistic Retrieval Model- Basic Idea,3.1,"The classic probabilistic model has led to the BM25 retrieval function,",00:01:22,1,The classic probabilistic model led BM25 retrieval function
1531,00:01:30,4,Probabilistic Retrieval Model- Basic Idea,3.1,"which we discussed in the vector space model,",00:01:27,1,discussed vector space model
1532,00:01:34,4,Probabilistic Retrieval Model- Basic Idea,3.1,because it's form is actually similar to a vector space model.,00:01:30,1,form actually similar vector space model
1533,00:01:39,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In this lecture, we're going to discuss another subclass in",00:01:34,1,In lecture going discuss another subclass
1534,00:01:45,4,Probabilistic Retrieval Model- Basic Idea,3.1,this big class called a language modeling approaches to retrieval.,00:01:39,1,big class called language modeling approaches retrieval
1535,00:01:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In particular, we're going to discuss the Query Likelihood retrieval model,",00:01:45,1,In particular going discuss Query Likelihood retrieval model
1536,00:01:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,which is one of the most effective models in probabilistic models.,00:01:51,1,one effective models probabilistic models
1537,00:02:02,4,Probabilistic Retrieval Model- Basic Idea,3.1,"There is also another line called a divergence-from-randomness model,",00:01:56,1,There also another line called divergence randomness model
1538,00:02:06,4,Probabilistic Retrieval Model- Basic Idea,3.1,which has latitude the PL2 function.,00:02:02,1,latitude PL2 function
1539,00:02:10,4,Probabilistic Retrieval Model- Basic Idea,3.1,It's also one of the most effective state of the art attribute functions.,00:02:06,1,It also one effective state art attribute functions
1540,00:02:16,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In query likelihood, our assumption is that this probability readiness",00:02:10,1,In query likelihood assumption probability readiness
1541,00:02:23,4,Probabilistic Retrieval Model- Basic Idea,3.1,can be approximated by the probability of query given a document and readiness.,00:02:16,1,approximated probability query given document readiness
1542,00:02:29,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So, intuitively, this probability just captures the following probability.",00:02:23,1,So intuitively probability captures following probability
1543,00:02:34,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And that is if a user likes document d, how likely would",00:02:29,1,And user likes document likely would
1544,00:02:39,4,Probabilistic Retrieval Model- Basic Idea,3.1,the user enter query q in order to retrieve document d.,00:02:34,1,user enter query q order retrieve document
1545,00:02:47,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So we'll assume that the user likes d, because we have a relevance value here.",00:02:39,1,So assume user likes relevance value
1546,00:02:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,And the we ask the question about how likely we will see this,00:02:47,1,And ask question likely see
1547,00:02:54,4,Probabilistic Retrieval Model- Basic Idea,3.1,particular query from this user?,00:02:51,1,particular query user
1548,00:02:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,So this is the basic idea.,00:02:54,1,So basic idea
1549,00:03:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Now to understand this idea, let's take a look at the general idea or",00:02:56,1,Now understand idea let take look general idea
1550,00:03:03,4,Probabilistic Retrieval Model- Basic Idea,3.1,the basic idea of probabilistic retrieval models.,00:03:00,1,basic idea probabilistic retrieval models
1551,00:03:10,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So here, I listed some imagined relevance status values or",00:03:03,1,So I listed imagined relevance status values
1552,00:03:14,4,Probabilistic Retrieval Model- Basic Idea,3.1,relevance judgments of queries and documents.,00:03:10,1,relevance judgments queries documents
1553,00:03:20,4,Probabilistic Retrieval Model- Basic Idea,3.1,"For example, in this slide, it shows that query one",00:03:14,1,For example slide shows query one
1554,00:03:27,4,Probabilistic Retrieval Model- Basic Idea,3.1,is a query that the user typed in and d1 is a document the user has seen and,00:03:20,1,query user typed d1 document user seen
1555,00:03:33,4,Probabilistic Retrieval Model- Basic Idea,3.1,one means the user thinks d1 is relevant to to q1.,00:03:27,1,one means user thinks d1 relevant q1
1556,00:03:38,4,Probabilistic Retrieval Model- Basic Idea,3.1,So this R here can be also approximated by the clicks little data that the search,00:03:33,1,So R also approximated clicks little data search
1557,00:03:44,4,Probabilistic Retrieval Model- Basic Idea,3.1,engine can collect it by watching how you interact with the search results.,00:03:38,1,engine collect watching interact search results
1558,00:03:48,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So, in this case, let's say, the user clicked on this document, so",00:03:44,1,So case let say user clicked document
1559,00:03:49,4,Probabilistic Retrieval Model- Basic Idea,3.1,there's a one here.,00:03:48,1,one
1560,00:03:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Similarly, the user clicked on d2 also, so there's a one here.",00:03:49,1,Similarly user clicked d2 also one
1561,00:04:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In other words, d2 is assumed to relevant at two, q1.",00:03:56,1,In words d2 assumed relevant two q1
1562,00:04:07,4,Probabilistic Retrieval Model- Basic Idea,3.1,"On the other hand, d3 is non relevant, there's a zero here.",00:04:00,1,On hand d3 non relevant zero
1563,00:04:13,4,Probabilistic Retrieval Model- Basic Idea,3.1,And d4 is non-relevant and then d 5 is again relevant and so on and so forth.,00:04:07,1,And d4 non relevant 5 relevant forth
1564,00:04:17,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And this part of maybe, they are collected from a different user.",00:04:13,1,And part maybe collected different user
1565,00:04:19,4,Probabilistic Retrieval Model- Basic Idea,3.1,Right. So this user typed in q1 and,00:04:17,1,Right So user typed q1
1566,00:04:25,4,Probabilistic Retrieval Model- Basic Idea,3.1,"then found that d1 is actually not useful, so d1 is actually non-relevant.",00:04:19,1,found d1 actually useful d1 actually non relevant
1567,00:04:30,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In contrast here we see it's relevant and,",00:04:25,1,In contrast see relevant
1568,00:04:38,4,Probabilistic Retrieval Model- Basic Idea,3.1,"or this could be the same query typing by the same user at different times,",00:04:30,1,could query typing user different times
1569,00:04:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,"but d2 is also relevant, et cetera.",00:04:38,1,d2 also relevant et cetera
1570,00:04:48,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And then here, we can see more data that about other queries.",00:04:42,1,And see data queries
1571,00:04:52,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Now we can imagine, we have a lot of search data.",00:04:48,1,Now imagine lot search data
1572,00:04:54,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Now we can ask the question,",00:04:52,1,Now ask question
1573,00:04:59,4,Probabilistic Retrieval Model- Basic Idea,3.1,how can we then estimated the probability of relevance?,00:04:54,1,estimated probability relevance
1574,00:05:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,Right. So,00:04:59,1,Right So
1575,00:05:03,4,Probabilistic Retrieval Model- Basic Idea,3.1,how can we compute this probability of relevance?,00:05:00,1,compute probability relevance
1576,00:05:07,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Well, intuitively, that just means if we look at the,",00:05:03,1,Well intuitively means look
1577,00:05:12,4,Probabilistic Retrieval Model- Basic Idea,3.1,"all the entries where we see this particular d and this particular q,",00:05:07,1,entries see particular particular q
1578,00:05:15,4,Probabilistic Retrieval Model- Basic Idea,3.1,how likely will we see a one on the third column?,00:05:12,1,likely see one third column
1579,00:05:19,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Basically, that just means we can correct the counts.",00:05:15,1,Basically means correct counts
1580,00:05:24,4,Probabilistic Retrieval Model- Basic Idea,3.1,We can first count how many times where we see q and,00:05:19,1,We first count many times see q
1581,00:05:29,4,Probabilistic Retrieval Model- Basic Idea,3.1,d as a pair in this table and then count how many times,00:05:24,1,pair table count many times
1582,00:05:35,4,Probabilistic Retrieval Model- Basic Idea,3.1,we actually have also seen one in the third column and,00:05:29,1,actually also seen one third column
1583,00:05:39,4,Probabilistic Retrieval Model- Basic Idea,3.1,then we just compute the ratio.,00:05:35,1,compute ratio
1584,00:05:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,So let's take a look at some specific examples.,00:05:39,1,So let take look specific examples
1585,00:05:50,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Suppose we are trying to computed this probability for d1, d2 and d3 for q1.",00:05:42,1,Suppose trying computed probability d1 d2 d3 q1
1586,00:05:53,4,Probabilistic Retrieval Model- Basic Idea,3.1,What is the estimated probability?,00:05:50,1,What estimated probability
1587,00:05:55,4,Probabilistic Retrieval Model- Basic Idea,3.1,Now think about that.,00:05:53,1,Now think
1588,00:05:58,4,Probabilistic Retrieval Model- Basic Idea,3.1,You can pause the video if needed.,00:05:55,1,You pause video needed
1589,00:06:02,4,Probabilistic Retrieval Model- Basic Idea,3.1,Try to take a look at the table and,00:05:58,1,Try take look table
1590,00:06:07,4,Probabilistic Retrieval Model- Basic Idea,3.1,try to give your estimate of the probability.,00:06:02,1,try give estimate probability
1591,00:06:13,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Have you seen that if we are interested in q1 and d1, we've been looking at the,",00:06:07,1,Have seen interested q1 d1 looking
1592,00:06:18,4,Probabilistic Retrieval Model- Basic Idea,3.1,"these two pairs and in both cases or actually in one of the cases,",00:06:13,1,two pairs cases actually one cases
1593,00:06:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,"the user has said that this is one, this is relevant.",00:06:18,1,user said one relevant
1594,00:06:26,4,Probabilistic Retrieval Model- Basic Idea,3.1,So R is equal to 1 in only one of the two cases.,00:06:22,1,So R equal 1 one two cases
1595,00:06:28,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In the other case, this is zero.",00:06:26,1,In case zero
1596,00:06:32,4,Probabilistic Retrieval Model- Basic Idea,3.1,So that's one out of two.,00:06:28,1,So one two
1597,00:06:35,4,Probabilistic Retrieval Model- Basic Idea,3.1,What about the d1 and the d2?,00:06:32,1,What d1 d2
1598,00:06:40,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Well, they're are here, you want d2, d1, d2.",00:06:35,1,Well want d2 d1 d2
1599,00:06:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In both cases, in this case R is equal to 1.",00:06:40,1,In cases case R equal 1
1600,00:06:46,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So, it's two out of two and so and so forth.",00:06:42,1,So two two forth
1601,00:06:48,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So you can see with this approach,",00:06:46,1,So see approach
1602,00:06:52,4,Probabilistic Retrieval Model- Basic Idea,3.1,we captured it score these documents for the query.,00:06:48,1,captured score documents query
1603,00:06:52,4,Probabilistic Retrieval Model- Basic Idea,3.1,Right?,00:06:52,1,Right
1604,00:06:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,"We now have a score for d1, d2 and d3 for this query.",00:06:52,1,We score d1 d2 d3 query
1605,00:07:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,We can simply ranked them based on these probabilities and so,00:06:56,1,We simply ranked based probabilities
1606,00:07:04,4,Probabilistic Retrieval Model- Basic Idea,3.1,that's the basic idea of probabilistic retrieval model.,00:07:00,1,basic idea probabilistic retrieval model
1607,00:07:06,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And you can see, it makes a lot of sense.",00:07:04,1,And see makes lot sense
1608,00:07:09,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In this case, it's going to rank d2 above all the other documents.",00:07:06,1,In case going rank d2 documents
1609,00:07:16,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Because in all the cases, when you have seen q1 and d2, R is equal to 1.",00:07:09,1,Because cases seen q1 d2 R equal 1
1610,00:07:19,4,Probabilistic Retrieval Model- Basic Idea,3.1,The user clicked on this document.,00:07:16,1,The user clicked document
1611,00:07:24,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So this also showed showed that with a lot of click through data,",00:07:19,1,So also showed showed lot click data
1612,00:07:30,4,Probabilistic Retrieval Model- Basic Idea,3.1,a search engine can learn a lot from the data to improve the search engine.,00:07:24,1,search engine learn lot data improve search engine
1613,00:07:35,4,Probabilistic Retrieval Model- Basic Idea,3.1,"This is a simple example that shows that with even a small number of entries here,",00:07:30,1,This simple example shows even small number entries
1614,00:07:38,4,Probabilistic Retrieval Model- Basic Idea,3.1,we can already estimate some probabilities.,00:07:35,1,already estimate probabilities
1615,00:07:43,4,Probabilistic Retrieval Model- Basic Idea,3.1,These probabilities would give us some sense about which document might be more,00:07:38,1,These probabilities would give us sense document might
1616,00:07:46,4,Probabilistic Retrieval Model- Basic Idea,3.1,read or more useful to a user for typing this query.,00:07:43,1,read useful user typing query
1617,00:07:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Now, of course, the problem is that we don't observe all the queries and",00:07:46,1,Now course problem observe queries
1618,00:07:54,4,Probabilistic Retrieval Model- Basic Idea,3.1,all of the documents and all the relevance values.,00:07:51,1,documents relevance values
1619,00:07:55,4,Probabilistic Retrieval Model- Basic Idea,3.1,Right?,00:07:54,1,Right
1620,00:07:57,4,Probabilistic Retrieval Model- Basic Idea,3.1,There will be a lot of unseen documents.,00:07:55,1,There lot unseen documents
1621,00:08:01,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In general, we can only collect data from the document's that we have shown to",00:07:57,1,In general collect data document shown
1622,00:08:02,4,Probabilistic Retrieval Model- Basic Idea,3.1,the users.,00:08:01,1,users
1623,00:08:05,4,Probabilistic Retrieval Model- Basic Idea,3.1,"There are even more unseen queries,",00:08:02,1,There even unseen queries
1624,00:08:09,4,Probabilistic Retrieval Model- Basic Idea,3.1,because you cannot predict what queries will be typed in by users.,00:08:05,1,cannot predict queries typed users
1625,00:08:13,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So, obviously, this approach won't work",00:08:09,1,So obviously approach work
1626,00:08:17,4,Probabilistic Retrieval Model- Basic Idea,3.1,if we apply it to unseen queries or unseen documents.,00:08:13,1,apply unseen queries unseen documents
1627,00:08:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Nevertheless, this shows the basic idea of the probabilistic retrieval model and",00:08:17,1,Nevertheless shows basic idea probabilistic retrieval model
1628,00:08:24,4,Probabilistic Retrieval Model- Basic Idea,3.1,it makes sense intuitively.,00:08:22,1,makes sense intuitively
1629,00:08:28,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So what do we do in such a case when we have a lot of unseen documents and, and",00:08:24,1,So case lot unseen documents
1630,00:08:29,4,Probabilistic Retrieval Model- Basic Idea,3.1,unseen queries?,00:08:28,1,unseen queries
1631,00:08:32,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Well, the solutions that we have to approximate in some way.",00:08:29,1,Well solutions approximate way
1632,00:08:36,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Right. So, in this particular case called",00:08:32,1,Right So particular case called
1633,00:08:41,4,Probabilistic Retrieval Model- Basic Idea,3.1,"the Query LIkelihood Retrieval Model, we just approximate this",00:08:36,1,Query LIkelihood Retrieval Model approximate
1634,00:08:47,4,Probabilistic Retrieval Model- Basic Idea,3.1,"by another conditional probability, p q | d, R is equal to 1.",00:08:41,1,another conditional probability p q R equal 1
1635,00:08:52,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So, in the condition part, we assume that the user likes the document,",00:08:47,1,So condition part assume user likes document
1636,00:08:55,4,Probabilistic Retrieval Model- Basic Idea,3.1,because we have seen that the user clicked on this document.,00:08:52,1,seen user clicked document
1637,00:08:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And this part,",00:08:55,1,And part
1638,00:09:01,4,Probabilistic Retrieval Model- Basic Idea,3.1,shows that we're interested in how likely the user would actually enter this query.,00:08:56,1,shows interested likely user would actually enter query
1639,00:09:04,4,Probabilistic Retrieval Model- Basic Idea,3.1,How likely we will see this query in the same row.,00:09:01,1,How likely see query row
1640,00:09:08,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So note that here, we have made an interesting assumption here.",00:09:04,1,So note made interesting assumption
1641,00:09:13,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Basically, we, we're going to assume that whether the user types in this",00:09:08,1,Basically going assume whether user types
1642,00:09:17,4,Probabilistic Retrieval Model- Basic Idea,3.1,query has something to do with whether user likes the document.,00:09:13,1,query something whether user likes document
1643,00:09:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,"In other words, we actually make the foreign assumption and",00:09:17,1,In words actually make foreign assumption
1644,00:09:27,4,Probabilistic Retrieval Model- Basic Idea,3.1,that is a user formula to query based on an imaginary relevant document.,00:09:22,1,user formula query based imaginary relevant document
1645,00:09:30,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Well, if you just look at this as a conditional probability,",00:09:27,1,Well look conditional probability
1646,00:09:32,4,Probabilistic Retrieval Model- Basic Idea,3.1,it's not obvious we are making this assumption.,00:09:30,1,obvious making assumption
1647,00:09:37,4,Probabilistic Retrieval Model- Basic Idea,3.1,So what I really meant is that to use this new,00:09:32,1,So I really meant use new
1648,00:09:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,conditional probability to help us score,00:09:37,1,conditional probability help us score
1649,00:09:47,4,Probabilistic Retrieval Model- Basic Idea,3.1,then this new condition of probability.,00:09:42,1,new condition probability
1650,00:09:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,We have to somehow be able to estimate this conditional,00:09:47,1,We somehow able estimate conditional
1651,00:09:54,4,Probabilistic Retrieval Model- Basic Idea,3.1,probability without relying on this big table.,00:09:51,1,probability without relying big table
1652,00:09:59,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Otherwise, it would be having similar problems as before.",00:09:54,1,Otherwise would similar problems
1653,00:10:04,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And by making this assumption, we have some way to bypass this big table and",00:09:59,1,And making assumption way bypass big table
1654,00:10:08,4,Probabilistic Retrieval Model- Basic Idea,3.1,try to just mortar how to use a formula to the query.,00:10:04,1,try mortar use formula query
1655,00:10:11,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Okay. So this is how you can simplify the,",00:10:08,1,Okay So simplify
1656,00:10:18,4,Probabilistic Retrieval Model- Basic Idea,3.1,the general model so that we can give either specific function later.,00:10:11,1,general model give either specific function later
1657,00:10:21,4,Probabilistic Retrieval Model- Basic Idea,3.1,So let's look at how this model works for our example.,00:10:18,1,So let look model works example
1658,00:10:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And basically,",00:10:21,1,And basically
1659,00:10:27,4,Probabilistic Retrieval Model- Basic Idea,3.1,what we are going to do in this case is to ask the following question.,00:10:22,1,going case ask following question
1660,00:10:31,4,Probabilistic Retrieval Model- Basic Idea,3.1,Which of these documents is most likely the imaginary relevant document in,00:10:27,1,Which documents likely imaginary relevant document
1661,00:10:33,4,Probabilistic Retrieval Model- Basic Idea,3.1,the user's mind when the user formulates this query?,00:10:31,1,user mind user formulates query
1662,00:10:37,4,Probabilistic Retrieval Model- Basic Idea,3.1,And so we ask this question and we quantify the probability and this,00:10:33,1,And ask question quantify probability
1663,00:10:42,4,Probabilistic Retrieval Model- Basic Idea,3.1,probability is a conditional probability of observing this query if a particular,00:10:37,1,probability conditional probability observing query particular
1664,00:10:46,4,Probabilistic Retrieval Model- Basic Idea,3.1,document is in fact the imaginary relevant document in the user's mind.,00:10:42,1,document fact imaginary relevant document user mind
1665,00:10:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Here you can see we compute all these query likelihood probabilities,",00:10:46,1,Here see compute query likelihood probabilities
1666,00:10:54,4,Probabilistic Retrieval Model- Basic Idea,3.1,the likelihood of queries given each document.,00:10:51,1,likelihood queries given document
1667,00:10:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Once we have these values,",00:10:54,1,Once values
1668,00:11:00,4,Probabilistic Retrieval Model- Basic Idea,3.1,we can then rank these documents based on these values.,00:10:56,1,rank documents based values
1669,00:11:05,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So to summarize, the general idea of modern relevance in the probability",00:11:00,1,So summarize general idea modern relevance probability
1670,00:11:11,4,Probabilistic Retrieval Model- Basic Idea,3.1,"risk model is to assume that we introduce a binary random variable, R here.",00:11:05,1,risk model assume introduce binary random variable R
1671,00:11:15,4,Probabilistic Retrieval Model- Basic Idea,3.1,And then let the scoring function be defined based on this conditional,00:11:11,1,And let scoring function defined based conditional
1672,00:11:16,4,Probabilistic Retrieval Model- Basic Idea,3.1,probability.,00:11:15,1,probability
1673,00:11:22,4,Probabilistic Retrieval Model- Basic Idea,3.1,We also talked about a proximate in this [SOUND] by using the query likelihood.,00:11:16,1,We also talked proximate SOUND using query likelihood
1674,00:11:26,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And in this case, we have a ranking function that's",00:11:22,1,And case ranking function
1675,00:11:30,4,Probabilistic Retrieval Model- Basic Idea,3.1,basically based on a probability of a query given the document.,00:11:26,1,basically based probability query given document
1676,00:11:35,4,Probabilistic Retrieval Model- Basic Idea,3.1,And this probability should be interpreted as the probability,00:11:30,1,And probability interpreted probability
1677,00:11:39,4,Probabilistic Retrieval Model- Basic Idea,3.1,that a user who likes document d would pose query q.,00:11:35,1,user likes document would pose query q
1678,00:11:44,4,Probabilistic Retrieval Model- Basic Idea,3.1,"Now the question, of course is how do we compute this additional probability?",00:11:39,1,Now question course compute additional probability
1679,00:11:49,4,Probabilistic Retrieval Model- Basic Idea,3.1,"At this in general has to do with how to compute the probability of text,",00:11:44,1,At general compute probability text
1680,00:11:51,4,Probabilistic Retrieval Model- Basic Idea,3.1,because q is a text.,00:11:49,1,q text
1681,00:11:56,4,Probabilistic Retrieval Model- Basic Idea,3.1,And this has to do with a model called a Language Model.,00:11:51,1,And model called Language Model
1682,00:12:01,4,Probabilistic Retrieval Model- Basic Idea,3.1,And this kind of models are proposed to model text.,00:11:56,1,And kind models proposed model text
1683,00:12:07,4,Probabilistic Retrieval Model- Basic Idea,3.1,"So most specifically, we will be very interested in the following",00:12:01,1,So specifically interested following
1684,00:12:11,4,Probabilistic Retrieval Model- Basic Idea,3.1,"conditional probability as I show you, you this here.",00:12:07,1,conditional probability I show
1685,00:12:19,4,Probabilistic Retrieval Model- Basic Idea,3.1,"If the user like this document, how likely the user would approve this query?",00:12:11,1,If user like document likely user would approve query
1686,00:12:25,4,Probabilistic Retrieval Model- Basic Idea,3.1,"And in the next lecture, we're going to give introduction to Language Model,",00:12:19,1,And next lecture going give introduction Language Model
1687,00:12:32,4,Probabilistic Retrieval Model- Basic Idea,3.1,so that we can see how we can model text with a probability risk model in general.,00:12:25,1,see model text probability risk model general
1688,00:00:03,4,Statistical Language Models (00-17-53),3.2,[SOUND].,00:00:00,2,SOUND
1689,00:00:10,4,Statistical Language Models (00-17-53),3.2,This lecture is about a statistical language model.,00:00:07,2,This lecture statistical language model
1690,00:00:14,4,Statistical Language Models (00-17-53),3.2,"In this lecture, we're go,",00:00:12,2,In lecture go
1691,00:00:18,4,Statistical Language Models (00-17-53),3.2,we're going to get an introduction to the probabilistic model.,00:00:14,2,going get introduction probabilistic model
1692,00:00:23,4,Statistical Language Models (00-17-53),3.2,This has to do with how many models have to go into these models.,00:00:18,2,This many models go models
1693,00:00:28,4,Statistical Language Models (00-17-53),3.2,"So, it's ready to how we model theory based on a document.",00:00:23,2,So ready model theory based document
1694,00:00:34,4,Statistical Language Models (00-17-53),3.2,"We're going to talk about, what is a language model and, then, we're going to",00:00:31,2,We going talk language model going
1695,00:00:38,4,Statistical Language Models (00-17-53),3.2,talk about the simplest language model called a unigram language model.,00:00:34,2,talk simplest language model called unigram language model
1696,00:00:42,4,Statistical Language Models (00-17-53),3.2,Which also happens to be the most useful model for text retrieval.,00:00:38,2,Which also happens useful model text retrieval
1697,00:00:45,4,Statistical Language Models (00-17-53),3.2,And finally we'll discuss possible uses of an m model.,00:00:42,2,And finally discuss possible uses model
1698,00:00:48,4,Statistical Language Models (00-17-53),3.2,What is a language model?,00:00:47,2,What language model
1699,00:00:53,4,Statistical Language Models (00-17-53),3.2,"Well, it's just a probability distribution over word sequences.",00:00:48,2,Well probability distribution word sequences
1700,00:00:54,4,Statistical Language Models (00-17-53),3.2,"So, here I show one.",00:00:53,2,So I show one
1701,00:01:03,4,Statistical Language Models (00-17-53),3.2,This model gives the sequence today's Wednesday a probability of 0.001 it gives,00:00:55,2,This model gives sequence today Wednesday probability 0 001 gives
1702,00:01:08,4,Statistical Language Models (00-17-53),3.2,"today Wednesday is a very very small probability, because it's algorithmatical.",00:01:03,2,today Wednesday small probability algorithmatical
1703,00:01:15,4,Statistical Language Models (00-17-53),3.2,You can see the probabilities given to these sentences or,00:01:11,2,You see probabilities given sentences
1704,00:01:19,4,Statistical Language Models (00-17-53),3.2,sequences of words can vary a lot depending on the model.,00:01:15,2,sequences words vary lot depending model
1705,00:01:22,4,Statistical Language Models (00-17-53),3.2,"Therefore, it's clearly context-dependent.",00:01:19,2,Therefore clearly context dependent
1706,00:01:24,4,Statistical Language Models (00-17-53),3.2,"In ordinary conversation,",00:01:22,2,In ordinary conversation
1707,00:01:28,4,Statistical Language Models (00-17-53),3.2,probably today is Wednesday is most popular among these sentences.,00:01:24,2,probably today Wednesday popular among sentences
1708,00:01:32,4,Statistical Language Models (00-17-53),3.2,"But imagine in the context of discussing a private math,",00:01:28,2,But imagine context discussing private math
1709,00:01:36,4,Statistical Language Models (00-17-53),3.2,maybe the higher values positive would have a higher probability.,00:01:32,2,maybe higher values positive would higher probability
1710,00:01:41,4,Statistical Language Models (00-17-53),3.2,This means it can be used to represent as a topic of a test.,00:01:37,2,This means used represent topic test
1711,00:01:45,4,Statistical Language Models (00-17-53),3.2,The model can also be regarded as a probabilistic mechanism for,00:01:42,2,The model also regarded probabilistic mechanism
1712,00:01:51,4,Statistical Language Models (00-17-53),3.2,"generating text, and this is why it is often called a generating model.",00:01:45,2,generating text often called generating model
1713,00:01:52,4,Statistical Language Models (00-17-53),3.2,"So, what does that mean?",00:01:51,2,So mean
1714,00:01:58,4,Statistical Language Models (00-17-53),3.2,We can image this is a mechanism that's visualized,00:01:52,2,We image mechanism visualized
1715,00:02:05,4,Statistical Language Models (00-17-53),3.2,here as a [INAUDIBLE] system that can generate a sequences of words.,00:01:58,2,INAUDIBLE system generate sequences words
1716,00:02:08,4,Statistical Language Models (00-17-53),3.2,So we can ask for a sequence and,00:02:05,2,So ask sequence
1717,00:02:12,4,Statistical Language Models (00-17-53),3.2,it's to sample a sequence from the device if you want.,00:02:08,2,sample sequence device want
1718,00:02:16,4,Statistical Language Models (00-17-53),3.2,"And it might generate, for example, today is Wednesday, but",00:02:12,2,And might generate example today Wednesday
1719,00:02:18,4,Statistical Language Models (00-17-53),3.2,it could have generated many other sequences.,00:02:16,2,could generated many sequences
1720,00:02:21,4,Statistical Language Models (00-17-53),3.2,"So for example, there are many possibilities, right?",00:02:18,2,So example many possibilities right
1721,00:02:25,4,Statistical Language Models (00-17-53),3.2,"So this, in this sense,",00:02:24,2,So sense
1722,00:02:32,4,Statistical Language Models (00-17-53),3.2,we can view our data as basically a sample observed from such a generated model.,00:02:25,2,view data basically sample observed generated model
1723,00:02:33,4,Statistical Language Models (00-17-53),3.2,So why is such a model useful?,00:02:32,2,So model useful
1724,00:02:39,4,Statistical Language Models (00-17-53),3.2,"Well, it's mainly because it can quantify the uncertainties in natural language.",00:02:33,2,Well mainly quantify uncertainties natural language
1725,00:02:41,4,Statistical Language Models (00-17-53),3.2,Where do uncertainties come from?,00:02:39,2,Where uncertainties come
1726,00:02:44,4,Statistical Language Models (00-17-53),3.2,"Well, one source is simply the ambiguity in",00:02:41,2,Well one source simply ambiguity
1727,00:02:48,4,Statistical Language Models (00-17-53),3.2,natural language that we discussed earlier in the lecture.,00:02:44,2,natural language discussed earlier lecture
1728,00:02:52,4,Statistical Language Models (00-17-53),3.2,Another source is because we don't have complete understanding.,00:02:48,2,Another source complete understanding
1729,00:02:55,4,Statistical Language Models (00-17-53),3.2,We lack all the knowledge to understand language.,00:02:52,2,We lack knowledge understand language
1730,00:02:58,4,Statistical Language Models (00-17-53),3.2,In that case there will be uncertainties as well.,00:02:55,2,In case uncertainties well
1731,00:03:01,4,Statistical Language Models (00-17-53),3.2,So let me show some examples of questions that we can answer with an average model,00:02:58,2,So let show examples questions answer average model
1732,00:03:06,4,Statistical Language Models (00-17-53),3.2,that would have an interesting application in different ways.,00:03:01,2,would interesting application different ways
1733,00:03:08,4,Statistical Language Models (00-17-53),3.2,Given that we see John and feels.,00:03:06,2,Given see John feels
1734,00:03:12,4,Statistical Language Models (00-17-53),3.2,How likely will we see happy as opposed to habit,00:03:09,2,How likely see happy opposed habit
1735,00:03:14,4,Statistical Language Models (00-17-53),3.2,as the next word in a sequence of words?,00:03:12,2,next word sequence words
1736,00:03:19,4,Statistical Language Models (00-17-53),3.2,Obviously this would be very useful speech recognition because happy and,00:03:16,2,Obviously would useful speech recognition happy
1737,00:03:23,4,Statistical Language Models (00-17-53),3.2,habit would have similar acoustical sound.,00:03:19,2,habit would similar acoustical sound
1738,00:03:25,4,Statistical Language Models (00-17-53),3.2,Acoustic signals.,00:03:23,2,Acoustic signals
1739,00:03:30,4,Statistical Language Models (00-17-53),3.2,But if we look at the language model we know that John feels happy would be,00:03:25,2,But look language model know John feels happy would
1740,00:03:32,4,Statistical Language Models (00-17-53),3.2,far more likely than John feels habit.,00:03:30,2,far likely John feels habit
1741,00:03:39,4,Statistical Language Models (00-17-53),3.2,"Another example, given that we observe baseball three times and",00:03:35,2,Another example given observe baseball three times
1742,00:03:43,4,Statistical Language Models (00-17-53),3.2,gained once in the news article how likely is it about the sports?,00:03:39,2,gained news article likely sports
1743,00:03:47,4,Statistical Language Models (00-17-53),3.2,This obviously is related to text categorization and information.,00:03:43,2,This obviously related text categorization information
1744,00:03:52,4,Statistical Language Models (00-17-53),3.2,"Also, given that a user is interested in sports news,",00:03:48,2,Also given user interested sports news
1745,00:03:55,4,Statistical Language Models (00-17-53),3.2,how likely would the user use baseball in a query?,00:03:52,2,likely would user use baseball query
1746,00:04:00,4,Statistical Language Models (00-17-53),3.2,Now this is clearly related to the query that we discussed in the previous lecture.,00:03:55,2,Now clearly related query discussed previous lecture
1747,00:04:05,4,Statistical Language Models (00-17-53),3.2,So now let's look at the simplest language model.,00:04:02,2,So let look simplest language model
1748,00:04:07,4,Statistical Language Models (00-17-53),3.2,"Called a lan, unigram language model.",00:04:05,2,Called lan unigram language model
1749,00:04:09,4,Statistical Language Models (00-17-53),3.2,"In such a case,",00:04:07,2,In case
1750,00:04:13,4,Statistical Language Models (00-17-53),3.2,we assume that we generate the text by generating each word independently.,00:04:09,2,assume generate text generating word independently
1751,00:04:19,4,Statistical Language Models (00-17-53),3.2,So this means the probability of a sequence of words would be then,00:04:14,2,So means probability sequence words would
1752,00:04:21,4,Statistical Language Models (00-17-53),3.2,the product of the probability of each word.,00:04:19,2,product probability word
1753,00:04:25,4,Statistical Language Models (00-17-53),3.2,"Now normally they are not independent, right?",00:04:21,2,Now normally independent right
1754,00:04:29,4,Statistical Language Models (00-17-53),3.2,So if you have seen a word like language.,00:04:25,2,So seen word like language
1755,00:04:32,4,Statistical Language Models (00-17-53),3.2,"Now, we'll make it far more likely to observe model",00:04:29,2,Now make far likely observe model
1756,00:04:33,4,Statistical Language Models (00-17-53),3.2,than if you haven't seen language.,00:04:32,2,seen language
1757,00:04:37,4,Statistical Language Models (00-17-53),3.2,So this assumption is not necessary sure but,00:04:35,2,So assumption necessary sure
1758,00:04:39,4,Statistical Language Models (00-17-53),3.2,we'll make this assumption to simplify the model.,00:04:37,2,make assumption simplify model
1759,00:04:47,4,Statistical Language Models (00-17-53),3.2,"So now, the model has precisely n parameters, where n is vocabulary size.",00:04:41,2,So model precisely n parameters n vocabulary size
1760,00:04:51,4,Statistical Language Models (00-17-53),3.2,"We have one probability for each word, and all these probabilities must sum to 1.",00:04:47,2,We one probability word probabilities must sum 1
1761,00:04:57,4,Statistical Language Models (00-17-53),3.2,"So strictly speaking, we actually have N minus 1 parameters.",00:04:51,2,So strictly speaking actually N minus 1 parameters
1762,00:05:00,4,Statistical Language Models (00-17-53),3.2,"As I said,",00:05:00,2,As I said
1763,00:05:06,4,Statistical Language Models (00-17-53),3.2,text can be then be assumed to be a sample drawn from this word distribution.,00:05:00,2,text assumed sample drawn word distribution
1764,00:05:12,4,Statistical Language Models (00-17-53),3.2,"So for example, now we can ask the device, or the model,",00:05:08,2,So example ask device model
1765,00:05:18,4,Statistical Language Models (00-17-53),3.2,to stochastically generate the words for us instead of in sequences.,00:05:12,2,stochastically generate words us instead sequences
1766,00:05:21,4,Statistical Language Models (00-17-53),3.2,"So instead of giving a whole sequence like today is Wednesday,",00:05:18,2,So instead giving whole sequence like today Wednesday
1767,00:05:23,4,Statistical Language Models (00-17-53),3.2,it now gives us just one word.,00:05:21,2,gives us one word
1768,00:05:26,4,Statistical Language Models (00-17-53),3.2,And we can get all kinds of words.,00:05:23,2,And get kinds words
1769,00:05:29,4,Statistical Language Models (00-17-53),3.2,And we can assemble these words in a sequence.,00:05:26,2,And assemble words sequence
1770,00:05:33,4,Statistical Language Models (00-17-53),3.2,"So, that would still allows you to compute the probability of today is Wed",00:05:29,2,So would still allows compute probability today Wed
1771,00:05:36,4,Statistical Language Models (00-17-53),3.2,as the product of the three probabilities.,00:05:33,2,product three probabilities
1772,00:05:41,4,Statistical Language Models (00-17-53),3.2,"As you can see even though we have not asked the model to generate the,",00:05:37,2,As see even though asked model generate
1773,00:05:46,4,Statistical Language Models (00-17-53),3.2,the sequences it actually allows us to compute the probability for,00:05:41,2,sequences actually allows us compute probability
1774,00:05:48,4,Statistical Language Models (00-17-53),3.2,all the sequences.,00:05:46,2,sequences
1775,00:05:53,4,Statistical Language Models (00-17-53),3.2,But this model now only needs N parameters to characterize.,00:05:48,2,But model needs N parameters characterize
1776,00:05:56,4,Statistical Language Models (00-17-53),3.2,That means if we specify all the probabilities for,00:05:53,2,That means specify probabilities
1777,00:06:01,4,Statistical Language Models (00-17-53),3.2,all the words then the model's behavior is completely specified.,00:05:56,2,words model behavior completely specified
1778,00:06:05,4,Statistical Language Models (00-17-53),3.2,"Whereas if you, we don't make this assumption we would have to specify.",00:06:01,2,Whereas make assumption would specify
1779,00:06:09,4,Statistical Language Models (00-17-53),3.2,Find probabilities for all kinds of combinations of words in sequences.,00:06:05,2,Find probabilities kinds combinations words sequences
1780,00:06:16,4,Statistical Language Models (00-17-53),3.2,"So by making this assumption, it makes it much easier to estimate these parameters.",00:06:11,2,So making assumption makes much easier estimate parameters
1781,00:06:18,4,Statistical Language Models (00-17-53),3.2,So let's see a specific example here.,00:06:16,2,So let see specific example
1782,00:06:25,4,Statistical Language Models (00-17-53),3.2,Here I show two unigram lambda models with some probabilities and,00:06:19,2,Here I show two unigram lambda models probabilities
1783,00:06:27,4,Statistical Language Models (00-17-53),3.2,these are high probability words that are shown on top.,00:06:25,2,high probability words shown top
1784,00:06:33,4,Statistical Language Models (00-17-53),3.2,The first one clearly suggests the topic of text mining,00:06:29,2,The first one clearly suggests topic text mining
1785,00:06:37,4,Statistical Language Models (00-17-53),3.2,because the high probability words are all related to this topic.,00:06:33,2,high probability words related topic
1786,00:06:38,4,Statistical Language Models (00-17-53),3.2,The second one is more related to health.,00:06:37,2,The second one related health
1787,00:06:44,4,Statistical Language Models (00-17-53),3.2,"Now, we can then ask the question how likely we'll observe a particular text",00:06:39,2,Now ask question likely observe particular text
1788,00:06:46,4,Statistical Language Models (00-17-53),3.2,from each of these three models.,00:06:44,2,three models
1789,00:06:49,4,Statistical Language Models (00-17-53),3.2,"Now suppose with sample words to form the document,",00:06:46,2,Now suppose sample words form document
1790,00:06:53,4,Statistical Language Models (00-17-53),3.2,let's say we take the first distribution which are the sample words.,00:06:49,2,let say take first distribution sample words
1791,00:06:56,4,Statistical Language Models (00-17-53),3.2,What words do you think it would be generated or maybe text?,00:06:53,2,What words think would generated maybe text
1792,00:06:58,4,Statistical Language Models (00-17-53),3.2,Or maybe mining maybe another word?,00:06:56,2,Or maybe mining maybe another word
1793,00:06:58,4,Statistical Language Models (00-17-53),3.2,"Even food,",00:06:58,2,Even food
1794,00:07:02,4,Statistical Language Models (00-17-53),3.2,"which has a very small probability, might still be able to show up.",00:06:58,2,small probability might still able show
1795,00:07:06,4,Statistical Language Models (00-17-53),3.2,"But in general, high probability words will likely show up more often.",00:07:03,2,But general high probability words likely show often
1796,00:07:11,4,Statistical Language Models (00-17-53),3.2,So we can imagine a generated text that looks like text mining.,00:07:08,2,So imagine generated text looks like text mining
1797,00:07:14,4,Statistical Language Models (00-17-53),3.2,"A factor with a small probability,",00:07:12,2,A factor small probability
1798,00:07:19,4,Statistical Language Models (00-17-53),3.2,you might be able to actually generate the actual text mining paper that,00:07:14,2,might able actually generate actual text mining paper
1799,00:07:24,4,Statistical Language Models (00-17-53),3.2,"would actually be meaningful, although the probability would be very, very small.",00:07:19,2,would actually meaningful although probability would small
1800,00:07:30,4,Statistical Language Models (00-17-53),3.2,"In the extreme case, you might imagine we might be able to generate a,",00:07:26,2,In extreme case might imagine might able generate
1801,00:07:35,4,Statistical Language Models (00-17-53),3.2,"a text paper, text mining paper that would be accepted by a major conference.",00:07:30,2,text paper text mining paper would accepted major conference
1802,00:07:38,4,Statistical Language Models (00-17-53),3.2,And in that case the probability would be even smaller.,00:07:35,2,And case probability would even smaller
1803,00:07:41,4,Statistical Language Models (00-17-53),3.2,"For instance nonzero probability,",00:07:38,2,For instance nonzero probability
1804,00:07:45,4,Statistical Language Models (00-17-53),3.2,if we assume none of the words will have a nonzero probability.,00:07:41,2,assume none words nonzero probability
1805,00:07:51,4,Statistical Language Models (00-17-53),3.2,"Similarly from the second topic, we can imagine we can generate a food and",00:07:47,2,Similarly second topic imagine generate food
1806,00:07:51,4,Statistical Language Models (00-17-53),3.2,nutrition paper.,00:07:51,2,nutrition paper
1807,00:07:58,4,Statistical Language Models (00-17-53),3.2,That doesn't mean we cannot generate this paper from text mining distribution.,00:07:51,2,That mean cannot generate paper text mining distribution
1808,00:08:05,4,Statistical Language Models (00-17-53),3.2,"We can, but the probability would be very, very small, maybe smaller than even",00:07:59,2,We probability would small maybe smaller even
1809,00:08:09,4,Statistical Language Models (00-17-53),3.2,generating a paper that can be accepted by a major conference on text mining.,00:08:05,2,generating paper accepted major conference text mining
1810,00:08:12,4,Statistical Language Models (00-17-53),3.2,"So the point of here is that given a distribution,",00:08:10,2,So point given distribution
1811,00:08:18,4,Statistical Language Models (00-17-53),3.2,we can talk about the probability of observing a certain kind of text.,00:08:13,2,talk probability observing certain kind text
1812,00:08:21,4,Statistical Language Models (00-17-53),3.2,Some text would have higher probabilities than others.,00:08:18,2,Some text would higher probabilities others
1813,00:08:23,4,Statistical Language Models (00-17-53),3.2,"Now, let's look at the problem in a different way.",00:08:21,2,Now let look problem different way
1814,00:08:28,4,Statistical Language Models (00-17-53),3.2,"Supposedly, we now have available a particular document.",00:08:23,2,Supposedly available particular document
1815,00:08:31,4,Statistical Language Models (00-17-53),3.2,"In this case, maybe the abstract or the text mining paper, and",00:08:28,2,In case maybe abstract text mining paper
1816,00:08:34,4,Statistical Language Models (00-17-53),3.2,we see these word accounts here.,00:08:31,2,see word accounts
1817,00:08:35,4,Statistical Language Models (00-17-53),3.2,The total number of words is 100.,00:08:34,2,The total number words 100
1818,00:08:39,4,Statistical Language Models (00-17-53),3.2,Now the question you ask here is a estimation question.,00:08:35,2,Now question ask estimation question
1819,00:08:42,4,Statistical Language Models (00-17-53),3.2,"We can ask the question, which model,",00:08:39,2,We ask question model
1820,00:08:46,4,Statistical Language Models (00-17-53),3.2,"which word distribution has been used to, to generate this text.",00:08:42,2,word distribution used generate text
1821,00:08:51,4,Statistical Language Models (00-17-53),3.2,Assuming the text has been generated by assembling words from the distribution.,00:08:46,2,Assuming text generated assembling words distribution
1822,00:08:53,4,Statistical Language Models (00-17-53),3.2,So what would be your guess?,00:08:51,2,So would guess
1823,00:08:58,4,Statistical Language Models (00-17-53),3.2,"What have to decide what probabilities test, mining, et cetera would have.",00:08:53,2,What decide probabilities test mining et cetera would
1824,00:09:05,4,Statistical Language Models (00-17-53),3.2,So pause a view for a second and try to think about your best guess.,00:09:01,2,So pause view second try think best guess
1825,00:09:14,4,Statistical Language Models (00-17-53),3.2,"If you're like a lot of people you would have guessed that well,",00:09:09,2,If like lot people would guessed well
1826,00:09:18,4,Statistical Language Models (00-17-53),3.2,my best guess is text has a probability of 10 out of 100,00:09:14,2,best guess text probability 10 100
1827,00:09:23,4,Statistical Language Models (00-17-53),3.2,because I have seen text ten times and there are a total of 100 words.,00:09:18,2,I seen text ten times total 100 words
1828,00:09:25,4,Statistical Language Models (00-17-53),3.2,"So we simply noticed, normalize these counts.",00:09:23,2,So simply noticed normalize counts
1829,00:09:29,4,Statistical Language Models (00-17-53),3.2,And that's in fact [INAUDIBLE] justified.,00:09:27,2,And fact INAUDIBLE justified
1830,00:09:33,4,Statistical Language Models (00-17-53),3.2,And your intuition is consistent with mathematical derivation.,00:09:29,2,And intuition consistent mathematical derivation
1831,00:09:36,4,Statistical Language Models (00-17-53),3.2,And this is called a maximum likelihood [INAUDIBLE].,00:09:33,2,And called maximum likelihood INAUDIBLE
1832,00:09:40,4,Statistical Language Models (00-17-53),3.2,"In this estimator, we'll assume that the parameter settings,.",00:09:36,2,In estimator assume parameter settings
1833,00:09:44,4,Statistical Language Models (00-17-53),3.2,Are those that would give our observer the maximum probability.,00:09:40,2,Are would give observer maximum probability
1834,00:09:47,4,Statistical Language Models (00-17-53),3.2,"That means if we change these probabilities,",00:09:44,2,That means change probabilities
1835,00:09:53,4,Statistical Language Models (00-17-53),3.2,then the probability of observing the particular text would be somewhat smaller.,00:09:47,2,probability observing particular text would somewhat smaller
1836,00:09:58,4,Statistical Language Models (00-17-53),3.2,So we can see this has a very simple formula.,00:09:55,2,So see simple formula
1837,00:10:03,4,Statistical Language Models (00-17-53),3.2,"Basically, we just need to look at the count of a word",00:09:58,2,Basically need look count word
1838,00:10:07,4,Statistical Language Models (00-17-53),3.2,in the document and then divide it by the total number of words in the document.,00:10:03,2,document divide total number words document
1839,00:10:08,4,Statistical Language Models (00-17-53),3.2,About the length.,00:10:07,2,About length
1840,00:10:10,4,Statistical Language Models (00-17-53),3.2,Normalize the frequency.,00:10:09,2,Normalize frequency
1841,00:10:12,4,Statistical Language Models (00-17-53),3.2,"Well a consequence of this,",00:10:11,2,Well consequence
1842,00:10:18,4,Statistical Language Models (00-17-53),3.2,"is of course, we're going to assign 0 probabilities to unseen words.",00:10:12,2,course going assign 0 probabilities unseen words
1843,00:10:20,4,Statistical Language Models (00-17-53),3.2,"If we have an observed word,",00:10:18,2,If observed word
1844,00:10:25,4,Statistical Language Models (00-17-53),3.2,there will be no incentive to assign a non-0 probability using this approach.,00:10:20,2,incentive assign non 0 probability using approach
1845,00:10:26,4,Statistical Language Models (00-17-53),3.2,Why?,00:10:25,2,Why
1846,00:10:30,4,Statistical Language Models (00-17-53),3.2,Because that would take away probability mass for this observed words.,00:10:26,2,Because would take away probability mass observed words
1847,00:10:34,4,Statistical Language Models (00-17-53),3.2,And that obviously wouldn't maximize the probability of this,00:10:30,2,And obviously maximize probability
1848,00:10:37,4,Statistical Language Models (00-17-53),3.2,particular observed [INAUDIBLE] data.,00:10:34,2,particular observed INAUDIBLE data
1849,00:10:42,4,Statistical Language Models (00-17-53),3.2,But one can still question whether this is our best estimator.,00:10:37,2,But one still question whether best estimator
1850,00:10:47,4,Statistical Language Models (00-17-53),3.2,"Well, the answer depends on what kind of model you want to find, right?",00:10:42,2,Well answer depends kind model want find right
1851,00:10:52,4,Statistical Language Models (00-17-53),3.2,This is made if it's a best model based on this particular layer.,00:10:47,2,This made best model based particular layer
1852,00:10:56,4,Statistical Language Models (00-17-53),3.2,But if you're interested in a model that can explain the content of the four,00:10:52,2,But interested model explain content four
1853,00:11:01,4,Statistical Language Models (00-17-53),3.2,"paper of, for this abstract, then you might have a second thought, right?",00:10:56,2,paper abstract might second thought right
1854,00:11:08,4,Statistical Language Models (00-17-53),3.2,So for one thing there should be other things in the body of that article.,00:11:01,2,So one thing things body article
1855,00:11:12,4,Statistical Language Models (00-17-53),3.2,"So they should not have, zero probabilities,",00:11:08,2,So zero probabilities
1856,00:11:14,4,Statistical Language Models (00-17-53),3.2,even though they are not observing the abstract.,00:11:12,2,even though observing abstract
1857,00:11:18,4,Statistical Language Models (00-17-53),3.2,"We're going to cover this later, in,",00:11:14,2,We going cover later
1858,00:11:24,4,Statistical Language Models (00-17-53),3.2,discussing the query model.,00:11:18,2,discussing query model
1859,00:11:29,4,Statistical Language Models (00-17-53),3.2,"So, let's take a look at some possible uses of these language models.",00:11:24,2,So let take look possible uses language models
1860,00:11:32,4,Statistical Language Models (00-17-53),3.2,One use is simply to use it to represent the topics.,00:11:29,2,One use simply use represent topics
1861,00:11:37,4,Statistical Language Models (00-17-53),3.2,So here it shows some general English background that text.,00:11:32,2,So shows general English background text
1862,00:11:39,4,Statistical Language Models (00-17-53),3.2,We can use this text to estimate a language model.,00:11:37,2,We use text estimate language model
1863,00:11:42,4,Statistical Language Models (00-17-53),3.2,And the model might look like this.,00:11:39,2,And model might look like
1864,00:11:45,4,Statistical Language Models (00-17-53),3.2,Right? So on the top we'll have those all common,00:11:42,2,Right So top common
1865,00:11:51,4,Statistical Language Models (00-17-53),3.2,"words, is we, is, and then we'll see some common words like these,",00:11:45,2,words see common words like
1866,00:11:55,4,Statistical Language Models (00-17-53),3.2,"and then some very, very real words in the bottom.",00:11:51,2,real words bottom
1867,00:11:57,4,Statistical Language Models (00-17-53),3.2,This is the background image model.,00:11:55,2,This background image model
1868,00:12:02,4,Statistical Language Models (00-17-53),3.2,"It represents the frequency on words, in English in general, right?",00:11:57,2,It represents frequency words English general right
1869,00:12:04,4,Statistical Language Models (00-17-53),3.2,This is the background model.,00:12:02,2,This background model
1870,00:12:06,4,Statistical Language Models (00-17-53),3.2,"Now, let's look at another text.",00:12:04,2,Now let look another text
1871,00:12:09,4,Statistical Language Models (00-17-53),3.2,"Maybe this time, we'll look at Computer Science research papers.",00:12:06,2,Maybe time look Computer Science research papers
1872,00:12:15,4,Statistical Language Models (00-17-53),3.2,"So we have a correction of computer science research papers, we do again,",00:12:11,2,So correction computer science research papers
1873,00:12:19,4,Statistical Language Models (00-17-53),3.2,we can just use the maximum where we simply normalize the frequencies.,00:12:15,2,use maximum simply normalize frequencies
1874,00:12:23,4,Statistical Language Models (00-17-53),3.2,"Now, in this case, we look at the distribution, that looks like this.",00:12:20,2,Now case look distribution looks like
1875,00:12:28,4,Statistical Language Models (00-17-53),3.2,"On the top, it looks similar, because these words occur everywhere,",00:12:23,2,On top looks similar words occur everywhere
1876,00:12:29,4,Statistical Language Models (00-17-53),3.2,they are very common.,00:12:28,2,common
1877,00:12:34,4,Statistical Language Models (00-17-53),3.2,But as we go down we'll see words that are more related to computer science.,00:12:29,2,But go see words related computer science
1878,00:12:36,4,Statistical Language Models (00-17-53),3.2,"Computer, or software, or text et cetera.",00:12:34,2,Computer software text et cetera
1879,00:12:43,4,Statistical Language Models (00-17-53),3.2,"So, although here, we might also see these words, for example, computer.",00:12:38,2,So although might also see words example computer
1880,00:12:47,4,Statistical Language Models (00-17-53),3.2,"But, we can imagine the probability here is much smaller than the probability here.",00:12:43,2,But imagine probability much smaller probability
1881,00:12:50,4,Statistical Language Models (00-17-53),3.2,"And we will see many other words here that,",00:12:47,2,And see many words
1882,00:12:53,4,Statistical Language Models (00-17-53),3.2,that would be more common in general in English.,00:12:50,2,would common general English
1883,00:12:58,4,Statistical Language Models (00-17-53),3.2,"So, you can see this distribution characterizes a topic",00:12:55,2,So see distribution characterizes topic
1884,00:13:00,4,Statistical Language Models (00-17-53),3.2,of the corresponding text.,00:12:58,2,corresponding text
1885,00:13:03,4,Statistical Language Models (00-17-53),3.2,"We can look at the, even the smaller text.",00:13:00,2,We look even smaller text
1886,00:13:06,4,Statistical Language Models (00-17-53),3.2,"So, in this case let's look at the text mining paper.",00:13:03,2,So case let look text mining paper
1887,00:13:09,4,Statistical Language Models (00-17-53),3.2,Now if we do the same we have another.,00:13:06,2,Now another
1888,00:13:13,4,Statistical Language Models (00-17-53),3.2,Distribution again the can be expected to occur on the top.,00:13:09,2,Distribution expected occur top
1889,00:13:16,4,Statistical Language Models (00-17-53),3.2,"Soon we will see text, mining, association, clustering,",00:13:13,2,Soon see text mining association clustering
1890,00:13:21,4,Statistical Language Models (00-17-53),3.2,these words have relatively high probabilities in contrast,00:13:16,2,words relatively high probabilities contrast
1891,00:13:27,4,Statistical Language Models (00-17-53),3.2,in this distribution has relatively small probability.,00:13:21,2,distribution relatively small probability
1892,00:13:30,4,Statistical Language Models (00-17-53),3.2,"So this means,",00:13:27,2,So means
1893,00:13:33,4,Statistical Language Models (00-17-53),3.2,again based on different text data that we can have a different model.,00:13:30,2,based different text data different model
1894,00:13:36,4,Statistical Language Models (00-17-53),3.2,And model captures the topic.,00:13:33,2,And model captures topic
1895,00:13:42,4,Statistical Language Models (00-17-53),3.2,So we call this document an LM model and we call this collection LM model.,00:13:38,2,So call document LM model call collection LM model
1896,00:13:46,4,Statistical Language Models (00-17-53),3.2,"And later, we'll see how they're used in a retrieval function.",00:13:42,2,And later see used retrieval function
1897,00:13:50,4,Statistical Language Models (00-17-53),3.2,"But now, let's look at the, another use of this model.",00:13:47,2,But let look another use model
1898,00:13:55,4,Statistical Language Models (00-17-53),3.2,Can we statistically find what words are semantically related to computer?,00:13:50,2,Can statistically find words semantically related computer
1899,00:13:58,4,Statistical Language Models (00-17-53),3.2,Now how do we find such words?,00:13:56,2,Now find words
1900,00:14:02,4,Statistical Language Models (00-17-53),3.2,Well our first thought is well let's take a look at the text that match.,00:13:58,2,Well first thought well let take look text match
1901,00:14:04,4,Statistical Language Models (00-17-53),3.2,Computer.,00:14:03,2,Computer
1902,00:14:08,4,Statistical Language Models (00-17-53),3.2,So we can take a look at all the documents that contain the word computer.,00:14:04,2,So take look documents contain word computer
1903,00:14:10,4,Statistical Language Models (00-17-53),3.2,Let's build a language model.,00:14:08,2,Let build language model
1904,00:14:13,4,Statistical Language Models (00-17-53),3.2,"Okay, see what words we see there.",00:14:10,2,Okay see words see
1905,00:14:19,4,Statistical Language Models (00-17-53),3.2,"Well, not surprisingly, we see these common words on top as we always do.",00:14:13,2,Well surprisingly see common words top always
1906,00:14:21,4,Statistical Language Models (00-17-53),3.2,"So in this case, this language model gives us the.",00:14:19,2,So case language model gives us
1907,00:14:25,4,Statistical Language Models (00-17-53),3.2,Conditional probability of seeing a word in the context of computer.,00:14:21,2,Conditional probability seeing word context computer
1908,00:14:29,4,Statistical Language Models (00-17-53),3.2,And these common words will naturally have high probabilities.,00:14:25,2,And common words naturally high probabilities
1909,00:14:31,4,Statistical Language Models (00-17-53),3.2,"Other words will see computer itself, and",00:14:29,2,Other words see computer
1910,00:14:35,4,Statistical Language Models (00-17-53),3.2,software will have relatively high probabilities.,00:14:31,2,software relatively high probabilities
1911,00:14:38,4,Statistical Language Models (00-17-53),3.2,"But we, if we just use this model we cannot.",00:14:35,2,But use model cannot
1912,00:14:42,4,Statistical Language Models (00-17-53),3.2,I just say all these words are semantically related to computer.,00:14:38,2,I say words semantically related computer
1913,00:14:50,4,Statistical Language Models (00-17-53),3.2,So intuitively what we'd like to get rid of these these common words.,00:14:43,2,So intuitively like get rid common words
1914,00:14:51,4,Statistical Language Models (00-17-53),3.2,How can we do that?,00:14:50,2,How
1915,00:14:55,4,Statistical Language Models (00-17-53),3.2,It turns out that it's possible to use language model to do that.,00:14:52,2,It turns possible use language model
1916,00:15:00,4,Statistical Language Models (00-17-53),3.2,Now I suggest you think about that.,00:14:57,2,Now I suggest think
1917,00:15:03,4,Statistical Language Models (00-17-53),3.2,So how can we know what words are very common so,00:15:00,2,So know words common
1918,00:15:06,4,Statistical Language Models (00-17-53),3.2,that we want to kind of get rid of them.,00:15:03,2,want kind get rid
1919,00:15:10,4,Statistical Language Models (00-17-53),3.2,What model will tell us that?,00:15:07,2,What model tell us
1920,00:15:14,4,Statistical Language Models (00-17-53),3.2,"Well, maybe you can think about that.",00:15:10,2,Well maybe think
1921,00:15:18,4,Statistical Language Models (00-17-53),3.2,So the background language model precisely tells us this information.,00:15:14,2,So background language model precisely tells us information
1922,00:15:21,4,Statistical Language Models (00-17-53),3.2,It tells us what words are common in general.,00:15:18,2,It tells us words common general
1923,00:15:23,4,Statistical Language Models (00-17-53),3.2,"So if we use this background model,",00:15:21,2,So use background model
1924,00:15:28,4,Statistical Language Models (00-17-53),3.2,we would know that these words are common words in general.,00:15:23,2,would know words common words general
1925,00:15:32,4,Statistical Language Models (00-17-53),3.2,So it's not surprising to observe them in the context of computer.,00:15:28,2,So surprising observe context computer
1926,00:15:35,4,Statistical Language Models (00-17-53),3.2,Whereas computer has a very small probability in general.,00:15:32,2,Whereas computer small probability general
1927,00:15:41,4,Statistical Language Models (00-17-53),3.2,"So it's very surprising that we have seen computer in, with this probability.",00:15:35,2,So surprising seen computer probability
1928,00:15:42,4,Statistical Language Models (00-17-53),3.2,And the same is true for software.,00:15:41,2,And true software
1929,00:15:48,4,Statistical Language Models (00-17-53),3.2,So then we can use these two models to somehow figure out.,00:15:44,2,So use two models somehow figure
1930,00:15:52,4,Statistical Language Models (00-17-53),3.2,The words that are related to computer.,00:15:48,2,The words related computer
1931,00:15:57,4,Statistical Language Models (00-17-53),3.2,For example we can simply take the ratio of these two probabilities and normalize,00:15:52,2,For example simply take ratio two probabilities normalize
1932,00:16:02,4,Statistical Language Models (00-17-53),3.2,the top of the model by the probability of the word in the background model.,00:15:57,2,top model probability word background model
1933,00:16:06,4,Statistical Language Models (00-17-53),3.2,"So if we do that, we take the ratio, we'll see that then on the top,",00:16:02,2,So take ratio see top
1934,00:16:09,4,Statistical Language Models (00-17-53),3.2,"computer, is ramped, and then followed by software,",00:16:06,2,computer ramped followed software
1935,00:16:14,4,Statistical Language Models (00-17-53),3.2,"program, all these words related to computer.",00:16:09,2,program words related computer
1936,00:16:17,4,Statistical Language Models (00-17-53),3.2,"Because they occur very frequently in the context of computer, but",00:16:14,2,Because occur frequently context computer
1937,00:16:20,4,Statistical Language Models (00-17-53),3.2,not frequently in whole connection.,00:16:17,2,frequently whole connection
1938,00:16:23,4,Statistical Language Models (00-17-53),3.2,Where as these common words will not have a high probability.,00:16:20,2,Where common words high probability
1939,00:16:27,4,Statistical Language Models (00-17-53),3.2,"In fact, they have a ratio of about one down there.",00:16:23,2,In fact ratio one
1940,00:16:30,4,Statistical Language Models (00-17-53),3.2,Because they are not really related to computer.,00:16:27,2,Because really related computer
1941,00:16:35,4,Statistical Language Models (00-17-53),3.2,By taking the same ball of text that contains the computer we don't,00:16:30,2,By taking ball text contains computer
1942,00:16:40,4,Statistical Language Models (00-17-53),3.2,really see more occurrences of that in general.,00:16:35,2,really see occurrences general
1943,00:16:43,4,Statistical Language Models (00-17-53),3.2,"So this shows that even with this simple LM models,",00:16:40,2,So shows even simple LM models
1944,00:16:46,4,Statistical Language Models (00-17-53),3.2,we can do some limited analysis of semantics.,00:16:43,2,limited analysis semantics
1945,00:16:53,4,Statistical Language Models (00-17-53),3.2,"So in this lecture, we talked about, language model,",00:16:48,2,So lecture talked language model
1946,00:16:56,4,Statistical Language Models (00-17-53),3.2,which is basically a probability distribution over the text.,00:16:53,2,basically probability distribution text
1947,00:17:00,4,Statistical Language Models (00-17-53),3.2,We talked about the simplistic language model called unigram language model.,00:16:56,2,We talked simplistic language model called unigram language model
1948,00:17:02,4,Statistical Language Models (00-17-53),3.2,Which is also just a word distribution.,00:17:00,2,Which also word distribution
1949,00:17:05,4,Statistical Language Models (00-17-53),3.2,We talked about the two uses of a language model.,00:17:02,2,We talked two uses language model
1950,00:17:10,4,Statistical Language Models (00-17-53),3.2,"One is to represent the, the topic in a document, in a classing or in general.",00:17:05,2,One represent topic document classing general
1951,00:17:12,4,Statistical Language Models (00-17-53),3.2,The other is discover word associations.,00:17:10,2,The discover word associations
1952,00:17:18,4,Statistical Language Models (00-17-53),3.2,In the next lecture we're going to talk about the how,00:17:16,2,In next lecture going talk
1953,00:17:21,4,Statistical Language Models (00-17-53),3.2,language model can be used to design a retrieval function.,00:17:18,2,language model used design retrieval function
1954,00:17:24,4,Statistical Language Models (00-17-53),3.2,Here are two additional readings.,00:17:23,2,Here two additional readings
1955,00:17:28,4,Statistical Language Models (00-17-53),3.2,The first is a textbook on statistical and natural language processing.,00:17:24,2,The first textbook statistical natural language processing
1956,00:17:35,4,Statistical Language Models (00-17-53),3.2,The second is a article that has a survey of statistical language,00:17:30,2,The second article survey statistical language
1957,00:17:39,4,Statistical Language Models (00-17-53),3.2,models with other pointers to research work.,00:17:35,2,models pointers research work
1958,00:00:10,2,Vector Space Model- Simplest Instantiation,1.6,[SOUND].,00:00:00,6,SOUND
1959,00:00:16,2,Vector Space Model- Simplest Instantiation,1.6,"In this lecture, we're going to talk about how to instantiate a vector space model,",00:00:11,6,In lecture going talk instantiate vector space model
1960,00:00:19,2,Vector Space Model- Simplest Instantiation,1.6,so that we can get a very specific ranking function.,00:00:16,6,get specific ranking function
1961,00:00:27,2,Vector Space Model- Simplest Instantiation,1.6,"So this is the, to continue the discussion of the vector space model.",00:00:23,6,So continue discussion vector space model
1962,00:00:32,2,Vector Space Model- Simplest Instantiation,1.6,Which is one particular approach to design ranking function.,00:00:27,6,Which one particular approach design ranking function
1963,00:00:38,2,Vector Space Model- Simplest Instantiation,1.6,And we are going to talk about how we use the general framework of,00:00:34,6,And going talk use general framework
1964,00:00:40,2,Vector Space Model- Simplest Instantiation,1.6,the the vector space model.,00:00:38,6,vector space model
1965,00:00:48,2,Vector Space Model- Simplest Instantiation,1.6,As a guidance to instantiate the framework to derive a specific ranking function.,00:00:40,6,As guidance instantiate framework derive specific ranking function
1966,00:00:53,2,Vector Space Model- Simplest Instantiation,1.6,And we're going to cover the simplest instantiation of the framework.,00:00:48,6,And going cover simplest instantiation framework
1967,00:00:58,2,Vector Space Model- Simplest Instantiation,1.6,So as we discussed in the previous lecture.,00:00:55,6,So discussed previous lecture
1968,00:01:00,2,Vector Space Model- Simplest Instantiation,1.6,The vector space model is really a framework.,00:00:58,6,The vector space model really framework
1969,00:01:02,2,Vector Space Model- Simplest Instantiation,1.6,"It isn't, didn't say.",00:01:00,6,It say
1970,00:01:11,2,Vector Space Model- Simplest Instantiation,1.6,"As we discussed in the previous lecture, vector space model is really a framework.",00:01:05,6,As discussed previous lecture vector space model really framework
1971,00:01:13,2,Vector Space Model- Simplest Instantiation,1.6,"It doesn't, say many things.",00:01:11,6,It say many things
1972,00:01:17,2,Vector Space Model- Simplest Instantiation,1.6,So for example here it shows that it did not say,00:01:14,6,So example shows say
1973,00:01:19,2,Vector Space Model- Simplest Instantiation,1.6,how we should define the dimension.,00:01:17,6,define dimension
1974,00:01:25,2,Vector Space Model- Simplest Instantiation,1.6,It also did not say how we place a documented vector in this space.,00:01:20,6,It also say place documented vector space
1975,00:01:31,2,Vector Space Model- Simplest Instantiation,1.6,It did not say how we place a query vector in this vector space.,00:01:27,6,It say place query vector vector space
1976,00:01:36,2,Vector Space Model- Simplest Instantiation,1.6,"And finally, it did not say how we should match a similarity",00:01:32,6,And finally say match similarity
1977,00:01:39,2,Vector Space Model- Simplest Instantiation,1.6,between the query vector and the document vector.,00:01:36,6,query vector document vector
1978,00:01:44,2,Vector Space Model- Simplest Instantiation,1.6,"So, you can imagine, in order to implement this model.",00:01:40,6,So imagine order implement model
1979,00:01:52,2,Vector Space Model- Simplest Instantiation,1.6,"We have to see specifically, how we are computing these vectors.",00:01:46,6,We see specifically computing vectors
1980,00:01:56,2,Vector Space Model- Simplest Instantiation,1.6,What is exactly xi and what is exactly yi?,00:01:52,6,What exactly xi exactly yi
1981,00:02:02,2,Vector Space Model- Simplest Instantiation,1.6,This will determine where we place the document vector.,00:01:58,6,This determine place document vector
1982,00:02:04,2,Vector Space Model- Simplest Instantiation,1.6,Where we place a query vector.,00:02:02,6,Where place query vector
1983,00:02:08,2,Vector Space Model- Simplest Instantiation,1.6,"And of course, we also need to say exactly what will be the similarity function.",00:02:04,6,And course also need say exactly similarity function
1984,00:02:15,2,Vector Space Model- Simplest Instantiation,1.6,So if we can provide a definition of the concepts that would,00:02:11,6,So provide definition concepts would
1985,00:02:18,2,Vector Space Model- Simplest Instantiation,1.6,"define the dimensions and these xi's, or yi's.",00:02:15,6,define dimensions xi yi
1986,00:02:22,2,Vector Space Model- Simplest Instantiation,1.6,"And then, the waits of terms for query and document.",00:02:18,6,And waits terms query document
1987,00:02:28,2,Vector Space Model- Simplest Instantiation,1.6,Then we will be able to place document vectors and,00:02:24,6,Then able place document vectors
1988,00:02:33,2,Vector Space Model- Simplest Instantiation,1.6,query vector in this well defined space.,00:02:28,6,query vector well defined space
1989,00:02:36,2,Vector Space Model- Simplest Instantiation,1.6,"And then, if we also specify similarity function,",00:02:33,6,And also specify similarity function
1990,00:02:39,2,Vector Space Model- Simplest Instantiation,1.6,then we'll have well defined ranking function.,00:02:36,6,well defined ranking function
1991,00:02:43,2,Vector Space Model- Simplest Instantiation,1.6,So let's see how we can do that.,00:02:41,6,So let see
1992,00:02:47,2,Vector Space Model- Simplest Instantiation,1.6,And think about the the simpliciter instantiation.,00:02:43,6,And think simpliciter instantiation
1993,00:02:52,2,Vector Space Model- Simplest Instantiation,1.6,"Actually, I would suggest you to pause the lecture at this point",00:02:47,6,Actually I would suggest pause lecture point
1994,00:02:54,2,Vector Space Model- Simplest Instantiation,1.6,spend a couple of minute to think about.,00:02:52,6,spend couple minute think
1995,00:02:59,2,Vector Space Model- Simplest Instantiation,1.6,Suppose you are asked to implement this idea.,00:02:54,6,Suppose asked implement idea
1996,00:03:02,2,Vector Space Model- Simplest Instantiation,1.6,You've come up with the idea of vector space model.,00:02:59,6,You come idea vector space model
1997,00:03:07,2,Vector Space Model- Simplest Instantiation,1.6,"But you still haven't figured out how to compute this vector exactly,",00:03:03,6,But still figured compute vector exactly
1998,00:03:10,2,Vector Space Model- Simplest Instantiation,1.6,how to define this similarity function.,00:03:07,6,define similarity function
1999,00:03:10,2,Vector Space Model- Simplest Instantiation,1.6,What would you do?,00:03:10,6,What would
2000,00:03:15,2,Vector Space Model- Simplest Instantiation,1.6,"So think for a couple of minutes and then, proceed.",00:03:12,6,So think couple minutes proceed
2001,00:03:26,2,Vector Space Model- Simplest Instantiation,1.6,So let's think about some simplest ways of instantiating this vector space model.,00:03:20,6,So let think simplest ways instantiating vector space model
2002,00:03:28,2,Vector Space Model- Simplest Instantiation,1.6,"First, how do we define a dimension.",00:03:26,6,First define dimension
2003,00:03:31,2,Vector Space Model- Simplest Instantiation,1.6,Well the obvious choice is we use,00:03:28,6,Well obvious choice use
2004,00:03:35,2,Vector Space Model- Simplest Instantiation,1.6,each word in our vocabulary to define a dimension.,00:03:31,6,word vocabulary define dimension
2005,00:03:39,2,Vector Space Model- Simplest Instantiation,1.6,"And a whole issue that there are n words in our vocabulary,",00:03:35,6,And whole issue n words vocabulary
2006,00:03:41,2,Vector Space Model- Simplest Instantiation,1.6,therefore there are n dimensions.,00:03:39,6,therefore n dimensions
2007,00:03:42,2,Vector Space Model- Simplest Instantiation,1.6,Each word defines one dimension.,00:03:41,6,Each word defines one dimension
2008,00:03:46,2,Vector Space Model- Simplest Instantiation,1.6,And this is basically the Bag of Words Instantiation.,00:03:42,6,And basically Bag Words Instantiation
2009,00:03:52,2,Vector Space Model- Simplest Instantiation,1.6,Now let's look at how we place vectors in this space.,00:03:48,6,Now let look place vectors space
2010,00:03:57,2,Vector Space Model- Simplest Instantiation,1.6,"Again here, the simplest of strategy is to",00:03:54,6,Again simplest strategy
2011,00:04:03,2,Vector Space Model- Simplest Instantiation,1.6,use a bit vector to represent both a query and a document.,00:03:58,6,use bit vector represent query document
2012,00:04:07,2,Vector Space Model- Simplest Instantiation,1.6,And that means each element xi and,00:04:04,6,And means element xi
2013,00:04:12,2,Vector Space Model- Simplest Instantiation,1.6,yi would be taking a value of either zero or one.,00:04:07,6,yi would taking value either zero one
2014,00:04:14,2,Vector Space Model- Simplest Instantiation,1.6,"When it's one,",00:04:13,6,When one
2015,00:04:20,2,Vector Space Model- Simplest Instantiation,1.6,it means the corresponding word is present in the document or in the query.,00:04:14,6,means corresponding word present document query
2016,00:04:25,2,Vector Space Model- Simplest Instantiation,1.6,"When it's zero, it's going to mean that it's absent.",00:04:20,6,When zero going mean absent
2017,00:04:31,2,Vector Space Model- Simplest Instantiation,1.6,So you can imagine if the user types in a few word in your query.,00:04:27,6,So imagine user types word query
2018,00:04:35,2,Vector Space Model- Simplest Instantiation,1.6,"Then the query vector, we only have a few ones, many, many zeros.",00:04:31,6,Then query vector ones many many zeros
2019,00:04:41,2,Vector Space Model- Simplest Instantiation,1.6,"The document vector in general we have more ones of course,",00:04:35,6,The document vector general ones course
2020,00:04:43,2,Vector Space Model- Simplest Instantiation,1.6,but we also have many zeros.,00:04:41,6,also many zeros
2021,00:04:46,2,Vector Space Model- Simplest Instantiation,1.6,So it seems the vocabulary is generally very large.,00:04:43,6,So seems vocabulary generally large
2022,00:04:50,2,Vector Space Model- Simplest Instantiation,1.6,Many words don't really occur in a document.,00:04:46,6,Many words really occur document
2023,00:04:56,2,Vector Space Model- Simplest Instantiation,1.6,Many words will only occasionally occur in the document.,00:04:52,6,Many words occasionally occur document
2024,00:05:02,2,Vector Space Model- Simplest Instantiation,1.6,A lot of words will be absent in a particular document.,00:04:58,6,A lot words absent particular document
2025,00:05:09,2,Vector Space Model- Simplest Instantiation,1.6,"So, now we have placed the documents and the query in the vector space.",00:05:04,6,So placed documents query vector space
2026,00:05:14,2,Vector Space Model- Simplest Instantiation,1.6,Let's look at how we match up the similarity.,00:05:11,6,Let look match similarity
2027,00:05:19,2,Vector Space Model- Simplest Instantiation,1.6,"So, a commonly used similarity measure here is Dot Product.",00:05:15,6,So commonly used similarity measure Dot Product
2028,00:05:25,2,Vector Space Model- Simplest Instantiation,1.6,The dot product of two vectors is simply defined as,00:05:20,6,The dot product two vectors simply defined
2029,00:05:30,2,Vector Space Model- Simplest Instantiation,1.6,the sum of the products of the corresponding elements of the two vectors.,00:05:25,6,sum products corresponding elements two vectors
2030,00:05:39,2,Vector Space Model- Simplest Instantiation,1.6,So here we see that it's the product of x1 and the y1.,00:05:30,6,So see product x1 y1
2031,00:05:41,2,Vector Space Model- Simplest Instantiation,1.6,So here.,00:05:39,6,So
2032,00:05:44,2,Vector Space Model- Simplest Instantiation,1.6,"And then, x2 multiplied by y2.",00:05:41,6,And x2 multiplied y2
2033,00:05:47,2,Vector Space Model- Simplest Instantiation,1.6,And then finally xn multiplied by yn.,00:05:44,6,And finally xn multiplied yn
2034,00:05:48,2,Vector Space Model- Simplest Instantiation,1.6,And then we take a sum here.,00:05:47,6,And take sum
2035,00:05:52,2,Vector Space Model- Simplest Instantiation,1.6,So that's the dot product.,00:05:50,6,So dot product
2036,00:05:57,2,Vector Space Model- Simplest Instantiation,1.6,"Now we can represent this in a more general way, using a sum here.",00:05:52,6,Now represent general way using sum
2037,00:06:04,2,Vector Space Model- Simplest Instantiation,1.6,So this only one of the many different ways of matching the similarity.,00:05:58,6,So one many different ways matching similarity
2038,00:06:10,2,Vector Space Model- Simplest Instantiation,1.6,"So now we see that we have defined the, the dimensions.",00:06:04,6,So see defined dimensions
2039,00:06:13,2,Vector Space Model- Simplest Instantiation,1.6,"We have defined the, the vectors.",00:06:10,6,We defined vectors
2040,00:06:16,2,Vector Space Model- Simplest Instantiation,1.6,And we have also defined the similarity function.,00:06:13,6,And also defined similarity function
2041,00:06:19,2,Vector Space Model- Simplest Instantiation,1.6,So now we finally have the Simplest Vector Space Model.,00:06:16,6,So finally Simplest Vector Space Model
2042,00:06:24,2,Vector Space Model- Simplest Instantiation,1.6,"Which is based on the bit vector representation, dot product similarity,",00:06:20,6,Which based bit vector representation dot product similarity
2043,00:06:25,2,Vector Space Model- Simplest Instantiation,1.6,and bag of words instantiation.,00:06:24,6,bag words instantiation
2044,00:06:30,2,Vector Space Model- Simplest Instantiation,1.6,And the formula looks like this.,00:06:27,6,And formula looks like
2045,00:06:32,2,Vector Space Model- Simplest Instantiation,1.6,So this is our formula.,00:06:30,6,So formula
2046,00:06:37,2,Vector Space Model- Simplest Instantiation,1.6,"And that's actually a particular retrieval function, a ranking function all right?",00:06:32,6,And actually particular retrieval function ranking function right
2047,00:06:42,2,Vector Space Model- Simplest Instantiation,1.6,"Now, we can finally implement this function using a program language and",00:06:37,6,Now finally implement function using program language
2048,00:06:45,2,Vector Space Model- Simplest Instantiation,1.6,then rank documents for query.,00:06:42,6,rank documents query
2049,00:06:50,2,Vector Space Model- Simplest Instantiation,1.6,Now at this point you should again pause the lecture,00:06:45,6,Now point pause lecture
2050,00:06:53,2,Vector Space Model- Simplest Instantiation,1.6,to think about how we can interpret this score.,00:06:50,6,think interpret score
2051,00:06:56,2,Vector Space Model- Simplest Instantiation,1.6,So we have gone through the process of modeling the retrieval problem,00:06:53,6,So gone process modeling retrieval problem
2052,00:07:00,2,Vector Space Model- Simplest Instantiation,1.6,using a vector space model.,00:06:58,6,using vector space model
2053,00:07:02,2,Vector Space Model- Simplest Instantiation,1.6,"And then, we make assumptions.",00:07:00,6,And make assumptions
2054,00:07:09,2,Vector Space Model- Simplest Instantiation,1.6,About how we place vectors in the vector space and how we define the similarity.,00:07:03,6,About place vectors vector space define similarity
2055,00:07:14,2,Vector Space Model- Simplest Instantiation,1.6,So in the end we've got a specific retrieval function shown here.,00:07:09,6,So end got specific retrieval function shown
2056,00:07:18,2,Vector Space Model- Simplest Instantiation,1.6,Now the next step is to think about what of this individual function,00:07:15,6,Now next step think individual function
2057,00:07:19,2,Vector Space Model- Simplest Instantiation,1.6,actually makes sense?,00:07:18,6,actually makes sense
2058,00:07:24,2,Vector Space Model- Simplest Instantiation,1.6,"I, can we expect this function to actually perform well?",00:07:20,6,I expect function actually perform well
2059,00:07:27,2,Vector Space Model- Simplest Instantiation,1.6,"Where we use it to ramp it up, for use in query.",00:07:24,6,Where use ramp use query
2060,00:07:35,2,Vector Space Model- Simplest Instantiation,1.6,"So, it's worth thinking about, what is this value that we are calculating?",00:07:28,6,So worth thinking value calculating
2061,00:07:40,2,Vector Space Model- Simplest Instantiation,1.6,"So in the end, we've got a number, but what does this number mean?",00:07:35,6,So end got number number mean
2062,00:07:40,2,Vector Space Model- Simplest Instantiation,1.6,Is it meaningful?,00:07:40,6,Is meaningful
2063,00:07:44,2,Vector Space Model- Simplest Instantiation,1.6,So spend a couple minutes to think about that.,00:07:42,6,So spend couple minutes think
2064,00:07:46,2,Vector Space Model- Simplest Instantiation,1.6,"And of course,",00:07:45,6,And course
2065,00:07:52,2,Vector Space Model- Simplest Instantiation,1.6,the general question here is do you believe this is a good ranking function?,00:07:46,6,general question believe good ranking function
2066,00:07:54,2,Vector Space Model- Simplest Instantiation,1.6,Would it actually work well?,00:07:52,6,Would actually work well
2067,00:07:58,2,Vector Space Model- Simplest Instantiation,1.6,"So again, think about how to interpret this value.",00:07:54,6,So think interpret value
2068,00:08:00,2,Vector Space Model- Simplest Instantiation,1.6,Is it actually meaningful?,00:07:58,6,Is actually meaningful
2069,00:08:03,2,Vector Space Model- Simplest Instantiation,1.6,Does it mean something?,00:08:01,6,Does mean something
2070,00:08:06,2,Vector Space Model- Simplest Instantiation,1.6,So related to how well that document matches the query.,00:08:03,6,So related well document matches query
2071,00:08:11,2,Vector Space Model- Simplest Instantiation,1.6,So in order to assess whether this simplest,00:08:08,6,So order assess whether simplest
2072,00:08:15,2,Vector Space Model- Simplest Instantiation,1.6,"vector space model actually works well, let's look at the example.",00:08:11,6,vector space model actually works well let look example
2073,00:08:22,2,Vector Space Model- Simplest Instantiation,1.6,So here I show some sample documents and a simple query.,00:08:17,6,So I show sample documents simple query
2074,00:08:26,2,Vector Space Model- Simplest Instantiation,1.6,The query is news about the presidential campaign.,00:08:22,6,The query news presidential campaign
2075,00:08:28,2,Vector Space Model- Simplest Instantiation,1.6,And we have five documents here.,00:08:26,6,And five documents
2076,00:08:32,2,Vector Space Model- Simplest Instantiation,1.6,"They cover different, terms in the query.",00:08:28,6,They cover different terms query
2077,00:08:37,2,Vector Space Model- Simplest Instantiation,1.6,"And if you look at the, these documents for a moment.",00:08:34,6,And look documents moment
2078,00:08:39,2,Vector Space Model- Simplest Instantiation,1.6,You may realize that,00:08:38,6,You may realize
2079,00:08:46,2,Vector Space Model- Simplest Instantiation,1.6,some documents are probably relevant in some cases or probably not relevant.,00:08:41,6,documents probably relevant cases probably relevant
2080,00:08:52,2,Vector Space Model- Simplest Instantiation,1.6,"Now if I ask you to rank these documents, how would you rank them?",00:08:48,6,Now I ask rank documents would rank
2081,00:08:56,2,Vector Space Model- Simplest Instantiation,1.6,This is basically our ideal ranking.,00:08:54,6,This basically ideal ranking
2082,00:08:57,2,Vector Space Model- Simplest Instantiation,1.6,Right.,00:08:56,6,Right
2083,00:09:01,2,Vector Space Model- Simplest Instantiation,1.6,When humans can examine the documents and then try to rank them.,00:08:57,6,When humans examine documents try rank
2084,00:09:07,2,Vector Space Model- Simplest Instantiation,1.6,"Now, so think for a moment and take a look at this slide.",00:09:03,6,Now think moment take look slide
2085,00:09:10,2,Vector Space Model- Simplest Instantiation,1.6,And perhaps by pausing the lecture.,00:09:07,6,And perhaps pausing lecture
2086,00:09:16,2,Vector Space Model- Simplest Instantiation,1.6,"So I think most of you would agree that d4,",00:09:12,6,So I think would agree d4
2087,00:09:19,2,Vector Space Model- Simplest Instantiation,1.6,"and d3, are probably better than others.",00:09:16,6,d3 probably better others
2088,00:09:23,2,Vector Space Model- Simplest Instantiation,1.6,Because they really cover the query well.,00:09:19,6,Because really cover query well
2089,00:09:26,2,Vector Space Model- Simplest Instantiation,1.6,"They match news, presidential, and campaign.",00:09:23,6,They match news presidential campaign
2090,00:09:33,2,Vector Space Model- Simplest Instantiation,1.6,"So, it looks like that these two documents are probably better than the others.",00:09:27,6,So looks like two documents probably better others
2091,00:09:34,2,Vector Space Model- Simplest Instantiation,1.6,They should be ranked on top.,00:09:33,6,They ranked top
2092,00:09:41,2,Vector Space Model- Simplest Instantiation,1.6,"And the other three, d1, d2, and d5, are really non-relavant.",00:09:35,6,And three d1 d2 d5 really non relavant
2093,00:09:46,2,Vector Space Model- Simplest Instantiation,1.6,"So we can also say d4 and d3 are relevent documents, and d1, d2, and",00:09:41,6,So also say d4 d3 relevent documents d1 d2
2094,00:09:48,2,Vector Space Model- Simplest Instantiation,1.6,d5 are non-relevant.,00:09:46,6,d5 non relevant
2095,00:09:55,2,Vector Space Model- Simplest Instantiation,1.6,"So, now lets see if our vector space model could do the same or",00:09:50,6,So lets see vector space model could
2096,00:09:57,2,Vector Space Model- Simplest Instantiation,1.6,could do something closer.,00:09:55,6,could something closer
2097,00:10:02,2,Vector Space Model- Simplest Instantiation,1.6,So let's first think about how we actually use this model to score documents.,00:09:57,6,So let first think actually use model score documents
2098,00:10:10,2,Vector Space Model- Simplest Instantiation,1.6,"Right here I show two documents, d1 and d3, and we have the query also here.",00:10:03,6,Right I show two documents d1 d3 query also
2099,00:10:15,2,Vector Space Model- Simplest Instantiation,1.6,"In the vector space model, of course we want to first compute the vectors for",00:10:10,6,In vector space model course want first compute vectors
2100,00:10:16,2,Vector Space Model- Simplest Instantiation,1.6,these documents and the query.,00:10:15,6,documents query
2101,00:10:19,2,Vector Space Model- Simplest Instantiation,1.6,"Now I issue with the vocabulary here as well, so",00:10:16,6,Now I issue vocabulary well
2102,00:10:22,2,Vector Space Model- Simplest Instantiation,1.6,these are the n dimensions that we'll be thinking about.,00:10:19,6,n dimensions thinking
2103,00:10:26,2,Vector Space Model- Simplest Instantiation,1.6,So what do you think is the vector representation for the query?,00:10:22,6,So think vector representation query
2104,00:10:32,2,Vector Space Model- Simplest Instantiation,1.6,Note that we are assuming that we only use zero and one,00:10:27,6,Note assuming use zero one
2105,00:10:39,2,Vector Space Model- Simplest Instantiation,1.6,to indicate whether a term is absent or present in the query or in the document.,00:10:32,6,indicate whether term absent present query document
2106,00:10:42,2,Vector Space Model- Simplest Instantiation,1.6,"So these are zero, one bit vectors.",00:10:39,6,So zero one bit vectors
2107,00:10:45,2,Vector Space Model- Simplest Instantiation,1.6,So what do you think is the query vector?,00:10:43,6,So think query vector
2108,00:10:51,2,Vector Space Model- Simplest Instantiation,1.6,Well the query has four words here.,00:10:47,6,Well query four words
2109,00:10:56,2,Vector Space Model- Simplest Instantiation,1.6,"So for these four words, there would be a one and for the rest, there will be zeros.",00:10:51,6,So four words would one rest zeros
2110,00:10:59,2,Vector Space Model- Simplest Instantiation,1.6,Now what about the documents?,00:10:57,6,Now documents
2111,00:11:00,2,Vector Space Model- Simplest Instantiation,1.6,It's the same.,00:10:59,6,It
2112,00:11:03,2,Vector Space Model- Simplest Instantiation,1.6,"So d1 has two rows, news and about.",00:11:00,6,So d1 two rows news
2113,00:11:06,2,Vector Space Model- Simplest Instantiation,1.6,So there are two ones here and the rest are zeros.,00:11:03,6,So two ones rest zeros
2114,00:11:12,2,Vector Space Model- Simplest Instantiation,1.6,"Similarly, so",00:11:06,6,Similarly
2115,00:11:16,2,Vector Space Model- Simplest Instantiation,1.6,"now that we have the two vectors, let's compute the similarity.",00:11:12,6,two vectors let compute similarity
2116,00:11:19,2,Vector Space Model- Simplest Instantiation,1.6,And we're going to use dot product.,00:11:17,6,And going use dot product
2117,00:11:25,2,Vector Space Model- Simplest Instantiation,1.6,"So you can see when we use dot product we just, multiply the corresponding elements.",00:11:19,6,So see use dot product multiply corresponding elements
2118,00:11:26,2,Vector Space Model- Simplest Instantiation,1.6,Right. So,00:11:25,6,Right So
2119,00:11:31,2,Vector Space Model- Simplest Instantiation,1.6,"these two would be, form a, be forming a product.",00:11:26,6,two would form forming product
2120,00:11:33,2,Vector Space Model- Simplest Instantiation,1.6,And these two will generate another product.,00:11:31,6,And two generate another product
2121,00:11:37,2,Vector Space Model- Simplest Instantiation,1.6,And these two would generate yet another product.,00:11:33,6,And two would generate yet another product
2122,00:11:38,2,Vector Space Model- Simplest Instantiation,1.6,And so on and so forth.,00:11:37,6,And forth
2123,00:11:42,2,Vector Space Model- Simplest Instantiation,1.6,"Now you can, you need to see if we do that.",00:11:40,6,Now need see
2124,00:11:48,2,Vector Space Model- Simplest Instantiation,1.6,We actually don't have to care about these zeroes,00:11:44,6,We actually care zeroes
2125,00:11:54,2,Vector Space Model- Simplest Instantiation,1.6,"because if whenever we have a zero, the product will be zero.",00:11:50,6,whenever zero product zero
2126,00:11:57,2,Vector Space Model- Simplest Instantiation,1.6,"So, when we take a sum over all these pairs,",00:11:54,6,So take sum pairs
2127,00:12:02,2,Vector Space Model- Simplest Instantiation,1.6,then the zero entries will be gone.,00:11:59,6,zero entries gone
2128,00:12:08,2,Vector Space Model- Simplest Instantiation,1.6,"As long as you have one zero, then the product would be zero.",00:12:04,6,As long one zero product would zero
2129,00:12:15,2,Vector Space Model- Simplest Instantiation,1.6,"So in the fact, we're just counting how many pairs of one and one, right?",00:12:08,6,So fact counting many pairs one one right
2130,00:12:16,2,Vector Space Model- Simplest Instantiation,1.6,"In this case, we have seen two.",00:12:15,6,In case seen two
2131,00:12:18,2,Vector Space Model- Simplest Instantiation,1.6,So the result will be two.,00:12:16,6,So result two
2132,00:12:20,2,Vector Space Model- Simplest Instantiation,1.6,"So, what does that mean?",00:12:18,6,So mean
2133,00:12:25,2,Vector Space Model- Simplest Instantiation,1.6,"Well that means, this number or the value of this scoring function.",00:12:20,6,Well means number value scoring function
2134,00:12:30,2,Vector Space Model- Simplest Instantiation,1.6,Is simply the count of how many unique query terms are matched in the document.,00:12:25,6,Is simply count many unique query terms matched document
2135,00:12:36,2,Vector Space Model- Simplest Instantiation,1.6,"Because if a document, if a term is matched in the document,",00:12:32,6,Because document term matched document
2136,00:12:39,2,Vector Space Model- Simplest Instantiation,1.6,then there will be two ones.,00:12:36,6,two ones
2137,00:12:44,2,Vector Space Model- Simplest Instantiation,1.6,"If it's not, then there will be zero on the document side.",00:12:41,6,If zero document side
2138,00:12:49,2,Vector Space Model- Simplest Instantiation,1.6,"Similarly, if the document has a term,.",00:12:46,6,Similarly document term
2139,00:12:53,2,Vector Space Model- Simplest Instantiation,1.6,But the terms not in the query there will be zero in the query vector.,00:12:49,6,But terms query zero query vector
2140,00:12:55,2,Vector Space Model- Simplest Instantiation,1.6,So those don't count.,00:12:53,6,So count
2141,00:12:58,2,Vector Space Model- Simplest Instantiation,1.6,So as a result this scoring function basically,00:12:55,6,So result scoring function basically
2142,00:13:03,2,Vector Space Model- Simplest Instantiation,1.6,meshes how many unique query terms are matched in a document.,00:12:58,6,meshes many unique query terms matched document
2143,00:13:05,2,Vector Space Model- Simplest Instantiation,1.6,This is how we interpret this score.,00:13:03,6,This interpret score
2144,00:13:10,2,Vector Space Model- Simplest Instantiation,1.6,Now we can also take a look at the d3.,00:13:07,6,Now also take look d3
2145,00:13:15,2,Vector Space Model- Simplest Instantiation,1.6,"In this case, you can see the result is three.",00:13:10,6,In case see result three
2146,00:13:21,2,Vector Space Model- Simplest Instantiation,1.6,"Because d3 matched the three distinctive query words, news, presidential, campaign.",00:13:15,6,Because d3 matched three distinctive query words news presidential campaign
2147,00:13:23,2,Vector Space Model- Simplest Instantiation,1.6,Whereas d1 only matched two.,00:13:21,6,Whereas d1 matched two
2148,00:13:28,2,Vector Space Model- Simplest Instantiation,1.6,"Now in this case, it seems reasonable to rank d3 on top of d1.",00:13:23,6,Now case seems reasonable rank d3 top d1
2149,00:13:33,2,Vector Space Model- Simplest Instantiation,1.6,And this simplest vector space model indeed does that.,00:13:29,6,And simplest vector space model indeed
2150,00:13:35,2,Vector Space Model- Simplest Instantiation,1.6,So that looks pretty good.,00:13:33,6,So looks pretty good
2151,00:13:40,2,Vector Space Model- Simplest Instantiation,1.6,"However, if we examine this model in detail, we likely will find some problems.",00:13:35,6,However examine model detail likely find problems
2152,00:13:45,2,Vector Space Model- Simplest Instantiation,1.6,So here I'm going to show all the scores for these five documents.,00:13:40,6,So I going show scores five documents
2153,00:13:49,2,Vector Space Model- Simplest Instantiation,1.6,And you can even verify they are correct.,00:13:46,6,And even verify correct
2154,00:13:51,2,Vector Space Model- Simplest Instantiation,1.6,Because we're basically counting,00:13:49,6,Because basically counting
2155,00:13:55,2,Vector Space Model- Simplest Instantiation,1.6,the number of unique query terms matched in each document.,00:13:51,6,number unique query terms matched document
2156,00:13:58,2,Vector Space Model- Simplest Instantiation,1.6,Now note that this method actually makes sense.,00:13:56,6,Now note method actually makes sense
2157,00:14:02,2,Vector Space Model- Simplest Instantiation,1.6,Right? It basically means if a document matches,00:13:58,6,Right It basically means document matches
2158,00:14:07,2,Vector Space Model- Simplest Instantiation,1.6,"more unique query terms, then the document will be assuming to be more relevant.",00:14:02,6,unique query terms document assuming relevant
2159,00:14:09,2,Vector Space Model- Simplest Instantiation,1.6,And that seems to make sense.,00:14:07,6,And seems make sense
2160,00:14:16,2,Vector Space Model- Simplest Instantiation,1.6,"The only problem is here, we can note set there are three documents, d2, d3, and d4.",00:14:09,6,The problem note set three documents d2 d3 d4
2161,00:14:22,2,Vector Space Model- Simplest Instantiation,1.6,"And they tied with a three, as a score.",00:14:16,6,And tied three score
2162,00:14:29,2,Vector Space Model- Simplest Instantiation,1.6,"So that's a problem, because if you look at them carefully it seems that",00:14:25,6,So problem look carefully seems
2163,00:14:35,2,Vector Space Model- Simplest Instantiation,1.6,d4 should be right above d3.,00:14:30,6,d4 right d3
2164,00:14:39,2,Vector Space Model- Simplest Instantiation,1.6,Because d3 only mentioned the presidential once.,00:14:35,6,Because d3 mentioned presidential
2165,00:14:42,2,Vector Space Model- Simplest Instantiation,1.6,But d4 mentioned it much more times.,00:14:39,6,But d4 mentioned much times
2166,00:14:47,2,Vector Space Model- Simplest Instantiation,1.6,"In case of d3, presidential could be extended mentioned.",00:14:42,6,In case d3 presidential could extended mentioned
2167,00:14:51,2,Vector Space Model- Simplest Instantiation,1.6,But d4 is clearly above presidential campaign.,00:14:47,6,But d4 clearly presidential campaign
2168,00:14:58,2,Vector Space Model- Simplest Instantiation,1.6,Another problem is that d2 and d3 also have the same soul.,00:14:51,6,Another problem d2 d3 also soul
2169,00:15:01,2,Vector Space Model- Simplest Instantiation,1.6,"But, if you look at the, the three words that are matched.",00:14:58,6,But look three words matched
2170,00:15:07,2,Vector Space Model- Simplest Instantiation,1.6,"In the case of d2, it matched the news, about, and the campaign.",00:15:01,6,In case d2 matched news campaign
2171,00:15:11,2,Vector Space Model- Simplest Instantiation,1.6,"But in the case of d3, it match the news, presidential, and campaign.",00:15:07,6,But case d3 match news presidential campaign
2172,00:15:15,2,Vector Space Model- Simplest Instantiation,1.6,"So intuitively, d3 is better.",00:15:12,6,So intuitively d3 better
2173,00:15:21,2,Vector Space Model- Simplest Instantiation,1.6,Because matching presidential is more important though than matching about.,00:15:15,6,Because matching presidential important though matching
2174,00:15:24,2,Vector Space Model- Simplest Instantiation,1.6,Even though about and the presidential are both in the query.,00:15:21,6,Even though presidential query
2175,00:15:29,2,Vector Space Model- Simplest Instantiation,1.6,"So intuitively, we would like d3 to be ranked above d2.",00:15:26,6,So intuitively would like d3 ranked d2
2176,00:15:32,2,Vector Space Model- Simplest Instantiation,1.6,"But this model, doesn't do that.",00:15:30,6,But model
2177,00:15:39,2,Vector Space Model- Simplest Instantiation,1.6,"So that means this is still not good enough, we have to solve these problems.",00:15:33,6,So means still good enough solve problems
2178,00:15:42,2,Vector Space Model- Simplest Instantiation,1.6,"To summarize,",00:15:41,6,To summarize
2179,00:15:45,2,Vector Space Model- Simplest Instantiation,1.6,in this lecture we talked about how to instantiate a vector space model.,00:15:42,6,lecture talked instantiate vector space model
2180,00:15:49,2,Vector Space Model- Simplest Instantiation,1.6,We may need to do three things.,00:15:47,6,We may need three things
2181,00:15:51,2,Vector Space Model- Simplest Instantiation,1.6,One is to define the dimension.,00:15:49,6,One define dimension
2182,00:15:55,2,Vector Space Model- Simplest Instantiation,1.6,The second is to,00:15:51,6,The second
2183,00:16:01,2,Vector Space Model- Simplest Instantiation,1.6,decide how to place documents as vectors in the vector space.,00:15:55,6,decide place documents vectors vector space
2184,00:16:05,2,Vector Space Model- Simplest Instantiation,1.6,And to also place a query in the vector space as a vector.,00:16:01,6,And also place query vector space vector
2185,00:16:13,2,Vector Space Model- Simplest Instantiation,1.6,"And third is to define the similarity between two vectors,",00:16:08,6,And third define similarity two vectors
2186,00:16:15,2,Vector Space Model- Simplest Instantiation,1.6,particularly the query vector and the document vector.,00:16:13,6,particularly query vector document vector
2187,00:16:22,2,Vector Space Model- Simplest Instantiation,1.6,We also talked about a very simple way to instantiate the vector space model.,00:16:17,6,We also talked simple way instantiate vector space model
2188,00:16:27,2,Vector Space Model- Simplest Instantiation,1.6,"Indeed, that's probably the simplest vector space model that we can derive.",00:16:22,6,Indeed probably simplest vector space model derive
2189,00:16:31,2,Vector Space Model- Simplest Instantiation,1.6,"In this case, we use each word to define a dimension.",00:16:27,6,In case use word define dimension
2190,00:16:37,2,Vector Space Model- Simplest Instantiation,1.6,We use a zero one bit vector to represent a document or a query.,00:16:31,6,We use zero one bit vector represent document query
2191,00:16:42,2,Vector Space Model- Simplest Instantiation,1.6,"In this case, we basically only care about word presence or absence.",00:16:37,6,In case basically care word presence absence
2192,00:16:43,2,Vector Space Model- Simplest Instantiation,1.6,We ignore the frequency.,00:16:42,6,We ignore frequency
2193,00:16:49,2,Vector Space Model- Simplest Instantiation,1.6,And we use the dot product as the similarity function.,00:16:45,6,And use dot product similarity function
2194,00:16:53,2,Vector Space Model- Simplest Instantiation,1.6,"And with such a, a, in situation.",00:16:50,6,And situation
2195,00:16:58,2,Vector Space Model- Simplest Instantiation,1.6,And we showed that the scoring function is basically to score,00:16:53,6,And showed scoring function basically score
2196,00:17:03,2,Vector Space Model- Simplest Instantiation,1.6,a document based on the number of distinct query words matched in the document.,00:16:58,6,document based number distinct query words matched document
2197,00:17:09,2,Vector Space Model- Simplest Instantiation,1.6,"We also show that such a single vector space model still doesn't work well,",00:17:04,6,We also show single vector space model still work well
2198,00:17:10,2,Vector Space Model- Simplest Instantiation,1.6,and we need to improve it.,00:17:09,6,need improve
2199,00:17:18,2,Vector Space Model- Simplest Instantiation,1.6,And this is the topic that we're going to cover in the next lecture.,00:17:12,6,And topic going cover next lecture
2200,00:00:02,3,System Implementation- Fast Search,2.3,[SOUND].,00:00:00,3,SOUND
2201,00:00:12,3,System Implementation- Fast Search,2.3,This lecture is about how to do fast research by using inverted index.,00:00:06,3,This lecture fast research using inverted index
2202,00:00:13,3,System Implementation- Fast Search,2.3,"In this lecture,",00:00:12,3,In lecture
2203,00:00:18,3,System Implementation- Fast Search,2.3,we are going to continue the discussion of the system implementation.,00:00:13,3,going continue discussion system implementation
2204,00:00:21,3,System Implementation- Fast Search,2.3,"In particular, we're going to talk about,",00:00:19,3,In particular going talk
2205,00:00:24,3,System Implementation- Fast Search,2.3,to how to support a faster search by using inverted index.,00:00:21,3,support faster search using inverted index
2206,00:00:31,3,System Implementation- Fast Search,2.3,"So, let's think about what a general scoring function might look like.",00:00:27,3,So let think general scoring function might look like
2207,00:00:37,3,System Implementation- Fast Search,2.3,"Now, of curse the vector space model is a special case of this.",00:00:32,3,Now curse vector space model special case
2208,00:00:40,3,System Implementation- Fast Search,2.3,But we can imagine many other retrieval functions of the same form.,00:00:37,3,But imagine many retrieval functions form
2209,00:00:45,3,System Implementation- Fast Search,2.3,"So, the form of this function is as follows.",00:00:42,3,So form function follows
2210,00:00:49,3,System Implementation- Fast Search,2.3,"We see this scoring function of document d, and",00:00:45,3,We see scoring function document
2211,00:00:56,3,System Implementation- Fast Search,2.3,"query q is defined as first, a function of f a that's adjustment in the function.",00:00:49,3,query q defined first function f adjustment function
2212,00:01:01,3,System Implementation- Fast Search,2.3,That what consider two factors that are shown,00:00:56,3,That consider two factors shown
2213,00:01:07,3,System Implementation- Fast Search,2.3,"here at the end, f sub d of d, and f sub q of q.",00:01:01,3,end f sub f sub q q
2214,00:01:14,3,System Implementation- Fast Search,2.3,"These are adjustment factors of a document and query, so",00:01:07,3,These adjustment factors document query
2215,00:01:19,3,System Implementation- Fast Search,2.3,"they're at the level of document, and query.",00:01:14,3,level document query
2216,00:01:20,3,System Implementation- Fast Search,2.3,"So, and",00:01:19,3,So
2217,00:01:25,3,System Implementation- Fast Search,2.3,then inside of this function we also see there's a another function called edge.,00:01:20,3,inside function also see another function called edge
2218,00:01:32,3,System Implementation- Fast Search,2.3,"So, this is the main part of",00:01:25,3,So main part
2219,00:01:37,3,System Implementation- Fast Search,2.3,"the scoring function, and these as I just said",00:01:32,3,scoring function I said
2220,00:01:44,3,System Implementation- Fast Search,2.3,"of the scoring factors at the level of the whole document, and the query.",00:01:37,3,scoring factors level whole document query
2221,00:01:47,3,System Implementation- Fast Search,2.3,"For example, document and",00:01:44,3,For example document
2222,00:01:52,3,System Implementation- Fast Search,2.3,this aggregate function would then combine all these.,00:01:47,3,aggregate function would combine
2223,00:02:00,3,System Implementation- Fast Search,2.3,"Now, inside this h function, there are functions that would compute",00:01:52,3,Now inside h function functions would compute
2224,00:02:05,3,System Implementation- Fast Search,2.3,the weights of the contribution of a matched query term t i.,00:02:00,3,weights contribution matched query term
2225,00:02:13,3,System Implementation- Fast Search,2.3,"So, this this g, the function g gives us",00:02:08,3,So g function g gives us
2226,00:02:19,3,System Implementation- Fast Search,2.3,the weight of a matched query term t i in document d.,00:02:13,3,weight matched query term document
2227,00:02:27,3,System Implementation- Fast Search,2.3,"And this h function with that aggregate all these weights, so",00:02:22,3,And h function aggregate weights
2228,00:02:34,3,System Implementation- Fast Search,2.3,"it were, for example, take a sum, but it of all the matched query in that terms.",00:02:27,3,example take sum matched query terms
2229,00:02:39,3,System Implementation- Fast Search,2.3,"But it can also be a product, or could be another way of aggregate them.",00:02:34,3,But also product could another way aggregate
2230,00:02:46,3,System Implementation- Fast Search,2.3,"And then finally, this adjustment function would then consider",00:02:41,3,And finally adjustment function would consider
2231,00:02:51,3,System Implementation- Fast Search,2.3,"the document level, or query level factors through further adjuster score,",00:02:46,3,document level query level factors adjuster score
2232,00:02:54,3,System Implementation- Fast Search,2.3,"for example, document lens [INAUDIBLE].",00:02:51,3,example document lens INAUDIBLE
2233,00:02:58,3,System Implementation- Fast Search,2.3,"So, this general form would cover many state of original functions.",00:02:54,3,So general form would cover many state original functions
2234,00:03:02,3,System Implementation- Fast Search,2.3,Let's look at how we can score such,00:02:58,3,Let look score
2235,00:03:06,3,System Implementation- Fast Search,2.3,score documents with such a function using inverted index.,00:03:02,3,score documents function using inverted index
2236,00:03:10,3,System Implementation- Fast Search,2.3,So here's the general algorithm that works as follows.,00:03:07,3,So general algorithm works follows
2237,00:03:13,3,System Implementation- Fast Search,2.3,First these these Query level and,00:03:10,3,First Query level
2238,00:03:19,3,System Implementation- Fast Search,2.3,document level factors can be pre-computed in the indexing term.,00:03:13,3,document level factors pre computed indexing term
2239,00:03:22,3,System Implementation- Fast Search,2.3,"Of course, for the query, we have to compute it as a query term.",00:03:19,3,Of course query compute query term
2240,00:03:28,3,System Implementation- Fast Search,2.3,"But for document, for example, document can be pre-computed.",00:03:22,3,But document example document pre computed
2241,00:03:32,3,System Implementation- Fast Search,2.3,And then we maintain a score accumulator for each document d to compute the h.,00:03:28,3,And maintain score accumulator document compute h
2242,00:03:39,3,System Implementation- Fast Search,2.3,And h is aggregation function of all the matching query terms.,00:03:34,3,And h aggregation function matching query terms
2243,00:03:40,3,System Implementation- Fast Search,2.3,So how do we do that?,00:03:39,3,So
2244,00:03:45,3,System Implementation- Fast Search,2.3,"Well, for each query term, we going to do fetch inverted list,",00:03:40,3,Well query term going fetch inverted list
2245,00:03:47,3,System Implementation- Fast Search,2.3,from the inverted index.,00:03:45,3,inverted index
2246,00:03:51,3,System Implementation- Fast Search,2.3,"This will give us all the documents that match this query term,",00:03:47,3,This give us documents match query term
2247,00:03:57,3,System Implementation- Fast Search,2.3,"and that includes d1, f1, and so, d and fn.",00:03:52,3,includes d1 f1 fn
2248,00:04:03,3,System Implementation- Fast Search,2.3,So each pair is document id and the frequency of the term in the document.,00:03:57,3,So pair document id frequency term document
2249,00:04:07,3,System Implementation- Fast Search,2.3,"Then for each entry d sub j and f sub j,",00:04:03,3,Then entry sub j f sub j
2250,00:04:12,3,System Implementation- Fast Search,2.3,"a particular match of the term in this particular document d sub j,",00:04:07,3,particular match term particular document sub j
2251,00:04:15,3,System Implementation- Fast Search,2.3,we're going to computer the function g.,00:04:12,3,going computer function g
2252,00:04:20,3,System Implementation- Fast Search,2.3,"That would give us something like a t of i, ef weights of this term.",00:04:15,3,That would give us something like ef weights term
2253,00:04:24,3,System Implementation- Fast Search,2.3,"So, we're computing the weight contribution of matching this query term",00:04:20,3,So computing weight contribution matching query term
2254,00:04:26,3,System Implementation- Fast Search,2.3,in this document.,00:04:24,3,document
2255,00:04:30,3,System Implementation- Fast Search,2.3,And then we're going to update the score accumulator for this document.,00:04:26,3,And going update score accumulator document
2256,00:04:36,3,System Implementation- Fast Search,2.3,"And this would allow us to add this to our accumulator,",00:04:31,3,And would allow us add accumulator
2257,00:04:40,3,System Implementation- Fast Search,2.3,that would incrementally compute function h.,00:04:36,3,would incrementally compute function h
2258,00:04:46,3,System Implementation- Fast Search,2.3,So this is basically a general way to allow sort of computer,00:04:41,3,So basically general way allow sort computer
2259,00:04:51,3,System Implementation- Fast Search,2.3,"all functions of this form, by using inverted index.",00:04:46,3,functions form using inverted index
2260,00:04:54,3,System Implementation- Fast Search,2.3,Note that we don't have to attach any document that that,00:04:51,3,Note attach document
2261,00:04:58,3,System Implementation- Fast Search,2.3,"didn't match any query term, but this is why it's fast.",00:04:54,3,match query term fast
2262,00:05:03,3,System Implementation- Fast Search,2.3,"We only need to process the documents that tap, that match at least one query term.",00:04:58,3,We need process documents tap match least one query term
2263,00:05:08,3,System Implementation- Fast Search,2.3,"In the end, then we're going to adjust the score to compute a,",00:05:04,3,In end going adjust score compute
2264,00:05:11,3,System Implementation- Fast Search,2.3,this function f of a and then we can sort.,00:05:08,3,function f sort
2265,00:05:14,3,System Implementation- Fast Search,2.3,So let's take a look at the specific example.,00:05:11,3,So let take look specific example
2266,00:05:17,3,System Implementation- Fast Search,2.3,"In this, case let's assume the scoring function's a very simple one.",00:05:14,3,In case let assume scoring function simple one
2267,00:05:25,3,System Implementation- Fast Search,2.3,"It just takes us sum of tf, the rule of tf, the count of, of term in the document.",00:05:17,3,It takes us sum tf rule tf count term document
2268,00:05:31,3,System Implementation- Fast Search,2.3,Now this simple equation with the help showing the algorithm clearly.,00:05:25,3,Now simple equation help showing algorithm clearly
2269,00:05:36,3,System Implementation- Fast Search,2.3,"It's very easy to extend the, the computation to include other weights",00:05:31,3,It easy extend computation include weights
2270,00:05:42,3,System Implementation- Fast Search,2.3,like the transformation of TF or document or IDF weighting.,00:05:36,3,like transformation TF document IDF weighting
2271,00:05:47,3,System Implementation- Fast Search,2.3,"So let's take a look at specific example with the query's information security,",00:05:42,3,So let take look specific example query information security
2272,00:05:54,3,System Implementation- Fast Search,2.3,and shows some entries of the inverted index on the right side.,00:05:48,3,shows entries inverted index right side
2273,00:05:56,3,System Implementation- Fast Search,2.3,Information occurring before documents and,00:05:54,3,Information occurring documents
2274,00:06:01,3,System Implementation- Fast Search,2.3,"the frequencies is also there, security is coding three documents.",00:05:56,3,frequencies also security coding three documents
2275,00:06:04,3,System Implementation- Fast Search,2.3,"So, let's see how the algorithm works, all right?",00:06:01,3,So let see algorithm works right
2276,00:06:09,3,System Implementation- Fast Search,2.3,"So, first we iterate all the query terms, and we fetch the first query then.",00:06:04,3,So first iterate query terms fetch first query
2277,00:06:09,3,System Implementation- Fast Search,2.3,What is that?,00:06:09,3,What
2278,00:06:11,3,System Implementation- Fast Search,2.3,That's information.,00:06:09,3,That information
2279,00:06:14,3,System Implementation- Fast Search,2.3,"Right? So, and imagine we have all these score",00:06:11,3,Right So imagine score
2280,00:06:19,3,System Implementation- Fast Search,2.3,"accumulators to score, score the, score the scores for these documents.",00:06:14,3,accumulators score score score scores documents
2281,00:06:21,3,System Implementation- Fast Search,2.3,"We can imagine there will be allocated, but",00:06:19,3,We imagine allocated
2282,00:06:24,3,System Implementation- Fast Search,2.3,then they will only be allocated as needed.,00:06:21,3,allocated needed
2283,00:06:32,3,System Implementation- Fast Search,2.3,So before we do any weighting of terms we don't even need a score accumulators.,00:06:24,3,So weighting terms even need score accumulators
2284,00:06:38,3,System Implementation- Fast Search,2.3,"But conceptual we have these score accumulators eventually allocated, right?",00:06:33,3,But conceptual score accumulators eventually allocated right
2285,00:06:43,3,System Implementation- Fast Search,2.3,"So let's fetch the, the entries from the inverted list for",00:06:38,3,So let fetch entries inverted list
2286,00:06:45,3,System Implementation- Fast Search,2.3,"information first, that's the first one.",00:06:43,3,information first first one
2287,00:06:50,3,System Implementation- Fast Search,2.3,So these score accumulators obviously would be initialized as zeros.,00:06:46,3,So score accumulators obviously would initialized zeros
2288,00:06:53,3,System Implementation- Fast Search,2.3,"So the first entry is d1 and 3,",00:06:51,3,So first entry d1 3
2289,00:06:58,3,System Implementation- Fast Search,2.3,3 is occurrences of information in this document.,00:06:53,3,3 occurrences information document
2290,00:07:03,3,System Implementation- Fast Search,2.3,Since our scoring function assume that the score is just a sum of these raw counts.,00:06:58,3,Since scoring function assume score sum raw counts
2291,00:07:08,3,System Implementation- Fast Search,2.3,We just need to add a 3 to the score accumulator to account for,00:07:03,3,We need add 3 score accumulator account
2292,00:07:14,3,System Implementation- Fast Search,2.3,"the increase of score, due to matching this term information, a document d1.",00:07:08,3,increase score due matching term information document d1
2293,00:07:18,3,System Implementation- Fast Search,2.3,And now we go to the next entry.,00:07:14,3,And go next entry
2294,00:07:22,3,System Implementation- Fast Search,2.3,That's d2 and 4 and then we'll add a 4 to the score accumulator of d2.,00:07:18,3,That d2 4 add 4 score accumulator d2
2295,00:07:26,3,System Implementation- Fast Search,2.3,"Of course, at this point we will allocate the score accumulator as needed.",00:07:22,3,Of course point allocate score accumulator needed
2296,00:07:33,3,System Implementation- Fast Search,2.3,"And so, at this point, we have located d1 and d2, and the next one is d3.",00:07:27,3,And point located d1 d2 next one d3
2297,00:07:39,3,System Implementation- Fast Search,2.3,"And we add 1, or we locate another score coming in the spot d3 and add 1 to it.",00:07:33,3,And add 1 locate another score coming spot d3 add 1
2298,00:07:44,3,System Implementation- Fast Search,2.3,"And finally, the d4 gets a 5 because the information",00:07:39,3,And finally d4 gets 5 information
2299,00:07:50,3,System Implementation- Fast Search,2.3,the term information occurred ti in five times in this document.,00:07:44,3,term information occurred ti five times document
2300,00:07:54,3,System Implementation- Fast Search,2.3,"Okay, so this completes the processing of all the entries in the,",00:07:50,3,Okay completes processing entries
2301,00:07:56,3,System Implementation- Fast Search,2.3,inverted index for information.,00:07:54,3,inverted index information
2302,00:08:00,3,System Implementation- Fast Search,2.3,It's processed all the contributions of matching information in this,00:07:56,3,It processed contributions matching information
2303,00:08:01,3,System Implementation- Fast Search,2.3,four documents.,00:08:00,3,four documents
2304,00:08:06,3,System Implementation- Fast Search,2.3,"So now our arrows will go to the next query term, that's security.",00:08:01,3,So arrows go next query term security
2305,00:08:10,3,System Implementation- Fast Search,2.3,"So, we're going to factor all the inverted index entries for security.",00:08:06,3,So going factor inverted index entries security
2306,00:08:13,3,System Implementation- Fast Search,2.3,"So in this case, there were three entries.",00:08:10,3,So case three entries
2307,00:08:15,3,System Implementation- Fast Search,2.3,And we're going to go through each of them.,00:08:13,3,And going go
2308,00:08:18,3,System Implementation- Fast Search,2.3,The first is d2 and 3.,00:08:15,3,The first d2 3
2309,00:08:22,3,System Implementation- Fast Search,2.3,"And that means security occurred three times in d2, and what do we do?",00:08:18,3,And means security occurred three times d2
2310,00:08:26,3,System Implementation- Fast Search,2.3,"Well, we do exactly the same as what we did for information.",00:08:22,3,Well exactly information
2311,00:08:28,3,System Implementation- Fast Search,2.3,"So this time we're going to do change the score,",00:08:26,3,So time going change score
2312,00:08:32,3,System Implementation- Fast Search,2.3,accumulating d2 sees it's already allocate.,00:08:28,3,accumulating d2 sees already allocate
2313,00:08:38,3,System Implementation- Fast Search,2.3,"And what we do is we'll add 3 to the existing value which is a 4,",00:08:32,3,And add 3 existing value 4
2314,00:08:41,3,System Implementation- Fast Search,2.3,so we now get the 7 for d2.,00:08:38,3,get 7 d2
2315,00:08:46,3,System Implementation- Fast Search,2.3,"D2 sc, score is increased because of the match both information and the security.",00:08:41,3,D2 sc score increased match information security
2316,00:08:53,3,System Implementation- Fast Search,2.3,"Go to the next step entry, that's d4 and 1, so we've updated the score for",00:08:47,3,Go next step entry d4 1 updated score
2317,00:08:59,3,System Implementation- Fast Search,2.3,"d4,and again we add 1 to d4, so d4 goes from 5 to 6.",00:08:53,3,d4 add 1 d4 d4 goes 5 6
2318,00:09:01,3,System Implementation- Fast Search,2.3,Finally we process d5 and 3.,00:08:59,3,Finally process d5 3
2319,00:09:06,3,System Implementation- Fast Search,2.3,"SInce we have not yet equated a score accumulator d4 to d5,",00:09:01,3,SInce yet equated score accumulator d4 d5
2320,00:09:12,3,System Implementation- Fast Search,2.3,"at this point, we allocate one, 45 and we're going to add 3 to it.",00:09:06,3,point allocate one 45 going add 3
2321,00:09:20,3,System Implementation- Fast Search,2.3,"So, those scores on the last row are the final scores for these documents.",00:09:12,3,So scores last row final scores documents
2322,00:09:27,3,System Implementation- Fast Search,2.3,"If our scoring function is just a, a simple sum of tf values.",00:09:20,3,If scoring function simple sum tf values
2323,00:09:31,3,System Implementation- Fast Search,2.3,"Now what if we actually would like to, to do lands normalization.",00:09:27,3,Now actually would like lands normalization
2324,00:09:35,3,System Implementation- Fast Search,2.3,Well we can do the normalization at this point for each document.,00:09:31,3,Well normalization point document
2325,00:09:40,3,System Implementation- Fast Search,2.3,"So to summarize this, all right so you can see we first",00:09:36,3,So summarize right see first
2326,00:09:44,3,System Implementation- Fast Search,2.3,"processed the information determine query term information, and",00:09:40,3,processed information determine query term information
2327,00:09:49,3,System Implementation- Fast Search,2.3,we process all the entries in the inverted index for this term.,00:09:44,3,process entries inverted index term
2328,00:09:54,3,System Implementation- Fast Search,2.3,"Then we process the security, all right, let's think about",00:09:49,3,Then process security right let think
2329,00:09:59,3,System Implementation- Fast Search,2.3,the what should be the order of processing here when we consider query terms?,00:09:54,3,order processing consider query terms
2330,00:10:02,3,System Implementation- Fast Search,2.3,"It might make difference,",00:10:00,3,It might make difference
2331,00:10:07,3,System Implementation- Fast Search,2.3,especially if we don't want to keep to keep all the score accumulators.,00:10:02,3,especially want keep keep score accumulators
2332,00:10:12,3,System Implementation- Fast Search,2.3,Let's say we only want to keep the most promising score accumulators.,00:10:07,3,Let say want keep promising score accumulators
2333,00:10:14,3,System Implementation- Fast Search,2.3,What do you think it would be a good order to go through?,00:10:12,3,What think would good order go
2334,00:10:20,3,System Implementation- Fast Search,2.3,Would you go would you process a common term first or,00:10:15,3,Would go would process common term first
2335,00:10:22,3,System Implementation- Fast Search,2.3,would you process a rare term first?,00:10:20,3,would process rare term first
2336,00:10:30,3,System Implementation- Fast Search,2.3,The answer is we should go through we should process the rare term first.,00:10:24,3,The answer go process rare term first
2337,00:10:36,3,System Implementation- Fast Search,2.3,"A rare term will match fewer documents and then the score confusion will be higher,",00:10:30,3,A rare term match fewer documents score confusion higher
2338,00:10:39,3,System Implementation- Fast Search,2.3,"because the IDF value will be higher and, and",00:10:36,3,IDF value higher
2339,00:10:45,3,System Implementation- Fast Search,2.3,then it allows us to attach the most diplomacy documents first.,00:10:39,3,allows us attach diplomacy documents first
2340,00:10:48,3,System Implementation- Fast Search,2.3,"So it helps pruning some non promising ones, if we don't need so",00:10:45,3,So helps pruning non promising ones need
2341,00:10:51,3,System Implementation- Fast Search,2.3,many documents to be returned to the user.,00:10:48,3,many documents returned user
2342,00:10:55,3,System Implementation- Fast Search,2.3,And so those are heuristics for further improving the accuracy.,00:10:52,3,And heuristics improving accuracy
2343,00:10:58,3,System Implementation- Fast Search,2.3,Here can also see how we can incorporate the idea of weighting.,00:10:55,3,Here also see incorporate idea weighting
2344,00:10:59,3,System Implementation- Fast Search,2.3,All right.,00:10:58,3,All right
2345,00:11:03,3,System Implementation- Fast Search,2.3,So they can [INAUDIBLE] when we incorporated a one way process each,00:10:59,3,So INAUDIBLE incorporated one way process
2346,00:11:04,3,System Implementation- Fast Search,2.3,query term.,00:11:03,3,query term
2347,00:11:07,3,System Implementation- Fast Search,2.3,"When we fetch in word index we can fetch the document frequency,",00:11:04,3,When fetch word index fetch document frequency
2348,00:11:09,3,System Implementation- Fast Search,2.3,and then we can compute the IDF.,00:11:07,3,compute IDF
2349,00:11:16,3,System Implementation- Fast Search,2.3,Or maybe perhapsIDF value has already been pre-computed when we index the document.,00:11:09,3,Or maybe perhapsIDF value already pre computed index document
2350,00:11:22,3,System Implementation- Fast Search,2.3,At that time we already computed the IDF value that we can just fetch it.,00:11:16,3,At time already computed IDF value fetch
2351,00:11:26,3,System Implementation- Fast Search,2.3,So all these can be down at this time.,00:11:22,3,So time
2352,00:11:30,3,System Implementation- Fast Search,2.3,So that will mean one will process all the entries for information these,00:11:26,3,So mean one process entries information
2353,00:11:35,3,System Implementation- Fast Search,2.3,"these weights would be adjusted by the same IDF, which is IDF for information.",00:11:30,3,weights would adjusted IDF IDF information
2354,00:11:40,3,System Implementation- Fast Search,2.3,"So this is the basic idea of using inverted index for faster search, and",00:11:36,3,So basic idea using inverted index faster search
2355,00:11:46,3,System Implementation- Fast Search,2.3,works well for all kinds of formulas that are of the general form and this generally,00:11:40,3,works well kinds formulas general form generally
2356,00:11:51,3,System Implementation- Fast Search,2.3,"cov, the general form covers actually most state of the art retrieval functions.",00:11:46,3,cov general form covers actually state art retrieval functions
2357,00:11:58,3,System Implementation- Fast Search,2.3,"So there are some tricks to further improve the efficiency ,some general mac",00:11:53,3,So tricks improve efficiency general mac
2358,00:12:02,3,System Implementation- Fast Search,2.3,"tech, techniques include caching.",00:11:58,3,tech techniques include caching
2359,00:12:07,3,System Implementation- Fast Search,2.3,"This is just a to store some results of popular query's, so",00:12:02,3,This store results popular query
2360,00:12:12,3,System Implementation- Fast Search,2.3,that next time when you see the same query you simply return the stored results.,00:12:07,3,next time see query simply return stored results
2361,00:12:17,3,System Implementation- Fast Search,2.3,"Similarly, you can also score the missed of inverted index in the memory for",00:12:12,3,Similarly also score missed inverted index memory
2362,00:12:19,3,System Implementation- Fast Search,2.3,popular term.,00:12:17,3,popular term
2363,00:12:22,3,System Implementation- Fast Search,2.3,And if the query comes popular you will assume,00:12:19,3,And query comes popular assume
2364,00:12:25,3,System Implementation- Fast Search,2.3,it will fetch the inverted index for the same term again.,00:12:22,3,fetch inverted index term
2365,00:12:28,3,System Implementation- Fast Search,2.3,So keeping that in the memory would help.,00:12:25,3,So keeping memory would help
2366,00:12:32,3,System Implementation- Fast Search,2.3,And these are general techniques for improving efficiency.,00:12:28,3,And general techniques improving efficiency
2367,00:12:36,3,System Implementation- Fast Search,2.3,We can also only keep the most promising accumulators because a user generally,00:12:32,3,We also keep promising accumulators user generally
2368,00:12:39,3,System Implementation- Fast Search,2.3,doesn't want to examine so many documents.,00:12:36,3,want examine many documents
2369,00:12:46,3,System Implementation- Fast Search,2.3,We only want to return high quality subset of documents that likely ranked,00:12:39,3,We want return high quality subset documents likely ranked
2370,00:12:51,3,System Implementation- Fast Search,2.3,"on the top, in,in for that purpose we can then prune the accumulators.",00:12:47,3,top purpose prune accumulators
2371,00:12:53,3,System Implementation- Fast Search,2.3,We don't have to store all the accumulators.,00:12:51,3,We store accumulators
2372,00:12:58,3,System Implementation- Fast Search,2.3,At some point we just keep the highest value accumulators.,00:12:53,3,At point keep highest value accumulators
2373,00:13:06,3,System Implementation- Fast Search,2.3,"Another technique is to do parallel processing, and that's needed for",00:13:00,3,Another technique parallel processing needed
2374,00:13:12,3,System Implementation- Fast Search,2.3,"really processing such a large data set, like the web data set.",00:13:06,3,really processing large data set like web data set
2375,00:13:15,3,System Implementation- Fast Search,2.3,And to scale up to the Web-scale we need to special,00:13:12,3,And scale Web scale need special
2376,00:13:18,3,System Implementation- Fast Search,2.3,to have the special techniques to do parallel processing and,00:13:15,3,special techniques parallel processing
2377,00:13:21,3,System Implementation- Fast Search,2.3,to distribute the storage of files on multiple machines.,00:13:18,3,distribute storage files multiple machines
2378,00:13:29,3,System Implementation- Fast Search,2.3,"So here as a, here is a list of some text retrieval toolkits.",00:13:25,3,So list text retrieval toolkits
2379,00:13:31,3,System Implementation- Fast Search,2.3,"It's, it's not a complete list.",00:13:29,3,It complete list
2380,00:13:36,3,System Implementation- Fast Search,2.3,You can find the more information at this URL on the bottom.,00:13:31,3,You find information URL bottom
2381,00:13:42,3,System Implementation- Fast Search,2.3,"Here I listed four here, lucene is one of the most popular toolkit",00:13:37,3,Here I listed four lucene one popular toolkit
2382,00:13:45,3,System Implementation- Fast Search,2.3,that can support a lot of applications.,00:13:42,3,support lot applications
2383,00:13:48,3,System Implementation- Fast Search,2.3,And it has very nice support for applications.,00:13:45,3,And nice support applications
2384,00:13:51,3,System Implementation- Fast Search,2.3,"You can use it to build a search engine very quickly,",00:13:48,3,You use build search engine quickly
2385,00:13:55,3,System Implementation- Fast Search,2.3,"the downside is that it's not that easy to extend it, and",00:13:51,3,downside easy extend
2386,00:14:00,3,System Implementation- Fast Search,2.3,the algorithms incremented there are not the most advanced algorithms.,00:13:55,3,algorithms incremented advanced algorithms
2387,00:14:03,3,System Implementation- Fast Search,2.3,Lemur or Indri is another toolkit that,00:14:00,3,Lemur Indri another toolkit
2388,00:14:08,3,System Implementation- Fast Search,2.3,that does not have such a nice support application as Lucene.,00:14:03,3,nice support application Lucene
2389,00:14:12,3,System Implementation- Fast Search,2.3,But it has many advanced search algorithms.,00:14:08,3,But many advanced search algorithms
2390,00:14:14,3,System Implementation- Fast Search,2.3,And it's also easy to extend.,00:14:12,3,And also easy extend
2391,00:14:21,3,System Implementation- Fast Search,2.3,Terrier is yet another toolkit that also has good support for,00:14:16,3,Terrier yet another toolkit also good support
2392,00:14:25,3,System Implementation- Fast Search,2.3,quotation capability and some advanced algorithms.,00:14:21,3,quotation capability advanced algorithms
2393,00:14:32,3,System Implementation- Fast Search,2.3,"So that's maybe in between Lemur, or Lucene or",00:14:25,3,So maybe Lemur Lucene
2394,00:14:38,3,System Implementation- Fast Search,2.3,"maybe rather combining the strands of both, so that's also useful toolkit.",00:14:32,3,maybe rather combining strands also useful toolkit
2395,00:14:43,3,System Implementation- Fast Search,2.3,"MeTA is the toolkit that we'll use for the programming assignment,",00:14:38,3,MeTA toolkit use programming assignment
2396,00:14:48,3,System Implementation- Fast Search,2.3,and this is a new toolkit that has a combination,00:14:43,3,new toolkit combination
2397,00:14:54,3,System Implementation- Fast Search,2.3,of both text retrieval algorithms and text mining algorithms.,00:14:48,3,text retrieval algorithms text mining algorithms
2398,00:14:59,3,System Implementation- Fast Search,2.3,"And so, toolkit models are implement, they are, there are a number of text analysis",00:14:54,3,And toolkit models implement number text analysis
2399,00:15:06,3,System Implementation- Fast Search,2.3,"algorithms, implemented in the toolkit, as well as basic research algorithms.",00:14:59,3,algorithms implemented toolkit well basic research algorithms
2400,00:15:10,3,System Implementation- Fast Search,2.3,"So, to summarize all the discussion about the system implementation,",00:15:06,3,So summarize discussion system implementation
2401,00:15:14,3,System Implementation- Fast Search,2.3,here are the major take away points.,00:15:11,3,major take away points
2402,00:15:20,3,System Implementation- Fast Search,2.3,Inverted index is the primary data structure for supporting a search engine.,00:15:14,3,Inverted index primary data structure supporting search engine
2403,00:15:25,3,System Implementation- Fast Search,2.3,That's the key to enable faster response to a user's query.,00:15:20,3,That key enable faster response user query
2404,00:15:31,3,System Implementation- Fast Search,2.3,"And the basic idea is process that, pre-process the data as much as we can,",00:15:26,3,And basic idea process pre process data much
2405,00:15:34,3,System Implementation- Fast Search,2.3,and we want to do compression when appropriate.,00:15:31,3,want compression appropriate
2406,00:15:40,3,System Implementation- Fast Search,2.3,So that we can save disk space and can speed up IO and,00:15:34,3,So save disk space speed IO
2407,00:15:43,3,System Implementation- Fast Search,2.3,processing of the inverted index in general.,00:15:40,3,processing inverted index general
2408,00:15:47,3,System Implementation- Fast Search,2.3,We'll talk about how we will construct the inverted index when the data,00:15:43,3,We talk construct inverted index data
2409,00:15:49,3,System Implementation- Fast Search,2.3,can fit into the memory.,00:15:47,3,fit memory
2410,00:15:55,3,System Implementation- Fast Search,2.3,"And then we talk about faster search using inverted index, basically to exploit",00:15:49,3,And talk faster search using inverted index basically exploit
2411,00:15:59,3,System Implementation- Fast Search,2.3,the inverted index to accumulate scores for documents matching a query term.,00:15:55,3,inverted index accumulate scores documents matching query term
2412,00:16:03,3,System Implementation- Fast Search,2.3,And we exploit Zipf's law avoid touching many documents,00:15:59,3,And exploit Zipf law avoid touching many documents
2413,00:16:05,3,System Implementation- Fast Search,2.3,that don't match any query term.,00:16:03,3,match query term
2414,00:16:11,3,System Implementation- Fast Search,2.3,"And this algorithm can, can support a wide range of ranking algorithms.",00:16:07,3,And algorithm support wide range ranking algorithms
2415,00:16:18,3,System Implementation- Fast Search,2.3,"So these basic techniques have mm, have great potential for further scanning",00:16:13,3,So basic techniques mm great potential scanning
2416,00:16:23,3,System Implementation- Fast Search,2.3,output using distribution to withstand parallel processing and the caching.,00:16:18,3,output using distribution withstand parallel processing caching
2417,00:16:28,3,System Implementation- Fast Search,2.3,"Here are two additional readings that you can take a look at if you have time,",00:16:23,3,Here two additional readings take look time
2418,00:16:31,3,System Implementation- Fast Search,2.3,and are interested in learning more about this.,00:16:28,3,interested learning
2419,00:16:38,3,System Implementation- Fast Search,2.3,The first one is a classic textbook on the scare the efficiency of inverted index and,00:16:31,3,The first one classic textbook scare efficiency inverted index
2420,00:16:43,3,System Implementation- Fast Search,2.3,"the compression techniques, and how to in general,",00:16:38,3,compression techniques general
2421,00:16:50,3,System Implementation- Fast Search,2.3,build a efficient search engine in terms of the space overhead and speed.,00:16:43,3,build efficient search engine terms space overhead speed
2422,00:16:54,3,System Implementation- Fast Search,2.3,The second one is a newer textbook that has a nice discussion of implementing and,00:16:50,3,The second one newer textbook nice discussion implementing
2423,00:16:56,3,System Implementation- Fast Search,2.3,evaluating search engines.,00:16:54,3,evaluating search engines
2424,00:00:05,4,Feedback in Vector Space Model- Rocchio,3.7,[SOUND] This lecture is about,00:00:00,9,SOUND This lecture
2425,00:00:12,4,Feedback in Vector Space Model- Rocchio,3.7,the feedback in the vector space model.,00:00:05,9,feedback vector space model
2426,00:00:18,4,Feedback in Vector Space Model- Rocchio,3.7,"In this lecture, we continue talking about the feedback and text retrieval.",00:00:12,9,In lecture continue talking feedback text retrieval
2427,00:00:21,4,Feedback in Vector Space Model- Rocchio,3.7,Particularly we're going to talk about feedback in the vector space model.,00:00:18,9,Particularly going talk feedback vector space model
2428,00:00:29,4,Feedback in Vector Space Model- Rocchio,3.7,As we have discussed before in the case of feedback the task of,00:00:23,9,As discussed case feedback task
2429,00:00:34,4,Feedback in Vector Space Model- Rocchio,3.7,a text retrieval system is relearned from examples to improve retrieval accuracy.,00:00:29,9,text retrieval system relearned examples improve retrieval accuracy
2430,00:00:37,4,Feedback in Vector Space Model- Rocchio,3.7,"We will have positive examples,",00:00:34,9,We positive examples
2431,00:00:40,4,Feedback in Vector Space Model- Rocchio,3.7,those are the documents that are assumed that will be random or,00:00:37,9,documents assumed random
2432,00:00:45,4,Feedback in Vector Space Model- Rocchio,3.7,judged with being random and all the documents that are viewed by users.,00:00:40,9,judged random documents viewed users
2433,00:00:48,4,Feedback in Vector Space Model- Rocchio,3.7,"We also have negative examples, those are documents known to be non-relevant.",00:00:45,9,We also negative examples documents known non relevant
2434,00:00:52,4,Feedback in Vector Space Model- Rocchio,3.7,They can also be the documents that are escaped by users.,00:00:48,9,They also documents escaped users
2435,00:00:58,4,Feedback in Vector Space Model- Rocchio,3.7,The general method in the vector space model for,00:00:55,9,The general method vector space model
2436,00:01:02,4,Feedback in Vector Space Model- Rocchio,3.7,feedback is to modify our query vector.,00:00:58,9,feedback modify query vector
2437,00:01:08,4,Feedback in Vector Space Model- Rocchio,3.7,Now we want to place the query vector in a better position to make that accurate,00:01:03,9,Now want place query vector better position make accurate
2438,00:01:11,4,Feedback in Vector Space Model- Rocchio,3.7,and what does that mean exactly?,00:01:10,9,mean exactly
2439,00:01:14,4,Feedback in Vector Space Model- Rocchio,3.7,"Well, if you think about the query vector that would mean you would have to do",00:01:11,9,Well think query vector would mean would
2440,00:01:17,4,Feedback in Vector Space Model- Rocchio,3.7,something to vector elements.,00:01:14,9,something vector elements
2441,00:01:21,4,Feedback in Vector Space Model- Rocchio,3.7,And in general that would mean we might add new terms.,00:01:17,9,And general would mean might add new terms
2442,00:01:27,4,Feedback in Vector Space Model- Rocchio,3.7,We might adjust weights of old terms or assign weights to new terms.,00:01:21,9,We might adjust weights old terms assign weights new terms
2443,00:01:32,4,Feedback in Vector Space Model- Rocchio,3.7,And as a result in general the query will have more terms so,00:01:28,9,And result general query terms
2444,00:01:35,4,Feedback in Vector Space Model- Rocchio,3.7,we often call this query expansion.,00:01:32,9,often call query expansion
2445,00:01:41,4,Feedback in Vector Space Model- Rocchio,3.7,The most effective method in the vector space model of feedback is called Rocchio,00:01:37,9,The effective method vector space model feedback called Rocchio
2446,00:01:44,4,Feedback in Vector Space Model- Rocchio,3.7,feedback which was actually proposed several decades ago.,00:01:41,9,feedback actually proposed several decades ago
2447,00:01:53,4,Feedback in Vector Space Model- Rocchio,3.7,"So, the idea is quite simple we illustrate this idea by using a two-dimensional",00:01:47,9,So idea quite simple illustrate idea using two dimensional
2448,00:01:58,4,Feedback in Vector Space Model- Rocchio,3.7,display of all the documents in the collection and also the query vector.,00:01:53,9,display documents collection also query vector
2449,00:02:03,4,Feedback in Vector Space Model- Rocchio,3.7,"So, now we can see the query vector is here in",00:01:58,9,So see query vector
2450,00:02:07,4,Feedback in Vector Space Model- Rocchio,3.7,the center and these are all of the documents.,00:02:03,9,center documents
2451,00:02:09,4,Feedback in Vector Space Model- Rocchio,3.7,So when we use a query vector and,00:02:07,9,So use query vector
2452,00:02:13,4,Feedback in Vector Space Model- Rocchio,3.7,use a similarity function to find the most similar documents.,00:02:09,9,use similarity function find similar documents
2453,00:02:14,4,Feedback in Vector Space Model- Rocchio,3.7,We are basically drawing a circle here and,00:02:13,9,We basically drawing circle
2454,00:02:18,4,Feedback in Vector Space Model- Rocchio,3.7,then these documents would be basically the top-ranked documents.,00:02:14,9,documents would basically top ranked documents
2455,00:02:23,4,Feedback in Vector Space Model- Rocchio,3.7,"And this process of relevant documents, right?",00:02:18,9,And process relevant documents right
2456,00:02:27,4,Feedback in Vector Space Model- Rocchio,3.7,"And these are random documents for example that's relevant, etc.",00:02:23,9,And random documents example relevant etc
2457,00:02:32,4,Feedback in Vector Space Model- Rocchio,3.7,And then these minuses are negative documents like this.,00:02:27,9,And minuses negative documents like
2458,00:02:37,4,Feedback in Vector Space Model- Rocchio,3.7,So our goal here is trying,00:02:34,9,So goal trying
2459,00:02:42,4,Feedback in Vector Space Model- Rocchio,3.7,to move this query vector to some position to improve the retrieval accuracy.,00:02:37,9,move query vector position improve retrieval accuracy
2460,00:02:48,4,Feedback in Vector Space Model- Rocchio,3.7,By looking at this diagram what do you think where,00:02:42,9,By looking diagram think
2461,00:02:53,4,Feedback in Vector Space Model- Rocchio,3.7,should we move the query vector so that we can improve the retrieval accuracy.,00:02:48,9,move query vector improve retrieval accuracy
2462,00:02:57,4,Feedback in Vector Space Model- Rocchio,3.7,"Intuitively, where do you want to move the query back to?",00:02:53,9,Intuitively want move query back
2463,00:03:01,4,Feedback in Vector Space Model- Rocchio,3.7,If you want to think more you can pause the video.,00:02:58,9,If want think pause video
2464,00:03:04,4,Feedback in Vector Space Model- Rocchio,3.7,Now if you think about,00:03:03,9,Now think
2465,00:03:10,4,Feedback in Vector Space Model- Rocchio,3.7,this picture you can realize that in order to work well in this case,00:03:05,9,picture realize order work well case
2466,00:03:15,4,Feedback in Vector Space Model- Rocchio,3.7,you want the query vector to be as close to the positive vectors as possible.,00:03:10,9,want query vector close positive vectors possible
2467,00:03:20,4,Feedback in Vector Space Model- Rocchio,3.7,"That means, ideally you want to place the query vector somewhere here or",00:03:15,9,That means ideally want place query vector somewhere
2468,00:03:24,4,Feedback in Vector Space Model- Rocchio,3.7,we want to move the query vector closer to this point.,00:03:20,9,want move query vector closer point
2469,00:03:29,4,Feedback in Vector Space Model- Rocchio,3.7,"Now, so what exactly at this point?",00:03:26,9,Now exactly point
2470,00:03:35,4,Feedback in Vector Space Model- Rocchio,3.7,"Well, if you want these relevant documents to be ranked on the top",00:03:29,9,Well want relevant documents ranked top
2471,00:03:41,4,Feedback in Vector Space Model- Rocchio,3.7,"you want this to be in the center of all of these relevant documents, right?",00:03:35,9,want center relevant documents right
2472,00:03:44,4,Feedback in Vector Space Model- Rocchio,3.7,Because then if you draw a circle around this one,00:03:41,9,Because draw circle around one
2473,00:03:47,4,Feedback in Vector Space Model- Rocchio,3.7,you get all these relevant documents.,00:03:44,9,get relevant documents
2474,00:03:52,4,Feedback in Vector Space Model- Rocchio,3.7,So that means we can move the query back toward the centroid of,00:03:47,9,So means move query back toward centroid
2475,00:03:54,4,Feedback in Vector Space Model- Rocchio,3.7,all the relevant document vectors.,00:03:52,9,relevant document vectors
2476,00:03:57,4,Feedback in Vector Space Model- Rocchio,3.7,"And this is basically the idea of Rocchio,",00:03:55,9,And basically idea Rocchio
2477,00:04:02,4,Feedback in Vector Space Model- Rocchio,3.7,of course you then can see that the centroid of negative documents.,00:03:59,9,course see centroid negative documents
2478,00:04:06,4,Feedback in Vector Space Model- Rocchio,3.7,And one move away from the negative documents.,00:04:02,9,And one move away negative documents
2479,00:04:09,4,Feedback in Vector Space Model- Rocchio,3.7,Now geometrically we're talking about a moving vector,00:04:06,9,Now geometrically talking moving vector
2480,00:04:12,4,Feedback in Vector Space Model- Rocchio,3.7,closer to some other vector and away from other vectors.,00:04:09,9,closer vector away vectors
2481,00:04:17,4,Feedback in Vector Space Model- Rocchio,3.7,Algebraically it just means we have this formula.,00:04:13,9,Algebraically means formula
2482,00:04:22,4,Feedback in Vector Space Model- Rocchio,3.7,Here you can see this is original query vector and,00:04:18,9,Here see original query vector
2483,00:04:29,4,Feedback in Vector Space Model- Rocchio,3.7,this average basically is the centroid vector of relevant documents.,00:04:22,9,average basically centroid vector relevant documents
2484,00:04:32,4,Feedback in Vector Space Model- Rocchio,3.7,When we take the average over these vectors,00:04:29,9,When take average vectors
2485,00:04:35,4,Feedback in Vector Space Model- Rocchio,3.7,then we're computing the centroid of these vectors.,00:04:32,9,computing centroid vectors
2486,00:04:41,4,Feedback in Vector Space Model- Rocchio,3.7,And similarly this is the average in that non-relevant document of vectors so,00:04:35,9,And similarly average non relevant document vectors
2487,00:04:46,4,Feedback in Vector Space Model- Rocchio,3.7,"it's essentially of now random, documents.",00:04:41,9,essentially random documents
2488,00:04:51,4,Feedback in Vector Space Model- Rocchio,3.7,"And we have these three parameters here, alpha, beta and gamma.",00:04:46,9,And three parameters alpha beta gamma
2489,00:04:55,4,Feedback in Vector Space Model- Rocchio,3.7,They're controlling the amount of movement.,00:04:51,9,They controlling amount movement
2490,00:05:00,4,Feedback in Vector Space Model- Rocchio,3.7,When we add these two vectors together we're moving the query at the closer,00:04:55,9,When add two vectors together moving query closer
2491,00:05:05,4,Feedback in Vector Space Model- Rocchio,3.7,"to the centroid, alright, so when we add them, together.",00:05:00,9,centroid alright add together
2492,00:05:12,4,Feedback in Vector Space Model- Rocchio,3.7,When we subtracted this part we kind of move the query vector away from that,00:05:05,9,When subtracted part kind move query vector away
2493,00:05:18,4,Feedback in Vector Space Model- Rocchio,3.7,centroid so this is the main idea of Rocchio Feedback.,00:05:13,9,centroid main idea Rocchio Feedback
2494,00:05:23,4,Feedback in Vector Space Model- Rocchio,3.7,And after we have done this we will get a new query vector,00:05:18,9,And done get new query vector
2495,00:05:25,4,Feedback in Vector Space Model- Rocchio,3.7,which can use it to store documents.,00:05:23,9,use store documents
2496,00:05:30,4,Feedback in Vector Space Model- Rocchio,3.7,This new New query vector will then,00:05:25,9,This new New query vector
2497,00:05:35,4,Feedback in Vector Space Model- Rocchio,3.7,reflect the move of this Original query vector toward,00:05:30,9,reflect move Original query vector toward
2498,00:05:40,4,Feedback in Vector Space Model- Rocchio,3.7,this Relevant centroid vector and,00:05:35,9,Relevant centroid vector
2499,00:05:45,4,Feedback in Vector Space Model- Rocchio,3.7,"away from the Non-relevant centroid vector, okay?",00:05:40,9,away Non relevant centroid vector okay
2500,00:05:48,4,Feedback in Vector Space Model- Rocchio,3.7,"So let's take a look at example, right?",00:05:45,9,So let take look example right
2501,00:05:54,4,Feedback in Vector Space Model- Rocchio,3.7,"This is the example that we have seen earlier only that I in the, the display",00:05:48,9,This example seen earlier I display
2502,00:05:59,4,Feedback in Vector Space Model- Rocchio,3.7,of the actual documents I only showed the vector representation of these documents.,00:05:54,9,actual documents I showed vector representation documents
2503,00:06:03,4,Feedback in Vector Space Model- Rocchio,3.7,We have five documents here and we have,00:05:59,9,We five documents
2504,00:06:09,4,Feedback in Vector Space Model- Rocchio,3.7,"true red in the documents here, right?",00:06:04,9,true red documents right
2505,00:06:15,4,Feedback in Vector Space Model- Rocchio,3.7,They are displayed in red and these are the term vectors.,00:06:09,9,They displayed red term vectors
2506,00:06:18,4,Feedback in Vector Space Model- Rocchio,3.7,"Now, I just assumed an idea of weights,",00:06:15,9,Now I assumed idea weights
2507,00:06:20,4,Feedback in Vector Space Model- Rocchio,3.7,a lot of times we have zero weights of course.,00:06:18,9,lot times zero weights course
2508,00:06:26,4,Feedback in Vector Space Model- Rocchio,3.7,"These are negative documents, there are two here, there is another one here.",00:06:20,9,These negative documents two another one
2509,00:06:30,4,Feedback in Vector Space Model- Rocchio,3.7,Now in this Rocchio method we first compute the centroid of,00:06:26,9,Now Rocchio method first compute centroid
2510,00:06:34,4,Feedback in Vector Space Model- Rocchio,3.7,each category and so let's see.,00:06:30,9,category let see
2511,00:06:39,4,Feedback in Vector Space Model- Rocchio,3.7,Look at the centroid of the positive document but,00:06:36,9,Look centroid positive document
2512,00:06:42,4,Feedback in Vector Space Model- Rocchio,3.7,we simply just so it's very easy to see.,00:06:39,9,simply easy see
2513,00:06:48,4,Feedback in Vector Space Model- Rocchio,3.7,We just add this with this one the corresponding element and,00:06:42,9,We add one corresponding element
2514,00:06:51,4,Feedback in Vector Space Model- Rocchio,3.7,that's down here and take the average.,00:06:48,9,take average
2515,00:06:54,4,Feedback in Vector Space Model- Rocchio,3.7,And then we're going to add the corresponding elements and,00:06:51,9,And going add corresponding elements
2516,00:06:56,4,Feedback in Vector Space Model- Rocchio,3.7,"then just take the average, right?",00:06:54,9,take average right
2517,00:06:58,4,Feedback in Vector Space Model- Rocchio,3.7,So we do this for all these.,00:06:56,9,So
2518,00:07:02,4,Feedback in Vector Space Model- Rocchio,3.7,"In the end, what we have is this one.",00:06:58,9,In end one
2519,00:07:08,4,Feedback in Vector Space Model- Rocchio,3.7,"This is the average vector of these two so it's a centroid of these two, right?",00:07:02,9,This average vector two centroid two right
2520,00:07:13,4,Feedback in Vector Space Model- Rocchio,3.7,Let's also look at the centroid of the nested documents.,00:07:10,9,Let also look centroid nested documents
2521,00:07:18,4,Feedback in Vector Space Model- Rocchio,3.7,This is basically the same we're going to take the average of three elements.,00:07:13,9,This basically going take average three elements
2522,00:07:22,4,Feedback in Vector Space Model- Rocchio,3.7,And these are the corresponding elements in these three vectors and,00:07:18,9,And corresponding elements three vectors
2523,00:07:22,4,Feedback in Vector Space Model- Rocchio,3.7,so on and so forth.,00:07:22,9,forth
2524,00:07:25,4,Feedback in Vector Space Model- Rocchio,3.7,"So in the end, that we have this one.",00:07:22,9,So end one
2525,00:07:29,4,Feedback in Vector Space Model- Rocchio,3.7,"Now, in the Rocchio feedback method we're going to combine all",00:07:26,9,Now Rocchio feedback method going combine
2526,00:07:32,4,Feedback in Vector Space Model- Rocchio,3.7,"these with original query vector, which is this.",00:07:29,9,original query vector
2527,00:07:35,4,Feedback in Vector Space Model- Rocchio,3.7,So now let's see how we combine them together.,00:07:32,9,So let see combine together
2528,00:07:38,4,Feedback in Vector Space Model- Rocchio,3.7,"Well, that's basically this, right?",00:07:36,9,Well basically right
2529,00:07:45,4,Feedback in Vector Space Model- Rocchio,3.7,So we have a parameter outlier controlling the original query term weight that's 1.,00:07:38,9,So parameter outlier controlling original query term weight 1
2530,00:07:50,4,Feedback in Vector Space Model- Rocchio,3.7,And now I've beta to control the inference of the positive,00:07:45,9,And I beta control inference positive
2531,00:07:55,4,Feedback in Vector Space Model- Rocchio,3.7,"centroid Vector weight that's 1.5 that comes from here, right?",00:07:50,9,centroid Vector weight 1 5 comes right
2532,00:08:00,4,Feedback in Vector Space Model- Rocchio,3.7,So this goes here and,00:07:55,9,So goes
2533,00:08:04,4,Feedback in Vector Space Model- Rocchio,3.7,we also have this negative wait here.,00:08:00,9,also negative wait
2534,00:08:08,4,Feedback in Vector Space Model- Rocchio,3.7,Conduit by a gamma here and,00:08:04,9,Conduit gamma
2535,00:08:12,4,Feedback in Vector Space Model- Rocchio,3.7,this weight has come from of course the nective centroid here.,00:08:08,9,weight come course nective centroid
2536,00:08:18,4,Feedback in Vector Space Model- Rocchio,3.7,And we do exactly the same for other terms each is for one term.,00:08:14,9,And exactly terms one term
2537,00:08:23,4,Feedback in Vector Space Model- Rocchio,3.7,And this is our new vector.,00:08:22,9,And new vector
2538,00:08:31,4,Feedback in Vector Space Model- Rocchio,3.7,"And we're going to use this new query vector, this one to run the documents.",00:08:25,9,And going use new query vector one run documents
2539,00:08:33,4,Feedback in Vector Space Model- Rocchio,3.7,"You can imagine what would happen, right?",00:08:31,9,You imagine would happen right
2540,00:08:35,4,Feedback in Vector Space Model- Rocchio,3.7,Because of the movement that this one or,00:08:33,9,Because movement one
2541,00:08:38,4,Feedback in Vector Space Model- Rocchio,3.7,the match of these red documents much better.,00:08:35,9,match red documents much better
2542,00:08:42,4,Feedback in Vector Space Model- Rocchio,3.7,Because we move this vector closer to them and,00:08:38,9,Because move vector closer
2543,00:08:47,4,Feedback in Vector Space Model- Rocchio,3.7,"it's going to penalize these black documents, these non-relevant documents.",00:08:42,9,going penalize black documents non relevant documents
2544,00:08:49,4,Feedback in Vector Space Model- Rocchio,3.7,So this is precisely what we want from feedback.,00:08:47,9,So precisely want feedback
2545,00:08:57,4,Feedback in Vector Space Model- Rocchio,3.7,"Now of course, if we apply this method in practice we will see one potential problem",00:08:50,9,Now course apply method practice see one potential problem
2546,00:09:05,4,Feedback in Vector Space Model- Rocchio,3.7,and that is the original query has only four times that are not zero.,00:08:58,9,original query four times zero
2547,00:09:07,4,Feedback in Vector Space Model- Rocchio,3.7,"But after we do queries,",00:09:05,9,But queries
2548,00:09:13,4,Feedback in Vector Space Model- Rocchio,3.7,imagine you can imagine we'll have many terms that would have a number of weights.,00:09:07,9,imagine imagine many terms would number weights
2549,00:09:16,4,Feedback in Vector Space Model- Rocchio,3.7,So the calculation would have to involve more terms.,00:09:13,9,So calculation would involve terms
2550,00:09:21,4,Feedback in Vector Space Model- Rocchio,3.7,"In practice, we often truncate this vector and",00:09:18,9,In practice often truncate vector
2551,00:09:25,4,Feedback in Vector Space Model- Rocchio,3.7,only retain the terms which is the highest weight.,00:09:21,9,retain terms highest weight
2552,00:09:29,4,Feedback in Vector Space Model- Rocchio,3.7,So let's talk about how we use this method in practice.,00:09:27,9,So let talk use method practice
2553,00:09:35,4,Feedback in Vector Space Model- Rocchio,3.7,I just mentioned that we often truncate the vector consider only a small number,00:09:30,9,I mentioned often truncate vector consider small number
2554,00:09:38,4,Feedback in Vector Space Model- Rocchio,3.7,of words that have highest weights in the centroid vector.,00:09:35,9,words highest weights centroid vector
2555,00:09:39,4,Feedback in Vector Space Model- Rocchio,3.7,This is for efficiency concern.,00:09:38,9,This efficiency concern
2556,00:09:45,4,Feedback in Vector Space Model- Rocchio,3.7,I also say that here that a negative examples or non-relevant examples,00:09:41,9,I also say negative examples non relevant examples
2557,00:09:49,4,Feedback in Vector Space Model- Rocchio,3.7,tend not to be very useful especially compared with positive examples.,00:09:45,9,tend useful especially compared positive examples
2558,00:09:52,4,Feedback in Vector Space Model- Rocchio,3.7,"Now you can think about the, why.",00:09:50,9,Now think
2559,00:09:57,4,Feedback in Vector Space Model- Rocchio,3.7,One reason is because negative documents,00:09:52,9,One reason negative documents
2560,00:10:02,4,Feedback in Vector Space Model- Rocchio,3.7,tend to distract the query in all directions so when you take,00:09:57,9,tend distract query directions take
2561,00:10:06,4,Feedback in Vector Space Model- Rocchio,3.7,the average it doesn't really tell you where exactly it should be moving to.,00:10:02,9,average really tell exactly moving
2562,00:10:10,4,Feedback in Vector Space Model- Rocchio,3.7,"Whereas, positive documents tend to be clustered together and",00:10:06,9,Whereas positive documents tend clustered together
2563,00:10:13,4,Feedback in Vector Space Model- Rocchio,3.7,they respond to you to consistent the direction.,00:10:10,9,respond consistent direction
2564,00:10:19,4,Feedback in Vector Space Model- Rocchio,3.7,So that also means that sometimesw we don't have those negative examples but,00:10:13,9,So also means sometimesw negative examples
2565,00:10:19,4,Feedback in Vector Space Model- Rocchio,3.7,"note that in,",00:10:19,9,note
2566,00:10:24,4,Feedback in Vector Space Model- Rocchio,3.7,in some cases in difficult queries where most top random results are negative.,00:10:19,9,cases difficult queries top random results negative
2567,00:10:26,4,Feedback in Vector Space Model- Rocchio,3.7,Negative feedback afterwards is very useful.,00:10:24,9,Negative feedback afterwards useful
2568,00:10:30,4,Feedback in Vector Space Model- Rocchio,3.7,Another thing is to avoid over-fitting that means we have to,00:10:27,9,Another thing avoid fitting means
2569,00:10:34,4,Feedback in Vector Space Model- Rocchio,3.7,keep relatively high weight on the original query terms.,00:10:30,9,keep relatively high weight original query terms
2570,00:10:35,4,Feedback in Vector Space Model- Rocchio,3.7,Why?,00:10:35,9,Why
2571,00:10:42,4,Feedback in Vector Space Model- Rocchio,3.7,Because the sample that we see in feedback is a relatively small sample.,00:10:35,9,Because sample see feedback relatively small sample
2572,00:10:46,4,Feedback in Vector Space Model- Rocchio,3.7,We don't want to overly trust the small sample and,00:10:42,9,We want overly trust small sample
2573,00:10:49,4,Feedback in Vector Space Model- Rocchio,3.7,the original query terms are still very important.,00:10:46,9,original query terms still important
2574,00:10:51,4,Feedback in Vector Space Model- Rocchio,3.7,Those terms are typed in by the user and,00:10:49,9,Those terms typed user
2575,00:10:55,4,Feedback in Vector Space Model- Rocchio,3.7,the user has decided that those terms are most important.,00:10:51,9,user decided terms important
2576,00:11:01,4,Feedback in Vector Space Model- Rocchio,3.7,So in order to prevent the us from over-fitting or drifting.,00:10:55,9,So order prevent us fitting drifting
2577,00:11:07,4,Feedback in Vector Space Model- Rocchio,3.7,"A type of drift prevent type of drifting due to the bias toward the,",00:11:01,9,A type drift prevent type drifting due bias toward
2578,00:11:08,4,Feedback in Vector Space Model- Rocchio,3.7,the feedback examples.,00:11:07,9,feedback examples
2579,00:11:12,4,Feedback in Vector Space Model- Rocchio,3.7,We generally would have to keep a pretty high weight on the original terms so,00:11:08,9,We generally would keep pretty high weight original terms
2580,00:11:13,4,Feedback in Vector Space Model- Rocchio,3.7,it is safe to do that.,00:11:12,9,safe
2581,00:11:19,4,Feedback in Vector Space Model- Rocchio,3.7,"And this is especially, true for pseudo awareness feedback.",00:11:15,9,And especially true pseudo awareness feedback
2582,00:11:22,4,Feedback in Vector Space Model- Rocchio,3.7,Now this method can be used for both relevance feedback and,00:11:19,9,Now method used relevance feedback
2583,00:11:23,4,Feedback in Vector Space Model- Rocchio,3.7,pseudo relevance feedback.,00:11:22,9,pseudo relevance feedback
2584,00:11:27,4,Feedback in Vector Space Model- Rocchio,3.7,"In the case of pseudo feedback, the parameter beta should be set to a,",00:11:23,9,In case pseudo feedback parameter beta set
2585,00:11:32,4,Feedback in Vector Space Model- Rocchio,3.7,a smaller value because the random examples are assumed,00:11:27,9,smaller value random examples assumed
2586,00:11:36,4,Feedback in Vector Space Model- Rocchio,3.7,"to be random there not as reliable as your relevance feedback, right?",00:11:32,9,random reliable relevance feedback right
2587,00:11:40,4,Feedback in Vector Space Model- Rocchio,3.7,"In the case of relevance feedback, we obviously could use a larger value.",00:11:36,9,In case relevance feedback obviously could use larger value
2588,00:11:45,4,Feedback in Vector Space Model- Rocchio,3.7,"So, those parameters still have to be set and.",00:11:40,9,So parameters still set
2589,00:11:48,4,Feedback in Vector Space Model- Rocchio,3.7,"And the ro, Rocchio method is usually robust and effective.",00:11:45,9,And ro Rocchio method usually robust effective
2590,00:11:52,4,Feedback in Vector Space Model- Rocchio,3.7,"It's, it's still a very popular method for feedback.",00:11:48,9,It still popular method feedback
2591,00:00:03,5, Recommender Systems- Content-based Filtering - Part 2,4.6,[SOUND].,00:00:00,11,SOUND
2592,00:00:12,5, Recommender Systems- Content-based Filtering - Part 2,4.6,There are some interesting challenges in threshold.,00:00:09,11,There interesting challenges threshold
2593,00:00:15,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Would have known in the filtering problem.,00:00:12,11,Would known filtering problem
2594,00:00:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So here I show the, sort of the data that you can collect in,",00:00:15,11,So I show sort data collect
2595,00:00:19,5, Recommender Systems- Content-based Filtering - Part 2,4.6,in the filtering system.,00:00:18,11,filtering system
2596,00:00:25,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So you can see the scores and the status of relevance.,00:00:19,11,So see scores status relevance
2597,00:00:30,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So the first one has a score 36.5, and it's relevant.",00:00:25,11,So first one score 36 5 relevant
2598,00:00:34,5, Recommender Systems- Content-based Filtering - Part 2,4.6,The second one is not relevant.,00:00:30,11,The second one relevant
2599,00:00:38,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Of course, we have a lot of documents for which we don't know the status,",00:00:34,11,Of course lot documents know status
2600,00:00:40,5, Recommender Systems- Content-based Filtering - Part 2,4.6,because we will have to the user.,00:00:38,11,user
2601,00:00:42,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So as you can see here,",00:00:40,11,So see
2602,00:00:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,we only see the judgements of documents delivered to the user.,00:00:42,11,see judgements documents delivered user
2603,00:00:50,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So this is not a random sample.,00:00:47,11,So random sample
2604,00:00:52,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So it's a censored data.,00:00:50,11,So censored data
2605,00:00:56,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"It's kind of biased, so that creates some difficulty for learning.",00:00:52,11,It kind biased creates difficulty learning
2606,00:01:03,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And secondly, there are in general very little labeled data and very few relevant",00:00:57,11,And secondly general little labeled data relevant
2607,00:01:07,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"data, so it's, it's also challenging for machine learning approaches.",00:01:03,11,data also challenging machine learning approaches
2608,00:01:12,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Typically they require require more training data.,00:01:07,11,Typically require require training data
2609,00:01:15,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And in the extreme case at the beginning,",00:01:13,11,And extreme case beginning
2610,00:01:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"we don't even have any, label there as well.",00:01:15,11,even label well
2611,00:01:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"The system still has to make a decision, so",00:01:18,11,The system still make decision
2612,00:01:24,5, Recommender Systems- Content-based Filtering - Part 2,4.6,that's a very difficult problem at the beginning.,00:01:21,11,difficult problem beginning
2613,00:01:29,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Finally, the results of this issue of exploration versus exploitation tradeoff.",00:01:24,11,Finally results issue exploration versus exploitation tradeoff
2614,00:01:35,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Now this means we also want to,00:01:30,11,Now means also want
2615,00:01:41,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"explore the document space a little bit, and to, to see if the user",00:01:35,11,explore document space little bit see user
2616,00:01:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,might be interested in the documents that we have not yet labeled.,00:01:41,11,might interested documents yet labeled
2617,00:01:51,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So, in other words, we're going to explore the space of user interests",00:01:47,11,So words going explore space user interests
2618,00:01:54,5, Recommender Systems- Content-based Filtering - Part 2,4.6,by testing whether the user might be interested in some other documents that,00:01:51,11,testing whether user might interested documents
2619,00:02:00,5, Recommender Systems- Content-based Filtering - Part 2,4.6,currently are not matching the user's interest.,00:01:56,11,currently matching user interest
2620,00:02:00,5, Recommender Systems- Content-based Filtering - Part 2,4.6,This so well.,00:02:00,11,This well
2621,00:02:02,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So how do we do that?,00:02:01,11,So
2622,00:02:07,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Well we could lower the threshold a little bit and do we just deliver some near,00:02:02,11,Well could lower threshold little bit deliver near
2623,00:02:13,5, Recommender Systems- Content-based Filtering - Part 2,4.6,misses to the user to see what the user would respond so,00:02:07,11,misses user see user would respond
2624,00:02:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"see how the user will, would respond to this extra document.",00:02:13,11,see user would respond extra document
2625,00:02:24,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And, and this is a trade off, because on the one hand, you want to explore,",00:02:19,11,And trade one hand want explore
2626,00:02:28,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"but on the other hand, you don't want to really explore too much,",00:02:24,11,hand want really explore much
2627,00:02:31,5, Recommender Systems- Content-based Filtering - Part 2,4.6,because then you would over-deliver non-relevant information.,00:02:28,11,would deliver non relevant information
2628,00:02:36,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So exploitation means you would, exploit what you learn about the user.",00:02:31,11,So exploitation means would exploit learn user
2629,00:02:39,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And let's say you know the user is interested in this particular topic, so",00:02:36,11,And let say know user interested particular topic
2630,00:02:42,5, Recommender Systems- Content-based Filtering - Part 2,4.6,you don't want to deviate that much.,00:02:39,11,want deviate much
2631,00:02:45,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And, but if you don't deviate at all, then you don't explore at all.",00:02:42,11,And deviate explore
2632,00:02:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,That's also not good.,00:02:45,11,That also good
2633,00:02:50,5, Recommender Systems- Content-based Filtering - Part 2,4.6,You might miss opportunity to learn another interest of the user.,00:02:47,11,You might miss opportunity learn another interest user
2634,00:02:53,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So this is a dilemma.,00:02:51,11,So dilemma
2635,00:02:57,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And that's also a difficult problem to solve.,00:02:54,11,And also difficult problem solve
2636,00:03:00,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Now how do we solve these problems?,00:02:58,11,Now solve problems
2637,00:03:04,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"In general, I think why can't I used the empirical utility optimization strategy?",00:03:00,11,In general I think I used empirical utility optimization strategy
2638,00:03:09,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And this strategy is basically to optimize the threshold based on, historical data,",00:03:04,11,And strategy basically optimize threshold based historical data
2639,00:03:13,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"just as you have seen on the previous slide, right?",00:03:09,11,seen previous slide right
2640,00:03:16,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So you can just compute the utility on the training data for,00:03:13,11,So compute utility training data
2641,00:03:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,each candidate score threshold.,00:03:16,11,candidate score threshold
2642,00:03:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Pretend that [INAUDIBLE] cut at this point.,00:03:18,11,Pretend INAUDIBLE cut point
2643,00:03:26,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"What if I cut out the [INAUDIBLE] threshold, what would happen?",00:03:21,11,What I cut INAUDIBLE threshold would happen
2644,00:03:28,5, Recommender Systems- Content-based Filtering - Part 2,4.6,What's utility?,00:03:26,11,What utility
2645,00:03:33,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Compute the utility, right?",00:03:29,11,Compute utility right
2646,00:03:37,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"We know the status, what's it based on",00:03:33,11,We know status based
2647,00:03:43,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"approximation of click-throughs, right?",00:03:37,11,approximation click throughs right
2648,00:03:46,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So then we can just choose this threshold that gives the maximum,00:03:43,11,So choose threshold gives maximum
2649,00:03:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,utility on the training data.,00:03:46,11,utility training data
2650,00:03:56,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Now but this of course doesn't account for exploration that we just talked about.,00:03:49,11,Now course account exploration talked
2651,00:03:59,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And there is also the difficulty of bias.,00:03:56,11,And also difficulty bias
2652,00:04:01,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Training sample, as we mentioned.",00:03:59,11,Training sample mentioned
2653,00:04:07,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So in general, we can only get an upper bound or, for the true optimal threshold",00:04:01,11,So general get upper bound true optimal threshold
2654,00:04:13,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"because the, the al, the threshold might be actually lower than this.",00:04:07,11,al threshold might actually lower
2655,00:04:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So it's possible that the discarded item might be actually interesting to the user.,00:04:13,11,So possible discarded item might actually interesting user
2656,00:04:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So how do we solve this problem?,00:04:19,11,So solve problem
2657,00:04:27,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Well we generally as I said we can lower the threshold to explore a little bit.,00:04:21,11,Well generally I said lower threshold explore little bit
2658,00:04:30,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So here's one particular approach called the beta-gamma threshold learning.,00:04:27,11,So one particular approach called beta gamma threshold learning
2659,00:04:32,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So the, the idea is foreign.",00:04:30,11,So idea foreign
2660,00:04:35,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So, here I show a ranked list of",00:04:32,11,So I show ranked list
2661,00:04:38,5, Recommender Systems- Content-based Filtering - Part 2,4.6,all the training documents that we have seen so far.,00:04:35,11,training documents seen far
2662,00:04:40,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And they are ranked by their positions.,00:04:38,11,And ranked positions
2663,00:04:43,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And on the Y-axis, we show the Utility.",00:04:40,11,And Y axis show Utility
2664,00:04:46,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Of course, this function depends on how you specify the coefficients in",00:04:43,11,Of course function depends specify coefficients
2665,00:04:48,5, Recommender Systems- Content-based Filtering - Part 2,4.6,the Utility function.,00:04:46,11,Utility function
2666,00:04:53,5, Recommender Systems- Content-based Filtering - Part 2,4.6,But we can not imagine depending on the cut off position we will have a utility.,00:04:48,11,But imagine depending cut position utility
2667,00:04:59,5, Recommender Systems- Content-based Filtering - Part 2,4.6,That means suppose I cut at this position and that will be the utility.,00:04:53,11,That means suppose I cut position utility
2668,00:05:05,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So we can for example I then find some cut off point.,00:05:01,11,So example I find cut point
2669,00:05:11,5, Recommender Systems- Content-based Filtering - Part 2,4.6,The optimal point theta optimal is the point,00:05:06,11,The optimal point theta optimal point
2670,00:05:16,5, Recommender Systems- Content-based Filtering - Part 2,4.6,when we would achieve the maximum utility if we had chosen this threshold.,00:05:11,11,would achieve maximum utility chosen threshold
2671,00:05:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And there is also 0 threshold, 0 utility threshold.",00:05:17,11,And also 0 threshold 0 utility threshold
2672,00:05:25,5, Recommender Systems- Content-based Filtering - Part 2,4.6,As you can see at this cut off.,00:05:21,11,As see cut
2673,00:05:27,5, Recommender Systems- Content-based Filtering - Part 2,4.6,The utility is 0.,00:05:25,11,The utility 0
2674,00:05:28,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Now, what does that mean?",00:05:27,11,Now mean
2675,00:05:33,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"That means if I lower the threshold, and then get the, and now I'm I reach this",00:05:28,11,That means I lower threshold get I I reach
2676,00:05:37,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"threshold, the utility would be lower, but it's still positive.",00:05:33,11,threshold utility would lower still positive
2677,00:05:41,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Still non-elective, at least.",00:05:37,11,Still non elective least
2678,00:05:44,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So it's not as high as the optimal utility, but",00:05:41,11,So high optimal utility
2679,00:05:51,5, Recommender Systems- Content-based Filtering - Part 2,4.6,it gives us a a safe point to explore the threshold.,00:05:45,11,gives us safe point explore threshold
2680,00:05:56,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"As I just explained, it's desirable to explore the interest space.",00:05:51,11,As I explained desirable explore interest space
2681,00:05:59,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So it's desirable to lower the threshold based on your training data.,00:05:56,11,So desirable lower threshold based training data
2682,00:06:04,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So that means, in general, we want to set the threshold somewhere in this range.",00:06:00,11,So means general want set threshold somewhere range
2683,00:06:09,5, Recommender Systems- Content-based Filtering - Part 2,4.6,It's the when user off fault to control the the deviation from,00:06:04,11,It user fault control deviation
2684,00:06:13,5, Recommender Systems- Content-based Filtering - Part 2,4.6,the optimal utility point.,00:06:09,11,optimal utility point
2685,00:06:16,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So you can see the formula of the threshold will be just the incorporation,00:06:13,11,So see formula threshold incorporation
2686,00:06:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,of the zero utility threshold and the optimal between the threshold.,00:06:16,11,zero utility threshold optimal threshold
2687,00:06:27,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Now the question is how, how should we set r form, you know and",00:06:22,11,Now question set r form know
2688,00:06:32,5, Recommender Systems- Content-based Filtering - Part 2,4.6,when should we deviate more from the optimal utility point.,00:06:27,11,deviate optimal utility point
2689,00:06:38,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Well this can depend on multiple factors and the one way to solve the problem is to,00:06:33,11,Well depend multiple factors one way solve problem
2690,00:06:43,5, Recommender Systems- Content-based Filtering - Part 2,4.6,encourage this threshold mechanism to explore,00:06:38,11,encourage threshold mechanism explore
2691,00:06:48,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"up the 0 point, and that's a safe point, but",00:06:43,11,0 point safe point
2692,00:06:52,5, Recommender Systems- Content-based Filtering - Part 2,4.6,we're not going to necessarily reach all the way to the 0 point.,00:06:48,11,going necessarily reach way 0 point
2693,00:06:58,5, Recommender Systems- Content-based Filtering - Part 2,4.6,But rather we're going to use other parameters to further define alpha.,00:06:52,11,But rather going use parameters define alpha
2694,00:07:01,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And this specifically is as follows.,00:06:58,11,And specifically follows
2695,00:07:04,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So there will be a beta parameter to control.,00:07:01,11,So beta parameter control
2696,00:07:07,5, Recommender Systems- Content-based Filtering - Part 2,4.6,The deviation from the optimal threshold.,00:07:04,11,The deviation optimal threshold
2697,00:07:10,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And this can be based on for example can be accounting for,00:07:07,11,And based example accounting
2698,00:07:14,5, Recommender Systems- Content-based Filtering - Part 2,4.6,the over throughout the training data let's say.,00:07:10,11,throughout training data let say
2699,00:07:17,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And so this can be just the adjustment factor.,00:07:14,11,And adjustment factor
2700,00:07:22,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"But what's more interesting is this gamma parameter here, and you can see in this",00:07:17,11,But interesting gamma parameter see
2701,00:07:30,5, Recommender Systems- Content-based Filtering - Part 2,4.6,formula gamma is controlling the the influence,00:07:24,11,formula gamma controlling influence
2702,00:07:36,5, Recommender Systems- Content-based Filtering - Part 2,4.6,of the number of examples in training data set.,00:07:30,11,number examples training data set
2703,00:07:43,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So you can see in this formula as N which denotes the number of training examples.,00:07:36,11,So see formula N denotes number training examples
2704,00:07:50,5, Recommender Systems- Content-based Filtering - Part 2,4.6,Becomes bigger than it would actually encourage less exploration.,00:07:43,11,Becomes bigger would actually encourage less exploration
2705,00:07:55,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"In other words, when N is very small, it will try to explore more.",00:07:50,11,In words N small try explore
2706,00:07:59,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And that just means if we have seen few examples,",00:07:55,11,And means seen examples
2707,00:08:04,5, Recommender Systems- Content-based Filtering - Part 2,4.6,we're not sure whether we have exhausted the space of interests.,00:07:59,11,sure whether exhausted space interests
2708,00:08:05,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So [INAUDIBLE].,00:08:04,11,So INAUDIBLE
2709,00:08:09,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"But as we have seen many examples from the user, many data points,",00:08:05,11,But seen many examples user many data points
2710,00:08:13,5, Recommender Systems- Content-based Filtering - Part 2,4.6,then we feel that we probably dont' have to explore more.,00:08:09,11,feel probably dont explore
2711,00:08:17,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So this gives us a dynamic of strategy for exploration, right?",00:08:13,11,So gives us dynamic strategy exploration right
2712,00:08:21,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"The more examples we have seen, the less exploration we are going to do.",00:08:17,11,The examples seen less exploration going
2713,00:08:25,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So, the threshold will be closer to the optimal threshold.",00:08:21,11,So threshold closer optimal threshold
2714,00:08:28,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So, that's the basic idea of this approach.",00:08:25,11,So basic idea approach
2715,00:08:34,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"Now, this approach actually, has been working well in some evaluation studies.",00:08:28,11,Now approach actually working well evaluation studies
2716,00:08:36,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And, particularly effective.",00:08:34,11,And particularly effective
2717,00:08:42,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And, also can welcome arbitrary utility with a appropriate lower bound.",00:08:36,11,And also welcome arbitrary utility appropriate lower bound
2718,00:08:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And explicitly addresses exploration-exploration tradeoff.,00:08:43,11,And explicitly addresses exploration exploration tradeoff
2719,00:08:53,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And it kind of uses a zero in this threshold point as a, a safeguard.",00:08:47,11,And kind uses zero threshold point safeguard
2720,00:08:56,5, Recommender Systems- Content-based Filtering - Part 2,4.6,For exploration and exploiting tradeoff.,00:08:53,11,For exploration exploiting tradeoff
2721,00:09:02,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"We're not, never going to explore further than the zero utility point.",00:08:56,11,We never going explore zero utility point
2722,00:09:05,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"So, if you take the analogy of gambling, and you,",00:09:02,11,So take analogy gambling
2723,00:09:08,5, Recommender Systems- Content-based Filtering - Part 2,4.6,you don't want to risk losing money.,00:09:05,11,want risk losing money
2724,00:09:12,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"You know, so it's a safe strategy, a conservative strategy for exploration.",00:09:08,11,You know safe strategy conservative strategy exploration
2725,00:09:18,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And the problem is, of course, this approach is purely heuristic.",00:09:13,11,And problem course approach purely heuristic
2726,00:09:22,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And the zero utility lower bound is also often too conservative.,00:09:18,11,And zero utility lower bound also often conservative
2727,00:09:26,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And there are, of course, calls are more advanced than machine learning",00:09:22,11,And course calls advanced machine learning
2728,00:09:30,5, Recommender Systems- Content-based Filtering - Part 2,4.6,projects that have been proposed for solving these problems.,00:09:26,11,projects proposed solving problems
2729,00:09:33,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And this is a very active research area.,00:09:30,11,And active research area
2730,00:09:40,5, Recommender Systems- Content-based Filtering - Part 2,4.6,So to summarize there are two strategies for,00:09:35,11,So summarize two strategies
2731,00:09:43,5, Recommender Systems- Content-based Filtering - Part 2,4.6,recommending systems or filtering systems.,00:09:40,11,recommending systems filtering systems
2732,00:09:47,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"One is content based, which is looking at the item similarity.",00:09:43,11,One content based looking item similarity
2733,00:09:51,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"And the other is collaborative filtering, which is looking at the user similarity.",00:09:47,11,And collaborative filtering looking user similarity
2734,00:09:56,5, Recommender Systems- Content-based Filtering - Part 2,4.6,In this lecture we have covered content-based filtering approach.,00:09:52,11,In lecture covered content based filtering approach
2735,00:09:59,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"In the next lecture, we're going to talk about collaborative filtering.",00:09:56,11,In next lecture going talk collaborative filtering
2736,00:10:05,5, Recommender Systems- Content-based Filtering - Part 2,4.6,The content-based filtering system we generally have to solve,00:09:59,11,The content based filtering system generally solve
2737,00:10:11,5, Recommender Systems- Content-based Filtering - Part 2,4.6,"several problems related to filtering decision and learning, etc.",00:10:05,11,several problems related filtering decision learning etc
2738,00:10:16,5, Recommender Systems- Content-based Filtering - Part 2,4.6,And such a system can actually be based on a search engine,00:10:11,11,And system actually based search engine
2739,00:10:20,5, Recommender Systems- Content-based Filtering - Part 2,4.6,system by adding a threshold mechanism and,00:10:16,11,system adding threshold mechanism
2740,00:10:25,5, Recommender Systems- Content-based Filtering - Part 2,4.6,adding adaptive learning algorithm to allow the system,00:10:20,11,adding adaptive learning algorithm allow system
2741,00:10:30,5, Recommender Systems- Content-based Filtering - Part 2,4.6,to learn from long term feedback from the user.,00:10:25,11,learn long term feedback user
2742,00:00:07,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,[SOUND] So to summarize our discussion of recommender systems,00:00:00,14,SOUND So summarize discussion recommender systems
2743,00:00:14,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,in some sense the filtering task of recommended is easy and,00:00:07,14,sense filtering task recommended easy
2744,00:00:20,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,in some other sense and the task is actually difficult.,00:00:14,14,sense task actually difficult
2745,00:00:24,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So its easy because the user dexpectations, though in this case,",00:00:20,14,So easy user dexpectations though case
2746,00:00:29,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,the system takes initiative to push the information to the user.,00:00:24,14,system takes initiative push information user
2747,00:00:33,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,So the user doesn't really make an effort.,00:00:29,14,So user really make effort
2748,00:00:36,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So any recommendation is better than nothing, right?",00:00:33,14,So recommendation better nothing right
2749,00:00:40,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So unless you recommend that all the you know,",00:00:36,14,So unless recommend know
2750,00:00:45,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"noisy items or useless documents, if you can recommend that",00:00:40,14,noisy items useless documents recommend
2751,00:00:49,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"some useful information uses general, would appreciate it, all right.",00:00:45,14,useful information uses general would appreciate right
2752,00:00:52,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So that's in that sense, that's easy.",00:00:49,14,So sense easy
2753,00:00:55,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"However, filtering is actually a much harder task.",00:00:52,14,However filtering actually much harder task
2754,00:01:00,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"Because you have to make a binary decision, and you can't afford waiting for",00:00:55,14,Because make binary decision afford waiting
2755,00:01:06,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,a lot of items and then you will whether one item is better than others.,00:01:00,14,lot items whether one item better others
2756,00:01:09,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,You have to make a decision when you see this item.,00:01:06,14,You make decision see item
2757,00:01:13,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,Let's think about news filtering as well as you see the news.,00:01:09,14,Let think news filtering well see news
2758,00:01:16,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,And you have to decide whether the news would be interesting to a user.,00:01:13,14,And decide whether news would interesting user
2759,00:01:21,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"If you wait for a few days, well, even if you can make accurate recommendation of",00:01:16,14,If wait days well even make accurate recommendation
2760,00:01:26,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"the most relevant news, only two days wouldn't be significantly decreased.",00:01:21,14,relevant news two days significantly decreased
2761,00:01:32,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"Another reason why it's hard, it's because of data sparseness.",00:01:28,14,Another reason hard data sparseness
2762,00:01:35,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"If you think of this as a learning problem in collaborative filtering, for",00:01:32,14,If think learning problem collaborative filtering
2763,00:01:41,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"example, it's purely based on learning from the past ratings.",00:01:35,14,example purely based learning past ratings
2764,00:01:45,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So if you don't have many ratings, there's really not much you can do, right?",00:01:41,14,So many ratings really much right
2765,00:01:51,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,And may I just mention this problem.,00:01:47,14,And may I mention problem
2766,00:01:54,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,This is actually a very serious problem.,00:01:51,14,This actually serious problem
2767,00:01:59,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,But of course there are strategies that have been proposed to solve the problem.,00:01:54,14,But course strategies proposed solve problem
2768,00:02:00,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"And there are,",00:01:59,14,And
2769,00:02:04,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,there are different strategies that we will use to alleviate the problem.,00:02:00,14,different strategies use alleviate problem
2770,00:02:09,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"We can use, for example, more user information to assess their similarity",00:02:04,14,We use example user information assess similarity
2771,00:02:11,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,instead of using the preferences.,00:02:09,14,instead using preferences
2772,00:02:16,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,Of these users on these items the immediate additional information or,00:02:13,14,Of users items immediate additional information
2773,00:02:21,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"better for about the user etcetera and, and",00:02:16,14,better user etcetera
2774,00:02:26,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,we also talked about the two strategies for filtering task.,00:02:21,14,also talked two strategies filtering task
2775,00:02:30,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,One is content based where we look at items in clarity you,00:02:26,14,One content based look items clarity
2776,00:02:34,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,know there's a clarity of filtering where we look at the user similarity.,00:02:30,14,know clarity filtering look user similarity
2777,00:02:36,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,And they obviously can be combined.,00:02:34,14,And obviously combined
2778,00:02:40,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"In a practical system, you can imagine, they generally would have to be combined.",00:02:36,14,In practical system imagine generally would combined
2779,00:02:44,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,So that will give us a hybrid strategy for filtering.,00:02:40,14,So give us hybrid strategy filtering
2780,00:02:49,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"A, and, we also could recall that we",00:02:44,14,A also could recall
2781,00:02:54,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,talked about push versus pull as two strategies for,00:02:49,14,talked push versus pull two strategies
2782,00:02:58,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,getting access to the text data.,00:02:54,14,getting access text data
2783,00:03:02,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"And recommend the system is it will help, users in the push mode.",00:02:58,14,And recommend system help users push mode
2784,00:03:06,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"And search engines are, certain users in the pull mode.",00:03:02,14,And search engines certain users pull mode
2785,00:03:11,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"Of using the tool should be combined, and they can be combined into have a system",00:03:06,14,Of using tool combined combined system
2786,00:03:16,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,that can support user with multiple mode and formation access.,00:03:11,14,support user multiple mode formation access
2787,00:03:22,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"So in the future, we could anticipate for such a system to be more usable to a user.",00:03:16,14,So future could anticipate system usable user
2788,00:03:27,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,And also this is a active research area so,00:03:22,14,And also active research area
2789,00:03:33,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"there are a lot of new algorithms being, being proposed over time.",00:03:27,14,lot new algorithms proposed time
2790,00:03:39,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"In particular, those new algorithms tend to use a lot of context information.",00:03:33,14,In particular new algorithms tend use lot context information
2791,00:03:42,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"Now the context here could be the context of the user, you know,",00:03:39,14,Now context could context user know
2792,00:03:45,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,it could also be context of documents or items.,00:03:42,14,could also context documents items
2793,00:03:47,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,The items are not isolated.,00:03:45,14,The items isolated
2794,00:03:49,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,They are connected in many ways.,00:03:47,14,They connected many ways
2795,00:03:57,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"The users might form social network as well, so there's a rich context there",00:03:51,14,The users might form social network well rich context
2796,00:04:02,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,"that we can leverage in order to really solve the problem well, and then that's",00:03:57,14,leverage order really solve problem well
2797,00:04:09,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,a active research area where also machine learning algorithms have been applied.,00:04:02,14,active research area also machine learning algorithms applied
2798,00:04:17,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,Here are some additional readings in the handbook called Recommender Systems.,00:04:09,14,Here additional readings handbook called Recommender Systems
2799,00:04:24,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,And has a collection of a lot of good articles that,00:04:17,14,And collection lot good articles
2800,00:04:28,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,can give you an overview of a number of specific,00:04:24,14,give overview number specific
2801,00:04:33,5,Recommender Systems- Collaborative Filtering - Part 3,4.7,approaches to recommender systems.,00:04:28,14,approaches recommender systems
2802,00:00:05,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,[SOUND] This lecture is about,00:00:00,12,SOUND This lecture
2803,00:00:10,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,Collaborative Filtering.,00:00:05,12,Collaborative Filtering
2804,00:00:16,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"In this lecture, we're going to continue the discussion of Recommender Systems.",00:00:10,12,In lecture going continue discussion Recommender Systems
2805,00:00:20,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"In particular, we're going to look at the approach of collaborative filtering.",00:00:16,12,In particular going look approach collaborative filtering
2806,00:00:25,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,You have seen this slide before when we talked about the two,00:00:20,12,You seen slide talked two
2807,00:00:30,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,strategies to answer the basic question will user U like item X.,00:00:25,12,strategies answer basic question user U like item X
2808,00:00:33,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"In the previous lecture, we looked at the item similarity,",00:00:30,12,In previous lecture looked item similarity
2809,00:00:35,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,that's content-based filtering.,00:00:33,12,content based filtering
2810,00:00:39,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"In this lecture, we're going to look at the user similarity.",00:00:35,12,In lecture going look user similarity
2811,00:00:42,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,This is a different strategy called collaborative filtering.,00:00:39,12,This different strategy called collaborative filtering
2812,00:00:46,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So first of all, what is collaborative filtering?",00:00:42,12,So first collaborative filtering
2813,00:00:48,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,It is to make filtering decisions for,00:00:46,12,It make filtering decisions
2814,00:00:53,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,individual user based on the judgement of other users and,00:00:48,12,individual user based judgement users
2815,00:00:57,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"that is to say, we will infer individual's interest or",00:00:53,12,say infer individual interest
2816,00:01:01,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"preferences from that, of other similar users.",00:00:57,12,preferences similar users
2817,00:01:04,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,So the general idea is the following.,00:01:01,12,So general idea following
2818,00:01:09,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"Given a user u, we are going to first find the similar users,",00:01:04,12,Given user u going first find similar users
2819,00:01:16,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,u1 through and then we're going to predict the used preferences based on,00:01:09,12,u1 going predict used preferences based
2820,00:01:21,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"the preferences of these similar users, u1 through.",00:01:16,12,preferences similar users u1
2821,00:01:26,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,Now the users similarity here can be judged based on their similarity.,00:01:21,12,Now users similarity judged based similarity
2822,00:01:29,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,The preference is on a common set of items.,00:01:26,12,The preference common set items
2823,00:01:35,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,Now here you'll see that the exact content of item doesn't really matter.,00:01:29,12,Now see exact content item really matter
2824,00:01:39,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"We're going to look at the only, the relationship between the users and",00:01:35,12,We going look relationship users
2825,00:01:40,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,the items.,00:01:39,12,items
2826,00:01:44,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,So this means this approach is very general if it can be,00:01:40,12,So means approach general
2827,00:01:49,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,applied to any items not just with text objects.,00:01:44,12,applied items text objects
2828,00:01:53,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So this approach, it would work well under the following assumptions.",00:01:49,12,So approach would work well following assumptions
2829,00:01:59,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,First users with the same interests will have similar preferences.,00:01:53,12,First users interests similar preferences
2830,00:02:03,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"Second, the users with similar preferences probably share the same interests.",00:01:59,12,Second users similar preferences probably share interests
2831,00:02:08,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So for example, if the interest of the user is in information retrieval,",00:02:03,12,So example interest user information retrieval
2832,00:02:12,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,then we can infer the user probably favor SIGIR papers.,00:02:08,12,infer user probably favor SIGIR papers
2833,00:02:17,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,And so those who are interested in information retrieval researches probably,00:02:12,12,And interested information retrieval researches probably
2834,00:02:21,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"all favor SIGIR papers, that's something that we make.",00:02:17,12,favor SIGIR papers something make
2835,00:02:23,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"And if this assumption is true,",00:02:21,12,And assumption true
2836,00:02:27,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,then it would help collaborative filtering to work well.,00:02:23,12,would help collaborative filtering work well
2837,00:02:32,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"We can also assume that if we see people favor SIGIR papers,",00:02:27,12,We also assume see people favor SIGIR papers
2838,00:02:38,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,then we can infer the interest is probably information retrieval.,00:02:32,12,infer interest probably information retrieval
2839,00:02:42,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So these simple examples, it seems what makes sense.",00:02:38,12,So simple examples seems makes sense
2840,00:02:47,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,And in many cases such as assumption actually does make sense.,00:02:42,12,And many cases assumption actually make sense
2841,00:02:52,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So, another assumption you have to make is that there are a sufficiently large",00:02:47,12,So another assumption make sufficiently large
2842,00:02:55,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,number of user preferences available to us.,00:02:52,12,number user preferences available us
2843,00:03:00,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So for example, if you see a lot of ratings of users for movies and",00:02:55,12,So example see lot ratings users movies
2844,00:03:03,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,those indicate their preferences in movies.,00:03:00,12,indicate preferences movies
2845,00:03:05,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"And if you have a lot of such data,",00:03:03,12,And lot data
2846,00:03:09,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,then collaborative filtering can be very effective.,00:03:05,12,collaborative filtering effective
2847,00:03:14,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"If not, there will be a problem and that's often called a cold start problem.",00:03:09,12,If problem often called cold start problem
2848,00:03:18,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"That means you don't have many preferences available, so",00:03:14,12,That means many preferences available
2849,00:03:23,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,the system could not fully take advantage of collaborative filtering yet.,00:03:18,12,system could fully take advantage collaborative filtering yet
2850,00:03:29,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,So let's look at the collaborative filtering problem in a more formal way.,00:03:23,12,So let look collaborative filtering problem formal way
2851,00:03:36,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,And so this picture shows that we are in general considering a lot of users and,00:03:29,12,And picture shows general considering lot users
2852,00:03:39,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,showing we're showing m users here.,00:03:36,12,showing showing users
2853,00:03:45,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So, u1 through and we're also considering a number of objects.",00:03:39,12,So u1 also considering number objects
2854,00:03:50,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"Let's say, n objects denoted as o1 through on and",00:03:45,12,Let say n objects denoted o1
2855,00:03:56,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,then we will assume that the users will be able to judge those objects and,00:03:50,12,assume users able judge objects
2856,00:04:01,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"the user could for example, give ratings to those items.",00:03:56,12,user could example give ratings items
2857,00:04:05,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"For example, those items could be movies, could be products and",00:04:01,12,For example items could movies could products
2858,00:04:10,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"then the users would give ratings one through five, let's say.",00:04:05,12,users would give ratings one five let say
2859,00:04:14,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,So what you see here is that we have assumed some ratings available for,00:04:10,12,So see assumed ratings available
2860,00:04:16,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,some combinations.,00:04:14,12,combinations
2861,00:04:21,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So some users have watched movies, they have rated those movies.",00:04:16,12,So users watched movies rated movies
2862,00:04:25,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,They obviously won't be able to watch all the movies and,00:04:21,12,They obviously able watch movies
2863,00:04:29,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,some users may actually only watch a few movies.,00:04:25,12,users may actually watch movies
2864,00:04:34,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So this is in general a response matrix, right?",00:04:29,12,So general response matrix right
2865,00:04:39,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,So many item many entries have unknown values and,00:04:34,12,So many item many entries unknown values
2866,00:04:44,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,what's interesting here is we could potentially infer,00:04:39,12,interesting could potentially infer
2867,00:04:49,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,the value of a element in this matrix based on other values and,00:04:44,12,value element matrix based values
2868,00:04:55,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,that's actually the central question in collaborative filtering.,00:04:49,12,actually central question collaborative filtering
2869,00:04:59,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"And that is, we assume an unknown function here f,",00:04:55,12,And assume unknown function f
2870,00:05:03,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,that would map a pair of user and object to a rating.,00:04:59,12,would map pair user object rating
2871,00:05:09,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,And we have observed there are some values of this function and,00:05:03,12,And observed values function
2872,00:05:13,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,we want to infer the value of this function for,00:05:09,12,want infer value function
2873,00:05:18,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"other pairs that we, that don't have values available here.",00:05:13,12,pairs values available
2874,00:05:24,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So this is ve, very similar to other machine learning problems,",00:05:18,12,So similar machine learning problems
2875,00:05:31,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,where we would know the values of the function on some training there that and,00:05:24,12,would know values function training
2876,00:05:37,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,we hope to predict the the values of this function on some test there.,00:05:31,12,hope predict values function test
2877,00:05:40,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,All right. So this is the function approximation.,00:05:37,12,All right So function approximation
2878,00:05:47,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,And how can we pick out the function based on the observed ratings?,00:05:40,12,And pick function based observed ratings
2879,00:05:49,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"So this is the, the setup.",00:05:47,12,So setup
2880,00:05:53,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,Now there are many approaches to solving this problem.,00:05:49,12,Now many approaches solving problem
2881,00:05:57,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,"And in fact, this is a very active research area.",00:05:53,12,And fact active research area
2882,00:06:02,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,A reason that there are special conferences dedicated to the problem,00:05:57,12,A reason special conferences dedicated problem
2883,00:06:06,5,Recommender Systems- Collaborative Filtering - Part 1,4.7,is a major conference devoted to the problem.,00:06:02,12,major conference devoted problem
2884,00:00:06,2,Natural Language Content Analysis,1.1,[SOUND] This lecture is about,00:00:00,1,SOUND This lecture
2885,00:00:13,2,Natural Language Content Analysis,1.1,natural language content analysis.,00:00:06,1,natural language content analysis
2886,00:00:15,2,Natural Language Content Analysis,1.1,"As you see from this picture,",00:00:13,1,As see picture
2887,00:00:19,2,Natural Language Content Analysis,1.1,this is really the first step to process any text data.,00:00:15,1,really first step process text data
2888,00:00:22,2,Natural Language Content Analysis,1.1,Text data are in natural languages.,00:00:19,1,Text data natural languages
2889,00:00:26,2,Natural Language Content Analysis,1.1,"So, computers have to understand natural languages to some extent in",00:00:22,1,So computers understand natural languages extent
2890,00:00:31,2,Natural Language Content Analysis,1.1,"order to make use of the data, so that's the topic of this lecture.",00:00:26,1,order make use data topic lecture
2891,00:00:33,2,Natural Language Content Analysis,1.1,We're going to cover three things.,00:00:31,1,We going cover three things
2892,00:00:38,2,Natural Language Content Analysis,1.1,"First, what is natural language processing, which is a main technique for",00:00:33,1,First natural language processing main technique
2893,00:00:41,2,Natural Language Content Analysis,1.1,processing natural language to obtain understanding?,00:00:38,1,processing natural language obtain understanding
2894,00:00:47,2,Natural Language Content Analysis,1.1,"The second is the State of the Art in NLP, which stands for",00:00:43,1,The second State Art NLP stands
2895,00:00:48,2,Natural Language Content Analysis,1.1,natural language processing.,00:00:47,1,natural language processing
2896,00:00:53,2,Natural Language Content Analysis,1.1,"Finally, we're going to cover the relation between natural language processing and",00:00:49,1,Finally going cover relation natural language processing
2897,00:00:54,2,Natural Language Content Analysis,1.1,text retrieval.,00:00:53,1,text retrieval
2898,00:00:57,2,Natural Language Content Analysis,1.1,"First, what is NLP?",00:00:54,1,First NLP
2899,00:01:00,2,Natural Language Content Analysis,1.1,"Well, the best way to explain it is to think about,",00:00:57,1,Well best way explain think
2900,00:01:05,2,Natural Language Content Analysis,1.1,if you see a text in a foreign language that you can't understand.,00:01:00,1,see text foreign language understand
2901,00:01:10,2,Natural Language Content Analysis,1.1,"Now, what you have to do in order to understand that text?",00:01:06,1,Now order understand text
2902,00:01:13,2,Natural Language Content Analysis,1.1,This is basically what computers are facing.,00:01:10,1,This basically computers facing
2903,00:01:15,2,Natural Language Content Analysis,1.1,"Right? So, looking at the simple sentence like,",00:01:13,1,Right So looking simple sentence like
2904,00:01:17,2,Natural Language Content Analysis,1.1,a dog is chasing a boy on the playground.,00:01:15,1,dog chasing boy playground
2905,00:01:22,2,Natural Language Content Analysis,1.1,"We don't have any problems understanding this sentence, but",00:01:18,1,We problems understanding sentence
2906,00:01:25,2,Natural Language Content Analysis,1.1,imagine what the computer would have to do in order to understand it.,00:01:22,1,imagine computer would order understand
2907,00:01:27,2,Natural Language Content Analysis,1.1,"For in general, it would have to do the following.",00:01:25,1,For general would following
2908,00:01:34,2,Natural Language Content Analysis,1.1,"First, it would have to know dog is a noun, chasing's a verb, et cetera.",00:01:27,1,First would know dog noun chasing verb et cetera
2909,00:01:38,2,Natural Language Content Analysis,1.1,"So, this is a code lexile analysis or part of speech tagging.",00:01:34,1,So code lexile analysis part speech tagging
2910,00:01:42,2,Natural Language Content Analysis,1.1,"And, we need to pick out the, the syntaxing categories of those words.",00:01:38,1,And need pick syntaxing categories words
2911,00:01:43,2,Natural Language Content Analysis,1.1,"So, that's a first step.",00:01:42,1,So first step
2912,00:01:47,2,Natural Language Content Analysis,1.1,"After that, we're going to figure out the structure of the sentence.",00:01:43,1,After going figure structure sentence
2913,00:01:50,2,Natural Language Content Analysis,1.1,"So for example, here it shows that a and",00:01:47,1,So example shows
2914,00:01:54,2,Natural Language Content Analysis,1.1,dog would go together to form a noun phrase.,00:01:50,1,dog would go together form noun phrase
2915,00:02:00,2,Natural Language Content Analysis,1.1,"And, we won't have dog and is to go first, right.",00:01:55,1,And dog go first right
2916,00:02:02,2,Natural Language Content Analysis,1.1,"And, there are some structures that are not just right.",00:02:00,1,And structures right
2917,00:02:09,2,Natural Language Content Analysis,1.1,"But, this structure shows what we might get if we look at the sentence and",00:02:04,1,But structure shows might get look sentence
2918,00:02:11,2,Natural Language Content Analysis,1.1,try to interpret the sentence.,00:02:09,1,try interpret sentence
2919,00:02:13,2,Natural Language Content Analysis,1.1,"Some words would go together first, and",00:02:11,1,Some words would go together first
2920,00:02:15,2,Natural Language Content Analysis,1.1,then they will go together with other words.,00:02:13,1,go together words
2921,00:02:20,2,Natural Language Content Analysis,1.1,"So here, we show we have noun phrases as intermediate components and",00:02:16,1,So show noun phrases intermediate components
2922,00:02:21,2,Natural Language Content Analysis,1.1,then verb phrases.,00:02:20,1,verb phrases
2923,00:02:23,2,Natural Language Content Analysis,1.1,"Finally, we have a sentence.",00:02:21,1,Finally sentence
2924,00:02:28,2,Natural Language Content Analysis,1.1,"And, you get this structure, we need to do something called a syntactic analysis,",00:02:23,1,And get structure need something called syntactic analysis
2925,00:02:29,2,Natural Language Content Analysis,1.1,or parsing.,00:02:28,1,parsing
2926,00:02:30,2,Natural Language Content Analysis,1.1,"And, we may have a parser,",00:02:29,1,And may parser
2927,00:02:34,2,Natural Language Content Analysis,1.1,a computer program that would automatically create this structure.,00:02:30,1,computer program would automatically create structure
2928,00:02:38,2,Natural Language Content Analysis,1.1,"At this point, you would know the structure of this sentence, but",00:02:34,1,At point would know structure sentence
2929,00:02:40,2,Natural Language Content Analysis,1.1,still you don't know the meaning of the sentence.,00:02:38,1,still know meaning sentence
2930,00:02:44,2,Natural Language Content Analysis,1.1,"So, we have to go further through semantic analysis.",00:02:40,1,So go semantic analysis
2931,00:02:45,2,Natural Language Content Analysis,1.1,"In our mind,",00:02:44,1,In mind
2932,00:02:51,2,Natural Language Content Analysis,1.1,we usually can map such a sentence to what we already know in our knowledge base.,00:02:45,1,usually map sentence already know knowledge base
2933,00:02:53,2,Natural Language Content Analysis,1.1,"And for example, you might imagine a dog that looks like that,",00:02:51,1,And example might imagine dog looks like
2934,00:02:56,2,Natural Language Content Analysis,1.1,there's a boy and there's some activity here.,00:02:53,1,boy activity
2935,00:02:59,2,Natural Language Content Analysis,1.1,"But for computer, will have to use symbols to denote that.",00:02:56,1,But computer use symbols denote
2936,00:03:01,2,Natural Language Content Analysis,1.1,All right.,00:02:59,1,All right
2937,00:03:05,2,Natural Language Content Analysis,1.1,"So, we would use the symbol d1 to denote a dog.",00:03:01,1,So would use symbol d1 denote dog
2938,00:03:10,2,Natural Language Content Analysis,1.1,"And, b1 to denote a boy, and then p1 to denote the playground, playground.",00:03:05,1,And b1 denote boy p1 denote playground playground
2939,00:03:15,2,Natural Language Content Analysis,1.1,"Now, there is also a chasing activity that's happening here, so",00:03:12,1,Now also chasing activity happening
2940,00:03:19,2,Natural Language Content Analysis,1.1,"we have the relation chasing here, that connects all these symbols.",00:03:15,1,relation chasing connects symbols
2941,00:03:23,2,Natural Language Content Analysis,1.1,"So, this is how a computer would obtain some understanding of this sentence.",00:03:19,1,So computer would obtain understanding sentence
2942,00:03:31,2,Natural Language Content Analysis,1.1,"Now from this representation, we could also further infer some other things,",00:03:25,1,Now representation could also infer things
2943,00:03:35,2,Natural Language Content Analysis,1.1,"and we might indeed, naturally think of something else when we read text.",00:03:31,1,might indeed naturally think something else read text
2944,00:03:37,2,Natural Language Content Analysis,1.1,"And, this is call inference.",00:03:35,1,And call inference
2945,00:03:42,2,Natural Language Content Analysis,1.1,"So for example, if you believe that if someone's being chased and",00:03:37,1,So example believe someone chased
2946,00:03:44,2,Natural Language Content Analysis,1.1,this person might be scared.,00:03:42,1,person might scared
2947,00:03:45,2,Natural Language Content Analysis,1.1,All right.,00:03:44,1,All right
2948,00:03:46,2,Natural Language Content Analysis,1.1,"With this rule,",00:03:45,1,With rule
2949,00:03:50,2,Natural Language Content Analysis,1.1,you can see computers could also infer that this boy may be scared.,00:03:46,1,see computers could also infer boy may scared
2950,00:03:54,2,Natural Language Content Analysis,1.1,"So, this is some extra knowledge that you would infer based on",00:03:50,1,So extra knowledge would infer based
2951,00:03:56,2,Natural Language Content Analysis,1.1,some understanding of the text.,00:03:54,1,understanding text
2952,00:04:02,2,Natural Language Content Analysis,1.1,"You can even go further to understand the, why the person said this sentence.",00:03:56,1,You even go understand person said sentence
2953,00:04:04,2,Natural Language Content Analysis,1.1,"So, this has to do with the use of language.",00:04:02,1,So use language
2954,00:04:05,2,Natural Language Content Analysis,1.1,All right.,00:04:04,1,All right
2955,00:04:08,2,Natural Language Content Analysis,1.1,This is called pragmatic analysis.,00:04:05,1,This called pragmatic analysis
2956,00:04:14,2,Natural Language Content Analysis,1.1,"In order to understand the speech actor of a sentence, all right,",00:04:08,1,In order understand speech actor sentence right
2957,00:04:18,2,Natural Language Content Analysis,1.1,we say something to basically achieve some goal.,00:04:14,1,say something basically achieve goal
2958,00:04:22,2,Natural Language Content Analysis,1.1,There's some purpose there and this has to do with the use of language.,00:04:18,1,There purpose use language
2959,00:04:24,2,Natural Language Content Analysis,1.1,"In this case, the person who said",00:04:22,1,In case person said
2960,00:04:29,2,Natural Language Content Analysis,1.1,the sentence might be reminding another person to bring back the dog.,00:04:24,1,sentence might reminding another person bring back dog
2961,00:04:31,2,Natural Language Content Analysis,1.1,That could be one possible intent.,00:04:29,1,That could one possible intent
2962,00:04:39,2,Natural Language Content Analysis,1.1,"To reach this level of understanding, we would require all these steps.",00:04:33,1,To reach level understanding would require steps
2963,00:04:43,2,Natural Language Content Analysis,1.1,"And, a computer would have to go through all these steps in order to",00:04:39,1,And computer would go steps order
2964,00:04:46,2,Natural Language Content Analysis,1.1,completely understand this sentence.,00:04:43,1,completely understand sentence
2965,00:04:49,2,Natural Language Content Analysis,1.1,"Yet, we humans have no trouble with understand that.",00:04:46,1,Yet humans trouble understand
2966,00:04:53,2,Natural Language Content Analysis,1.1,"We instantly, will get everything, and there is a reason for that.",00:04:49,1,We instantly get everything reason
2967,00:04:57,2,Natural Language Content Analysis,1.1,"That's because we have a large knowledge base in our brain, and",00:04:53,1,That large knowledge base brain
2968,00:05:01,2,Natural Language Content Analysis,1.1,we use common sense knowledge to help interpret the sentence.,00:04:57,1,use common sense knowledge help interpret sentence
2969,00:05:06,2,Natural Language Content Analysis,1.1,"Computers, unfortunately, are hard to obtain such understanding.",00:05:01,1,Computers unfortunately hard obtain understanding
2970,00:05:08,2,Natural Language Content Analysis,1.1,They don't have such a knowledge base.,00:05:06,1,They knowledge base
2971,00:05:12,2,Natural Language Content Analysis,1.1,They are still incapable of doing reasoning and uncertainties.,00:05:08,1,They still incapable reasoning uncertainties
2972,00:05:18,2,Natural Language Content Analysis,1.1,"So, that makes natural language processing difficult for computers.",00:05:14,1,So makes natural language processing difficult computers
2973,00:05:21,2,Natural Language Content Analysis,1.1,"But, the fundamental reason why the natural language processing is difficult",00:05:18,1,But fundamental reason natural language processing difficult
2974,00:05:25,2,Natural Language Content Analysis,1.1,for computers is simple because natural language has not been designed for,00:05:21,1,computers simple natural language designed
2975,00:05:26,2,Natural Language Content Analysis,1.1,computers.,00:05:25,1,computers
2976,00:05:30,2,Natural Language Content Analysis,1.1,"They, they, natural languages are designed for us to communicate.",00:05:26,1,They natural languages designed us communicate
2977,00:05:33,2,Natural Language Content Analysis,1.1,There are other languages designed for computers.,00:05:30,1,There languages designed computers
2978,00:05:36,2,Natural Language Content Analysis,1.1,"For example, program languages.",00:05:33,1,For example program languages
2979,00:05:38,2,Natural Language Content Analysis,1.1,"Those are harder for us, right.",00:05:36,1,Those harder us right
2980,00:05:43,2,Natural Language Content Analysis,1.1,"So, natural languages is designed to make our communication efficient.",00:05:38,1,So natural languages designed make communication efficient
2981,00:05:46,2,Natural Language Content Analysis,1.1,"As a result, we omit a lot of common sense knowledge",00:05:43,1,As result omit lot common sense knowledge
2982,00:05:49,2,Natural Language Content Analysis,1.1,because we assume everyone knows about that.,00:05:46,1,assume everyone knows
2983,00:05:54,2,Natural Language Content Analysis,1.1,"We also keep a lot of ambiguities because we assume the receiver, or",00:05:49,1,We also keep lot ambiguities assume receiver
2984,00:05:59,2,Natural Language Content Analysis,1.1,"the hearer could know how to discern an ambiguous word,",00:05:54,1,hearer could know discern ambiguous word
2985,00:06:02,2,Natural Language Content Analysis,1.1,based on the knowledge or the context.,00:05:59,1,based knowledge context
2986,00:06:05,2,Natural Language Content Analysis,1.1,There's no need to invent a different word for different meanings.,00:06:02,1,There need invent different word different meanings
2987,00:06:08,2,Natural Language Content Analysis,1.1,We could overload the same word with different meanings without the problem.,00:06:05,1,We could overload word different meanings without problem
2988,00:06:11,2,Natural Language Content Analysis,1.1,"Because of these reasons,",00:06:10,1,Because reasons
2989,00:06:15,2,Natural Language Content Analysis,1.1,this makes every step in natural language of processing difficult for computers.,00:06:11,1,makes every step natural language processing difficult computers
2990,00:06:18,2,Natural Language Content Analysis,1.1,"Ambiguity's the main difficulty, and",00:06:15,1,Ambiguity main difficulty
2991,00:06:22,2,Natural Language Content Analysis,1.1,"common sense reasoning is often required, that's also hard.",00:06:18,1,common sense reasoning often required also hard
2992,00:06:26,2,Natural Language Content Analysis,1.1,"So, let me give you some examples of challenges here.",00:06:23,1,So let give examples challenges
2993,00:06:30,2,Natural Language Content Analysis,1.1,Conceded the word-level ambiguities.,00:06:26,1,Conceded word level ambiguities
2994,00:06:34,2,Natural Language Content Analysis,1.1,The same word can have different syntactical categories.,00:06:30,1,The word different syntactical categories
2995,00:06:36,2,Natural Language Content Analysis,1.1,"For example, design can be a noun or a verb.",00:06:34,1,For example design noun verb
2996,00:06:42,2,Natural Language Content Analysis,1.1,The word root may have multiple meanings.,00:06:39,1,The word root may multiple meanings
2997,00:06:45,2,Natural Language Content Analysis,1.1,"So, square root in math sense, or the root of a plant.",00:06:42,1,So square root math sense root plant
2998,00:06:48,2,Natural Language Content Analysis,1.1,You might be able to think of other meanings.,00:06:46,1,You might able think meanings
2999,00:06:52,2,Natural Language Content Analysis,1.1,There are also syntactical ambiguities.,00:06:49,1,There also syntactical ambiguities
3000,00:06:57,2,Natural Language Content Analysis,1.1,"For example, the main topic of this lecture, natural language processing,",00:06:52,1,For example main topic lecture natural language processing
3001,00:07:01,2,Natural Language Content Analysis,1.1,"can actually be interpreted in two ways, in terms of the structure.",00:06:57,1,actually interpreted two ways terms structure
3002,00:07:03,2,Natural Language Content Analysis,1.1,Think for a moment and see if you can figure that out.,00:07:01,1,Think moment see figure
3003,00:07:09,2,Natural Language Content Analysis,1.1,"We usually think of this as processing of natural languages, but",00:07:05,1,We usually think processing natural languages
3004,00:07:14,2,Natural Language Content Analysis,1.1,"you could also think of this as you say, language process is natural.",00:07:09,1,could also think say language process natural
3005,00:07:20,2,Natural Language Content Analysis,1.1,"Right. So, this is example of syntatic ambiguity.",00:07:15,1,Right So example syntatic ambiguity
3006,00:07:23,2,Natural Language Content Analysis,1.1,Where we have different structures that can be,00:07:20,1,Where different structures
3007,00:07:27,2,Natural Language Content Analysis,1.1,applied to the same sequence of words.,00:07:24,1,applied sequence words
3008,00:07:31,2,Natural Language Content Analysis,1.1,"Another example of ambiguous sentence is the following,",00:07:27,1,Another example ambiguous sentence following
3009,00:07:34,2,Natural Language Content Analysis,1.1,a man saw a boy with a telescope.,00:07:31,1,man saw boy telescope
3010,00:07:37,2,Natural Language Content Analysis,1.1,"Now, in this case, the question is, who had the telescope?",00:07:34,1,Now case question telescope
3011,00:07:42,2,Natural Language Content Analysis,1.1,"All right, this is called a prepositional phrase attachment ambiguity,",00:07:37,1,All right called prepositional phrase attachment ambiguity
3012,00:07:45,2,Natural Language Content Analysis,1.1,or PP attachment ambiguity.,00:07:42,1,PP attachment ambiguity
3013,00:07:49,2,Natural Language Content Analysis,1.1,"Now, we generally don't have a problem with these ambiguities because we have",00:07:45,1,Now generally problem ambiguities
3014,00:07:54,2,Natural Language Content Analysis,1.1,a lot of background knowledge to help us disintegrate the ambiguity.,00:07:49,1,lot background knowledge help us disintegrate ambiguity
3015,00:07:57,2,Natural Language Content Analysis,1.1,Another example of difficulty is anaphora resolution.,00:07:55,1,Another example difficulty anaphora resolution
3016,00:08:03,2,Natural Language Content Analysis,1.1,"So, think about the sentence like John persuaded Bill to buy a TV for himself.",00:07:57,1,So think sentence like John persuaded Bill buy TV
3017,00:08:07,2,Natural Language Content Analysis,1.1,"The question here is, does himself refer to John or Bill?",00:08:03,1,The question refer John Bill
3018,00:08:10,2,Natural Language Content Analysis,1.1,"So again, this is something that you have to use some background or",00:08:07,1,So something use background
3019,00:08:12,2,Natural Language Content Analysis,1.1,the context to figure out.,00:08:10,1,context figure
3020,00:08:15,2,Natural Language Content Analysis,1.1,"Finally, presupposition is another problem.",00:08:12,1,Finally presupposition another problem
3021,00:08:18,2,Natural Language Content Analysis,1.1,"Consider the sentence, he has quit smoking.",00:08:15,1,Consider sentence quit smoking
3022,00:08:20,2,Natural Language Content Analysis,1.1,Now this obviously implies he smoked before.,00:08:18,1,Now obviously implies smoked
3023,00:08:27,2,Natural Language Content Analysis,1.1,"So, imagine a computer wants to understand all the subtle differences and meanings.",00:08:22,1,So imagine computer wants understand subtle differences meanings
3024,00:08:30,2,Natural Language Content Analysis,1.1,They would have to use a lot of knowledge to figure that out.,00:08:27,1,They would use lot knowledge figure
3025,00:08:35,2,Natural Language Content Analysis,1.1,"It also would have to maintain a large knowl, knowledge base of odd meanings of",00:08:30,1,It also would maintain large knowl knowledge base odd meanings
3026,00:08:41,2,Natural Language Content Analysis,1.1,words and how they are connected to our common sense knowledge of the word.,00:08:35,1,words connected common sense knowledge word
3027,00:08:44,2,Natural Language Content Analysis,1.1,So this is why it's very difficult.,00:08:41,1,So difficult
3028,00:08:49,2,Natural Language Content Analysis,1.1,So as a result we are still not perfect.,00:08:45,1,So result still perfect
3029,00:08:54,2,Natural Language Content Analysis,1.1,"In fact, far from perfect in understanding natural languages using computers.",00:08:49,1,In fact far perfect understanding natural languages using computers
3030,00:09:00,2,Natural Language Content Analysis,1.1,So this slide sort of gives a simplified view of state of the art technologies.,00:08:54,1,So slide sort gives simplified view state art technologies
3031,00:09:05,2,Natural Language Content Analysis,1.1,We can do part of speech tagging pretty well.,00:09:01,1,We part speech tagging pretty well
3032,00:09:09,2,Natural Language Content Analysis,1.1,"So, I showed minus 7% accuracy here.",00:09:05,1,So I showed minus 7 accuracy
3033,00:09:13,2,Natural Language Content Analysis,1.1,"Now this number is obviously based on a certain data set, so",00:09:09,1,Now number obviously based certain data set
3034,00:09:15,2,Natural Language Content Analysis,1.1,don't take this literally.,00:09:13,1,take literally
3035,00:09:18,2,Natural Language Content Analysis,1.1,"All right, this just shows that we could do it pretty well.",00:09:15,1,All right shows could pretty well
3036,00:09:20,2,Natural Language Content Analysis,1.1,But it's still not perfect.,00:09:18,1,But still perfect
3037,00:09:23,2,Natural Language Content Analysis,1.1,"In terms of parsing, we can do partial parsing pretty well.",00:09:20,1,In terms parsing partial parsing pretty well
3038,00:09:28,2,Natural Language Content Analysis,1.1,"That means we can get noun phrase structures or verb phrase structure, or",00:09:23,1,That means get noun phrase structures verb phrase structure
3039,00:09:33,2,Natural Language Content Analysis,1.1,some segment of the sentence understood correctly in terms of the structure.,00:09:28,1,segment sentence understood correctly terms structure
3040,00:09:38,2,Natural Language Content Analysis,1.1,"And, in some evaluation results we have seen about 90%",00:09:34,1,And evaluation results seen 90
3041,00:09:43,2,Natural Language Content Analysis,1.1,accuracy in terms of partial parsing of sentences.,00:09:38,1,accuracy terms partial parsing sentences
3042,00:09:46,2,Natural Language Content Analysis,1.1,"Again, I have to say, these numbers are relative to the data set.",00:09:43,1,Again I say numbers relative data set
3043,00:09:50,2,Natural Language Content Analysis,1.1,"In some other data sets, the numbers might be lower.",00:09:46,1,In data sets numbers might lower
3044,00:09:54,2,Natural Language Content Analysis,1.1,Most of existing work has been evaluated using news data set.,00:09:50,1,Most existing work evaluated using news data set
3045,00:09:59,2,Natural Language Content Analysis,1.1,"And so, a lot of these numbers are more or less biased towards news data.",00:09:54,1,And lot numbers less biased towards news data
3046,00:10:01,2,Natural Language Content Analysis,1.1,Think about social media data.,00:09:59,1,Think social media data
3047,00:10:02,2,Natural Language Content Analysis,1.1,The accuracy likely is lower.,00:10:01,1,The accuracy likely lower
3048,00:10:07,2,Natural Language Content Analysis,1.1,"In terms of semantic analysis,",00:10:05,1,In terms semantic analysis
3049,00:10:13,2,Natural Language Content Analysis,1.1,we are far from being able to do a complete understanding of a sentence.,00:10:07,1,far able complete understanding sentence
3050,00:10:16,2,Natural Language Content Analysis,1.1,But we have some techniques that would allow us to do,00:10:13,1,But techniques would allow us
3051,00:10:18,2,Natural Language Content Analysis,1.1,partial understanding of the sentence.,00:10:16,1,partial understanding sentence
3052,00:10:22,2,Natural Language Content Analysis,1.1,"So, I could mention some of them.",00:10:18,1,So I could mention
3053,00:10:27,2,Natural Language Content Analysis,1.1,"For example, we have techniques that can allow us to extract the entities and",00:10:22,1,For example techniques allow us extract entities
3054,00:10:30,2,Natural Language Content Analysis,1.1,relations mentioned in text or articles.,00:10:27,1,relations mentioned text articles
3055,00:10:34,2,Natural Language Content Analysis,1.1,"For example, recognizing the mentions of people, locations,",00:10:30,1,For example recognizing mentions people locations
3056,00:10:37,2,Natural Language Content Analysis,1.1,"organizations, et cetera in text.",00:10:34,1,organizations et cetera text
3057,00:10:40,2,Natural Language Content Analysis,1.1,Right? So this is called entity extraction.,00:10:37,1,Right So called entity extraction
3058,00:10:42,2,Natural Language Content Analysis,1.1,We may be able to recognize the relations.,00:10:40,1,We may able recognize relations
3059,00:10:47,2,Natural Language Content Analysis,1.1,"For example, this person visited that per, that place.",00:10:42,1,For example person visited per place
3060,00:10:51,2,Natural Language Content Analysis,1.1,"Or, this person met that person, or this company acquired another company.",00:10:47,1,Or person met person company acquired another company
3061,00:10:54,2,Natural Language Content Analysis,1.1,Such relations can be extracted by using the current and,00:10:51,1,Such relations extracted using current
3062,00:10:56,2,Natural Language Content Analysis,1.1,natural languaging processing techniques.,00:10:54,1,natural languaging processing techniques
3063,00:11:00,2,Natural Language Content Analysis,1.1,"They are not perfect, but they can do well for some entities.",00:10:56,1,They perfect well entities
3064,00:11:03,2,Natural Language Content Analysis,1.1,Some entities are harder than others.,00:11:00,1,Some entities harder others
3065,00:11:05,2,Natural Language Content Analysis,1.1,We can also do word sentence disintegration to some extent.,00:11:03,1,We also word sentence disintegration extent
3066,00:11:10,2,Natural Language Content Analysis,1.1,"We have to figure out whether this word in this sentence would have certain meaning,",00:11:05,1,We figure whether word sentence would certain meaning
3067,00:11:12,2,Natural Language Content Analysis,1.1,"and in another context,",00:11:10,1,another context
3068,00:11:15,2,Natural Language Content Analysis,1.1,the computer could figure out that it has a different meaning.,00:11:12,1,computer could figure different meaning
3069,00:11:18,2,Natural Language Content Analysis,1.1,"Again, it's not perfect but you can do something in that direction.",00:11:15,1,Again perfect something direction
3070,00:11:22,2,Natural Language Content Analysis,1.1,We can also do sentiment analysis meaning,00:11:19,1,We also sentiment analysis meaning
3071,00:11:25,2,Natural Language Content Analysis,1.1,to figure out whether sentence is positive or negative.,00:11:22,1,figure whether sentence positive negative
3072,00:11:28,2,Natural Language Content Analysis,1.1,"This is a special use for, for review analysis for example.",00:11:25,1,This special use review analysis example
3073,00:11:33,2,Natural Language Content Analysis,1.1,So these examples of semantic analysis.,00:11:30,1,So examples semantic analysis
3074,00:11:38,2,Natural Language Content Analysis,1.1,And they help us to obtain partial understanding of the sentences.,00:11:33,1,And help us obtain partial understanding sentences
3075,00:11:39,2,Natural Language Content Analysis,1.1,Right? It's not,00:11:38,1,Right It
3076,00:11:44,2,Natural Language Content Analysis,1.1,"giving us a complete understanding as I showed before for the sentence, but",00:11:39,1,giving us complete understanding I showed sentence
3077,00:11:49,2,Natural Language Content Analysis,1.1,it will still help us gain understanding of the content and these can be useful.,00:11:44,1,still help us gain understanding content useful
3078,00:11:54,2,Natural Language Content Analysis,1.1,"In terms of inference, we are not yet there,",00:11:51,1,In terms inference yet
3079,00:11:59,2,Natural Language Content Analysis,1.1,probably because of the general difficulty of inference and uncertainties.,00:11:54,1,probably general difficulty inference uncertainties
3080,00:12:03,2,Natural Language Content Analysis,1.1,This is a general challenge in artificial intelligence.,00:11:59,1,This general challenge artificial intelligence
3081,00:12:08,2,Natural Language Content Analysis,1.1,That's probably also because we don't have complete semantic reimplementation for,00:12:03,1,That probably also complete semantic reimplementation
3082,00:12:10,2,Natural Language Content Analysis,1.1,natural language text.,00:12:08,1,natural language text
3083,00:12:11,2,Natural Language Content Analysis,1.1,So this is hard.,00:12:10,1,So hard
3084,00:12:16,2,Natural Language Content Analysis,1.1,"Yet in some domains, perhaps in limited domains when you have a lot of",00:12:11,1,Yet domains perhaps limited domains lot
3085,00:12:21,2,Natural Language Content Analysis,1.1,"restrictions on the world of users, you may be to may be able to perform",00:12:16,1,restrictions world users may may able perform
3086,00:12:27,2,Natural Language Content Analysis,1.1,"inference to some extent, but in general we cannot really do that reliably.",00:12:21,1,inference extent general cannot really reliably
3087,00:12:31,2,Natural Language Content Analysis,1.1,"Speech act analysis is also far from being done, and",00:12:27,1,Speech act analysis also far done
3088,00:12:36,2,Natural Language Content Analysis,1.1,we can only do that analysis for very special cases.,00:12:31,1,analysis special cases
3089,00:12:40,2,Natural Language Content Analysis,1.1,"So, this roughly gives you some idea about the state of the art.",00:12:36,1,So roughly gives idea state art
3090,00:12:44,2,Natural Language Content Analysis,1.1,And let me also talk a little bit about what we can't do.,00:12:41,1,And let also talk little bit
3091,00:12:52,2,Natural Language Content Analysis,1.1,"And, and so we can't even do 100% part of speech tagging.",00:12:44,1,And even 100 part speech tagging
3092,00:12:56,2,Natural Language Content Analysis,1.1,"This looks like a simple task, but think about the example here,",00:12:52,1,This looks like simple task think example
3093,00:13:01,2,Natural Language Content Analysis,1.1,the two uses of off may have different syntactic categories if you try,00:12:56,1,two uses may different syntactic categories try
3094,00:13:04,2,Natural Language Content Analysis,1.1,to make a fine grain distinctions.,00:13:01,1,make fine grain distinctions
3095,00:13:07,2,Natural Language Content Analysis,1.1,It's not that easy to figure out such differences.,00:13:04,1,It easy figure differences
3096,00:13:12,2,Natural Language Content Analysis,1.1,It's also hard to do general complete the parsing.,00:13:10,1,It also hard general complete parsing
3097,00:13:16,2,Natural Language Content Analysis,1.1,"And, again this same sentence that you saw before is example.",00:13:12,1,And sentence saw example
3098,00:13:21,2,Natural Language Content Analysis,1.1,"This, this ambiguity can be very hard to disambiguate.",00:13:18,1,This ambiguity hard disambiguate
3099,00:13:26,2,Natural Language Content Analysis,1.1,"And you can imagine example where you have to use a lot of knowledge i,",00:13:21,1,And imagine example use lot knowledge
3100,00:13:27,2,Natural Language Content Analysis,1.1,in the context of the sentence or,00:13:26,1,context sentence
3101,00:13:33,2,Natural Language Content Analysis,1.1,"from the background in order to figure out the, who actually had the telescope.",00:13:27,1,background order figure actually telescope
3102,00:13:37,2,Natural Language Content Analysis,1.1,"So is, i, although sentence looks very simple, it actually is pretty hard.",00:13:33,1,So although sentence looks simple actually pretty hard
3103,00:13:42,2,Natural Language Content Analysis,1.1,"And in cases when the sentence is very long, imagine it has four or",00:13:37,1,And cases sentence long imagine four
3104,00:13:46,2,Natural Language Content Analysis,1.1,"five prepositional phrases, then there are even more possibilities to figure out.",00:13:42,1,five prepositional phrases even possibilities figure
3105,00:13:51,2,Natural Language Content Analysis,1.1,It's also harder to precise deep semantic analysis.,00:13:48,1,It also harder precise deep semantic analysis
3106,00:13:52,2,Natural Language Content Analysis,1.1,So here's example.,00:13:51,1,So example
3107,00:14:00,2,Natural Language Content Analysis,1.1,"In this sentence, John owns a restaurant, how do we define owns exactly?",00:13:53,1,In sentence John owns restaurant define owns exactly
3108,00:14:05,2,Natural Language Content Analysis,1.1,"The word, own, you know, is something that we can understand but",00:14:00,1,The word know something understand
3109,00:14:10,2,Natural Language Content Analysis,1.1,it's very hard to precisely describe the meaning of own for computers.,00:14:05,1,hard precisely describe meaning computers
3110,00:14:16,2,Natural Language Content Analysis,1.1,So as a result we have robust and,00:14:11,1,So result robust
3111,00:14:20,2,Natural Language Content Analysis,1.1,general natural language processing techniques that can process a lot of text,00:14:16,1,general natural language processing techniques process lot text
3112,00:14:25,2,Natural Language Content Analysis,1.1,"data in a shallow way, meaning we only do superficial analysis.",00:14:20,1,data shallow way meaning superficial analysis
3113,00:14:28,2,Natural Language Content Analysis,1.1,"For example, part of s, of speech tagging, or",00:14:25,1,For example part speech tagging
3114,00:14:33,2,Natural Language Content Analysis,1.1,"partial parsing, or recognizing sentiment.",00:14:28,1,partial parsing recognizing sentiment
3115,00:14:36,2,Natural Language Content Analysis,1.1,And those are not deep understanding because we're not really,00:14:33,1,And deep understanding really
3116,00:14:39,2,Natural Language Content Analysis,1.1,understanding the exact meaning of the sentence.,00:14:36,1,understanding exact meaning sentence
3117,00:14:45,2,Natural Language Content Analysis,1.1,"On the other hand, the deep understanding techniques tend not to scale up well,",00:14:41,1,On hand deep understanding techniques tend scale well
3118,00:14:50,2,Natural Language Content Analysis,1.1,meaning that they would fail on some unrestricted text.,00:14:45,1,meaning would fail unrestricted text
3119,00:14:54,2,Natural Language Content Analysis,1.1,And if you don't restrict the text domain or,00:14:50,1,And restrict text domain
3120,00:14:59,2,Natural Language Content Analysis,1.1,"the use of words, then these techniques tend not to work well.",00:14:54,1,use words techniques tend work well
3121,00:15:04,2,Natural Language Content Analysis,1.1,"They may work well, based on machine learning techniques on the data",00:14:59,1,They may work well based machine learning techniques data
3122,00:15:08,2,Natural Language Content Analysis,1.1,that are similar to the training data that the program has been trained on.,00:15:04,1,similar training data program trained
3123,00:15:13,2,Natural Language Content Analysis,1.1,But they generally wouldn't work well on the data that are very different from,00:15:08,1,But generally work well data different
3124,00:15:14,2,Natural Language Content Analysis,1.1,the training data.,00:15:13,1,training data
3125,00:15:19,2,Natural Language Content Analysis,1.1,So this pretty much summarizes the state of the art of natural language processing.,00:15:14,1,So pretty much summarizes state art natural language processing
3126,00:15:23,2,Natural Language Content Analysis,1.1,"Of course, within such a short amount of time, we can't really give you a,",00:15:19,1,Of course within short amount time really give
3127,00:15:29,2,Natural Language Content Analysis,1.1,"a complete view of any of it, which is a big field, and either expect that to have,",00:15:23,1,complete view big field either expect
3128,00:15:35,2,Natural Language Content Analysis,1.1,to see multiple courses on natural language processing topic itself.,00:15:29,1,see multiple courses natural language processing topic
3129,00:15:40,2,Natural Language Content Analysis,1.1,"But, because of it's relevance to the topic that we talked about it's useful for",00:15:35,1,But relevance topic talked useful
3130,00:15:45,2,Natural Language Content Analysis,1.1,you to know the background in case you haven't been exposed to that.,00:15:40,1,know background case exposed
3131,00:15:47,2,Natural Language Content Analysis,1.1,"So, what does that mean for text retrieval?",00:15:45,1,So mean text retrieval
3132,00:15:53,2,Natural Language Content Analysis,1.1,"Well, in text retrieval we are dealing with all kinds of text.",00:15:48,1,Well text retrieval dealing kinds text
3133,00:15:56,2,Natural Language Content Analysis,1.1,It's very hard to restrict the text to a certain domain.,00:15:53,1,It hard restrict text certain domain
3134,00:16:01,2,Natural Language Content Analysis,1.1,"And we also are often dealing with a lot of text data, so that means.",00:15:56,1,And also often dealing lot text data means
3135,00:16:06,2,Natural Language Content Analysis,1.1,"The NLP techniques must be general, robust, and efficient and that",00:16:01,1,The NLP techniques must general robust efficient
3136,00:16:12,2,Natural Language Content Analysis,1.1,just implies today we can only use fairly shallow NLP techniques for text retrieval.,00:16:06,1,implies today use fairly shallow NLP techniques text retrieval
3137,00:16:14,2,Natural Language Content Analysis,1.1,"In fact,",00:16:12,1,In fact
3138,00:16:19,2,Natural Language Content Analysis,1.1,most search engines today use something called a bag of words representation.,00:16:14,1,search engines today use something called bag words representation
3139,00:16:25,2,Natural Language Content Analysis,1.1,Now this is probably the simplest representation you can probably think of.,00:16:20,1,Now probably simplest representation probably think
3140,00:16:29,2,Natural Language Content Analysis,1.1,That is to turn text data into simply a bag of words.,00:16:25,1,That turn text data simply bag words
3141,00:16:33,2,Natural Language Content Analysis,1.1,Meaning we will keep the individual words but we'll ignore all the orders of words.,00:16:29,1,Meaning keep individual words ignore orders words
3142,00:16:37,2,Natural Language Content Analysis,1.1,And we'll keep duplicated occurrences of words.,00:16:33,1,And keep duplicated occurrences words
3143,00:16:39,2,Natural Language Content Analysis,1.1,So this is called a bag of words representation.,00:16:37,1,So called bag words representation
3144,00:16:44,2,Natural Language Content Analysis,1.1,"When you represent the text in this way, you ignore a lot about the information,",00:16:39,1,When represent text way ignore lot information
3145,00:16:48,2,Natural Language Content Analysis,1.1,and that just makes it harder,00:16:44,1,makes harder
3146,00:16:52,2,Natural Language Content Analysis,1.1,to understand the exact meaning of a sentence because we've lost the order.,00:16:48,1,understand exact meaning sentence lost order
3147,00:16:57,2,Natural Language Content Analysis,1.1,"But yet, this representation tends to actually work pretty well for",00:16:53,1,But yet representation tends actually work pretty well
3148,00:16:59,2,Natural Language Content Analysis,1.1,most search tasks.,00:16:57,1,search tasks
3149,00:17:03,2,Natural Language Content Analysis,1.1,And this is partly because the search task is not all that difficult.,00:16:59,1,And partly search task difficult
3150,00:17:08,2,Natural Language Content Analysis,1.1,"If you see matching of some of the query words in a text document, chances",00:17:03,1,If see matching query words text document chances
3151,00:17:13,2,Natural Language Content Analysis,1.1,"are that that document is about the topic, although there are exceptions, right?",00:17:08,1,document topic although exceptions right
3152,00:17:18,2,Natural Language Content Analysis,1.1,"So in comparison some other tasks, for example machine translation, would require",00:17:13,1,So comparison tasks example machine translation would require
3153,00:17:22,2,Natural Language Content Analysis,1.1,"you to understand the language accurately, otherwise the translation would be wrong.",00:17:18,1,understand language accurately otherwise translation would wrong
3154,00:17:23,2,Natural Language Content Analysis,1.1,"So in comparison,",00:17:22,1,So comparison
3155,00:17:28,2,Natural Language Content Analysis,1.1,search tasks are solved relatively easy such a representation is often sufficient.,00:17:23,1,search tasks solved relatively easy representation often sufficient
3156,00:17:32,2,Natural Language Content Analysis,1.1,"And that's also the representation that the major search engines today,",00:17:28,1,And also representation major search engines today
3157,00:17:34,2,Natural Language Content Analysis,1.1,like Google or Bing are using.,00:17:32,1,like Google Bing using
3158,00:17:38,2,Natural Language Content Analysis,1.1,Of course I put in in parentheses but not all.,00:17:35,1,Of course I put parentheses
3159,00:17:42,2,Natural Language Content Analysis,1.1,Of course there are many queries that are not answered well by the current search,00:17:38,1,Of course many queries answered well current search
3160,00:17:45,2,Natural Language Content Analysis,1.1,"engines, and they do require a representation",00:17:42,1,engines require representation
3161,00:17:48,2,Natural Language Content Analysis,1.1,that would go beyond bag of words representation.,00:17:45,1,would go beyond bag words representation
3162,00:17:52,2,Natural Language Content Analysis,1.1,"That would require more natural language processing, to be done.",00:17:48,1,That would require natural language processing done
3163,00:17:56,2,Natural Language Content Analysis,1.1,There is another reason why we have not used the sophisticated NLP,00:17:52,1,There another reason used sophisticated NLP
3164,00:18:01,2,Natural Language Content Analysis,1.1,"techniques in modern search engines, and that's because some retrieval techniques",00:17:56,1,techniques modern search engines retrieval techniques
3165,00:18:05,2,Natural Language Content Analysis,1.1,actually naturally solve the problem of NLP.,00:18:01,1,actually naturally solve problem NLP
3166,00:18:09,2,Natural Language Content Analysis,1.1,"So, one example, is word sense disambiguation.",00:18:05,1,So one example word sense disambiguation
3167,00:18:11,2,Natural Language Content Analysis,1.1,Think about a word like java.,00:18:09,1,Think word like java
3168,00:18:13,2,Natural Language Content Analysis,1.1,It could mean coffee or it could mean program language.,00:18:11,1,It could mean coffee could mean program language
3169,00:18:18,2,Natural Language Content Analysis,1.1,If you look at the word alone it would be ambiguous.,00:18:15,1,If look word alone would ambiguous
3170,00:18:23,2,Natural Language Content Analysis,1.1,"But when the user uses the water in the query, usually there are other words.",00:18:18,1,But user uses water query usually words
3171,00:18:26,2,Natural Language Content Analysis,1.1,For example I'm looking for usage of Java applet.,00:18:23,1,For example I looking usage Java applet
3172,00:18:31,2,Natural Language Content Analysis,1.1,When I have applet there that implies Java means program language.,00:18:26,1,When I applet implies Java means program language
3173,00:18:35,2,Natural Language Content Analysis,1.1,And that context can help us naturally,00:18:31,1,And context help us naturally
3174,00:18:39,2,Natural Language Content Analysis,1.1,"prefer documents where Java is referring to program language,",00:18:35,1,prefer documents Java referring program language
3175,00:18:43,2,Natural Language Content Analysis,1.1,because those documents would probably match applet as well.,00:18:39,1,documents would probably match applet well
3176,00:18:47,2,Natural Language Content Analysis,1.1,"If java occurs in the document in a way that means coffee,",00:18:43,1,If java occurs document way means coffee
3177,00:18:53,2,Natural Language Content Analysis,1.1,"then you would never match applet, or with very small probability.",00:18:47,1,would never match applet small probability
3178,00:18:55,2,Natural Language Content Analysis,1.1,Right. So this is a case when some retrieval,00:18:53,1,Right So case retrieval
3179,00:18:59,2,Natural Language Content Analysis,1.1,techniques naturally achieve the goal of word sense disambiguation.,00:18:55,1,techniques naturally achieve goal word sense disambiguation
3180,00:19:05,2,Natural Language Content Analysis,1.1,Another example is some technique called,00:19:01,1,Another example technique called
3181,00:19:11,2,Natural Language Content Analysis,1.1,feedback which we will talk about later in some of the lectures.,00:19:05,1,feedback talk later lectures
3182,00:19:17,2,Natural Language Content Analysis,1.1,"This tech, technique would allow us to add additional words to the query.",00:19:11,1,This tech technique would allow us add additional words query
3183,00:19:22,2,Natural Language Content Analysis,1.1,And those additional words could be related to the query words.,00:19:17,1,And additional words could related query words
3184,00:19:26,2,Natural Language Content Analysis,1.1,And these words can help match documents where the original query words,00:19:22,1,And words help match documents original query words
3185,00:19:27,2,Natural Language Content Analysis,1.1,have not occurred.,00:19:26,1,occurred
3186,00:19:32,2,Natural Language Content Analysis,1.1,"So this achieves, to some extent, semantic matching of terms.",00:19:27,1,So achieves extent semantic matching terms
3187,00:19:35,2,Natural Language Content Analysis,1.1,So those techniques also helped us,00:19:32,1,So techniques also helped us
3188,00:19:38,2,Natural Language Content Analysis,1.1,bypass some of the difficulties in natural language processing.,00:19:35,1,bypass difficulties natural language processing
3189,00:19:43,2,Natural Language Content Analysis,1.1,"However, in the long run, we still need deeper natural language processing",00:19:40,1,However long run still need deeper natural language processing
3190,00:19:47,2,Natural Language Content Analysis,1.1,techniques in order to improve the accuracy of the current search engines.,00:19:43,1,techniques order improve accuracy current search engines
3191,00:19:53,2,Natural Language Content Analysis,1.1,"And it's particularly needed for complex search tasks, or for question answering.",00:19:47,1,And particularly needed complex search tasks question answering
3192,00:19:58,2,Natural Language Content Analysis,1.1,Google has recently launched a knowledge graph.,00:19:55,1,Google recently launched knowledge graph
3193,00:20:01,2,Natural Language Content Analysis,1.1,"And this is one step toward that goal,",00:19:58,1,And one step toward goal
3194,00:20:05,2,Natural Language Content Analysis,1.1,because knowledge graph would contain entities and their relations.,00:20:01,1,knowledge graph would contain entities relations
3195,00:20:09,2,Natural Language Content Analysis,1.1,And this goes beyond the simple bag of words representation.,00:20:05,1,And goes beyond simple bag words representation
3196,00:20:14,2,Natural Language Content Analysis,1.1,"And such technique should help us improve the search engine utility significantly,",00:20:09,1,And technique help us improve search engine utility significantly
3197,00:20:19,2,Natural Language Content Analysis,1.1,although this is a still open topic for research and exploration.,00:20:14,1,although still open topic research exploration
3198,00:20:25,2,Natural Language Content Analysis,1.1,"In sum, in this lecture we'll talk about what is NLP and we've talked",00:20:19,1,In sum lecture talk NLP talked
3199,00:20:30,2,Natural Language Content Analysis,1.1,"about the state of the art techniques, what we can do, what we cannot do.",00:20:25,1,state art techniques cannot
3200,00:20:34,2,Natural Language Content Analysis,1.1,"And finally, we also explained why bag of words representation",00:20:30,1,And finally also explained bag words representation
3201,00:20:39,2,Natural Language Content Analysis,1.1,remains the dominant representation used in modern search engines even though,00:20:34,1,remains dominant representation used modern search engines even though
3202,00:20:42,2,Natural Language Content Analysis,1.1,deeper NLP would be needed for future search engines.,00:20:39,1,deeper NLP would needed future search engines
3203,00:20:46,2,Natural Language Content Analysis,1.1,If you want to know more you can take a look at some additional readings.,00:20:42,1,If want know take look additional readings
3204,00:20:47,2,Natural Language Content Analysis,1.1,I only cited one here.,00:20:46,1,I cited one
3205,00:20:48,2,Natural Language Content Analysis,1.1,And that's a good starting point though.,00:20:47,1,And good starting point though
3206,00:20:50,2,Natural Language Content Analysis,1.1,Thanks.,00:20:48,1,Thanks
3207,00:00:04,5,Course Summary,4.8,[SOUND] This lecture is,00:00:00,15,SOUND This lecture
3208,00:00:10,5,Course Summary,4.8,a summary of this course.,00:00:04,15,summary course
3209,00:00:17,5,Course Summary,4.8,This map shows the major topics we have covered in this course.,00:00:10,15,This map shows major topics covered course
3210,00:00:22,5,Course Summary,4.8,And here are some key high-level take-away messages.,00:00:17,15,And key high level take away messages
3211,00:00:28,5,Course Summary,4.8,First we talk about natural language content analysis.,00:00:22,15,First talk natural language content analysis
3212,00:00:34,5,Course Summary,4.8,Here the main take-away message is natural language processing is the foundation for,00:00:28,15,Here main take away message natural language processing foundation
3213,00:00:38,5,Course Summary,4.8,"textual retrieval, but current NLP isn't robust enough.",00:00:34,15,textual retrieval current NLP robust enough
3214,00:00:43,5,Course Summary,4.8,So the back of words replenishing is generally,00:00:38,15,So back words replenishing generally
3215,00:00:47,5,Course Summary,4.8,the main method used in modern search engines and,00:00:43,15,main method used modern search engines
3216,00:00:52,5,Course Summary,4.8,it's often sufficient for most of the search tasks.,00:00:47,15,often sufficient search tasks
3217,00:00:56,5,Course Summary,4.8,"But obviously, for more compass search tasks,",00:00:52,15,But obviously compass search tasks
3218,00:01:01,5,Course Summary,4.8,then we need a deeper measurement processing techniques.,00:00:56,15,need deeper measurement processing techniques
3219,00:01:05,5,Course Summary,4.8,And we then talked about a high-level strategies for,00:01:01,15,And talked high level strategies
3220,00:01:09,5,Course Summary,4.8,text access and we talked about push versus pull in plural.,00:01:05,15,text access talked push versus pull plural
3221,00:01:12,5,Course Summary,4.8,"We talked about a query, which is browsing.",00:01:09,15,We talked query browsing
3222,00:01:17,5,Course Summary,4.8,"Now, in general in future search engines, we should integrate",00:01:12,15,Now general future search engines integrate
3223,00:01:22,5,Course Summary,4.8,all these techniques to provide a multiple information access and,00:01:17,15,techniques provide multiple information access
3224,00:01:27,5,Course Summary,4.8,then we talked about a number of issues related to search engines.,00:01:22,15,talked number issues related search engines
3225,00:01:33,5,Course Summary,4.8,We talked about the search problem and we framed that as a ranking problem and,00:01:27,15,We talked search problem framed ranking problem
3226,00:01:38,5,Course Summary,4.8,we talked about the a number of retrieval methods.,00:01:33,15,talked number retrieval methods
3227,00:01:42,5,Course Summary,4.8,We start with an overview of the vector space model and,00:01:38,15,We start overview vector space model
3228,00:01:47,5,Course Summary,4.8,probabilistic model and then we talked about the vector space model in that.,00:01:42,15,probabilistic model talked vector space model
3229,00:01:52,5,Course Summary,4.8,We also later talked about leverageable learning approach and,00:01:47,15,We also later talked leverageable learning approach
3230,00:01:55,5,Course Summary,4.8,that's probabilistic model.,00:01:52,15,probabilistic model
3231,00:02:02,5,Course Summary,4.8,"And here, the main take-away message is that model retrieval functions tend to",00:01:55,15,And main take away message model retrieval functions tend
3232,00:02:07,5,Course Summary,4.8,look similar and they generally use various heuristics.,00:02:02,15,look similar generally use various heuristics
3233,00:02:13,5,Course Summary,4.8,Most important ones are TF-IDF waiting document length normalization and,00:02:07,15,Most important ones TF IDF waiting document length normalization
3234,00:02:21,5,Course Summary,4.8,that TF is often transformed through a sub-linear transformation function and,00:02:13,15,TF often transformed sub linear transformation function
3235,00:02:26,5,Course Summary,4.8,then we talked about how to implement a retrieval system.,00:02:21,15,talked implement retrieval system
3236,00:02:33,5,Course Summary,4.8,And here the main technique that we talked about how to construct an inverted index.,00:02:26,15,And main technique talked construct inverted index
3237,00:02:38,5,Course Summary,4.8,So that we can prepare the system to answer a query quickly and,00:02:33,15,So prepare system answer query quickly
3238,00:02:44,5,Course Summary,4.8,"we talked about how to, to fast research by using the inverted index and",00:02:38,15,talked fast research using inverted index
3239,00:02:49,5,Course Summary,4.8,we then talked about how to evaluate the text retrieval system,00:02:44,15,talked evaluate text retrieval system
3240,00:02:54,5,Course Summary,4.8,mainly introduced the Cranfield evaluation methodology.,00:02:49,15,mainly introduced Cranfield evaluation methodology
3241,00:02:59,5,Course Summary,4.8,This was a very important the various methodology of that can be applied to,00:02:54,15,This important various methodology applied
3242,00:03:00,5,Course Summary,4.8,many tasks.,00:02:59,15,many tasks
3243,00:03:05,5,Course Summary,4.8,We talked about the major evaluation measures.,00:03:00,15,We talked major evaluation measures
3244,00:03:09,5,Course Summary,4.8,So the most important measures for,00:03:05,15,So important measures
3245,00:03:14,5,Course Summary,4.8,a search engine are MAP mean average precision and nDCG.,00:03:09,15,search engine MAP mean average precision nDCG
3246,00:03:18,5,Course Summary,4.8,Normalized discounted accumulative gain and also precision and,00:03:14,15,Normalized discounted accumulative gain also precision
3247,00:03:20,5,Course Summary,4.8,record the two basic measures.,00:03:18,15,record two basic measures
3248,00:03:24,5,Course Summary,4.8,And we then talked about feedback techniques.,00:03:20,15,And talked feedback techniques
3249,00:03:28,5,Course Summary,4.8,And we talked about the rock you in the vector space model and,00:03:24,15,And talked rock vector space model
3250,00:03:31,5,Course Summary,4.8,the mixture model in the language modeling approach.,00:03:28,15,mixture model language modeling approach
3251,00:03:36,5,Course Summary,4.8,Feedback is very important technique especially considering,00:03:31,15,Feedback important technique especially considering
3252,00:03:41,5,Course Summary,4.8,the opportunity of learning from a lot of pixels on the web.,00:03:36,15,opportunity learning lot pixels web
3253,00:03:45,5,Course Summary,4.8,We then talked about the web search.,00:03:41,15,We talked web search
3254,00:03:49,5,Course Summary,4.8,"And here, we talk about the how to use parallel indexing to resolve",00:03:45,15,And talk use parallel indexing resolve
3255,00:03:53,5,Course Summary,4.8,"the scalability issue in indexing, we introduce a MapReduce and",00:03:49,15,scalability issue indexing introduce MapReduce
3256,00:03:59,5,Course Summary,4.8,then we talked about the how to using information interacting pull search.,00:03:53,15,talked using information interacting pull search
3257,00:04:01,5,Course Summary,4.8,We talked about page random,00:03:59,15,We talked page random
3258,00:04:06,5,Course Summary,4.8,hits as the major algorithms to analyze links on the web.,00:04:01,15,hits major algorithms analyze links web
3259,00:04:09,5,Course Summary,4.8,We then talked about learning to rank.,00:04:06,15,We talked learning rank
3260,00:04:14,5,Course Summary,4.8,This is a use of machine learning to combine multiple features for,00:04:09,15,This use machine learning combine multiple features
3261,00:04:16,5,Course Summary,4.8,improving scoring.,00:04:14,15,improving scoring
3262,00:04:21,5,Course Summary,4.8,Not only the effectiveness can be improved using this approach but,00:04:16,15,Not effectiveness improved using approach
3263,00:04:26,5,Course Summary,4.8,"we can also improve the robustness of the ranking function,",00:04:21,15,also improve robustness ranking function
3264,00:04:31,5,Course Summary,4.8,"so that it's not easy to spam a search engine with just a,",00:04:26,15,easy spam search engine
3265,00:04:35,5,Course Summary,4.8,a some features to promote a page.,00:04:31,15,features promote page
3266,00:04:39,5,Course Summary,4.8,"And finally, we talked about the future of web search.",00:04:35,15,And finally talked future web search
3267,00:04:44,5,Course Summary,4.8,We talked about some major interactions that we might assume,00:04:39,15,We talked major interactions might assume
3268,00:04:49,5,Course Summary,4.8,in the future in improving the current generation of search engines.,00:04:44,15,future improving current generation search engines
3269,00:04:55,5,Course Summary,4.8,"And then finally, we talked about the Recommender System and these are systems",00:04:49,15,And finally talked Recommender System systems
3270,00:05:00,5,Course Summary,4.8,to implement the push mode and we'll talk about the two approaches.,00:04:55,15,implement push mode talk two approaches
3271,00:05:04,5,Course Summary,4.8,"One is content based, one is collaborative filtering and",00:05:00,15,One content based one collaborative filtering
3272,00:05:06,5,Course Summary,4.8,they can be combined together.,00:05:04,15,combined together
3273,00:05:11,5,Course Summary,4.8,Now an obvious missing piece in this,00:05:06,15,Now obvious missing piece
3274,00:05:16,5,Course Summary,4.8,"picture is the user, you can see.",00:05:11,15,picture user see
3275,00:05:20,5,Course Summary,4.8,"So user interface is also a important component in any search engine,",00:05:16,15,So user interface also important component search engine
3276,00:05:24,5,Course Summary,4.8,even though the current search interface is relatively simple.,00:05:20,15,even though current search interface relatively simple
3277,00:05:29,5,Course Summary,4.8,There actually have been a lot of studies of user interfaces,00:05:24,15,There actually lot studies user interfaces
3278,00:05:35,5,Course Summary,4.8,"related to visualization for example and this is topic to that,",00:05:29,15,related visualization example topic
3279,00:05:39,5,Course Summary,4.8,you can learn more by reading this book.,00:05:35,15,learn reading book
3280,00:05:47,5,Course Summary,4.8,It's a excellent book about all kind of studies of search user interface.,00:05:39,15,It excellent book kind studies search user interface
3281,00:05:52,5,Course Summary,4.8,"If you want to know more about the, the topics that we talked about,",00:05:47,15,If want know topics talked
3282,00:05:56,5,Course Summary,4.8,you can also read some additional readings that are listed here.,00:05:52,15,also read additional readings listed
3283,00:06:01,5,Course Summary,4.8,"In this short course, we are only managing to cover some basic topics in text",00:05:56,15,In short course managing cover basic topics text
3284,00:06:03,5,Course Summary,4.8,retrieval in search engines.,00:06:01,15,retrieval search engines
3285,00:06:09,5,Course Summary,4.8,And these resources provide additional information about more advanced topics and,00:06:03,15,And resources provide additional information advanced topics
3286,00:06:15,5,Course Summary,4.8,they give more thorough treatment of some of the topics that we talked about.,00:06:09,15,give thorough treatment topics talked
3287,00:06:21,5,Course Summary,4.8,And a main source is synthesis digital library,00:06:15,15,And main source synthesis digital library
3288,00:06:26,5,Course Summary,4.8,where you can see a lot of short textbook or,00:06:21,15,see lot short textbook
3289,00:06:29,5,Course Summary,4.8,textbooks or long tutorials.,00:06:26,15,textbooks long tutorials
3290,00:06:35,5,Course Summary,4.8,They tend to provide us with a lot of information to explain a topic and,00:06:29,15,They tend provide us lot information explain topic
3291,00:06:40,5,Course Summary,4.8,there are multiple series that are related to this course.,00:06:35,15,multiple series related course
3292,00:06:43,5,Course Summary,4.8,"One is information concepts, retrieval and services.",00:06:40,15,One information concepts retrieval services
3293,00:06:45,5,Course Summary,4.8,Another is human Language technology and,00:06:43,15,Another human Language technology
3294,00:06:49,5,Course Summary,4.8,"yet, another is artificial intelligence and machine learning.",00:06:45,15,yet another artificial intelligence machine learning
3295,00:06:54,5,Course Summary,4.8,There are also some major journals and conferences listed over here that,00:06:49,15,There also major journals conferences listed
3296,00:06:59,5,Course Summary,4.8,tend to have a lot of research papers related to the topic of this course.,00:06:54,15,tend lot research papers related topic course
3297,00:07:01,5,Course Summary,4.8,And finally for,00:06:59,15,And finally
3298,00:07:07,5,Course Summary,4.8,"more information about resources including readings and tool kits, etc.",00:07:01,15,information resources including readings tool kits etc
3299,00:07:09,5,Course Summary,4.8,You can check out this URL.,00:07:07,15,You check URL
3300,00:07:14,5,Course Summary,4.8,"So, if you have not taken the text mining course in this",00:07:09,15,So taken text mining course
3301,00:07:18,5,Course Summary,4.8,"in this data mining specialization series,",00:07:14,15,data mining specialization series
3302,00:07:23,5,Course Summary,4.8,"then naturally, the next step is to take that calls.",00:07:18,15,naturally next step take calls
3303,00:07:27,5,Course Summary,4.8,"As this picture shows to mine the text data,",00:07:23,15,As picture shows mine text data
3304,00:07:31,5,Course Summary,4.8,we generally need two kinds of techniques.,00:07:27,15,generally need two kinds techniques
3305,00:07:34,5,Course Summary,4.8,"One is text retrieval, which is covered in this course.",00:07:31,15,One text retrieval covered course
3306,00:07:39,5,Course Summary,4.8,"And these techniques will help us convert raw big text data into small,",00:07:34,15,And techniques help us convert raw big text data small
3307,00:07:44,5,Course Summary,4.8,"relevant text data, which are actually needed in the specific application.",00:07:39,15,relevant text data actually needed specific application
3308,00:07:49,5,Course Summary,4.8,"And human plays important role in mining any text data,",00:07:44,15,And human plays important role mining text data
3309,00:07:53,5,Course Summary,4.8,because text data is written for humans to consume.,00:07:49,15,text data written humans consume
3310,00:07:59,5,Course Summary,4.8,"So, involving humans in the process of data mining is very important.",00:07:53,15,So involving humans process data mining important
3311,00:08:04,5,Course Summary,4.8,"And in this course, we have covered various strategies to",00:07:59,15,And course covered various strategies
3312,00:08:07,5,Course Summary,4.8,help users get access to the most relevant data.,00:08:04,15,help users get access relevant data
3313,00:08:13,5,Course Summary,4.8,These techniques are also essential in any text mining system to help,00:08:07,15,These techniques also essential text mining system help
3314,00:08:18,5,Course Summary,4.8,provide providence and to help users interpret the inner,00:08:13,15,provide providence help users interpret inner
3315,00:08:23,5,Course Summary,4.8,patterns that the user would find through text data mining.,00:08:18,15,patterns user would find text data mining
3316,00:08:27,5,Course Summary,4.8,"So, in general, the user would have to go back to the original data to better",00:08:23,15,So general user would go back original data better
3317,00:08:29,5,Course Summary,4.8,understand the patterns.,00:08:27,15,understand patterns
3318,00:08:33,5,Course Summary,4.8,"So the text mining course or rather text mining and ana,",00:08:29,15,So text mining course rather text mining ana
3319,00:08:36,5,Course Summary,4.8,"analytics course will be deal,",00:08:33,15,analytics course deal
3320,00:08:41,5,Course Summary,4.8,dealing with what to do once the user has found the information.,00:08:36,15,dealing user found information
3321,00:08:46,5,Course Summary,4.8,So this is a in this picture where we would convert,00:08:41,15,So picture would convert
3322,00:08:49,5,Course Summary,4.8,the text data into action or knowledge.,00:08:46,15,text data action knowledge
3323,00:08:53,5,Course Summary,4.8,And this has to do with helping users to go further digest with,00:08:49,15,And helping users go digest
3324,00:08:58,5,Course Summary,4.8,a found information or to find the patterns and,00:08:53,15,found information find patterns
3325,00:09:03,5,Course Summary,4.8,to reveal knowledge buried in text and such knowledge can be used in,00:08:58,15,reveal knowledge buried text knowledge used
3326,00:09:09,5,Course Summary,4.8,application system to help decision-making or to help user finish a task.,00:09:03,15,application system help decision making help user finish task
3327,00:09:16,5,Course Summary,4.8,"So, if you have not taken that course the natural step and",00:09:09,15,So taken course natural step
3328,00:09:22,5,Course Summary,4.8,the natural next step would be to take that course.,00:09:16,15,natural next step would take course
3329,00:09:25,5,Course Summary,4.8,Thank you for taking this course.,00:09:22,15,Thank taking course
3330,00:09:30,5,Course Summary,4.8,I hope you have found this course to be useful to you and,00:09:25,15,I hope found course useful
3331,00:09:36,5,Course Summary,4.8,I look forward to interacting with you at a future activity.,00:09:30,15,I look forward interacting future activity
3332,00:00:01,1,Course Introduction,,[MUSIC],00:00:00,2,MUSIC
3333,00:00:10,1,Course Introduction,,Text data is very special.,00:00:09,2,Text data special
3334,00:00:16,1,Course Introduction,,"In contrast to the data captured by machines such as sensors,",00:00:10,2,In contrast data captured machines sensors
3335,00:00:19,1,Course Introduction,,text data is produced by humans.,00:00:16,2,text data produced humans
3336,00:00:23,1,Course Introduction,,And they also are meant to be consumed by humans.,00:00:19,2,And also meant consumed humans
3337,00:00:26,1,Course Introduction,,And this has some interesting consequences.,00:00:23,2,And interesting consequences
3338,00:00:31,1,Course Introduction,,"Because it is produced by humans, it tends to have a lot of useful knowledge about",00:00:26,2,Because produced humans tends lot useful knowledge
3339,00:00:34,1,Course Introduction,,"people's' preferences, people's' opinions about everything.",00:00:31,2,people preferences people opinions everything
3340,00:00:38,1,Course Introduction,,And that makes it possible to mine,00:00:35,2,And makes possible mine
3341,00:00:41,1,Course Introduction,,"text data to discover those latent prefaces of people,",00:00:38,2,text data discover latent prefaces people
3342,00:00:46,1,Course Introduction,,which could be very useful to build an intelligent system to help people.,00:00:41,2,could useful build intelligent system help people
3343,00:00:48,1,Course Introduction,,You can think about scientific literature or,00:00:46,2,You think scientific literature
3344,00:00:53,1,Course Introduction,,so and it's a way to encode our knowledge about the world.,00:00:48,2,way encode knowledge world
3345,00:01:00,1,Course Introduction,,"So it's very high quality content, yet we have difficulty digesting all the content.",00:00:53,2,So high quality content yet difficulty digesting content
3346,00:01:05,1,Course Introduction,,"Now as a result of the fact that text is consumed by we humans,",00:01:00,2,Now result fact text consumed humans
3347,00:01:10,1,Course Introduction,,"we also need intelligent software tools to help people digest the content, or",00:01:05,2,also need intelligent software tools help people digest content
3348,00:01:13,1,Course Introduction,,otherwise we'd miss a lot of useful content.,00:01:10,2,otherwise miss lot useful content
3349,00:01:19,1,Course Introduction,,This slide shows that the human really plays important role in test data mining.,00:01:13,2,This slide shows human really plays important role test data mining
3350,00:01:21,1,Course Introduction,,"We have to consider human in the loop, and",00:01:19,2,We consider human loop
3351,00:01:24,1,Course Introduction,,we have to consider the fact that the text is generated by human.,00:01:21,2,consider fact text generated human
3352,00:01:30,1,Course Introduction,,"So, here are some examples of useful text information systems.",00:01:27,2,So examples useful text information systems
3353,00:01:35,1,Course Introduction,,This is by no means a complete list of all applications.,00:01:30,2,This means complete list applications
3354,00:01:38,1,Course Introduction,,I categorize them into different categories.,00:01:35,2,I categorize different categories
3355,00:01:41,1,Course Introduction,,But you can probably imagine other kinds of applications.,00:01:38,2,But probably imagine kinds applications
3356,00:01:42,1,Course Introduction,,So let's take a look at some of them.,00:01:41,2,So let take look
3357,00:01:45,1,Course Introduction,,"Search for example, we all know search engines is special.",00:01:42,2,Search example know search engines special
3358,00:01:49,1,Course Introduction,,"Web search engines, iPad, all of you are using Google, or Bing, or",00:01:45,2,Web search engines iPad using Google Bing
3359,00:01:51,1,Course Introduction,,another web search engine all the time.,00:01:49,2,another web search engine time
3360,00:01:54,1,Course Introduction,,And we also have live research assistants.,00:01:51,2,And also live research assistants
3361,00:01:58,1,Course Introduction,,"And in fact, wherever you have a lot of text data, you would have a search engine.",00:01:54,2,And fact wherever lot text data would search engine
3362,00:02:00,1,Course Introduction,,"So for example, you might have a search box on your laptop.",00:01:58,2,So example might search box laptop
3363,00:02:03,1,Course Introduction,,"All right, to search content on your computer.",00:02:00,2,All right search content computer
3364,00:02:07,1,Course Introduction,,"So that's one kind of application systems, but",00:02:04,2,So one kind application systems
3365,00:02:10,1,Course Introduction,,we also have filtering systems or recommended systems.,00:02:07,2,also filtering systems recommended systems
3366,00:02:12,1,Course Introduction,,Those systems can push information to users.,00:02:10,2,Those systems push information users
3367,00:02:15,1,Course Introduction,,They can recommend useful information to users.,00:02:12,2,They recommend useful information users
3368,00:02:19,1,Course Introduction,,"So again, use filters, spam filters.",00:02:15,2,So use filters spam filters
3369,00:02:20,1,Course Introduction,,Literature the movie recommenders.,00:02:19,2,Literature movie recommenders
3370,00:02:24,1,Course Introduction,,Now not of them are necessary recommending the information to you.,00:02:20,2,Now necessary recommending information
3371,00:02:27,1,Course Introduction,,"For example email filter, spam email filter,",00:02:24,2,For example email filter spam email filter
3372,00:02:33,1,Course Introduction,,"this is actually to filter out the spams from your inbox, all right.",00:02:27,2,actually filter spams inbox right
3373,00:02:38,1,Course Introduction,,But in nature these are similar systems in that they have to make a binary decision,00:02:33,2,But nature similar systems make binary decision
3374,00:02:42,1,Course Introduction,,regarding whether to retain a particular document or discard it.,00:02:38,2,regarding whether retain particular document discard
3375,00:02:47,1,Course Introduction,,Another kind of systems are categorization systems.,00:02:43,2,Another kind systems categorization systems
3376,00:02:51,1,Course Introduction,,"So for example, in handling emails,",00:02:47,2,So example handling emails
3377,00:02:56,1,Course Introduction,,"you might prefer automatic, sorter that would automatically",00:02:51,2,might prefer automatic sorter would automatically
3378,00:03:00,1,Course Introduction,,sort incoming emails into a proper folders that you created.,00:02:56,2,sort incoming emails proper folders created
3379,00:03:07,1,Course Introduction,,Or we might want to categorize product reviews into positive or negative.,00:03:02,2,Or might want categorize product reviews positive negative
3380,00:03:11,1,Course Introduction,,News agencies might be interested in,00:03:07,2,News agencies might interested
3381,00:03:15,1,Course Introduction,,categorizing news articles into all kinds of subject categories.,00:03:11,2,categorizing news articles kinds subject categories
3382,00:03:17,1,Course Introduction,,Those are all categorization systems.,00:03:15,2,Those categorization systems
3383,00:03:22,1,Course Introduction,,Finally there are also systems that might do more analysis.,00:03:18,2,Finally also systems might analysis
3384,00:03:26,1,Course Introduction,,"And oh, you can say mine text data.",00:03:22,2,And oh say mine text data
3385,00:03:29,1,Course Introduction,,And these can be text mining systems or,00:03:26,2,And text mining systems
3386,00:03:32,1,Course Introduction,,"information extraction systems, and they can be",00:03:29,2,information extraction systems
3387,00:03:37,1,Course Introduction,,used to analyze text data in more detail to discover potentially useful knowledge.,00:03:32,2,used analyze text data detail discover potentially useful knowledge
3388,00:03:41,1,Course Introduction,,For example companies might be interested in discovering,00:03:37,2,For example companies might interested discovering
3389,00:03:45,1,Course Introduction,,"major complaints from their customers based on the email messages that the,",00:03:41,2,major complaints customers based email messages
3390,00:03:47,1,Course Introduction,,they have received from the customers.,00:03:45,2,received customers
3391,00:03:52,1,Course Introduction,,"All right, so having a system to support that would",00:03:47,2,All right system support would
3392,00:03:57,1,Course Introduction,,really help improve their productivity and the customer relations.,00:03:52,2,really help improve productivity customer relations
3393,00:04:02,1,Course Introduction,,"Also in business, intelligence companies are often interested in analyzing product",00:03:59,2,Also business intelligence companies often interested analyzing product
3394,00:04:06,1,Course Introduction,,reviews to understand the relative strengths of their own products,00:04:02,2,reviews understand relative strengths products
3395,00:04:09,1,Course Introduction,,in comparison with competitors.,00:04:06,2,comparison competitors
3396,00:04:14,1,Course Introduction,,"And, and so these are all examples of these test mining systems.",00:04:10,2,And examples test mining systems
3397,00:04:19,1,Course Introduction,,[INAUDIBLE] we have a lot of data in particular literature data.,00:04:14,2,INAUDIBLE lot data particular literature data
3398,00:04:23,1,Course Introduction,,"So, there's also great opportunity of using computer systems",00:04:19,2,So also great opportunity using computer systems
3399,00:04:26,1,Course Introduction,,"to analyze the data to automatically read literature, and",00:04:23,2,analyze data automatically read literature
3400,00:04:30,1,Course Introduction,,"to gain knowledge, and to help biologists make discoveries.",00:04:26,2,gain knowledge help biologists make discoveries
3401,00:04:32,1,Course Introduction,,And you can imagine many others.,00:04:30,2,And imagine many others
3402,00:04:35,1,Course Introduction,,"So the point is that with so much text data,",00:04:32,2,So point much text data
3403,00:04:41,1,Course Introduction,,we can build very useful systems to help people in many different ways.,00:04:35,2,build useful systems help people many different ways
3404,00:04:44,1,Course Introduction,,Now how do we build this systems?,00:04:41,2,Now build systems
3405,00:04:49,1,Course Introduction,,Well these actually are the main technologies that we'll be talking,00:04:44,2,Well actually main technologies talking
3406,00:04:54,1,Course Introduction,,about in this course and the other course that I'm teaching for this specialization.,00:04:49,2,course course I teaching specialization
3407,00:04:57,1,Course Introduction,,The main techniques for building these systems and also for,00:04:54,2,The main techniques building systems also
3408,00:05:02,1,Course Introduction,,harnessing the text data are text retrieval and text data mining.,00:04:57,2,harnessing text data text retrieval text data mining
3409,00:05:07,1,Course Introduction,,So I use this picture to show the relation between these two,00:05:02,2,So I use picture show relation two
3410,00:05:08,1,Course Introduction,,some of the different techniques.,00:05:07,2,different techniques
3411,00:05:11,1,Course Introduction,,"We started with big text data, right?",00:05:08,2,We started big text data right
3412,00:05:15,1,Course Introduction,,"But for any applications, we don't necessarily need to use all the data.",00:05:11,2,But applications necessarily need use data
3413,00:05:22,1,Course Introduction,,"Often we only need the small subset of the most relevant data, and that's shown here.",00:05:16,2,Often need small subset relevant data shown
3414,00:05:28,1,Course Introduction,,"So text retrieval is to convert big, raw text data into that small",00:05:22,2,So text retrieval convert big raw text data small
3415,00:05:33,1,Course Introduction,,subset of most relevant data that are most useful for a particular application.,00:05:28,2,subset relevant data useful particular application
3416,00:05:35,1,Course Introduction,,And this is usually done by search engines.,00:05:33,2,And usually done search engines
3417,00:05:37,1,Course Introduction,,And so this will be covered in this course.,00:05:35,2,And covered course
3418,00:05:42,1,Course Introduction,,"After we have got a small amount of relevant data,",00:05:37,2,After got small amount relevant data
3419,00:05:46,1,Course Introduction,,"we also need to further analyze the data to help people digest the data, or",00:05:42,2,also need analyze data help people digest data
3420,00:05:50,1,Course Introduction,,to turn the data into actionable knowledge.,00:05:46,2,turn data actionable knowledge
3421,00:05:54,1,Course Introduction,,"And this step is called text mining, where we use a number of techniques to",00:05:50,2,And step called text mining use number techniques
3422,00:05:57,1,Course Introduction,,mine the data to get useful knowledge or pairings.,00:05:54,2,mine data get useful knowledge pairings
3423,00:06:01,1,Course Introduction,,And the knowledge can then be used in many different applications.,00:05:57,2,And knowledge used many different applications
3424,00:06:05,1,Course Introduction,,"And this part, text mining, will be covered in the other course that I'm",00:06:01,2,And part text mining covered course I
3425,00:06:08,1,Course Introduction,,teaching called Text Mining and Analytics.,00:06:05,2,teaching called Text Mining Analytics
3426,00:06:12,1,Course Introduction,,The emphasis of this course is on basic concepts and,00:06:08,2,The emphasis course basic concepts
3427,00:06:14,1,Course Introduction,,practical techniques in text retrieval.,00:06:12,2,practical techniques text retrieval
3428,00:06:19,1,Course Introduction,,More specifically we will cover how search engines work.,00:06:15,2,More specifically cover search engines work
3429,00:06:21,1,Course Introduction,,How to implement a search engine.,00:06:19,2,How implement search engine
3430,00:06:24,1,Course Introduction,,"How to evaluate a search engine, so that you know one search engine is",00:06:21,2,How evaluate search engine know one search engine
3431,00:06:27,1,Course Introduction,,better than another or one method is better than another.,00:06:24,2,better another one method better another
3432,00:06:30,1,Course Introduction,,How to improve and optimize a search engine system.,00:06:27,2,How improve optimize search engine system
3433,00:06:32,1,Course Introduction,,And how to build a recommender system.,00:06:30,2,And build recommender system
3434,00:06:38,1,Course Introduction,,We also hope to provide a hands on experience on multiple aspects.,00:06:34,2,We also hope provide hands experience multiple aspects
3435,00:06:42,1,Course Introduction,,One is to create a test collection for evaluating search engines.,00:06:38,2,One create test collection evaluating search engines
3436,00:06:48,1,Course Introduction,,This is very important for knowing which technique actually worked well.,00:06:42,2,This important knowing technique actually worked well
3437,00:06:51,1,Course Introduction,,And whether your search engine system is really good for your application.,00:06:48,2,And whether search engine system really good application
3438,00:06:57,1,Course Introduction,,The other aspect is to experiment with search engine algorithms.,00:06:53,2,The aspect experiment search engine algorithms
3439,00:07:01,1,Course Introduction,,"In practice, you will have to face choices of different algorithms.",00:06:57,2,In practice face choices different algorithms
3440,00:07:04,1,Course Introduction,,So it's important to know how to compare them and,00:07:01,2,So important know compare
3441,00:07:08,1,Course Introduction,,"to figure out how they work or maybe potentially, how to improve them.",00:07:04,2,figure work maybe potentially improve
3442,00:07:15,1,Course Introduction,,"And finally, we'll provide a platform for you to do search engine competition.",00:07:08,2,And finally provide platform search engine competition
3443,00:07:20,1,Course Introduction,,Where you can compare your different ideas to see which idea works better,00:07:15,2,Where compare different ideas see idea works better
3444,00:07:21,1,Course Introduction,,on some data set.,00:07:20,2,data set
3445,00:07:25,1,Course Introduction,,The prerequisites for this course are minimum.,00:07:21,2,The prerequisites course minimum
3446,00:07:28,1,Course Introduction,,"Basically we hope you have some basic concepts of computer science, for",00:07:25,2,Basically hope basic concepts computer science
3447,00:07:30,1,Course Introduction,,example data structures.,00:07:28,2,example data structures
3448,00:07:34,1,Course Introduction,,"And we hope you will be comfortable with programming, especially in C++.",00:07:30,2,And hope comfortable programming especially C
3449,00:07:38,1,Course Introduction,,because that's the language that we'll use for some of the programming assignments.,00:07:34,2,language use programming assignments
3450,00:07:46,1,Course Introduction,,"The format is lectures plus quizzes, as often happens in MOOCs.",00:07:40,2,The format lectures plus quizzes often happens MOOCs
3451,00:07:49,1,Course Introduction,,And we also will provide a program assignments for,00:07:46,2,And also provide program assignments
3452,00:07:52,1,Course Introduction,,those of you that have the resources to do that.,00:07:49,2,resources
3453,00:07:55,1,Course Introduction,,We don't really have any required readings for this course.,00:07:52,2,We really required readings course
3454,00:07:58,1,Course Introduction,,"That just means if you follow all the lecture videos carefully,",00:07:55,2,That means follow lecture videos carefully
3455,00:08:02,1,Course Introduction,,and you're suppose to know all the basic concepts and the basic techniques.,00:07:58,2,suppose know basic concepts basic techniques
3456,00:08:06,1,Course Introduction,,"But it's always useful to read more, so",00:08:02,2,But always useful read
3457,00:08:10,1,Course Introduction,,here we provide a list of some useful reference books.,00:08:06,2,provide list useful reference books
3458,00:08:16,1,Course Introduction,,"And this in time order, and that also includes a book that",00:08:10,2,And time order also includes book
3459,00:08:19,1,Course Introduction,,"and I are co-authoring now, and",00:08:16,2,I co authoring
3460,00:08:25,1,Course Introduction,,we make some draft chapters available on this website.,00:08:19,2,make draft chapters available website
3461,00:08:28,1,Course Introduction,,And we can find more readings and reference books on this website.,00:08:25,2,And find readings reference books website
3462,00:08:31,1,Course Introduction,,"Finally, and this is the course schedule.",00:08:29,2,Finally course schedule
3463,00:08:36,1,Course Introduction,,"That's just the top of the map for the rest of the course,",00:08:31,2,That top map rest course
3464,00:08:40,1,Course Introduction,,and it shows the topics that we will cover in the remaining lectures.,00:08:36,2,shows topics cover remaining lectures
3465,00:08:46,1,Course Introduction,,This picture also shows basic flow of information in a text information system.,00:08:40,2,This picture also shows basic flow information text information system
3466,00:08:51,1,Course Introduction,,"So starting from the big text data, the first step is to do some natural language",00:08:46,2,So starting big text data first step natural language
3467,00:08:57,1,Course Introduction,,"content analysis, because text data is in the form of natural language text.",00:08:51,2,content analysis text data form natural language text
3468,00:09:01,1,Course Introduction,,So we need to understand the text to some extent,00:08:57,2,So need understand text extent
3469,00:09:03,1,Course Introduction,,in order to do something useful for the users.,00:09:01,2,order something useful users
3470,00:09:05,1,Course Introduction,,So this is the first topic that we will cover.,00:09:03,2,So first topic cover
3471,00:09:11,1,Course Introduction,,And then on top of that as you can see there are two boxes here.,00:09:05,2,And top see two boxes
3472,00:09:16,1,Course Introduction,,Those are two types of systems that can be used to help people,00:09:11,2,Those two types systems used help people
3473,00:09:19,1,Course Introduction,,get access to the most relevant data.,00:09:16,2,get access relevant data
3474,00:09:23,1,Course Introduction,,"Or in other words, those are the two kinds of systems that will convert",00:09:19,2,Or words two kinds systems convert
3475,00:09:26,1,Course Introduction,,big text data into small relevant text data.,00:09:23,2,big text data small relevant text data
3476,00:09:31,1,Course Introduction,,Search engines are helping users to search or,00:09:28,2,Search engines helping users search
3477,00:09:36,1,Course Introduction,,to query the data to get the most relevant documents out.,00:09:31,2,query data get relevant documents
3478,00:09:40,1,Course Introduction,,"Recommender systems are to recommend information to users,",00:09:36,2,Recommender systems recommend information users
3479,00:09:42,1,Course Introduction,,to push information to users.,00:09:40,2,push information users
3480,00:09:45,1,Course Introduction,,"So those are two, complementary was of",00:09:42,2,So two complementary
3481,00:09:50,1,Course Introduction,,getting users connected to the most relevant data at the right time.,00:09:45,2,getting users connected relevant data right time
3482,00:09:55,1,Course Introduction,,"So this part is called text access, and this will be the next topic.",00:09:50,2,So part called text access next topic
3483,00:10:00,1,Course Introduction,,"And after we cover that we are going to cover a number of topics,",00:09:55,2,And cover going cover number topics
3484,00:10:02,1,Course Introduction,,all about the search engines.,00:10:00,2,search engines
3485,00:10:05,1,Course Introduction,,"Now the text access topic is a brief topic,",00:10:02,2,Now text access topic brief topic
3486,00:10:09,1,Course Introduction,,a brief coverage of the two kinds of systems.,00:10:05,2,brief coverage two kinds systems
3487,00:10:12,1,Course Introduction,,"In the remaining topics, we'll cover search engines in much more detail.",00:10:09,2,In remaining topics cover search engines much detail
3488,00:10:17,1,Course Introduction,,"That includes text retrieval problem, text retrieval methods, how to evaluate",00:10:12,2,That includes text retrieval problem text retrieval methods evaluate
3489,00:10:21,1,Course Introduction,,"these methods, implementation of the system, and web search applications.",00:10:17,2,methods implementation system web search applications
3490,00:10:25,1,Course Introduction,,"And after these, we're going to go cover the recommender system.",00:10:23,2,And going go cover recommender system
3491,00:10:31,1,Course Introduction,,So this is what you expect in the rest of this course.,00:10:25,2,So expect rest course
3492,00:10:32,1,Course Introduction,,Thanks.,00:10:31,2,Thanks
3493,00:00:04,2,Vector Space Model- Improved Instantiation,1.7,[SOUND].,00:00:00,7,SOUND
3494,00:00:12,2,Vector Space Model- Improved Instantiation,1.7,"In this lecture, we're going to talk about how to improve the instant changing of",00:00:07,7,In lecture going talk improve instant changing
3495,00:00:14,2,Vector Space Model- Improved Instantiation,1.7,the Vector Space Model.,00:00:12,7,Vector Space Model
3496,00:00:22,2,Vector Space Model- Improved Instantiation,1.7,This is the continued discussion of the Vector Space Model.,00:00:17,7,This continued discussion Vector Space Model
3497,00:00:30,2,Vector Space Model- Improved Instantiation,1.7,We're going to focus on how to improve the instant changing of this model.,00:00:22,7,We going focus improve instant changing model
3498,00:00:34,2,Vector Space Model- Improved Instantiation,1.7,"In a previous lecture, you have seen that with simple",00:00:30,7,In previous lecture seen simple
3499,00:00:39,2,Vector Space Model- Improved Instantiation,1.7,"situations of the Vector Space Model, we can come up with",00:00:34,7,situations Vector Space Model come
3500,00:00:44,2,Vector Space Model- Improved Instantiation,1.7,"a simple scoring function that would give us, basically,",00:00:39,7,simple scoring function would give us basically
3501,00:00:49,2,Vector Space Model- Improved Instantiation,1.7,a count of how many unique query terms are matching the document.,00:00:44,7,count many unique query terms matching document
3502,00:00:56,2,Vector Space Model- Improved Instantiation,1.7,We also have seen that this function has a problem as shown on this slide.,00:00:50,7,We also seen function problem shown slide
3503,00:01:00,2,Vector Space Model- Improved Instantiation,1.7,"In particular, if you look at these three documents,",00:00:56,7,In particular look three documents
3504,00:01:05,2,Vector Space Model- Improved Instantiation,1.7,they will all get the same score because they match the three unique query words.,00:01:00,7,get score match three unique query words
3505,00:01:10,2,Vector Space Model- Improved Instantiation,1.7,"But intuitively we would like, d4 to be ranked above d3.",00:01:06,7,But intuitively would like d4 ranked d3
3506,00:01:13,2,Vector Space Model- Improved Instantiation,1.7,And d2 is really non relevant.,00:01:10,7,And d2 really non relevant
3507,00:01:20,2,Vector Space Model- Improved Instantiation,1.7,So the problem here is that this function couldn't capture,00:01:14,7,So problem function capture
3508,00:01:22,2,Vector Space Model- Improved Instantiation,1.7,the following characteristics.,00:01:20,7,following characteristics
3509,00:01:27,2,Vector Space Model- Improved Instantiation,1.7,"First, we would like to give more gratitude to d4",00:01:22,7,First would like give gratitude d4
3510,00:01:32,2,Vector Space Model- Improved Instantiation,1.7,because it matches the presidential more times than d3.,00:01:27,7,matches presidential times d3
3511,00:01:38,2,Vector Space Model- Improved Instantiation,1.7,"Second, intuitively matching presidential should be more important",00:01:32,7,Second intuitively matching presidential important
3512,00:01:43,2,Vector Space Model- Improved Instantiation,1.7,"than matching about, because about is a very common word that occurs everywhere.",00:01:38,7,matching common word occurs everywhere
3513,00:01:44,2,Vector Space Model- Improved Instantiation,1.7,It doesn't really carry that much content.,00:01:43,7,It really carry much content
3514,00:01:48,2,Vector Space Model- Improved Instantiation,1.7,"So, in this lecture,",00:01:47,7,So lecture
3515,00:01:52,2,Vector Space Model- Improved Instantiation,1.7,let's see how we can improve the model to solve these two problems.,00:01:48,7,let see improve model solve two problems
3516,00:01:59,2,Vector Space Model- Improved Instantiation,1.7,It's worth thinking at this point about why do we have these four problems.,00:01:53,7,It worth thinking point four problems
3517,00:02:04,2,Vector Space Model- Improved Instantiation,1.7,If we look back at the assumptions we have made,00:02:01,7,If look back assumptions made
3518,00:02:08,2,Vector Space Model- Improved Instantiation,1.7,"while substantiating the Vector Space Model, we will realize that",00:02:04,7,substantiating Vector Space Model realize
3519,00:02:15,2,Vector Space Model- Improved Instantiation,1.7,the problem is really coming from some of the assumptions.,00:02:08,7,problem really coming assumptions
3520,00:02:19,2,Vector Space Model- Improved Instantiation,1.7,"In particular, it has to do with how we place the vectors in the vector space.",00:02:15,7,In particular place vectors vector space
3521,00:02:25,2,Vector Space Model- Improved Instantiation,1.7,"So then, naturally, in order to fix these problems,",00:02:22,7,So naturally order fix problems
3522,00:02:27,2,Vector Space Model- Improved Instantiation,1.7,we have to revisit those assumptions.,00:02:25,7,revisit assumptions
3523,00:02:31,2,Vector Space Model- Improved Instantiation,1.7,"Perhaps, you will have to use different ways to",00:02:27,7,Perhaps use different ways
3524,00:02:34,2,Vector Space Model- Improved Instantiation,1.7,instantiate the Vector Space Model.,00:02:31,7,instantiate Vector Space Model
3525,00:02:41,2,Vector Space Model- Improved Instantiation,1.7,"In particular, we have to place the vectors in a different way.",00:02:34,7,In particular place vectors different way
3526,00:02:45,2,Vector Space Model- Improved Instantiation,1.7,"So, let's see how can we prove this?",00:02:41,7,So let see prove
3527,00:02:50,2,Vector Space Model- Improved Instantiation,1.7,"Well, our natural thought is in order to consider multiple times of a term",00:02:45,7,Well natural thought order consider multiple times term
3528,00:02:52,2,Vector Space Model- Improved Instantiation,1.7,in a document.,00:02:50,7,document
3529,00:02:57,2,Vector Space Model- Improved Instantiation,1.7,We should consider the term frequency instead of just the absence or presence.,00:02:52,7,We consider term frequency instead absence presence
3530,00:03:02,2,Vector Space Model- Improved Instantiation,1.7,In order to consider the difference between a document where a query,00:02:57,7,In order consider difference document query
3531,00:03:07,2,Vector Space Model- Improved Instantiation,1.7,term occurred multiple times and the one where the query term occurred just once.,00:03:02,7,term occurred multiple times one query term occurred
3532,00:03:13,2,Vector Space Model- Improved Instantiation,1.7,"We have to concede a term frequency, the count of a term being in the document.",00:03:07,7,We concede term frequency count term document
3533,00:03:18,2,Vector Space Model- Improved Instantiation,1.7,"In the simplest model, we only model the presence and absence of a term.",00:03:13,7,In simplest model model presence absence term
3534,00:03:25,2,Vector Space Model- Improved Instantiation,1.7,We ignore the actual number of times that a term occurs in a document.,00:03:18,7,We ignore actual number times term occurs document
3535,00:03:26,2,Vector Space Model- Improved Instantiation,1.7,So let's add this back.,00:03:25,7,So let add back
3536,00:03:31,2,Vector Space Model- Improved Instantiation,1.7,So we're going to do then represent a document by a vector with,00:03:26,7,So going represent document vector
3537,00:03:34,2,Vector Space Model- Improved Instantiation,1.7,term frequency as element.,00:03:31,7,term frequency element
3538,00:03:40,2,Vector Space Model- Improved Instantiation,1.7,"So, that is to say, now, the elements of both the query vector and",00:03:34,7,So say elements query vector
3539,00:03:43,2,Vector Space Model- Improved Instantiation,1.7,"the document vector will not be zero once, but",00:03:40,7,document vector zero
3540,00:03:49,2,Vector Space Model- Improved Instantiation,1.7,instead there will be the counts of a word in the query or the document.,00:03:43,7,instead counts word query document
3541,00:03:55,2,Vector Space Model- Improved Instantiation,1.7,So this would bring additional information about the document.,00:03:52,7,So would bring additional information document
3542,00:04:00,2,Vector Space Model- Improved Instantiation,1.7,So this can be seen as a more accurate representation of our documents.,00:03:55,7,So seen accurate representation documents
3543,00:04:03,2,Vector Space Model- Improved Instantiation,1.7,"So, now let's see what the formula would look like if we change",00:04:00,7,So let see formula would look like change
3544,00:04:05,2,Vector Space Model- Improved Instantiation,1.7,this representation.,00:04:03,7,representation
3545,00:04:10,2,Vector Space Model- Improved Instantiation,1.7,"So as you see on this slide, we still use that product, and,",00:04:05,7,So see slide still use product
3546,00:04:14,2,Vector Space Model- Improved Instantiation,1.7,so the formula looks very similar in the form.,00:04:10,7,formula looks similar form
3547,00:04:18,2,Vector Space Model- Improved Instantiation,1.7,"In fact, it looks identical, but inside of the sum of cos xi and",00:04:14,7,In fact looks identical inside sum cos xi
3548,00:04:20,2,Vector Space Model- Improved Instantiation,1.7,yi are now different.,00:04:18,7,yi different
3549,00:04:28,2,Vector Space Model- Improved Instantiation,1.7,They're now the counts of words i in the query and the document.,00:04:21,7,They counts words query document
3550,00:04:35,2,Vector Space Model- Improved Instantiation,1.7,"Now at this point, I also suggest you to pause the lecture for moment and",00:04:30,7,Now point I also suggest pause lecture moment
3551,00:04:41,2,Vector Space Model- Improved Instantiation,1.7,just we'll think about how we have interpret the score of this new function.,00:04:35,7,think interpret score new function
3552,00:04:47,2,Vector Space Model- Improved Instantiation,1.7,It's doing something very similar to what the simplest VSM is doing.,00:04:41,7,It something similar simplest VSM
3553,00:04:50,2,Vector Space Model- Improved Instantiation,1.7,"But because of the change of the vector,",00:04:47,7,But change vector
3554,00:04:53,2,Vector Space Model- Improved Instantiation,1.7,now the new score has a different interpretation.,00:04:51,7,new score different interpretation
3555,00:04:55,2,Vector Space Model- Improved Instantiation,1.7,Can you see the difference?,00:04:54,7,Can see difference
3556,00:05:00,2,Vector Space Model- Improved Instantiation,1.7,And it has to do with the consideration of multiple,00:04:56,7,And consideration multiple
3557,00:05:03,2,Vector Space Model- Improved Instantiation,1.7,occurrences of the same time in the document.,00:05:00,7,occurrences time document
3558,00:05:06,2,Vector Space Model- Improved Instantiation,1.7,"More importantly, we''ll try to know whether this would fix the problem of",00:05:03,7,More importantly try know whether would fix problem
3559,00:05:07,2,Vector Space Model- Improved Instantiation,1.7,the simplest vector space model.,00:05:06,7,simplest vector space model
3560,00:05:12,2,Vector Space Model- Improved Instantiation,1.7,"So, let's look at the this example again.",00:05:07,7,So let look example
3561,00:05:16,2,Vector Space Model- Improved Instantiation,1.7,"So suppose, we change the vector to term frequency vectors.",00:05:12,7,So suppose change vector term frequency vectors
3562,00:05:20,2,Vector Space Model- Improved Instantiation,1.7,"Now, let's look at these three documents again.",00:05:16,7,Now let look three documents
3563,00:05:25,2,Vector Space Model- Improved Instantiation,1.7,The query vector is the same because all these words occurred exactly once,00:05:20,7,The query vector words occurred exactly
3564,00:05:27,2,Vector Space Model- Improved Instantiation,1.7,in the query.,00:05:25,7,query
3565,00:05:29,2,Vector Space Model- Improved Instantiation,1.7,So the vector is still 0 1 vector.,00:05:27,7,So vector still 0 1 vector
3566,00:05:35,2,Vector Space Model- Improved Instantiation,1.7,"And in fact, d2 is also essential in representing",00:05:31,7,And fact d2 also essential representing
3567,00:05:40,2,Vector Space Model- Improved Instantiation,1.7,the same way because none of these words has been repeated many times.,00:05:35,7,way none words repeated many times
3568,00:05:44,2,Vector Space Model- Improved Instantiation,1.7,"As a result, the score is also the same, still three.",00:05:40,7,As result score also still three
3569,00:05:49,2,Vector Space Model- Improved Instantiation,1.7,The same issue for d3 and we still have a 3.,00:05:44,7,The issue d3 still 3
3570,00:05:53,2,Vector Space Model- Improved Instantiation,1.7,"But d4 would be different, because now,",00:05:49,7,But d4 would different
3571,00:05:57,2,Vector Space Model- Improved Instantiation,1.7,presidential occurred twice here.,00:05:53,7,presidential occurred twice
3572,00:06:04,2,Vector Space Model- Improved Instantiation,1.7,So the end in the four presidential in the [INAUDIBLE] would be 2 instead of 1.,00:05:57,7,So end four presidential INAUDIBLE would 2 instead 1
3573,00:06:08,2,Vector Space Model- Improved Instantiation,1.7,"As a result, now the score for d4 is higher.",00:06:04,7,As result score d4 higher
3574,00:06:09,2,Vector Space Model- Improved Instantiation,1.7,It's a four now.,00:06:08,7,It four
3575,00:06:14,2,Vector Space Model- Improved Instantiation,1.7,"So this means, by using term frequency,",00:06:09,7,So means using term frequency
3576,00:06:19,2,Vector Space Model- Improved Instantiation,1.7,we can now rank d4 above d2 and d3 as we hope to.,00:06:14,7,rank d4 d2 d3 hope
3577,00:06:23,2,Vector Space Model- Improved Instantiation,1.7,So this solve the problem with default.,00:06:19,7,So solve problem default
3578,00:06:32,2,Vector Space Model- Improved Instantiation,1.7,"But, we can also see that d2 and d3 are still featured in the same way.",00:06:26,7,But also see d2 d3 still featured way
3579,00:06:40,2,Vector Space Model- Improved Instantiation,1.7,"They still have identical scores, so it did not fix the problem here.",00:06:32,7,They still identical scores fix problem
3580,00:06:42,2,Vector Space Model- Improved Instantiation,1.7,"So, how can we fix this problem?",00:06:40,7,So fix problem
3581,00:06:49,2,Vector Space Model- Improved Instantiation,1.7,"We would like, to give more credit for matching presidential than matching about.",00:06:42,7,We would like give credit matching presidential matching
3582,00:06:52,2,Vector Space Model- Improved Instantiation,1.7,But how can we solve the problem in a general way?,00:06:49,7,But solve problem general way
3583,00:06:57,2,Vector Space Model- Improved Instantiation,1.7,Is there any way to determine which word should be treated more importantly and,00:06:53,7,Is way determine word treated importantly
3584,00:07:01,2,Vector Space Model- Improved Instantiation,1.7,"which word can be, basically ignored.",00:06:57,7,word basically ignored
3585,00:07:03,2,Vector Space Model- Improved Instantiation,1.7,About is such a word.,00:07:02,7,About word
3586,00:07:09,2,Vector Space Model- Improved Instantiation,1.7,"And which it does not really carry that much content,",00:07:05,7,And really carry much content
3587,00:07:11,2,Vector Space Model- Improved Instantiation,1.7,we can essentially ignore that.,00:07:09,7,essentially ignore
3588,00:07:14,2,Vector Space Model- Improved Instantiation,1.7,"We sometimes call such a word, a stock word.",00:07:11,7,We sometimes call word stock word
3589,00:07:18,2,Vector Space Model- Improved Instantiation,1.7,"Those are generally very frequent and they occur everywhere,",00:07:14,7,Those generally frequent occur everywhere
3590,00:07:21,2,Vector Space Model- Improved Instantiation,1.7,"matching it, doesn't really mean anything.",00:07:18,7,matching really mean anything
3591,00:07:24,2,Vector Space Model- Improved Instantiation,1.7,But computation how can we capture that?,00:07:21,7,But computation capture
3592,00:07:27,2,Vector Space Model- Improved Instantiation,1.7,"So again, I encourage you to think a little bit about this.",00:07:24,7,So I encourage think little bit
3593,00:07:32,2,Vector Space Model- Improved Instantiation,1.7,Can you come up with any statistical approaches to somehow,00:07:29,7,Can come statistical approaches somehow
3594,00:07:34,2,Vector Space Model- Improved Instantiation,1.7,distinguish presidential from about.,00:07:32,7,distinguish presidential
3595,00:07:41,2,Vector Space Model- Improved Instantiation,1.7,"If you think about it for a moment, you realize that,",00:07:37,7,If think moment realize
3596,00:07:46,2,Vector Space Model- Improved Instantiation,1.7,one difference is that a word like above occurs everywhere.,00:07:41,7,one difference word like occurs everywhere
3597,00:07:50,2,Vector Space Model- Improved Instantiation,1.7,So if you count the currents of the water in the whole collection that we,00:07:46,7,So count currents water whole collection
3598,00:07:54,2,Vector Space Model- Improved Instantiation,1.7,would see that about as much higher for,00:07:50,7,would see much higher
3599,00:07:58,2,Vector Space Model- Improved Instantiation,1.7,"this than presidential, which it tends to occur only in some documents.",00:07:54,7,presidential tends occur documents
3600,00:08:05,2,Vector Space Model- Improved Instantiation,1.7,So this idea suggests that we could somehow,00:08:01,7,So idea suggests could somehow
3601,00:08:10,2,Vector Space Model- Improved Instantiation,1.7,use the global statistics of terms or some other formation to try to,00:08:05,7,use global statistics terms formation try
3602,00:08:15,2,Vector Space Model- Improved Instantiation,1.7,down weight the element for,00:08:10,7,weight element
3603,00:08:18,2,Vector Space Model- Improved Instantiation,1.7,about in the vector representation of d2.,00:08:15,7,vector representation d2
3604,00:08:21,2,Vector Space Model- Improved Instantiation,1.7,"At the same time,",00:08:18,7,At time
3605,00:08:27,2,Vector Space Model- Improved Instantiation,1.7,we hope to somehow increase the weight of presidential in the vector of d3.,00:08:21,7,hope somehow increase weight presidential vector d3
3606,00:08:34,2,Vector Space Model- Improved Instantiation,1.7,"If we can do that, then, we can expect that d2 will get",00:08:29,7,If expect d2 get
3607,00:08:39,2,Vector Space Model- Improved Instantiation,1.7,"the overall score to be less than three, while d3 will get the score about three.",00:08:34,7,overall score less three d3 get score three
3608,00:08:42,2,Vector Space Model- Improved Instantiation,1.7,"Then, we'll be able to rank d3 on top of d2.",00:08:39,7,Then able rank d3 top d2
3609,00:08:47,2,Vector Space Model- Improved Instantiation,1.7,So how can we do this systematically?,00:08:45,7,So systematically
3610,00:08:51,2,Vector Space Model- Improved Instantiation,1.7,"Again, we can rely on some steps that people count.",00:08:47,7,Again rely steps people count
3611,00:08:55,2,Vector Space Model- Improved Instantiation,1.7,"And in this case, the particular idea is called the Inverse Document Frequency.",00:08:51,7,And case particular idea called Inverse Document Frequency
3612,00:08:59,2,Vector Space Model- Improved Instantiation,1.7,We have seen document frequency.,00:08:57,7,We seen document frequency
3613,00:09:04,2,Vector Space Model- Improved Instantiation,1.7,"As one signal used in, the moding retrieval functions.",00:08:59,7,As one signal used moding retrieval functions
3614,00:09:08,2,Vector Space Model- Improved Instantiation,1.7,We discussed this in a previous lecture.,00:09:05,7,We discussed previous lecture
3615,00:09:11,2,Vector Space Model- Improved Instantiation,1.7,So here's the specific way of using it.,00:09:08,7,So specific way using
3616,00:09:15,2,Vector Space Model- Improved Instantiation,1.7,Document frequency is the count of documents that contain a particular term.,00:09:11,7,Document frequency count documents contain particular term
3617,00:09:20,2,Vector Space Model- Improved Instantiation,1.7,"Here, we say inverse document frequency because we actually want to reword a word",00:09:15,7,Here say inverse document frequency actually want reword word
3618,00:09:22,2,Vector Space Model- Improved Instantiation,1.7,that doesn't occur in many documents.,00:09:20,7,occur many documents
3619,00:09:30,2,Vector Space Model- Improved Instantiation,1.7,"And so, the way to incorporate this into our vector [INAUDIBLE] is",00:09:24,7,And way incorporate vector INAUDIBLE
3620,00:09:35,2,Vector Space Model- Improved Instantiation,1.7,then to modify the frequency count by multiplying,00:09:30,7,modify frequency count multiplying
3621,00:09:40,2,Vector Space Model- Improved Instantiation,1.7,it by the idea of the corresponding word as shown here.,00:09:35,7,idea corresponding word shown
3622,00:09:43,2,Vector Space Model- Improved Instantiation,1.7,"If we didn't do that, then we can penalize common",00:09:40,7,If penalize common
3623,00:09:48,2,Vector Space Model- Improved Instantiation,1.7,"words which generally have a low idea of, and",00:09:43,7,words generally low idea
3624,00:09:54,2,Vector Space Model- Improved Instantiation,1.7,"reward real words, which we're have a higher IDF.",00:09:48,7,reward real words higher IDF
3625,00:10:01,2,Vector Space Model- Improved Instantiation,1.7,So most specific [INAUDIBLE] IDF can be defined as the logarithm,00:09:56,7,So specific INAUDIBLE IDF defined logarithm
3626,00:10:06,2,Vector Space Model- Improved Instantiation,1.7,"of M plus one divided by k, where M is the total number of",00:10:01,7,M plus one divided k M total number
3627,00:10:11,2,Vector Space Model- Improved Instantiation,1.7,"documents in the collection,k is df or document frequency.",00:10:06,7,documents collection k df document frequency
3628,00:10:15,2,Vector Space Model- Improved Instantiation,1.7,The total number of documents containing the word W.,00:10:11,7,The total number documents containing word W
3629,00:10:18,2,Vector Space Model- Improved Instantiation,1.7,"Now, if you plot this function by varying k,",00:10:15,7,Now plot function varying k
3630,00:10:23,2,Vector Space Model- Improved Instantiation,1.7,then you will see the curve that look like this.,00:10:18,7,see curve look like
3631,00:10:28,2,Vector Space Model- Improved Instantiation,1.7,"In general, you can see it would give a higher value for",00:10:23,7,In general see would give higher value
3632,00:10:30,2,Vector Space Model- Improved Instantiation,1.7,"a low DF word, a rare word.",00:10:28,7,low DF word rare word
3633,00:10:38,2,Vector Space Model- Improved Instantiation,1.7,You can also see the maximum value of this function is log of M plus 1.,00:10:30,7,You also see maximum value function log M plus 1
3634,00:10:46,2,Vector Space Model- Improved Instantiation,1.7,Will be interesting for you to think about what's minimum value for this function?,00:10:38,7,Will interesting think minimum value function
3635,00:10:48,2,Vector Space Model- Improved Instantiation,1.7,This could be interesting exercise.,00:10:46,7,This could interesting exercise
3636,00:10:55,2,Vector Space Model- Improved Instantiation,1.7,"Now, the specific function may not be as important as",00:10:51,7,Now specific function may important
3637,00:10:59,2,Vector Space Model- Improved Instantiation,1.7,the heuristic to simply penalize popular terms.,00:10:55,7,heuristic simply penalize popular terms
3638,00:11:05,2,Vector Space Model- Improved Instantiation,1.7,But it turns out this particular function form has also worked very well.,00:11:01,7,But turns particular function form also worked well
3639,00:11:12,2,Vector Space Model- Improved Instantiation,1.7,"Now, whether there is a better form of function here,",00:11:07,7,Now whether better form function
3640,00:11:13,2,Vector Space Model- Improved Instantiation,1.7,is the open research question.,00:11:12,7,open research question
3641,00:11:20,2,Vector Space Model- Improved Instantiation,1.7,"But, it's also clear that if we use a linear kernalization like what's",00:11:15,7,But also clear use linear kernalization like
3642,00:11:27,2,Vector Space Model- Improved Instantiation,1.7,"shown here with this line, then, it may not be as reasonable as the standard IDF.",00:11:20,7,shown line may reasonable standard IDF
3643,00:11:34,2,Vector Space Model- Improved Instantiation,1.7,"In particular, you can see the difference in the standard IDF,",00:11:27,7,In particular see difference standard IDF
3644,00:11:41,2,Vector Space Model- Improved Instantiation,1.7,"and we, somehow have a [INAUDIBLE] point here.",00:11:35,7,somehow INAUDIBLE point
3645,00:11:45,2,Vector Space Model- Improved Instantiation,1.7,"After this point, we're going to say these terms are essentially not very useful.",00:11:41,7,After point going say terms essentially useful
3646,00:11:48,2,Vector Space Model- Improved Instantiation,1.7,They can be essentially ignored.,00:11:45,7,They essentially ignored
3647,00:11:52,2,Vector Space Model- Improved Instantiation,1.7,"And this makes sense when the term occurs so frequently, and",00:11:48,7,And makes sense term occurs frequently
3648,00:11:57,2,Vector Space Model- Improved Instantiation,1.7,let's say a term occurs in more than 50% of the documents.,00:11:52,7,let say term occurs 50 documents
3649,00:12:01,2,Vector Space Model- Improved Instantiation,1.7,"Then the term is unlikely very important and it's, it's basically, a common term.",00:11:57,7,Then term unlikely important basically common term
3650,00:12:07,2,Vector Space Model- Improved Instantiation,1.7,"It's not very important to match this word, so with the standard IDF, you can",00:12:01,7,It important match word standard IDF
3651,00:12:14,2,Vector Space Model- Improved Instantiation,1.7,"see it's, basically, assumed that they all have lower weights, there's no difference.",00:12:07,7,see basically assumed lower weights difference
3652,00:12:17,2,Vector Space Model- Improved Instantiation,1.7,"But if you look at the linear kernelization, at this point there is,",00:12:14,7,But look linear kernelization point
3653,00:12:17,2,Vector Space Model- Improved Instantiation,1.7,there's some difference.,00:12:17,7,difference
3654,00:12:26,2,Vector Space Model- Improved Instantiation,1.7,"So intuitively, we want to focus more on the discrimination of low DF words,",00:12:19,7,So intuitively want focus discrimination low DF words
3655,00:12:32,2,Vector Space Model- Improved Instantiation,1.7,rather than these common words.,00:12:26,7,rather common words
3656,00:12:38,2,Vector Space Model- Improved Instantiation,1.7,"Well, of course, which one works better, still has to be validated",00:12:32,7,Well course one works better still validated
3657,00:12:43,2,Vector Space Model- Improved Instantiation,1.7,by using the empirically related data set.,00:12:38,7,using empirically related data set
3658,00:12:46,2,Vector Space Model- Improved Instantiation,1.7,And we have to use users to judge which results of that.,00:12:43,7,And use users judge results
3659,00:12:53,2,Vector Space Model- Improved Instantiation,1.7,So now let's see how this can solve problem two.,00:12:48,7,So let see solve problem two
3660,00:12:55,2,Vector Space Model- Improved Instantiation,1.7,"So now, let's look at the two documents again.",00:12:53,7,So let look two documents
3661,00:13:00,2,Vector Space Model- Improved Instantiation,1.7,"Now without IDF weighting, before, we just have [INAUDIBLE] vectors,",00:12:56,7,Now without IDF weighting INAUDIBLE vectors
3662,00:13:05,2,Vector Space Model- Improved Instantiation,1.7,but with IDF weighting we now can adjust the DF weight,00:13:00,7,IDF weighting adjust DF weight
3663,00:13:09,2,Vector Space Model- Improved Instantiation,1.7,"by multiplying the, with the IDF value.",00:13:05,7,multiplying IDF value
3664,00:13:14,2,Vector Space Model- Improved Instantiation,1.7,"For example here, you can see is the adjustment in particular for",00:13:09,7,For example see adjustment particular
3665,00:13:19,2,Vector Space Model- Improved Instantiation,1.7,"about, there is an adjustment by using the IDF value of about",00:13:14,7,adjustment using IDF value
3666,00:13:23,2,Vector Space Model- Improved Instantiation,1.7,which is smaller than the IDF value of presidential.,00:13:19,7,smaller IDF value presidential
3667,00:13:28,2,Vector Space Model- Improved Instantiation,1.7,"So if you look at these, the IDF will distinguish these two words.",00:13:23,7,So look IDF distinguish two words
3668,00:13:34,2,Vector Space Model- Improved Instantiation,1.7,"As a result, adjustment here would be larger, would make this weight larger.",00:13:28,7,As result adjustment would larger would make weight larger
3669,00:13:42,2,Vector Space Model- Improved Instantiation,1.7,"So if we score with these new vectors, and",00:13:37,7,So score new vectors
3670,00:13:48,2,Vector Space Model- Improved Instantiation,1.7,"what would happen is that the, of course, they share the same weights for news and",00:13:42,7,would happen course share weights news
3671,00:13:53,2,Vector Space Model- Improved Instantiation,1.7,"the campaign, but the margin of about and presidential with this grouping may.",00:13:48,7,campaign margin presidential grouping may
3672,00:14:01,2,Vector Space Model- Improved Instantiation,1.7,"So now as a result of IDF weighting, we will have d3 to be ranked above d2.",00:13:53,7,So result IDF weighting d3 ranked d2
3673,00:14:06,2,Vector Space Model- Improved Instantiation,1.7,"Because it matched rail word, where as d2 matched common word.",00:14:01,7,Because matched rail word d2 matched common word
3674,00:14:10,2,Vector Space Model- Improved Instantiation,1.7,So this shows that the idea of weighting can solve problem two.,00:14:06,7,So shows idea weighting solve problem two
3675,00:14:19,2,Vector Space Model- Improved Instantiation,1.7,"So, how effective is this model in general when we use TF-IDF weighting?",00:14:13,7,So effective model general use TF IDF weighting
3676,00:14:23,2,Vector Space Model- Improved Instantiation,1.7,"Well, let's look at all these documents that we have seen before.",00:14:19,7,Well let look documents seen
3677,00:14:28,2,Vector Space Model- Improved Instantiation,1.7,These are the new scores of the new documents.,00:14:23,7,These new scores new documents
3678,00:14:31,2,Vector Space Model- Improved Instantiation,1.7,But how effective is this new weighting method and,00:14:28,7,But effective new weighting method
3679,00:14:32,2,Vector Space Model- Improved Instantiation,1.7,"new scoring function, all right?",00:14:31,7,new scoring function right
3680,00:14:38,2,Vector Space Model- Improved Instantiation,1.7,So now let's see overall how effective is this new ranking function,00:14:33,7,So let see overall effective new ranking function
3681,00:14:40,2,Vector Space Model- Improved Instantiation,1.7,with TF-IDF Weighting?,00:14:38,7,TF IDF Weighting
3682,00:14:44,2,Vector Space Model- Improved Instantiation,1.7,"Here, we show all the five documents that we have seen before, and",00:14:40,7,Here show five documents seen
3683,00:14:45,2,Vector Space Model- Improved Instantiation,1.7,these are their scores.,00:14:44,7,scores
3684,00:14:51,2,Vector Space Model- Improved Instantiation,1.7,"Now, we can see the scores for the first four",00:14:47,7,Now see scores first four
3685,00:14:56,2,Vector Space Model- Improved Instantiation,1.7,documents here seem to be quite reasonable.,00:14:51,7,documents seem quite reasonable
3686,00:14:57,2,Vector Space Model- Improved Instantiation,1.7,They are as we expected.,00:14:56,7,They expected
3687,00:15:01,2,Vector Space Model- Improved Instantiation,1.7,"However, we also see a new problem.",00:14:58,7,However also see new problem
3688,00:15:06,2,Vector Space Model- Improved Instantiation,1.7,"Because now d5, here, which did not have a very high",00:15:01,7,Because d5 high
3689,00:15:10,2,Vector Space Model- Improved Instantiation,1.7,score with our simplest vector space model.,00:15:06,7,score simplest vector space model
3690,00:15:13,2,Vector Space Model- Improved Instantiation,1.7,"Now, after it has a very high score.",00:15:10,7,Now high score
3691,00:15:15,2,Vector Space Model- Improved Instantiation,1.7,"In fact, it has the highest score here.",00:15:13,7,In fact highest score
3692,00:15:18,2,Vector Space Model- Improved Instantiation,1.7,"So, this creates a new problem.",00:15:16,7,So creates new problem
3693,00:15:23,2,Vector Space Model- Improved Instantiation,1.7,This actually a common phenomenon in designing material functions.,00:15:18,7,This actually common phenomenon designing material functions
3694,00:15:25,2,Vector Space Model- Improved Instantiation,1.7,"Basically, when you try to fix one problem,",00:15:23,7,Basically try fix one problem
3695,00:15:27,2,Vector Space Model- Improved Instantiation,1.7,you tend to introduce other problems.,00:15:25,7,tend introduce problems
3696,00:15:33,2,Vector Space Model- Improved Instantiation,1.7,And that's why it's very tricky how to design effective ranking function.,00:15:27,7,And tricky design effective ranking function
3697,00:15:39,2,Vector Space Model- Improved Instantiation,1.7,And what's what's the best ranking function is the open research question.,00:15:33,7,And best ranking function open research question
3698,00:15:42,2,Vector Space Model- Improved Instantiation,1.7,Researchers are still working on that.,00:15:39,7,Researchers still working
3699,00:15:47,2,Vector Space Model- Improved Instantiation,1.7,"But in the next few lecture, we're going to also talk about some additional",00:15:42,7,But next lecture going also talk additional
3700,00:15:52,2,Vector Space Model- Improved Instantiation,1.7,ideas to further improve this model and try to fix this problem.,00:15:47,7,ideas improve model try fix problem
3701,00:15:57,2,Vector Space Model- Improved Instantiation,1.7,"So to summarize this lecture,",00:15:55,7,So summarize lecture
3702,00:16:01,2,Vector Space Model- Improved Instantiation,1.7,we've talked about how to improve this vector space model.,00:15:57,7,talked improve vector space model
3703,00:16:05,2,Vector Space Model- Improved Instantiation,1.7,And we've got to improve the [INAUDIBLE] of the vector space model based on,00:16:01,7,And got improve INAUDIBLE vector space model based
3704,00:16:07,2,Vector Space Model- Improved Instantiation,1.7,TF-IDF weighting.,00:16:05,7,TF IDF weighting
3705,00:16:12,2,Vector Space Model- Improved Instantiation,1.7,"So the improvement, most of it, is on the placement of the vector.",00:16:08,7,So improvement placement vector
3706,00:16:20,2,Vector Space Model- Improved Instantiation,1.7,"Where we give higher weight to a term that occurred many times in the document,",00:16:14,7,Where give higher weight term occurred many times document
3707,00:16:23,2,Vector Space Model- Improved Instantiation,1.7,but infrequently in the whole collection.,00:16:20,7,infrequently whole collection
3708,00:16:28,2,Vector Space Model- Improved Instantiation,1.7,And we have seen that this improved model indeed works better than,00:16:23,7,And seen improved model indeed works better
3709,00:16:33,2,Vector Space Model- Improved Instantiation,1.7,"the simplest vector space model, but it also still has some problems.",00:16:28,7,simplest vector space model also still problems
3710,00:16:35,2,Vector Space Model- Improved Instantiation,1.7,"In the next lecture,",00:16:33,7,In next lecture
3711,00:16:41,2,Vector Space Model- Improved Instantiation,1.7,we're going to look at the how to address these additional problems.,00:16:35,7,going look address additional problems
3712,00:00:08,5,Learning to Rank - Part 2,4.4,"[SOUND] So now let's take a look at the specific,",00:00:00,7,SOUND So let take look specific
3713,00:00:14,5,Learning to Rank - Part 2,4.4,method that's based on regression.,00:00:08,7,method based regression
3714,00:00:17,5,Learning to Rank - Part 2,4.4,"Now this is one of the many different methods in fact,",00:00:14,7,Now one many different methods fact
3715,00:00:19,5,Learning to Rank - Part 2,4.4,it's the one of the simplest methods.,00:00:17,7,one simplest methods
3716,00:00:24,5,Learning to Rank - Part 2,4.4,And I choose this to explain the idea because it's it's so simple.,00:00:19,7,And I choose explain idea simple
3717,00:00:34,5,Learning to Rank - Part 2,4.4,So in this approach we simply assume that the relevance of a document,00:00:26,7,So approach simply assume relevance document
3718,00:00:39,5,Learning to Rank - Part 2,4.4,"with respect to the query, is related to a linear combination of all the features.",00:00:34,7,respect query related linear combination features
3719,00:00:47,5,Learning to Rank - Part 2,4.4,Here I used the Xi to emote the feature.,00:00:39,7,Here I used Xi emote feature
3720,00:00:51,5,Learning to Rank - Part 2,4.4,So Xi of Q and D is a feature.,00:00:47,7,So Xi Q D feature
3721,00:00:54,5,Learning to Rank - Part 2,4.4,"And we can have as many features as, we would like.",00:00:51,7,And many features would like
3722,00:01:01,5,Learning to Rank - Part 2,4.4,And we assume that these features can be combined in a linear manner.,00:00:55,7,And assume features combined linear manner
3723,00:01:06,5,Learning to Rank - Part 2,4.4,And each feature is controlled by a parameter here.,00:01:03,7,And feature controlled parameter
3724,00:01:10,5,Learning to Rank - Part 2,4.4,"And this beta is a parameter, that's a weighting parameter.",00:01:06,7,And beta parameter weighting parameter
3725,00:01:16,5,Learning to Rank - Part 2,4.4,A larger value would mean the feature would have a higher weight and,00:01:10,7,A larger value would mean feature would higher weight
3726,00:01:18,5,Learning to Rank - Part 2,4.4,it would contribute more to the scoring function.,00:01:16,7,would contribute scoring function
3727,00:01:23,5,Learning to Rank - Part 2,4.4,The specific form of the function actually also involves,00:01:18,7,The specific form function actually also involves
3728,00:01:27,5,Learning to Rank - Part 2,4.4,a transformation of the probability of relevance.,00:01:23,7,transformation probability relevance
3729,00:01:29,5,Learning to Rank - Part 2,4.4,So this is the probability of relevance.,00:01:27,7,So probability relevance
3730,00:01:36,5,Learning to Rank - Part 2,4.4,We know that the probability of relevance is within the range from 0 to 1.,00:01:30,7,We know probability relevance within range 0 1
3731,00:01:40,5,Learning to Rank - Part 2,4.4,And we could have just assumed that the scoring function is,00:01:36,7,And could assumed scoring function
3732,00:01:43,5,Learning to Rank - Part 2,4.4,related to this linear combination.,00:01:40,7,related linear combination
3733,00:01:47,5,Learning to Rank - Part 2,4.4,"Right, so we can do a, a linear regression but",00:01:43,7,Right linear regression
3734,00:01:53,5,Learning to Rank - Part 2,4.4,then the value of this linear combination could easily go beyond 1.,00:01:47,7,value linear combination could easily go beyond 1
3735,00:01:58,5,Learning to Rank - Part 2,4.4,"So this transformation here would map ze,",00:01:53,7,So transformation would map ze
3736,00:02:05,5,Learning to Rank - Part 2,4.4,0 to 1 range through the whole range of real values.,00:01:58,7,0 1 range whole range real values
3737,00:02:08,5,Learning to Rank - Part 2,4.4,"You can, you can verify it, it by yourself.",00:02:05,7,You verify
3738,00:02:17,5,Learning to Rank - Part 2,4.4,So this allows us then to connect to the probability of relevance,00:02:10,7,So allows us connect probability relevance
3739,00:02:23,5,Learning to Rank - Part 2,4.4,which is between 0 and 1 to a linear combination of arbitrary efficients.,00:02:17,7,0 1 linear combination arbitrary efficients
3740,00:02:28,5,Learning to Rank - Part 2,4.4,"And if we rewrite this into a probability function, we will get the next one.",00:02:23,7,And rewrite probability function get next one
3741,00:02:34,5,Learning to Rank - Part 2,4.4,"So on this side on this equation, we will have the probability of relevance.",00:02:28,7,So side equation probability relevance
3742,00:02:38,5,Learning to Rank - Part 2,4.4,"And on the right hand side, we will have this form.",00:02:35,7,And right hand side form
3743,00:02:42,5,Learning to Rank - Part 2,4.4,Now this form is created non-active.,00:02:39,7,Now form created non active
3744,00:02:46,5,Learning to Rank - Part 2,4.4,And it still involves the linear combination of features.,00:02:42,7,And still involves linear combination features
3745,00:02:51,5,Learning to Rank - Part 2,4.4,"And it's also clear that is, if this value is,",00:02:46,7,And also clear value
3746,00:02:54,5,Learning to Rank - Part 2,4.4,is.,00:02:51,7,
3747,00:02:59,5,Learning to Rank - Part 2,4.4,Of the linear combination in the equation above.,00:02:54,7,Of linear combination equation
3748,00:03:03,5,Learning to Rank - Part 2,4.4,"If this this, this value here,",00:02:59,7,If value
3749,00:03:12,5,Learning to Rank - Part 2,4.4,if this value is large then it will mean this value is small.,00:03:05,7,value large mean value small
3750,00:03:16,5,Learning to Rank - Part 2,4.4,"And therefore, this probability, this whole probability, would be large.",00:03:12,7,And therefore probability whole probability would large
3751,00:03:18,5,Learning to Rank - Part 2,4.4,And that's what we expect.,00:03:16,7,And expect
3752,00:03:22,5,Learning to Rank - Part 2,4.4,"Basically, it would be if this combination gives us a high value,",00:03:18,7,Basically would combination gives us high value
3753,00:03:25,5,Learning to Rank - Part 2,4.4,then the document's more likely relevant.,00:03:22,7,document likely relevant
3754,00:03:28,5,Learning to Rank - Part 2,4.4,So this is our hypothesis.,00:03:26,7,So hypothesis
3755,00:03:32,5,Learning to Rank - Part 2,4.4,"Again, this is not necessarily the best hypothesis.",00:03:28,7,Again necessarily best hypothesis
3756,00:03:35,5,Learning to Rank - Part 2,4.4,That this is a simple way to connect,00:03:32,7,That simple way connect
3757,00:03:39,5,Learning to Rank - Part 2,4.4,these features with the probability of relevance.,00:03:35,7,features probability relevance
3758,00:03:44,5,Learning to Rank - Part 2,4.4,So now we have this this combination function.,00:03:40,7,So combination function
3759,00:03:49,5,Learning to Rank - Part 2,4.4,The next task is to see how we need to estimate the parameters so,00:03:44,7,The next task see need estimate parameters
3760,00:03:52,5,Learning to Rank - Part 2,4.4,that the function can truly be applied.,00:03:49,7,function truly applied
3761,00:03:53,5,Learning to Rank - Part 2,4.4,Right. Without them knowing,00:03:52,7,Right Without knowing
3762,00:03:58,5,Learning to Rank - Part 2,4.4,"that they have values, it's, it's harder to apply this function, okay.",00:03:53,7,values harder apply function okay
3763,00:04:01,5,Learning to Rank - Part 2,4.4,"So let's how we can estimate, beta values.",00:03:58,7,So let estimate beta values
3764,00:04:07,5,Learning to Rank - Part 2,4.4,"All right. Let's take a look, at a simple example.",00:04:04,7,All right Let take look simple example
3765,00:04:11,5,Learning to Rank - Part 2,4.4,"In this example, we have three features.",00:04:08,7,In example three features
3766,00:04:15,5,Learning to Rank - Part 2,4.4,One is BM25 score of the document under the query.,00:04:11,7,One BM25 score document query
3767,00:04:19,5,Learning to Rank - Part 2,4.4,"One is the page rank score of the document, which might or",00:04:15,7,One page rank score document might
3768,00:04:20,5,Learning to Rank - Part 2,4.4,might not depend on the query.,00:04:19,7,might depend query
3769,00:04:24,5,Learning to Rank - Part 2,4.4,"Hm, we might have a top sensitive page rank.",00:04:20,7,Hm might top sensitive page rank
3770,00:04:25,5,Learning to Rank - Part 2,4.4,That would depend on the query.,00:04:24,7,That would depend query
3771,00:04:30,5,Learning to Rank - Part 2,4.4,"Otherwise, the general page rank doesn't really depend on the query.",00:04:25,7,Otherwise general page rank really depend query
3772,00:04:34,5,Learning to Rank - Part 2,4.4,And then we have BM25 score on the Anchor task of the document.,00:04:30,7,And BM25 score Anchor task document
3773,00:04:40,5,Learning to Rank - Part 2,4.4,"These are then the feature values for a particular doc, document query pair.",00:04:35,7,These feature values particular doc document query pair
3774,00:04:44,5,Learning to Rank - Part 2,4.4,And in this case the document is D1.,00:04:41,7,And case document D1
3775,00:04:47,5,Learning to Rank - Part 2,4.4,"And the, the judgment says that it's relevant.",00:04:44,7,And judgment says relevant
3776,00:04:52,5,Learning to Rank - Part 2,4.4,"Here's another training instance, and these features values.",00:04:47,7,Here another training instance features values
3777,00:04:58,5,Learning to Rank - Part 2,4.4,"But in this case it's non-relevant, okay?",00:04:54,7,But case non relevant okay
3778,00:05:02,5,Learning to Rank - Part 2,4.4,"This is a overly simplified case, where we just have two instances.",00:04:58,7,This overly simplified case two instances
3779,00:05:06,5,Learning to Rank - Part 2,4.4,"But it, it's sufficient to illustrate the point.",00:05:03,7,But sufficient illustrate point
3780,00:05:11,5,Learning to Rank - Part 2,4.4,So what we can do is we use the maximum likelihood estimator to actually estimate,00:05:06,7,So use maximum likelihood estimator actually estimate
3781,00:05:13,5,Learning to Rank - Part 2,4.4,the parameters.,00:05:11,7,parameters
3782,00:05:18,5,Learning to Rank - Part 2,4.4,"Basically, we're going to do, predict the relevance status of the document,",00:05:13,7,Basically going predict relevance status document
3783,00:05:22,5,Learning to Rank - Part 2,4.4,"the, based on the feature values.",00:05:18,7,based feature values
3784,00:05:25,5,Learning to Rank - Part 2,4.4,That is given that we observe these feature values here.,00:05:22,7,That given observe feature values
3785,00:05:32,5,Learning to Rank - Part 2,4.4,Can we predict the relevance?,00:05:28,7,Can predict relevance
3786,00:05:32,5,Learning to Rank - Part 2,4.4,Yeah.,00:05:32,7,Yeah
3787,00:05:39,5,Learning to Rank - Part 2,4.4,"And of course, the prediction will be using this function that you see here.",00:05:32,7,And course prediction using function see
3788,00:05:42,5,Learning to Rank - Part 2,4.4,And we hypothesize this that the probability of relevance is related,00:05:39,7,And hypothesize probability relevance related
3789,00:05:43,5,Learning to Rank - Part 2,4.4,features in this way.,00:05:42,7,features way
3790,00:05:46,5,Learning to Rank - Part 2,4.4,So we're going to see for,00:05:43,7,So going see
3791,00:05:51,5,Learning to Rank - Part 2,4.4,what values of beta we can predict that the relevance well.,00:05:46,7,values beta predict relevance well
3792,00:05:52,5,Learning to Rank - Part 2,4.4,What do we mean?,00:05:51,7,What mean
3793,00:05:58,5,Learning to Rank - Part 2,4.4,"Well, what, what do we mean by predicting the relevance well?",00:05:52,7,Well mean predicting relevance well
3794,00:05:59,5,Learning to Rank - Part 2,4.4,Well we just mean.,00:05:58,7,Well mean
3795,00:06:03,5,Learning to Rank - Part 2,4.4,"In the first case for D1, this expression here,",00:05:59,7,In first case D1 expression
3796,00:06:06,5,Learning to Rank - Part 2,4.4,"right here, should give higher values.",00:06:03,7,right give higher values
3797,00:06:10,5,Learning to Rank - Part 2,4.4,"In fact, they would hope this to give a value close to one.",00:06:06,7,In fact would hope give value close one
3798,00:06:14,5,Learning to Rank - Part 2,4.4,Why? Because this is a relevant document.,00:06:10,7,Why Because relevant document
3799,00:06:21,5,Learning to Rank - Part 2,4.4,"On the other hand, in the second case for D2 we hope this value would be small.",00:06:14,7,On hand second case D2 hope value would small
3800,00:06:22,5,Learning to Rank - Part 2,4.4,Right.,00:06:21,7,Right
3801,00:06:25,5,Learning to Rank - Part 2,4.4,Why? It's because it's a non-relevant document.,00:06:22,7,Why It non relevant document
3802,00:06:30,5,Learning to Rank - Part 2,4.4,So now let's see how this can be mathematical expressed.,00:06:26,7,So let see mathematical expressed
3803,00:06:35,5,Learning to Rank - Part 2,4.4,"And this is similar to, expressing the probability of a document.",00:06:30,7,And similar expressing probability document
3804,00:06:38,5,Learning to Rank - Part 2,4.4,Only that we are not talking about the probability of words but,00:06:35,7,Only talking probability words
3805,00:06:41,5,Learning to Rank - Part 2,4.4,"talking about the probability of relevance, 1 or 0.",00:06:38,7,talking probability relevance 1 0
3806,00:06:46,5,Learning to Rank - Part 2,4.4,So what's the probability of this document?,00:06:41,7,So probability document
3807,00:06:52,5,Learning to Rank - Part 2,4.4,The relevant if it has these feature values.,00:06:48,7,The relevant feature values
3808,00:06:55,5,Learning to Rank - Part 2,4.4,Well this is.,00:06:54,7,Well
3809,00:06:57,5,Learning to Rank - Part 2,4.4,"Just this expression, right?",00:06:55,7,Just expression right
3810,00:07:00,5,Learning to Rank - Part 2,4.4,"We just need to pluck in the X, the Xis.",00:06:57,7,We need pluck X Xis
3811,00:07:02,5,Learning to Rank - Part 2,4.4,So that's what we'll get.,00:07:00,7,So get
3812,00:07:07,5,Learning to Rank - Part 2,4.4,"It's exactly like, what we have seen that,",00:07:02,7,It exactly like seen
3813,00:07:11,5,Learning to Rank - Part 2,4.4,only that we replace these Xis.,00:07:07,7,replace Xis
3814,00:07:14,5,Learning to Rank - Part 2,4.4,With now specific values.,00:07:11,7,With specific values
3815,00:07:20,5,Learning to Rank - Part 2,4.4,"And so, for example, this 0.7 goes to here and this 0.11 goes to here.",00:07:14,7,And example 0 7 goes 0 11 goes
3816,00:07:28,5,Learning to Rank - Part 2,4.4,And these are different feature values and we'll combine them in this particular way.,00:07:20,7,And different feature values combine particular way
3817,00:07:31,5,Learning to Rank - Part 2,4.4,The beta values are still unknown.,00:07:28,7,The beta values still unknown
3818,00:07:34,5,Learning to Rank - Part 2,4.4,But this gives us the probability,00:07:31,7,But gives us probability
3819,00:07:39,5,Learning to Rank - Part 2,4.4,that this document is relevant if we assume such a model.,00:07:34,7,document relevant assume model
3820,00:07:42,5,Learning to Rank - Part 2,4.4,"Okay, and we want to maximize this probability since",00:07:39,7,Okay want maximize probability since
3821,00:07:43,5,Learning to Rank - Part 2,4.4,this is a random document.,00:07:42,7,random document
3822,00:07:48,5,Learning to Rank - Part 2,4.4,What we do for the second document.,00:07:44,7,What second document
3823,00:07:52,5,Learning to Rank - Part 2,4.4,"Well, we want to compute to the probability that the predictions is, is n,",00:07:48,7,Well want compute probability predictions n
3824,00:07:53,5,Learning to Rank - Part 2,4.4,non-relevant.,00:07:52,7,non relevant
3825,00:08:02,5,Learning to Rank - Part 2,4.4,"So, this would mean, we have to compute a 1 minus, right this expression.",00:07:53,7,So would mean compute 1 minus right expression
3826,00:08:04,5,Learning to Rank - Part 2,4.4,Since this expression.,00:08:02,7,Since expression
3827,00:08:10,5,Learning to Rank - Part 2,4.4,"Is actually the probability of relevance, so to compute the non relevance",00:08:05,7,Is actually probability relevance compute non relevance
3828,00:08:18,5,Learning to Rank - Part 2,4.4,"from relevance, we just do 1 minus the probability of relevance, okay?",00:08:10,7,relevance 1 minus probability relevance okay
3829,00:08:20,5,Learning to Rank - Part 2,4.4,So this whole expression then.,00:08:18,7,So whole expression
3830,00:08:29,5,Learning to Rank - Part 2,4.4,Just is our probability of predicting these two relevance values.,00:08:20,7,Just probability predicting two relevance values
3831,00:08:30,5,Learning to Rank - Part 2,4.4,One is 1.,00:08:29,7,One 1
3832,00:08:31,5,Learning to Rank - Part 2,4.4,"Here, one is a 0.",00:08:30,7,Here one 0
3833,00:08:37,5,Learning to Rank - Part 2,4.4,And this whole equation is our probability.,00:08:31,7,And whole equation probability
3834,00:08:42,5,Learning to Rank - Part 2,4.4,Of observing a 1 here and observing a 0 here.,00:08:37,7,Of observing 1 observing 0
3835,00:08:50,5,Learning to Rank - Part 2,4.4,"Of course this probability depends on the beta values, right?",00:08:44,7,Of course probability depends beta values right
3836,00:08:52,5,Learning to Rank - Part 2,4.4,So then our goal is to,00:08:50,7,So goal
3837,00:08:57,5,Learning to Rank - Part 2,4.4,adjust the beta values to make this whole thing reach its maximum.,00:08:52,7,adjust beta values make whole thing reach maximum
3838,00:08:59,5,Learning to Rank - Part 2,4.4,Make that as large as possible.,00:08:57,7,Make large possible
3839,00:09:02,5,Learning to Rank - Part 2,4.4,So that means we are going to compute this.,00:09:00,7,So means going compute
3840,00:09:08,5,Learning to Rank - Part 2,4.4,"The beta is just the, the parameter values that would maximize this for",00:09:02,7,The beta parameter values would maximize
3841,00:09:10,5,Learning to Rank - Part 2,4.4,like holder expression.,00:09:08,7,like holder expression
3842,00:09:16,5,Learning to Rank - Part 2,4.4,And what it means is if look at the function is,00:09:12,7,And means look function
3843,00:09:20,5,Learning to Rank - Part 2,4.4,we're going to choose betas to make this as large as possible.,00:09:16,7,going choose betas make large possible
3844,00:09:25,5,Learning to Rank - Part 2,4.4,And make this also as large as possible,00:09:20,7,And make also large possible
3845,00:09:29,5,Learning to Rank - Part 2,4.4,which is equivalent to say make this the part as small as possible.,00:09:25,7,equivalent say make part small possible
3846,00:09:32,5,Learning to Rank - Part 2,4.4,And this is precisely what we want.,00:09:30,7,And precisely want
3847,00:09:39,5,Learning to Rank - Part 2,4.4,"So once we do the training, now we will know the beta values.",00:09:34,7,So training know beta values
3848,00:09:45,5,Learning to Rank - Part 2,4.4,So then this function will be well defined once their values are known.,00:09:39,7,So function well defined values known
3849,00:09:50,5,Learning to Rank - Part 2,4.4,Both this and this will become pretty less specified.,00:09:45,7,Both become pretty less specified
3850,00:09:55,5,Learning to Rank - Part 2,4.4,So for any new query and new document we can simply compute the features [NOISE],00:09:50,7,So new query new document simply compute features NOISE
3851,00:10:00,5,Learning to Rank - Part 2,4.4,For that pair and then we just use this formula to generate a ranking score.,00:09:55,7,For pair use formula generate ranking score
3852,00:10:06,5,Learning to Rank - Part 2,4.4,And this scoring function can be used in for rank documents for a particular query.,00:10:00,7,And scoring function used rank documents particular query
3853,00:10:10,5,Learning to Rank - Part 2,4.4,"So that's the basic idea of, learning to rank.",00:10:06,7,So basic idea learning rank
3854,00:00:04,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,[SOUND] This lecture is about how to,00:00:00,8,SOUND This lecture
3855,00:00:10,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,evaluate the text retrieval system when,00:00:04,8,evaluate text retrieval system
3856,00:00:15,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,we have multiple levels of judgments.,00:00:10,8,multiple levels judgments
3857,00:00:19,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,In this lecture we will continue the discussion of evaluation.,00:00:15,8,In lecture continue discussion evaluation
3858,00:00:23,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,We're going to look at how to evaluate the text retrieval system.,00:00:19,8,We going look evaluate text retrieval system
3859,00:00:26,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And we have multiple level of judgements.,00:00:23,8,And multiple level judgements
3860,00:00:31,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So, so far we have talked about binding judgements,",00:00:27,8,So far talked binding judgements
3861,00:00:34,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,that means a documents is judged as being relevant or not-relevant.,00:00:31,8,means documents judged relevant relevant
3862,00:00:40,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"But earlier we will also talk about, relevance as a matter of degree.",00:00:35,8,But earlier also talk relevance matter degree
3863,00:00:45,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So we often can distinguish it very higher relevant options,",00:00:40,8,So often distinguish higher relevant options
3864,00:00:50,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"those are very useful options, from you know, lower rated relevant options.",00:00:45,8,useful options know lower rated relevant options
3865,00:00:53,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"They are okay, they are useful perhaps.",00:00:50,8,They okay useful perhaps
3866,00:00:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And further from non-relevant documents.,00:00:53,8,And non relevant documents
3867,00:00:56,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Those are not useful.,00:00:55,8,Those useful
3868,00:00:57,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Right?,00:00:56,8,Right
3869,00:01:01,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So imagine you can have ratings for these pages.,00:00:57,8,So imagine ratings pages
3870,00:01:05,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Then you would have much more levels of ratings.,00:01:01,8,Then would much levels ratings
3871,00:01:10,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"For example, here I show an example of three levels, three were relevant.",00:01:05,8,For example I show example three levels three relevant
3872,00:01:12,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Sorry, three were very relevant.",00:01:10,8,Sorry three relevant
3873,00:01:15,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Two for marginally relevant and one for non-relevant.,00:01:12,8,Two marginally relevant one non relevant
3874,00:01:20,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Now how do we evaluate such a new system using these judgements of use of the map,00:01:15,8,Now evaluate new system using judgements use map
3875,00:01:23,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"doesn't work, average of precision doesn't work, precision and",00:01:20,8,work average precision work precision
3876,00:01:26,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,record doesn't work because they rely on vinyl judgement.,00:01:23,8,record work rely vinyl judgement
3877,00:01:33,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So let's look at the sum top regular results when using these judgments.,00:01:28,8,So let look sum top regular results using judgments
3878,00:01:36,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Right? Imagine the user would be mostly,00:01:33,8,Right Imagine user would mostly
3879,00:01:38,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,care about the top ten results here.,00:01:36,8,care top ten results
3880,00:01:40,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Right.,00:01:39,8,Right
3881,00:01:46,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And we mark the the rating levels or,00:01:42,8,And mark rating levels
3882,00:01:51,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,relevance levels for these documents as shown here.,00:01:46,8,relevance levels documents shown
3883,00:01:54,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Three, two, one, one, three, et cetera.",00:01:51,8,Three two one one three et cetera
3884,00:01:56,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And we call these gain.,00:01:54,8,And call gain
3885,00:02:03,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And the reason why we call it a gain, is because the measure that",00:01:57,8,And reason call gain measure
3886,00:02:08,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"we are infusing is called, NTCG, normalizer discount of accumulative gain.",00:02:03,8,infusing called NTCG normalizer discount accumulative gain
3887,00:02:14,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So this gain basically can mesh your, how much gain of random",00:02:10,8,So gain basically mesh much gain random
3888,00:02:19,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"information a user can obtain by looking at each document, alright.",00:02:14,8,information user obtain looking document alright
3889,00:02:24,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So looking after the first document the user can gain three points.,00:02:19,8,So looking first document user gain three points
3890,00:02:28,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Looking at the non-relevant document the user would only gain one point.,00:02:24,8,Looking non relevant document user would gain one point
3891,00:02:31,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Right. Looking at the multi-level relevant or,00:02:28,8,Right Looking multi level relevant
3892,00:02:35,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,marginally relevant document the user would get two points et cetera.,00:02:31,8,marginally relevant document user would get two points et cetera
3893,00:02:41,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So this gain usually matches the utility of a document from a user's perspective.,00:02:35,8,So gain usually matches utility document user perspective
3894,00:02:46,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Of course if we assume the user stops at the ten documents, and",00:02:41,8,Of course assume user stops ten documents
3895,00:02:51,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,we're looking at the cutoff at ten we can look after the total gain of the user.,00:02:46,8,looking cutoff ten look total gain user
3896,00:02:53,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And what's that, well that's simply the sum of these and",00:02:51,8,And well simply sum
3897,00:02:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,we call it the cumulative gain.,00:02:53,8,call cumulative gain
3898,00:02:59,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So if we use a stops at the positua that's just a three.,00:02:55,8,So use stops positua three
3899,00:03:03,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,If the user looks at another document that's a 3 plus 2.,00:02:59,8,If user looks another document 3 plus 2
3900,00:03:05,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,If the user looks at the more documents.,00:03:03,8,If user looks documents
3901,00:03:08,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Then the cumulative gain is more.,00:03:05,8,Then cumulative gain
3902,00:03:13,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Of course, this is at the cost of spending more time to examine the list.",00:03:08,8,Of course cost spending time examine list
3903,00:03:16,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So cumulative gain gives us some idea about,00:03:13,8,So cumulative gain gives us idea
3904,00:03:21,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,how much total gain the user would have if the user examines all these documents.,00:03:16,8,much total gain user would user examines documents
3905,00:03:28,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Now, in NDCG, we also have another letter here, D, discounted cumulative gain.",00:03:21,8,Now NDCG also another letter D discounted cumulative gain
3906,00:03:32,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So why do we want to do discounting?,00:03:29,8,So want discounting
3907,00:03:36,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Well, if you look at this cumulative gain, there is one deficiency which is",00:03:32,8,Well look cumulative gain one deficiency
3908,00:03:41,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,it did not consider the rank position of these these documents.,00:03:36,8,consider rank position documents
3909,00:03:46,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So, for example looking at the, this sum here",00:03:41,8,So example looking sum
3910,00:03:51,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"and we only know there is only one highly relevant document,",00:03:48,8,know one highly relevant document
3911,00:03:54,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"one marginally relevant document, two non-relevant documents.",00:03:51,8,one marginally relevant document two non relevant documents
3912,00:03:57,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,We don't really care where they are ranked.,00:03:54,8,We really care ranked
3913,00:04:00,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Ideally, we want these two to be ranked on the top.",00:03:57,8,Ideally want two ranked top
3914,00:04:03,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Which is the case here.,00:04:00,8,Which case
3915,00:04:06,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,But how can we capture that intuition?,00:04:03,8,But capture intuition
3916,00:04:12,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Well we have to say, well this 3 here is not as good as this 3 on the top.",00:04:06,8,Well say well 3 good 3 top
3917,00:04:16,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And that means the contribution of,",00:04:12,8,And means contribution
3918,00:04:22,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"the game from different positions, has to be weight by their position.",00:04:16,8,game different positions weight position
3919,00:04:25,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And this is the idea of discounting, basically.",00:04:22,8,And idea discounting basically
3920,00:04:29,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So, we're going to say, well, the first one, doesn't it need to be discounted",00:04:25,8,So going say well first one need discounted
3921,00:04:34,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"because the user can be assume that you always see this document, but",00:04:29,8,user assume always see document
3922,00:04:38,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"the second one, this one will be discounted a little bit,",00:04:34,8,second one one discounted little bit
3923,00:04:42,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,because there's a small possibility that the user wouldn't notice it.,00:04:38,8,small possibility user notice
3924,00:04:48,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So, we divide this gain by the weight, based on the position.",00:04:42,8,So divide gain weight based position
3925,00:04:52,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So, log of two, two is the rank position of this document and,",00:04:48,8,So log two two rank position document
3926,00:04:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"when we go to the third position, we,",00:04:52,8,go third position
3927,00:05:01,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"discount even more because the numbers is log of three, and so on and so forth.",00:04:55,8,discount even numbers log three forth
3928,00:05:06,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So when we take a such a sum then a lowly ranked document would not contribute,00:05:01,8,So take sum lowly ranked document would contribute
3929,00:05:10,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,contribute that much as a highly ranked document.,00:05:06,8,contribute much highly ranked document
3930,00:05:15,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So that means if you, for example, switch the position of this and let's say",00:05:10,8,So means example switch position let say
3931,00:05:20,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"this position and this one, and then you would get more discount if you put",00:05:15,8,position one would get discount put
3932,00:05:27,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"for example, very relevant document here as opposed to two here.",00:05:22,8,example relevant document opposed two
3933,00:05:31,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Imagine if you put the three here, then it would have to be discounted.",00:05:27,8,Imagine put three would discounted
3934,00:05:34,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So it's not as good as if you would put the three here.,00:05:31,8,So good would put three
3935,00:05:36,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So this is the idea of discounting.,00:05:34,8,So idea discounting
3936,00:05:43,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Okay, so n, now at this point that we have got this discounted cumulative gain for",00:05:37,8,Okay n point got discounted cumulative gain
3937,00:05:50,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,measuring the utility of this ranked list with multiple levels of judgments.,00:05:43,8,measuring utility ranked list multiple levels judgments
3938,00:05:53,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So are we happy with this?,00:05:51,8,So happy
3939,00:05:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Well we can use this rank systems.,00:05:53,8,Well use rank systems
3940,00:05:59,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Now we still need to do a little bit more in order to,00:05:55,8,Now still need little bit order
3941,00:06:03,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,make this measure comfortable across different topics.,00:05:59,8,make measure comfortable across different topics
3942,00:06:04,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And this is the last step.,00:06:03,8,And last step
3943,00:06:10,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And by the way, here we just show that DCG at the ten.",00:06:06,8,And way show DCG ten
3944,00:06:11,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Alright.,00:06:10,8,Alright
3945,00:06:16,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So this is the total sum of DCG over all these ten documents.,00:06:11,8,So total sum DCG ten documents
3946,00:06:20,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So the last step is called N, normalization.",00:06:16,8,So last step called N normalization
3947,00:06:25,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And if we do that then we get normalized DCG.,00:06:20,8,And get normalized DCG
3948,00:06:26,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So how do we do that?,00:06:25,8,So
3949,00:06:30,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Well, the idea here is within the Normalized DCG",00:06:26,8,Well idea within Normalized DCG
3950,00:06:34,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,by the Ideal DCG at the same cutoff.,00:06:30,8,Ideal DCG cutoff
3951,00:06:37,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,What is the Ideal DCG?,00:06:35,8,What Ideal DCG
3952,00:06:40,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Well this is a DCG of ideal ranking.,00:06:37,8,Well DCG ideal ranking
3953,00:06:46,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So imagine if we have nine documents in the whole collection,00:06:40,8,So imagine nine documents whole collection
3954,00:06:53,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,rated a three here and that means in total we have nine documents rated three.,00:06:46,8,rated three means total nine documents rated three
3955,00:06:56,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Then, our ideal ranked the Lister",00:06:53,8,Then ideal ranked Lister
3956,00:07:00,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,would have put all these nine documents on the very top.,00:06:56,8,would put nine documents top
3957,00:07:05,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So all these would have to be three and then this would be followed by a two here,",00:07:00,8,So would three would followed two
3958,00:07:10,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,because that's the best we could do after we have run out of threes.,00:07:05,8,best could run threes
3959,00:07:11,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,But all these positions would be threes.,00:07:10,8,But positions would threes
3960,00:07:12,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Right?,00:07:11,8,Right
3961,00:07:16,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So this would be our ideal ranked list.,00:07:14,8,So would ideal ranked list
3962,00:07:21,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And then we can compute the DCG for this ideal rank list.,00:07:18,8,And compute DCG ideal rank list
3963,00:07:28,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So this would be given by this formula you see here, and so",00:07:23,8,So would given formula see
3964,00:07:32,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,this idea DCG would be used as the normalizer DCG.,00:07:28,8,idea DCG would used normalizer DCG
3965,00:07:40,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Like so here, and this IdealDCG would be used as a normalizer.",00:07:33,8,Like IdealDCG would used normalizer
3966,00:07:44,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So you can imagine now normalization essentially is to compare the actual DCG,00:07:40,8,So imagine normalization essentially compare actual DCG
3967,00:07:49,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,with the best decision you can possibly get for this topic.,00:07:44,8,best decision possibly get topic
3968,00:07:51,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Now why do we want to do this?,00:07:49,8,Now want
3969,00:07:57,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Well by doing this we'll map the DCG values in to a range of zero through one,",00:07:51,8,Well map DCG values range zero one
3970,00:08:01,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"so the best value, or the highest value for every query would be one.",00:07:57,8,best value highest value every query would one
3971,00:08:06,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,That's when you're relevance is in fact the idealist.,00:08:01,8,That relevance fact idealist
3972,00:08:13,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,But otherwise in general you will be lower than one.,00:08:07,8,But otherwise general lower one
3973,00:08:14,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Now what if we don't do that?,00:08:13,8,Now
3974,00:08:18,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"Well, you can see this transformation or this numberization,",00:08:14,8,Well see transformation numberization
3975,00:08:23,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,doesn't really affect the relative comparison of systems for,00:08:18,8,really affect relative comparison systems
3976,00:08:28,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"just one topic, because this ideal DCG is the same for all the systems.",00:08:23,8,one topic ideal DCG systems
3977,00:08:33,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So the ranking of systems based on only DCG would be exactly the same.,00:08:28,8,So ranking systems based DCG would exactly
3978,00:08:36,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,As if you rank them based on the normalized decision.,00:08:33,8,As rank based normalized decision
3979,00:08:41,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,The difference however is when we have multiple topics because,00:08:36,8,The difference however multiple topics
3980,00:08:45,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"if we don't do normalization, different topics will have different scales of DCG.",00:08:41,8,normalization different topics different scales DCG
3981,00:08:52,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,For a topic like this one we have nine highly relevant documents.,00:08:46,8,For topic like one nine highly relevant documents
3982,00:08:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,The DCG can get really high.,00:08:52,8,The DCG get really high
3983,00:09:00,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,But imagine that in another case there are only two very relevant documents.,00:08:55,8,But imagine another case two relevant documents
3984,00:09:02,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,In total in the whole collection.,00:09:00,8,In total whole collection
3985,00:09:06,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Then the highest DCG that any system could achieve for,00:09:02,8,Then highest DCG system could achieve
3986,00:09:09,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,such a topic would not be very high.,00:09:06,8,topic would high
3987,00:09:16,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So again we face the problem of different scales of DCG values and when we,00:09:09,8,So face problem different scales DCG values
3988,00:09:21,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,take an average we don't want the average to be dominated by those high values.,00:09:16,8,take average want average dominated high values
3989,00:09:23,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Those are again easy quires.,00:09:21,8,Those easy quires
3990,00:09:27,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So by doing the normalization we have all, avoid the problem.",00:09:23,8,So normalization avoid problem
3991,00:09:31,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,Making all the purists contribute equal to the average.,00:09:27,8,Making purists contribute equal average
3992,00:09:33,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,So this is the idea of NDCG.,00:09:31,8,So idea NDCG
3993,00:09:40,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,It's used for measuring relevance based on much more level relevance judgments.,00:09:33,8,It used measuring relevance based much level relevance judgments
3994,00:09:44,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So more in the more general way,",00:09:41,8,So general way
3995,00:09:50,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,this is basically a measure that can be applied through,00:09:44,8,basically measure applied
3996,00:09:55,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"any ranked task with much more level of, of judgments.",00:09:50,8,ranked task much level judgments
3997,00:10:01,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,And the scale of the judgments can be multiple,00:09:55,8,And scale judgments multiple
3998,00:10:07,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"can be more than binary, not only more than binary, they can be multiple levels,",00:10:03,8,binary binary multiple levels
3999,00:10:11,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"like one's or five, or even more depending on your application.",00:10:07,8,like one five even depending application
4000,00:10:15,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And the main idea of this measure just to summarize,",00:10:11,8,And main idea measure summarize
4001,00:10:20,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,is to measure the total utility of the top k documents.,00:10:15,8,measure total utility top k documents
4002,00:10:24,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"So you always choose a cutoff, and then you measure the total utility.",00:10:20,8,So always choose cutoff measure total utility
4003,00:10:29,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"And it would discount the contribution from a lowly ranked document,",00:10:24,8,And would discount contribution lowly ranked document
4004,00:10:35,3,Evaluation of TR Systems- Multi-Level Judgements,2.7,"and finally, it would do normalization to ensure comparability across queries",00:10:29,8,finally would normalization ensure comparability across queries
4005,00:00:05,3,Evaluation of TR Systems,2.4,[SOUND] This lecture is about,00:00:00,4,SOUND This lecture
4006,00:00:13,3,Evaluation of TR Systems,2.4,evaluation of text retrieval systems.,00:00:05,4,evaluation text retrieval systems
4007,00:00:20,3,Evaluation of TR Systems,2.4,"In the previous lectures, we have talked about a number of text retrieval methods.",00:00:13,4,In previous lectures talked number text retrieval methods
4008,00:00:22,3,Evaluation of TR Systems,2.4,Different kinds of ranking functions.,00:00:20,4,Different kinds ranking functions
4009,00:00:26,3,Evaluation of TR Systems,2.4,But how do we know which one works the best?,00:00:23,4,But know one works best
4010,00:00:29,3,Evaluation of TR Systems,2.4,"In order to answer this question, we have to compare them,",00:00:27,4,In order answer question compare
4011,00:00:33,3,Evaluation of TR Systems,2.4,and that means we'll have to evaluate these retrieval methods.,00:00:29,4,means evaluate retrieval methods
4012,00:00:37,3,Evaluation of TR Systems,2.4,So this is the main topic of this lecture.,00:00:34,4,So main topic lecture
4013,00:00:42,3,Evaluation of TR Systems,2.4,"First, let's think about why do we have to do evaluation?",00:00:40,4,First let think evaluation
4014,00:00:44,3,Evaluation of TR Systems,2.4,I already gave one reason.,00:00:42,4,I already gave one reason
4015,00:00:45,3,Evaluation of TR Systems,2.4,"And that is,",00:00:44,4,And
4016,00:00:50,3,Evaluation of TR Systems,2.4,we have to use evaluation to figure out which retrieval method works better.,00:00:45,4,use evaluation figure retrieval method works better
4017,00:00:54,3,Evaluation of TR Systems,2.4,Now this is very important for advancing our knowledge.,00:00:50,4,Now important advancing knowledge
4018,00:00:58,3,Evaluation of TR Systems,2.4,Otherwise we wouldn't know whether a new idea works better than old idea.,00:00:54,4,Otherwise know whether new idea works better old idea
4019,00:01:03,3,Evaluation of TR Systems,2.4,"In the beginning of this course we talked about the,",00:00:58,4,In beginning course talked
4020,00:01:07,3,Evaluation of TR Systems,2.4,the problem of text retrieval we compare it with database retrieval.,00:01:03,4,problem text retrieval compare database retrieval
4021,00:01:14,3,Evaluation of TR Systems,2.4,"There, we mentioned that text retrieval is imperative to find the problem.",00:01:08,4,There mentioned text retrieval imperative find problem
4022,00:01:17,3,Evaluation of TR Systems,2.4,"So, evaluation must rely on users,",00:01:14,4,So evaluation must rely users
4023,00:01:22,3,Evaluation of TR Systems,2.4,"which system works better, that would have to be judged by our users.",00:01:17,4,system works better would judged users
4024,00:01:28,3,Evaluation of TR Systems,2.4,So this becomes very challenging problem.,00:01:25,4,So becomes challenging problem
4025,00:01:32,3,Evaluation of TR Systems,2.4,"Because how can we get users involved in, in matters, and",00:01:28,4,Because get users involved matters
4026,00:01:34,3,Evaluation of TR Systems,2.4,how can we draw a fair comparison of different methods.,00:01:32,4,draw fair comparison different methods
4027,00:01:39,3,Evaluation of TR Systems,2.4,So just go back to the reasons for evaluation.,00:01:37,4,So go back reasons evaluation
4028,00:01:42,3,Evaluation of TR Systems,2.4,I listed two reasons here.,00:01:41,4,I listed two reasons
4029,00:01:47,3,Evaluation of TR Systems,2.4,"The second reason is basically what I just said but there is also another reason,",00:01:42,4,The second reason basically I said also another reason
4030,00:01:51,3,Evaluation of TR Systems,2.4,which is to assess the actual utility of a test regional system.,00:01:47,4,assess actual utility test regional system
4031,00:01:55,3,Evaluation of TR Systems,2.4,Now imagine you're building your own applications.,00:01:51,4,Now imagine building applications
4032,00:02:01,3,Evaluation of TR Systems,2.4,Would be interested in knowing how well your search engine works for your users.,00:01:55,4,Would interested knowing well search engine works users
4033,00:02:04,3,Evaluation of TR Systems,2.4,So in this case measures must,00:02:01,4,So case measures must
4034,00:02:07,3,Evaluation of TR Systems,2.4,reflect the utility to the actual users in the the real application.,00:02:04,4,reflect utility actual users real application
4035,00:02:11,3,Evaluation of TR Systems,2.4,"And typically, this has been done by using user studies and",00:02:07,4,And typically done using user studies
4036,00:02:13,3,Evaluation of TR Systems,2.4,using the real search engine.,00:02:11,4,using real search engine
4037,00:02:21,3,Evaluation of TR Systems,2.4,"In the second case or for the second reason, the measures",00:02:16,4,In second case second reason measures
4038,00:02:25,3,Evaluation of TR Systems,2.4,actually all need to be correlated with the utility to actual users.,00:02:21,4,actually need correlated utility actual users
4039,00:02:30,3,Evaluation of TR Systems,2.4,"Thus they don't have to accurately reflect the, the exact utility to users.",00:02:26,4,Thus accurately reflect exact utility users
4040,00:02:37,3,Evaluation of TR Systems,2.4,So the measure only needs to be good enough to tell which method works better.,00:02:31,4,So measure needs good enough tell method works better
4041,00:02:41,3,Evaluation of TR Systems,2.4,And this is usually done through test collection.,00:02:38,4,And usually done test collection
4042,00:02:48,3,Evaluation of TR Systems,2.4,And this is the main idea that we'll be talking about in this course.,00:02:41,4,And main idea talking course
4043,00:02:53,3,Evaluation of TR Systems,2.4,This has been very important for comparing different algorithms and,00:02:48,4,This important comparing different algorithms
4044,00:02:56,3,Evaluation of TR Systems,2.4,for improving search engines systems in general.,00:02:53,4,improving search engines systems general
4045,00:03:01,3,Evaluation of TR Systems,2.4,So next we will talk about what to measure.,00:02:58,4,So next talk measure
4046,00:03:07,3,Evaluation of TR Systems,2.4,"There are many aspects of a search engine we can measure, we can evaluate and",00:03:01,4,There many aspects search engine measure evaluate
4047,00:03:09,3,Evaluation of TR Systems,2.4,here I list the three major aspects.,00:03:07,4,I list three major aspects
4048,00:03:13,3,Evaluation of TR Systems,2.4,"One is effectiveness or accuracy, how accurate are the search results?",00:03:09,4,One effectiveness accuracy accurate search results
4049,00:03:18,3,Evaluation of TR Systems,2.4,In this case we're measuring a system's capability of ranking relevant documents,00:03:13,4,In case measuring system capability ranking relevant documents
4050,00:03:20,3,Evaluation of TR Systems,2.4,on top of non relevant ones.,00:03:18,4,top non relevant ones
4051,00:03:21,3,Evaluation of TR Systems,2.4,The second is efficiency.,00:03:20,4,The second efficiency
4052,00:03:24,3,Evaluation of TR Systems,2.4,How quickly can a user get the results?,00:03:21,4,How quickly user get results
4053,00:03:27,3,Evaluation of TR Systems,2.4,How much computing resources are needed to answer a query?,00:03:24,4,How much computing resources needed answer query
4054,00:03:31,3,Evaluation of TR Systems,2.4,So in this case we need to measure the space and time overhead of the system.,00:03:27,4,So case need measure space time overhead system
4055,00:03:34,3,Evaluation of TR Systems,2.4,The third aspect is usability.,00:03:32,4,The third aspect usability
4056,00:03:38,3,Evaluation of TR Systems,2.4,Basically the question is how useful is the system for real user tasks?,00:03:34,4,Basically question useful system real user tasks
4057,00:03:44,3,Evaluation of TR Systems,2.4,"Here, obviously, interfaces and many other things are also important and",00:03:38,4,Here obviously interfaces many things also important
4058,00:03:45,3,Evaluation of TR Systems,2.4,we typically would have to do user studies.,00:03:44,4,typically would user studies
4059,00:03:51,3,Evaluation of TR Systems,2.4,"Now, in this course, we're going to talk more, mostly about the effectiveness and",00:03:47,4,Now course going talk mostly effectiveness
4060,00:03:55,3,Evaluation of TR Systems,2.4,"accuracy measures because, the efficiency and",00:03:51,4,accuracy measures efficiency
4061,00:04:00,3,Evaluation of TR Systems,2.4,"usability dimensions are, not really unique to search engines, and so,",00:03:55,4,usability dimensions really unique search engines
4062,00:04:08,3,Evaluation of TR Systems,2.4,"they are, needed for evaluating any other software systems.",00:04:02,4,needed evaluating software systems
4063,00:04:12,3,Evaluation of TR Systems,2.4,And there is also good coverage of such materials in other courses.,00:04:08,4,And also good coverage materials courses
4064,00:04:18,3,Evaluation of TR Systems,2.4,"But how to evaluate a search engine is quite, you know accuracy is",00:04:13,4,But evaluate search engine quite know accuracy
4065,00:04:23,3,Evaluation of TR Systems,2.4,"something you need to text retrieval, and we're going to talk a lot about this.",00:04:18,4,something need text retrieval going talk lot
4066,00:04:28,3,Evaluation of TR Systems,2.4,"The main idea that people have proposed before using a attitude, evaluate",00:04:23,4,The main idea people proposed using attitude evaluate
4067,00:04:33,3,Evaluation of TR Systems,2.4,"a text retrieval algorithm, is called the Cranfield Evaluation Methodology.",00:04:28,4,text retrieval algorithm called Cranfield Evaluation Methodology
4068,00:04:40,3,Evaluation of TR Systems,2.4,"This one actually was developed long time ago, developed in the 1960s.",00:04:33,4,This one actually developed long time ago developed 1960s
4069,00:04:46,3,Evaluation of TR Systems,2.4,"It's a methodology for laboratory test of system components, it's actually",00:04:40,4,It methodology laboratory test system components actually
4070,00:04:50,3,Evaluation of TR Systems,2.4,"a methodology that has been very useful, not just for search engine evaluation.",00:04:46,4,methodology useful search engine evaluation
4071,00:04:55,3,Evaluation of TR Systems,2.4,But also for evaluating virtually all kinds of empirical tasks.,00:04:50,4,But also evaluating virtually kinds empirical tasks
4072,00:05:01,3,Evaluation of TR Systems,2.4,"And, for example in processing or in other fields where the problem",00:04:55,4,And example processing fields problem
4073,00:05:05,3,Evaluation of TR Systems,2.4,is empirically defined we typically would need to use to use such a methodology.,00:05:01,4,empirically defined typically would need use use methodology
4074,00:05:12,3,Evaluation of TR Systems,2.4,And today was the big data challenge with the use of machine learning every where.,00:05:05,4,And today big data challenge use machine learning every
4075,00:05:17,3,Evaluation of TR Systems,2.4,"We general, this methodology has been very popular, but it was first developed for",00:05:12,4,We general methodology popular first developed
4076,00:05:20,3,Evaluation of TR Systems,2.4,search engine application in the 1960s.,00:05:17,4,search engine application 1960s
4077,00:05:25,3,Evaluation of TR Systems,2.4,So the basic idea of this approach is it'll build a reusable test collections,00:05:20,4,So basic idea approach build reusable test collections
4078,00:05:26,3,Evaluation of TR Systems,2.4,and define measures.,00:05:25,4,define measures
4079,00:05:30,3,Evaluation of TR Systems,2.4,Once such a test collection is build it can be used again and,00:05:27,4,Once test collection build used
4080,00:05:32,3,Evaluation of TR Systems,2.4,again to test the different algorithms.,00:05:30,4,test different algorithms
4081,00:05:36,3,Evaluation of TR Systems,2.4,And we're going to define measures that would allow you to quantify,00:05:32,4,And going define measures would allow quantify
4082,00:05:39,3,Evaluation of TR Systems,2.4,"performance of a system or an, an algorithm.",00:05:36,4,performance system algorithm
4083,00:05:42,3,Evaluation of TR Systems,2.4,So how exactly would this work?,00:05:41,4,So exactly would work
4084,00:05:45,3,Evaluation of TR Systems,2.4,"Well, we're going to do, have assembled collection of documents and",00:05:42,4,Well going assembled collection documents
4085,00:05:50,3,Evaluation of TR Systems,2.4,this is just similar to real document collection in your search application.,00:05:45,4,similar real document collection search application
4086,00:05:53,3,Evaluation of TR Systems,2.4,We can also have a sample set of queries or topics.,00:05:50,4,We also sample set queries topics
4087,00:05:55,3,Evaluation of TR Systems,2.4,This is to simulate the user's queries.,00:05:53,4,This simulate user queries
4088,00:05:58,3,Evaluation of TR Systems,2.4,Then we'll have to have relevance judgments.,00:05:56,4,Then relevance judgments
4089,00:06:03,3,Evaluation of TR Systems,2.4,These are judgments of which documents should be returned for which queries.,00:05:58,4,These judgments documents returned queries
4090,00:06:08,3,Evaluation of TR Systems,2.4,"Ideally, they have to made by users who formulated the queries",00:06:03,4,Ideally made users formulated queries
4091,00:06:12,3,Evaluation of TR Systems,2.4,because those are the people that know exactly what documents would be used for.,00:06:08,4,people know exactly documents would used
4092,00:06:17,3,Evaluation of TR Systems,2.4,And then finally we have to have measures to quantify how well a system's result,00:06:12,4,And finally measures quantify well system result
4093,00:06:19,3,Evaluation of TR Systems,2.4,matches the ideal ranked list.,00:06:17,4,matches ideal ranked list
4094,00:06:25,3,Evaluation of TR Systems,2.4,That would be constructed and based on users' relevant judgements.,00:06:19,4,That would constructed based users relevant judgements
4095,00:06:30,3,Evaluation of TR Systems,2.4,So this methodology is very useful for starting retrieval,00:06:25,4,So methodology useful starting retrieval
4096,00:06:36,3,Evaluation of TR Systems,2.4,"algorithms because the test can actually, can be reused many times.",00:06:30,4,algorithms test actually reused many times
4097,00:06:41,3,Evaluation of TR Systems,2.4,And it will also provide a fair comparison for all the methods.,00:06:36,4,And also provide fair comparison methods
4098,00:06:45,3,Evaluation of TR Systems,2.4,"We have the same criteria, same data set to use and",00:06:41,4,We criteria data set use
4099,00:06:47,3,Evaluation of TR Systems,2.4,to compare different algorithms.,00:06:45,4,compare different algorithms
4100,00:06:51,3,Evaluation of TR Systems,2.4,"This allows us to compare a new algorithm with an old algorithm,",00:06:47,4,This allows us compare new algorithm old algorithm
4101,00:06:53,3,Evaluation of TR Systems,2.4,that was the method of many years ago.,00:06:51,4,method many years ago
4102,00:06:55,3,Evaluation of TR Systems,2.4,By using the same standard.,00:06:53,4,By using standard
4103,00:06:58,3,Evaluation of TR Systems,2.4,"So this is the illustration of how this works, so",00:06:55,4,So illustration works
4104,00:07:03,3,Evaluation of TR Systems,2.4,"as I said, we need a queries that are shown here.",00:06:58,4,I said need queries shown
4105,00:07:05,3,Evaluation of TR Systems,2.4,"We have Q1, Q2, et cetera.",00:07:03,4,We Q1 Q2 et cetera
4106,00:07:07,3,Evaluation of TR Systems,2.4,"We also need a documents, and that's called the document collection,",00:07:05,4,We also need documents called document collection
4107,00:07:11,3,Evaluation of TR Systems,2.4,"and on the right side, you see we need relevance judgment.",00:07:07,4,right side see need relevance judgment
4108,00:07:19,3,Evaluation of TR Systems,2.4,These are basically the binary judgments of documents with respect to a query.,00:07:11,4,These basically binary judgments documents respect query
4109,00:07:23,3,Evaluation of TR Systems,2.4,"So, for example D1 is judged as being relevant to Q1,",00:07:19,4,So example D1 judged relevant Q1
4110,00:07:26,3,Evaluation of TR Systems,2.4,D2 is judged as being relevant as well.,00:07:23,4,D2 judged relevant well
4111,00:07:29,3,Evaluation of TR Systems,2.4,"And D3 is judged as non relevant in the two, Q1, et cetera.",00:07:26,4,And D3 judged non relevant two Q1 et cetera
4112,00:07:32,3,Evaluation of TR Systems,2.4,These would be created by users.,00:07:30,4,These would created users
4113,00:07:38,3,Evaluation of TR Systems,2.4,"Once we have these, and we basically have a test, correction, and",00:07:34,4,Once basically test correction
4114,00:07:42,3,Evaluation of TR Systems,2.4,"then, if you have two systems, you want to, compare them.",00:07:38,4,two systems want compare
4115,00:07:47,3,Evaluation of TR Systems,2.4,Then you can just run each system on these queries and,00:07:42,4,Then run system queries
4116,00:07:50,3,Evaluation of TR Systems,2.4,documents and each system will then return results.,00:07:47,4,documents system return results
4117,00:07:56,3,Evaluation of TR Systems,2.4,"Let's say if the query is Q1 and then we would have the results here,",00:07:50,4,Let say query Q1 would results
4118,00:08:02,3,Evaluation of TR Systems,2.4,here I show R sub A as results from system A.,00:07:56,4,I show R sub A results system A
4119,00:08:08,3,Evaluation of TR Systems,2.4,"So, this is remember we talked about task of computing approximation of the,",00:08:02,4,So remember talked task computing approximation
4120,00:08:09,3,Evaluation of TR Systems,2.4,relevant document setter.,00:08:08,4,relevant document setter
4121,00:08:15,3,Evaluation of TR Systems,2.4,"So A is, the system A's approximation here, and",00:08:09,4,So A system A approximation
4122,00:08:20,3,Evaluation of TR Systems,2.4,also B is system B's approximation of relevant documents.,00:08:15,4,also B system B approximation relevant documents
4123,00:08:22,3,Evaluation of TR Systems,2.4,Now let's take a look at these results.,00:08:21,4,Now let take look results
4124,00:08:24,3,Evaluation of TR Systems,2.4,So which is better?,00:08:22,4,So better
4125,00:08:26,3,Evaluation of TR Systems,2.4,Now imagine for a user which one would you like?,00:08:24,4,Now imagine user one would like
4126,00:08:31,3,Evaluation of TR Systems,2.4,All right lets take a look at both results.,00:08:26,4,All right lets take look results
4127,00:08:33,3,Evaluation of TR Systems,2.4,And there are some differences and,00:08:31,4,And differences
4128,00:08:40,3,Evaluation of TR Systems,2.4,there are some documents that are return to both systems.,00:08:33,4,documents return systems
4129,00:08:42,3,Evaluation of TR Systems,2.4,"But if you look at the results you will feel that well,",00:08:40,4,But look results feel well
4130,00:08:48,3,Evaluation of TR Systems,2.4,maybe an A is better in the sense that we don't have many number in documents.,00:08:42,4,maybe A better sense many number documents
4131,00:08:52,3,Evaluation of TR Systems,2.4,"And among the three documents returned the two of them are relevant, so",00:08:48,4,And among three documents returned two relevant
4132,00:08:55,3,Evaluation of TR Systems,2.4,"that's good, it's precise.",00:08:52,4,good precise
4133,00:08:59,3,Evaluation of TR Systems,2.4,On the other hand can also say maybe B is better because,00:08:55,4,On hand also say maybe B better
4134,00:09:02,3,Evaluation of TR Systems,2.4,"we've got more relevant documents, we've got three instead of two.",00:08:59,4,got relevant documents got three instead two
4135,00:09:06,3,Evaluation of TR Systems,2.4,So which one is better and how do we quantify this?,00:09:04,4,So one better quantify
4136,00:09:12,3,Evaluation of TR Systems,2.4,"Well obviously, this question highly depends on a user's task.",00:09:08,4,Well obviously question highly depends user task
4137,00:09:14,3,Evaluation of TR Systems,2.4,"And, it depends on users as well.",00:09:12,4,And depends users well
4138,00:09:19,3,Evaluation of TR Systems,2.4,"You might be able to imagine, for some users may be system made is better.",00:09:14,4,You might able imagine users may system made better
4139,00:09:23,3,Evaluation of TR Systems,2.4,"If the user is not interested in getting all the relevant documents,",00:09:19,4,If user interested getting relevant documents
4140,00:09:27,3,Evaluation of TR Systems,2.4,"right, in this case this is the user doesn't have to read.",00:09:23,4,right case user read
4141,00:09:31,3,Evaluation of TR Systems,2.4,User would see most relevant documents.,00:09:27,4,User would see relevant documents
4142,00:09:35,3,Evaluation of TR Systems,2.4,"On the other hand on one count, imagine user might need to have",00:09:31,4,On hand one count imagine user might need
4143,00:09:39,3,Evaluation of TR Systems,2.4,"as many relevant documents as possible, for example, taking a literature survey.",00:09:35,4,many relevant documents possible example taking literature survey
4144,00:09:40,3,Evaluation of TR Systems,2.4,"You might be in the second category, and",00:09:39,4,You might second category
4145,00:09:44,3,Evaluation of TR Systems,2.4,then you might find that system B's better.,00:09:40,4,might find system B better
4146,00:09:48,3,Evaluation of TR Systems,2.4,"So in either case, we'll have to also define measures that would quantify them.",00:09:44,4,So either case also define measures would quantify
4147,00:09:53,3,Evaluation of TR Systems,2.4,And we might need to define multiple measures because,00:09:48,4,And might need define multiple measures
4148,00:09:57,3,Evaluation of TR Systems,2.4,users have different perspectives of looking at results.,00:09:53,4,users different perspectives looking results
4149,00:00:09,5,Link Analysis - Part 3,4.3,[SOUND] So we talked about a page rank as a way to,00:00:00,5,SOUND So talked page rank way
4150,00:00:14,5,Link Analysis - Part 3,4.3,to capture the Authorities.,00:00:09,5,capture Authorities
4151,00:00:21,5,Link Analysis - Part 3,4.3,"Now we also looked at the, some other examples where a hub might be interesting.",00:00:14,5,Now also looked examples hub might interesting
4152,00:00:23,5,Link Analysis - Part 3,4.3,"So, there is another algorithm called the HITS and",00:00:21,5,So another algorithm called HITS
4153,00:00:26,5,Link Analysis - Part 3,4.3,that's going to do compute the scores for us.,00:00:23,5,going compute scores us
4154,00:00:28,5,Link Analysis - Part 3,4.3,Authorities & Hubs.,00:00:26,5,Authorities Hubs
4155,00:00:32,5,Link Analysis - Part 3,4.3,"Intuitions of, pages that are widely cited, good, sorry,",00:00:28,5,Intuitions pages widely cited good sorry
4156,00:00:34,5,Link Analysis - Part 3,4.3,"there is, then, there is pages that are cited.",00:00:32,5,pages cited
4157,00:00:36,5,Link Analysis - Part 3,4.3,"Many other pages are good Hubs, right?",00:00:34,5,Many pages good Hubs right
4158,00:00:40,5,Link Analysis - Part 3,4.3,"But there, I think that the.",00:00:36,5,But I think
4159,00:00:44,5,Link Analysis - Part 3,4.3,"Most interesting idea of this algorithm HITS is, it's going to use,",00:00:40,5,Most interesting idea algorithm HITS going use
4160,00:00:49,5,Link Analysis - Part 3,4.3,a reinforcement mechanism to kind of help improve the scoring for,00:00:44,5,reinforcement mechanism kind help improve scoring
4161,00:00:51,5,Link Analysis - Part 3,4.3,Hubs and the Authorities.,00:00:49,5,Hubs Authorities
4162,00:00:54,5,Link Analysis - Part 3,4.3,"And here, so here's the idea,",00:00:51,5,And idea
4163,00:00:57,5,Link Analysis - Part 3,4.3,it will assume that good authorities are cited by good hubs.,00:00:54,5,assume good authorities cited good hubs
4164,00:01:04,5,Link Analysis - Part 3,4.3,"That means if you're cited by many pages with good hub scores,",00:00:58,5,That means cited many pages good hub scores
4165,00:01:07,5,Link Analysis - Part 3,4.3,then that increases your authority score.,00:01:04,5,increases authority score
4166,00:01:11,5,Link Analysis - Part 3,4.3,"And similarly, good hubs are those that pointed to good authorities.",00:01:07,5,And similarly good hubs pointed good authorities
4167,00:01:15,5,Link Analysis - Part 3,4.3,"So if you get you point it to a lot of good authority pages,",00:01:11,5,So get point lot good authority pages
4168,00:01:17,5,Link Analysis - Part 3,4.3,then your hub score would be increased.,00:01:15,5,hub score would increased
4169,00:01:20,5,Link Analysis - Part 3,4.3,"So you then, you would have iterative reinforce each other,",00:01:17,5,So would iterative reinforce
4170,00:01:23,5,Link Analysis - Part 3,4.3,because you can point it to some good hubs.,00:01:20,5,point good hubs
4171,00:01:25,5,Link Analysis - Part 3,4.3,"Sorry, you can point it to some good authorities.",00:01:23,5,Sorry point good authorities
4172,00:01:26,5,Link Analysis - Part 3,4.3,To get a good hub score.,00:01:25,5,To get good hub score
4173,00:01:31,5,Link Analysis - Part 3,4.3,"Whereas those authority scores, would be also improved,",00:01:26,5,Whereas authority scores would also improved
4174,00:01:33,5,Link Analysis - Part 3,4.3,because they are pointed to by a good hub.,00:01:31,5,pointed good hub
4175,00:01:38,5,Link Analysis - Part 3,4.3,"And this hub is also general, it can have many applications in graph and",00:01:33,5,And hub also general many applications graph
4176,00:01:40,5,Link Analysis - Part 3,4.3,network analysis.,00:01:38,5,network analysis
4177,00:01:43,5,Link Analysis - Part 3,4.3,"So just briefly, here's how it works.",00:01:40,5,So briefly works
4178,00:01:45,5,Link Analysis - Part 3,4.3,"We first also construct the matrix, but",00:01:43,5,We first also construct matrix
4179,00:01:48,5,Link Analysis - Part 3,4.3,this time we're going to construct the Adjacency matrix.,00:01:45,5,time going construct Adjacency matrix
4180,00:01:51,5,Link Analysis - Part 3,4.3,"We're not going to normalize the values, so if there's a link there's a y.",00:01:48,5,We going normalize values link
4181,00:01:54,5,Link Analysis - Part 3,4.3,If there's no link that's zero.,00:01:51,5,If link zero
4182,00:02:01,5,Link Analysis - Part 3,4.3,"Right again, it's the same graph and then, we're going to define the top score of",00:01:54,5,Right graph going define top score
4183,00:02:06,5,Link Analysis - Part 3,4.3,page as a sum of the authority scores of all the pages that it appoints to.,00:02:01,5,page sum authority scores pages appoints
4184,00:02:12,5,Link Analysis - Part 3,4.3,"So whether you are hub that really depends on whether you are pointing to a lot of,",00:02:08,5,So whether hub really depends whether pointing lot
4185,00:02:14,5,Link Analysis - Part 3,4.3,good authority pages.,00:02:12,5,good authority pages
4186,00:02:17,5,Link Analysis - Part 3,4.3,That's what it says in the first equation.,00:02:14,5,That says first equation
4187,00:02:22,5,Link Analysis - Part 3,4.3,"Your second equation, will define the authority score of a page",00:02:17,5,Your second equation define authority score page
4188,00:02:25,5,Link Analysis - Part 3,4.3,as a sum of the hub scores of all those pages.,00:02:22,5,sum hub scores pages
4189,00:02:29,5,Link Analysis - Part 3,4.3,"That they point to, so whether you are a good authority would depend on",00:02:25,5,That point whether good authority would depend
4190,00:02:33,5,Link Analysis - Part 3,4.3,whether those pages that are pointing to you are good Hubs.,00:02:29,5,whether pages pointing good Hubs
4191,00:02:37,5,Link Analysis - Part 3,4.3,So you can see this a forms a iterative reinforcement mechanism.,00:02:33,5,So see forms iterative reinforcement mechanism
4192,00:02:41,5,Link Analysis - Part 3,4.3,Now these two equations can be also written.,00:02:38,5,Now two equations also written
4193,00:02:44,5,Link Analysis - Part 3,4.3,"In the matrix fo-, format.",00:02:41,5,In matrix fo format
4194,00:02:49,5,Link Analysis - Part 3,4.3,"Right, so what we get here is then the hub vector is",00:02:44,5,Right get hub vector
4195,00:02:54,5,Link Analysis - Part 3,4.3,equal to the product of the Adjacency matrix.,00:02:49,5,equal product Adjacency matrix
4196,00:02:57,5,Link Analysis - Part 3,4.3,And the authority vector.,00:02:54,5,And authority vector
4197,00:02:59,5,Link Analysis - Part 3,4.3,And this is basically the first equation.,00:02:57,5,And basically first equation
4198,00:03:00,5,Link Analysis - Part 3,4.3,Right.,00:02:59,5,Right
4199,00:03:05,5,Link Analysis - Part 3,4.3,"And similarly, the second equation can be returned as the authority vector",00:03:00,5,And similarly second equation returned authority vector
4200,00:03:11,5,Link Analysis - Part 3,4.3,is equal to the product of A transpose multiplied by the hub vector.,00:03:05,5,equal product A transpose multiplied hub vector
4201,00:03:15,5,Link Analysis - Part 3,4.3,And these are just different ways of expressing these equations.,00:03:11,5,And different ways expressing equations
4202,00:03:19,5,Link Analysis - Part 3,4.3,But what's interesting is that if you look at to the matrix form.,00:03:15,5,But interesting look matrix form
4203,00:03:26,5,Link Analysis - Part 3,4.3,You can also plug-in the authority equation into the first one.,00:03:19,5,You also plug authority equation first one
4204,00:03:30,5,Link Analysis - Part 3,4.3,"So if you do that, you can actually make it limited to the authority vector",00:03:26,5,So actually make limited authority vector
4205,00:03:34,5,Link Analysis - Part 3,4.3,"completely, and you get the equation of only hub scores.",00:03:30,5,completely get equation hub scores
4206,00:03:38,5,Link Analysis - Part 3,4.3,"Right, the hub score vector is equal to A multiplied by A transpose.",00:03:34,5,Right hub score vector equal A multiplied A transpose
4207,00:03:42,5,Link Analysis - Part 3,4.3,Multiplied by the hub score vector again.,00:03:38,5,Multiplied hub score vector
4208,00:03:47,5,Link Analysis - Part 3,4.3,And similarly we can do a transformation to have equation for,00:03:43,5,And similarly transformation equation
4209,00:03:49,5,Link Analysis - Part 3,4.3,just the authorities scores.,00:03:47,5,authorities scores
4210,00:03:54,5,Link Analysis - Part 3,4.3,"So although we framed the problem as computing Hubs & Authorities,",00:03:49,5,So although framed problem computing Hubs Authorities
4211,00:03:58,5,Link Analysis - Part 3,4.3,we can actually eliminate the one of them to obtain equation just for one of them.,00:03:54,5,actually eliminate one obtain equation one
4212,00:04:03,5,Link Analysis - Part 3,4.3,"Now the difference between this and page is that, now the matrix",00:03:59,5,Now difference page matrix
4213,00:04:07,5,Link Analysis - Part 3,4.3,"is actually a multiplication of the mer-, Adjacency matrix and its transpose.",00:04:03,5,actually multiplication mer Adjacency matrix transpose
4214,00:04:10,5,Link Analysis - Part 3,4.3,So this is different from page rank.,00:04:07,5,So different page rank
4215,00:04:11,5,Link Analysis - Part 3,4.3,Right?,00:04:10,5,Right
4216,00:04:15,5,Link Analysis - Part 3,4.3,But mathematically then we would be computing the same problem.,00:04:11,5,But mathematically would computing problem
4217,00:04:17,5,Link Analysis - Part 3,4.3,"So in ha, in hits,",00:04:15,5,So ha hits
4218,00:04:22,5,Link Analysis - Part 3,4.3,we're keeping would initialize the values that state one for all these values.,00:04:17,5,keeping would initialize values state one values
4219,00:04:26,5,Link Analysis - Part 3,4.3,"And then with the algorithm will apply these, these equations essentially and",00:04:22,5,And algorithm apply equations essentially
4220,00:04:29,5,Link Analysis - Part 3,4.3,this is equivalent if you multiply that.,00:04:26,5,equivalent multiply
4221,00:04:31,5,Link Analysis - Part 3,4.3,"By, by the matrix.",00:04:29,5,By matrix
4222,00:04:33,5,Link Analysis - Part 3,4.3,A and A transpose.,00:04:31,5,A A transpose
4223,00:04:34,5,Link Analysis - Part 3,4.3,Right.,00:04:33,5,Right
4224,00:04:37,5,Link Analysis - Part 3,4.3,And so the arrows of these are exactly the same in the debate rank.,00:04:34,5,And arrows exactly debate rank
4225,00:04:42,5,Link Analysis - Part 3,4.3,"But here, because the Adjacency matrix is not normalized, so what we have to do",00:04:37,5,But Adjacency matrix normalized
4226,00:04:47,5,Link Analysis - Part 3,4.3,"is to, what we have to do is after each iteration we have to do normalize.",00:04:42,5,iteration normalize
4227,00:04:50,5,Link Analysis - Part 3,4.3,And this would allow us to control the grooves of value.,00:04:47,5,And would allow us control grooves value
4228,00:04:53,5,Link Analysis - Part 3,4.3,"Otherwise they would, grew larger and larger.",00:04:50,5,Otherwise would grew larger larger
4229,00:04:58,5,Link Analysis - Part 3,4.3,"And if we do that, and then we will basically get a, HITS.",00:04:53,5,And basically get HITS
4230,00:05:02,5,Link Analysis - Part 3,4.3,"I was in the computer, the hub scores and also the scores for all of the pages.",00:04:58,5,I computer hub scores also scores pages
4231,00:05:08,5,Link Analysis - Part 3,4.3,"And these scores can then be used, in ranging to start the PageRank scores.",00:05:03,5,And scores used ranging start PageRank scores
4232,00:05:14,5,Link Analysis - Part 3,4.3,"So to summarize, in this lecture we have seen that link information is very useful.",00:05:09,5,So summarize lecture seen link information useful
4233,00:05:18,5,Link Analysis - Part 3,4.3,"In particular, the Anchor text base is very useful.",00:05:14,5,In particular Anchor text base useful
4234,00:05:23,5,Link Analysis - Part 3,4.3,To increase the the text representation of a page.,00:05:18,5,To increase text representation page
4235,00:05:25,5,Link Analysis - Part 3,4.3,And we also talk about the PageRank and,00:05:23,5,And also talk PageRank
4236,00:05:29,5,Link Analysis - Part 3,4.3,HITS algorithm as two major link analysis algorithms.,00:05:25,5,HITS algorithm two major link analysis algorithms
4237,00:05:32,5,Link Analysis - Part 3,4.3,Both can generate scores for.,00:05:29,5,Both generate scores
4238,00:05:35,5,Link Analysis - Part 3,4.3,"What pages that can be used for the, the ranking function.",00:05:32,5,What pages used ranking function
4239,00:05:41,5,Link Analysis - Part 3,4.3,"Those that PageRank and the HITS also very general algorithms, so",00:05:35,5,Those PageRank HITS also general algorithms
4240,00:05:46,5,Link Analysis - Part 3,4.3,they have many applications in analyzing other graphs or networks.,00:05:41,5,many applications analyzing graphs networks
4241,00:00:07,5,Web Indexing,4.2,[SOUND].,00:00:00,2,SOUND
4242,00:00:11,5,Web Indexing,4.2,This lecture is about web indexing.,00:00:07,2,This lecture web indexing
4243,00:00:18,5,Web Indexing,4.2,"In this lecture, we will continue talking about web search, and",00:00:11,2,In lecture continue talking web search
4244,00:00:24,5,Web Indexing,4.2,we're going to talk about how to create a web scale index.,00:00:18,2,going talk create web scale index
4245,00:00:29,5,Web Indexing,4.2,So once we crawl the web we've got a lot of web pages.,00:00:24,2,So crawl web got lot web pages
4246,00:00:33,5,Web Indexing,4.2,The next step is we use the indexer to create the inverted index.,00:00:29,2,The next step use indexer create inverted index
4247,00:00:41,5,Web Indexing,4.2,"In general, we can use the standard information retrieval techniques for",00:00:36,2,In general use standard information retrieval techniques
4248,00:00:45,5,Web Indexing,4.2,"creating the index, and that is what we talked about in the previous lecture.",00:00:41,2,creating index talked previous lecture
4249,00:00:50,5,Web Indexing,4.2,"But there are new challenges that we have to solve for web scale indexing,",00:00:45,2,But new challenges solve web scale indexing
4250,00:00:54,5,Web Indexing,4.2,and the two main challenges of scalability and efficiency.,00:00:50,2,two main challenges scalability efficiency
4251,00:01:00,5,Web Indexing,4.2,The index will be so large that it cannot actually fit into any single machine or,00:00:54,2,The index large cannot actually fit single machine
4252,00:01:05,5,Web Indexing,4.2,"single disk, so we have to store the data on multiple machines.",00:01:00,2,single disk store data multiple machines
4253,00:01:11,5,Web Indexing,4.2,"Also, because the data is so large, it's beneficial to process the data in",00:01:06,2,Also data large beneficial process data
4254,00:01:15,5,Web Indexing,4.2,parallel so that we can produce the index quickly.,00:01:11,2,parallel produce index quickly
4255,00:01:20,5,Web Indexing,4.2,"To address these challenges, Google has made a number of innovations.",00:01:15,2,To address challenges Google made number innovations
4256,00:01:25,5,Web Indexing,4.2,"One is the Google File System, that's a general distributed file system",00:01:20,2,One Google File System general distributed file system
4257,00:01:30,5,Web Indexing,4.2,that can help programmers manage files stored on a cluster of machines.,00:01:25,2,help programmers manage files stored cluster machines
4258,00:01:33,5,Web Indexing,4.2,The second is MapReduce.,00:01:32,2,The second MapReduce
4259,00:01:37,5,Web Indexing,4.2,This is a general software framework for supporting parallel computation.,00:01:33,2,This general software framework supporting parallel computation
4260,00:01:44,5,Web Indexing,4.2,"Hadoop is the most well known open source implementation of MapReduce,",00:01:38,2,Hadoop well known open source implementation MapReduce
4261,00:01:47,5,Web Indexing,4.2,now used in many applications.,00:01:44,2,used many applications
4262,00:01:52,5,Web Indexing,4.2,So this is the architecture of the Google File System.,00:01:50,2,So architecture Google File System
4263,00:01:58,5,Web Indexing,4.2,It uses a very simple centralized management mechanism to manage,00:01:53,2,It uses simple centralized management mechanism manage
4264,00:02:01,5,Web Indexing,4.2,all the specific locations of files.,00:01:58,2,specific locations files
4265,00:02:04,5,Web Indexing,4.2,So it maintains the file namespace and,00:02:01,2,So maintains file namespace
4266,00:02:09,5,Web Indexing,4.2,look up table to know where exactly each file is stored.,00:02:04,2,look table know exactly file stored
4267,00:02:15,5,Web Indexing,4.2,The application client would then talk to this GFS master.,00:02:11,2,The application client would talk GFS master
4268,00:02:21,5,Web Indexing,4.2,And that obtains specific locations of the files that they want to process.,00:02:15,2,And obtains specific locations files want process
4269,00:02:31,5,Web Indexing,4.2,"And once the GFS client obtained the specific information about the files,",00:02:22,2,And GFS client obtained specific information files
4270,00:02:35,5,Web Indexing,4.2,then the application client can talk to the specific,00:02:31,2,application client talk specific
4271,00:02:40,5,Web Indexing,4.2,servers where the data actually sits directly.,00:02:35,2,servers data actually sits directly
4272,00:02:46,5,Web Indexing,4.2,So that you can avoid avoid involving other nodes in the network.,00:02:40,2,So avoid avoid involving nodes network
4273,00:02:49,5,Web Indexing,4.2,So when this file system stores the files on machines,00:02:46,2,So file system stores files machines
4274,00:02:57,5,Web Indexing,4.2,the system also would create a fixed sizes of chunks.,00:02:51,2,system also would create fixed sizes chunks
4275,00:03:01,5,Web Indexing,4.2,"So the data files are separate into many chunks,",00:02:57,2,So data files separate many chunks
4276,00:03:05,5,Web Indexing,4.2,"each chunk is 64 megabytes, so it's pretty big.",00:03:01,2,chunk 64 megabytes pretty big
4277,00:03:09,5,Web Indexing,4.2,And that's appropriate for large data processing.,00:03:05,2,And appropriate large data processing
4278,00:03:12,5,Web Indexing,4.2,These chunks are replicated to ensure reliability.,00:03:09,2,These chunks replicated ensure reliability
4279,00:03:17,5,Web Indexing,4.2,"So this is something that the, the programmer doesn't have to worry about,",00:03:12,2,So something programmer worry
4280,00:03:22,5,Web Indexing,4.2,and it's all taken care of by this file system.,00:03:17,2,taken care file system
4281,00:03:24,5,Web Indexing,4.2,"So from the application perspective,",00:03:22,2,So application perspective
4282,00:03:29,5,Web Indexing,4.2,the programmer would see this as if it's a normal file.,00:03:24,2,programmer would see normal file
4283,00:03:33,5,Web Indexing,4.2,"The program doesn't have to know where exactly it's stored, and",00:03:29,2,The program know exactly stored
4284,00:03:38,5,Web Indexing,4.2,can just invoke high level operators to process the file.,00:03:33,2,invoke high level operators process file
4285,00:03:43,5,Web Indexing,4.2,And another feature is that the data transfer is directly between,00:03:38,2,And another feature data transfer directly
4286,00:03:48,5,Web Indexing,4.2,"application and chunk servers, so it's, it's efficient in this sense.",00:03:43,2,application chunk servers efficient sense
4287,00:03:55,5,Web Indexing,4.2,"On top of the Google file system, and Google also proposed MapReduce as",00:03:51,2,On top Google file system Google also proposed MapReduce
4288,00:03:59,5,Web Indexing,4.2,a general framework for parallel programming.,00:03:55,2,general framework parallel programming
4289,00:04:05,5,Web Indexing,4.2,"Now, this is very useful to support a task like building inverted index.",00:03:59,2,Now useful support task like building inverted index
4290,00:04:10,5,Web Indexing,4.2,And so this framework is hiding a lot of,00:04:05,2,And framework hiding lot
4291,00:04:15,5,Web Indexing,4.2,low level features from the programmer.,00:04:10,2,low level features programmer
4292,00:04:21,5,Web Indexing,4.2,"As a result, the programmer can make minimum effort to create",00:04:15,2,As result programmer make minimum effort create
4293,00:04:26,5,Web Indexing,4.2,a application that can be run on a large cluster in parallel.,00:04:21,2,application run large cluster parallel
4294,00:04:33,5,Web Indexing,4.2,"So, some of the low level details hidden in the framework,",00:04:28,2,So low level details hidden framework
4295,00:04:39,5,Web Indexing,4.2,"including the specific natural communications, or load balancing,",00:04:33,2,including specific natural communications load balancing
4296,00:04:46,5,Web Indexing,4.2,"or where the tasks are executed, all these details are hidden from the programmer.",00:04:39,2,tasks executed details hidden programmer
4297,00:04:52,5,Web Indexing,4.2,There is also a nice feature which is the built-in fault tolerance.,00:04:47,2,There also nice feature built fault tolerance
4298,00:04:56,5,Web Indexing,4.2,"If one server is broken, let's say, so it's down, and",00:04:52,2,If one server broken let say
4299,00:04:59,5,Web Indexing,4.2,"then some tasks may not be finished,",00:04:56,2,tasks may finished
4300,00:05:05,5,Web Indexing,4.2,then the MapReduce mechanism would know that the task has not been done.,00:04:59,2,MapReduce mechanism would know task done
4301,00:05:11,5,Web Indexing,4.2,So it would automatically dispatch the task on other servers that can do the job.,00:05:05,2,So would automatically dispatch task servers job
4302,00:05:15,5,Web Indexing,4.2,"And therefore, again, the programmer doesn't have to worry about that.",00:05:11,2,And therefore programmer worry
4303,00:05:17,5,Web Indexing,4.2,So here's how MapReduce works.,00:05:15,2,So MapReduce works
4304,00:05:23,5,Web Indexing,4.2,"The input data will be separated into a number of key, value pairs.",00:05:17,2,The input data separated number key value pairs
4305,00:05:26,5,Web Indexing,4.2,"Now, what exactly is in the value will depend on the data.",00:05:23,2,Now exactly value depend data
4306,00:05:29,5,Web Indexing,4.2,And it's actually a fairly general framework to allow you to,00:05:26,2,And actually fairly general framework allow
4307,00:05:33,5,Web Indexing,4.2,just partition the data into different parts.,00:05:29,2,partition data different parts
4308,00:05:35,5,Web Indexing,4.2,And each part can be then processed in parallel.,00:05:33,2,And part processed parallel
4309,00:05:41,5,Web Indexing,4.2,"Each key, value pair will be then sent to a map function.",00:05:37,2,Each key value pair sent map function
4310,00:05:45,5,Web Indexing,4.2,"The programmer will write the map function, of course.",00:05:41,2,The programmer write map function course
4311,00:05:50,5,Web Indexing,4.2,And then the map function will then process this key value pair and,00:05:45,2,And map function process key value pair
4312,00:05:53,5,Web Indexing,4.2,"generate the, a number of other key value pairs.",00:05:50,2,generate number key value pairs
4313,00:05:58,5,Web Indexing,4.2,"Of course, the new key is usually different from the old key",00:05:53,2,Of course new key usually different old key
4314,00:06:02,5,Web Indexing,4.2,that's given to the map as input.,00:05:58,2,given map input
4315,00:06:05,5,Web Indexing,4.2,And these key value pairs are the output of the map function.,00:06:02,2,And key value pairs output map function
4316,00:06:10,5,Web Indexing,4.2,And all the outputs of all the map functions will be then collected.,00:06:05,2,And outputs map functions collected
4317,00:06:16,5,Web Indexing,4.2,And then they will be further sorted based on the key.,00:06:10,2,And sorted based key
4318,00:06:21,5,Web Indexing,4.2,And the result is that all the values that are associated with the same,00:06:16,2,And result values associated
4319,00:06:24,5,Web Indexing,4.2,key will be then grouped together.,00:06:21,2,key grouped together
4320,00:06:30,5,Web Indexing,4.2,So now we've got a pair of a key and a set of values that are attached to this key.,00:06:24,2,So got pair key set values attached key
4321,00:06:34,5,Web Indexing,4.2,So this will then be sent to a reduce function.,00:06:31,2,So sent reduce function
4322,00:06:41,5,Web Indexing,4.2,"Now, of course, each reduce function will handle a different each a different key.",00:06:36,2,Now course reduce function handle different different key
4323,00:06:45,5,Web Indexing,4.2,"So we will send this, these output values to",00:06:41,2,So send output values
4324,00:06:50,5,Web Indexing,4.2,"multiple reduce functions, each handling a unique key.",00:06:45,2,multiple reduce functions handling unique key
4325,00:06:58,5,Web Indexing,4.2,"A reduce function would then process the input, which is a key and",00:06:52,2,A reduce function would process input key
4326,00:07:04,5,Web Indexing,4.2,"a set of values, to produce another set of key values as the output.",00:06:58,2,set values produce another set key values output
4327,00:07:10,5,Web Indexing,4.2,"So these output values would be then collected together to form the,",00:07:04,2,So output values would collected together form
4328,00:07:11,5,Web Indexing,4.2,the final output.,00:07:10,2,final output
4329,00:07:17,5,Web Indexing,4.2,"Right, so this is the, the general framework of MapReduce.",00:07:12,2,Right general framework MapReduce
4330,00:07:21,5,Web Indexing,4.2,"Now, the programmer only needs to write the the map function and",00:07:17,2,Now programmer needs write map function
4331,00:07:23,5,Web Indexing,4.2,the reduce function.,00:07:21,2,reduce function
4332,00:07:28,5,Web Indexing,4.2,Everything else is actually taken care of by the MapReduce framework.,00:07:23,2,Everything else actually taken care MapReduce framework
4333,00:07:32,5,Web Indexing,4.2,"So, you can see the programmer really only needs to do minimum work.",00:07:28,2,So see programmer really needs minimum work
4334,00:07:38,5,Web Indexing,4.2,"And with such a framework, the input data can be partitioned into multiple parts.",00:07:32,2,And framework input data partitioned multiple parts
4335,00:07:43,5,Web Indexing,4.2,"Each is processed in parallel first by map, and",00:07:38,2,Each processed parallel first map
4336,00:07:48,5,Web Indexing,4.2,"then in the process after we reach the reduce stage,",00:07:43,2,process reach reduce stage
4337,00:07:53,5,Web Indexing,4.2,then much more reduce functions can also further process,00:07:48,2,much reduce functions also process
4338,00:08:00,5,Web Indexing,4.2,the different keys and their associated values in parallel.,00:07:53,2,different keys associated values parallel
4339,00:08:05,5,Web Indexing,4.2,So it achieves some it,00:08:00,2,So achieves
4340,00:08:10,5,Web Indexing,4.2,achieves the purpose of parallel processing of a large dataset.,00:08:05,2,achieves purpose parallel processing large dataset
4341,00:08:15,5,Web Indexing,4.2,"So let's take a look at a simple example, and that's word counting.",00:08:10,2,So let take look simple example word counting
4342,00:08:21,5,Web Indexing,4.2,The input is is files containing words.,00:08:16,2,The input files containing words
4343,00:08:25,5,Web Indexing,4.2,"And the output that we want to generate is the number of occurrences of each word, so",00:08:21,2,And output want generate number occurrences word
4344,00:08:27,5,Web Indexing,4.2,it's the word count.,00:08:25,2,word count
4345,00:08:31,5,Web Indexing,4.2,"Right, we know this, this kind of counting would be useful to,",00:08:27,2,Right know kind counting would useful
4346,00:08:36,5,Web Indexing,4.2,"for example, assess the popularity of a word in a large collection.",00:08:31,2,example assess popularity word large collection
4347,00:08:42,5,Web Indexing,4.2,And this is useful for achieving a factor of IDF weighting for search.,00:08:36,2,And useful achieving factor IDF weighting search
4348,00:08:44,5,Web Indexing,4.2,So how can we solve this problem?,00:08:42,2,So solve problem
4349,00:08:48,5,Web Indexing,4.2,"Well, one natural thought is that, well, this task can be done in",00:08:44,2,Well one natural thought well task done
4350,00:08:53,5,Web Indexing,4.2,parallel by simply counting different parts of the file in parallel and,00:08:48,2,parallel simply counting different parts file parallel
4351,00:08:57,5,Web Indexing,4.2,"then in the end, we just combine all the counts.",00:08:53,2,end combine counts
4352,00:09:01,5,Web Indexing,4.2,And that's precisely the idea of what we can do with MapReduce.,00:08:57,2,And precisely idea MapReduce
4353,00:09:06,5,Web Indexing,4.2,We can parallelize lines in this input file.,00:09:02,2,We parallelize lines input file
4354,00:09:12,5,Web Indexing,4.2,"So more specifically, we can assume the input to each map function",00:09:07,2,So specifically assume input map function
4355,00:09:20,5,Web Indexing,4.2,is a key value pair that represents the line number and the stream on that line.,00:09:12,2,key value pair represents line number stream line
4356,00:09:25,5,Web Indexing,4.2,"So the first line, for example, has a key of one.",00:09:20,2,So first line example key one
4357,00:09:32,5,Web Indexing,4.2,"And the value is Hello World Bye World, and just four words on that line.",00:09:25,2,And value Hello World Bye World four words line
4358,00:09:36,5,Web Indexing,4.2,So this key-value pair will be sent to a map function.,00:09:32,2,So key value pair sent map function
4359,00:09:40,5,Web Indexing,4.2,The map function would then just count the words in this line.,00:09:36,2,The map function would count words line
4360,00:09:43,5,Web Indexing,4.2,"And in this case, of course, there are only four words.",00:09:41,2,And case course four words
4361,00:09:45,5,Web Indexing,4.2,Each word gets a count of one.,00:09:43,2,Each word gets count one
4362,00:09:52,5,Web Indexing,4.2,"And these are the output that you see here on this slide, from this map function.",00:09:45,2,And output see slide map function
4363,00:09:55,5,Web Indexing,4.2,"So, the map function is really very simple.",00:09:52,2,So map function really simple
4364,00:10:00,5,Web Indexing,4.2,"If you look at the, what the pseudocode looks like on the right side, you see,",00:09:55,2,If look pseudocode looks like right side see
4365,00:10:04,5,Web Indexing,4.2,"it simply needs to iterate over all the words in this line,",00:10:00,2,simply needs iterate words line
4366,00:10:08,5,Web Indexing,4.2,"and then just call a Collect function,",00:10:04,2,call Collect function
4367,00:10:14,5,Web Indexing,4.2,which means it would then send the word and the counter to the collector.,00:10:09,2,means would send word counter collector
4368,00:10:19,5,Web Indexing,4.2,The collector would then try to sort all these key value pairs,00:10:14,2,The collector would try sort key value pairs
4369,00:10:20,5,Web Indexing,4.2,from different map functions.,00:10:19,2,different map functions
4370,00:10:21,5,Web Indexing,4.2,Right?,00:10:20,2,Right
4371,00:10:22,5,Web Indexing,4.2,So the functions are very simple.,00:10:21,2,So functions simple
4372,00:10:30,5,Web Indexing,4.2,And the programmer specifies this function as a way to process each part of the data.,00:10:22,2,And programmer specifies function way process part data
4373,00:10:34,5,Web Indexing,4.2,"Of course, the second line will be handled by a different map function,",00:10:31,2,Of course second line handled different map function
4374,00:10:36,5,Web Indexing,4.2,which will produce a similar output.,00:10:34,2,produce similar output
4375,00:10:40,5,Web Indexing,4.2,"Okay, now the output from the map functions will be then sent to",00:10:36,2,Okay output map functions sent
4376,00:10:41,5,Web Indexing,4.2,a collector.,00:10:40,2,collector
4377,00:10:45,5,Web Indexing,4.2,And the collector will do the internal grouping or sorting.,00:10:41,2,And collector internal grouping sorting
4378,00:10:50,5,Web Indexing,4.2,"So at this stage, you can see we have collected multiple pairs.",00:10:45,2,So stage see collected multiple pairs
4379,00:10:53,5,Web Indexing,4.2,Each pair is a word and its count in the line.,00:10:50,2,Each pair word count line
4380,00:10:58,5,Web Indexing,4.2,"So once we see all these these pairs,",00:10:53,2,So see pairs
4381,00:11:03,5,Web Indexing,4.2,"then we can sort them based on the key, which is the word.",00:10:58,2,sort based key word
4382,00:11:08,5,Web Indexing,4.2,"So we will collect all the counts of a word, like bye, here, together.",00:11:03,2,So collect counts word like bye together
4383,00:11:12,5,Web Indexing,4.2,"And similarly, we do that for other words.",00:11:09,2,And similarly words
4384,00:11:13,5,Web Indexing,4.2,"Like Hadoop, hello, etc.",00:11:12,2,Like Hadoop hello etc
4385,00:11:19,5,Web Indexing,4.2,"So each word now is attached to a number of values, a number of counts.",00:11:13,2,So word attached number values number counts
4386,00:11:27,5,Web Indexing,4.2,And these counts represent the occurrences of this word in different lines.,00:11:20,2,And counts represent occurrences word different lines
4387,00:11:33,5,Web Indexing,4.2,"So now we have got a new pair of a key and a set of values,",00:11:27,2,So got new pair key set values
4388,00:11:37,5,Web Indexing,4.2,and this pair will then be fed into a reduce function.,00:11:33,2,pair fed reduce function
4389,00:11:42,5,Web Indexing,4.2,So the reduce function now will have to finish the job of counting,00:11:37,2,So reduce function finish job counting
4390,00:11:44,5,Web Indexing,4.2,the total occurrences of this word.,00:11:42,2,total occurrences word
4391,00:11:47,5,Web Indexing,4.2,"Now it has already got all these partial counts, so",00:11:44,2,Now already got partial counts
4392,00:11:50,5,Web Indexing,4.2,all it needs to do is simply to add them up.,00:11:47,2,needs simply add
4393,00:11:53,5,Web Indexing,4.2,So the reduce function shown here is very simple as well.,00:11:50,2,So reduce function shown simple well
4394,00:11:58,5,Web Indexing,4.2,"You have a counter and then iterate over all the words that you see in this array,",00:11:53,2,You counter iterate words see array
4395,00:12:01,5,Web Indexing,4.2,"and then you just accumulate these counts, right.",00:11:58,2,accumulate counts right
4396,00:12:06,5,Web Indexing,4.2,"And then finally, you output the key and and the total count,",00:12:03,2,And finally output key total count
4397,00:12:11,5,Web Indexing,4.2,and that's precisely what we want as the output of this whole program.,00:12:06,2,precisely want output whole program
4398,00:12:17,5,Web Indexing,4.2,"So, you can see, this is already very similar to building a inverted index,",00:12:12,2,So see already similar building inverted index
4399,00:12:21,5,Web Indexing,4.2,"and if you think about it, the output here is indexed by a word, and",00:12:17,2,think output indexed word
4400,00:12:24,5,Web Indexing,4.2,"we have already got a dictionary, basically.",00:12:21,2,already got dictionary basically
4401,00:12:26,5,Web Indexing,4.2,We have got the count.,00:12:24,2,We got count
4402,00:12:32,5,Web Indexing,4.2,But what's missing is the document IDs and the specific,00:12:26,2,But missing document IDs specific
4403,00:12:38,5,Web Indexing,4.2,frequency counts of words in those documents.,00:12:34,2,frequency counts words documents
4404,00:12:43,5,Web Indexing,4.2,So we can modify this slightly to actually build a inverted index in parallel.,00:12:38,2,So modify slightly actually build inverted index parallel
4405,00:12:45,5,Web Indexing,4.2,So here's one way to do that.,00:12:43,2,So one way
4406,00:12:51,5,Web Indexing,4.2,"So in this case, we can assume the input to a map function is a pair",00:12:45,2,So case assume input map function pair
4407,00:12:54,5,Web Indexing,4.2,of a key which denotes the document ID and,00:12:51,2,key denotes document ID
4408,00:12:59,5,Web Indexing,4.2,the value denoting the string for that document.,00:12:54,2,value denoting string document
4409,00:13:02,5,Web Indexing,4.2,So it's all the words in that document.,00:12:59,2,So words document
4410,00:13:05,5,Web Indexing,4.2,And so the map function will do something very similar to,00:13:02,2,And map function something similar
4411,00:13:07,5,Web Indexing,4.2,what we have seen in the water company example.,00:13:05,2,seen water company example
4412,00:13:14,5,Web Indexing,4.2,It simply groups all the counts of this word in this document together.,00:13:07,2,It simply groups counts word document together
4413,00:13:17,5,Web Indexing,4.2,And it will then generate a set of key value pairs.,00:13:14,2,And generate set key value pairs
4414,00:13:19,5,Web Indexing,4.2,Each key is a word.,00:13:17,2,Each key word
4415,00:13:27,5,Web Indexing,4.2,And the value is the count of this word in this document plus the document ID.,00:13:20,2,And value count word document plus document ID
4416,00:13:31,5,Web Indexing,4.2,"Now, you can easily see why we need to add document ID here.",00:13:27,2,Now easily see need add document ID
4417,00:13:36,5,Web Indexing,4.2,"Of course, later, in the inverted index, we would like to keep this information, so",00:13:31,2,Of course later inverted index would like keep information
4418,00:13:38,5,Web Indexing,4.2,the map function should keep track of it.,00:13:36,2,map function keep track
4419,00:13:41,5,Web Indexing,4.2,And this can then be sent to the reduce function later.,00:13:38,2,And sent reduce function later
4420,00:13:46,5,Web Indexing,4.2,"Now, similarly another document D2 can be processed in the same way.",00:13:41,2,Now similarly another document D2 processed way
4421,00:13:50,5,Web Indexing,4.2,"So in the end, again, there is a sorting mechanism that would group them together.",00:13:46,2,So end sorting mechanism would group together
4422,00:13:55,5,Web Indexing,4.2,And then we will have just a key like java associated,00:13:50,2,And key like java associated
4423,00:14:00,5,Web Indexing,4.2,"with all the documents that match this key, or",00:13:55,2,documents match key
4424,00:14:04,5,Web Indexing,4.2,"all the documents where java occurred, and their counts,",00:14:00,2,documents java occurred counts
4425,00:14:09,5,Web Indexing,4.2,"right, so the counts of java in those documents.",00:14:04,2,right counts java documents
4426,00:14:11,5,Web Indexing,4.2,And this will be collected together.,00:14:09,2,And collected together
4427,00:14:15,5,Web Indexing,4.2,"And this will be, so fed into the reduced function.",00:14:11,2,And fed reduced function
4428,00:14:19,5,Web Indexing,4.2,"So, now you can see, the reduce function has already got input",00:14:15,2,So see reduce function already got input
4429,00:14:21,5,Web Indexing,4.2,"that looks like a inverted index entry, right?",00:14:19,2,looks like inverted index entry right
4430,00:14:27,5,Web Indexing,4.2,"So, it's just the word and all the documents that contain the word and",00:14:21,2,So word documents contain word
4431,00:14:30,5,Web Indexing,4.2,the frequency of the word in those documents.,00:14:27,2,frequency word documents
4432,00:14:37,5,Web Indexing,4.2,"So, all you need to do is simply to concatenate them into a continuous chunk",00:14:30,2,So need simply concatenate continuous chunk
4433,00:14:43,5,Web Indexing,4.2,"of data, and this can be then retained into a file system.",00:14:37,2,data retained file system
4434,00:14:47,5,Web Indexing,4.2,"So basically, the reduce function is going to do very minimal work.",00:14:43,2,So basically reduce function going minimal work
4435,00:14:58,5,Web Indexing,4.2,"And so, this is pseudo-code for inverted index construction.",00:14:49,2,And pseudo code inverted index construction
4436,00:15:05,5,Web Indexing,4.2,"Here we see two functions, procedure Map and procedure Reduce.",00:14:58,2,Here see two functions procedure Map procedure Reduce
4437,00:15:13,5,Web Indexing,4.2,And a programmer would specify these two functions to program on top of MapReduce.,00:15:05,2,And programmer would specify two functions program top MapReduce
4438,00:15:18,5,Web Indexing,4.2,"And you can see, basically, they are doing what I just described.",00:15:13,2,And see basically I described
4439,00:15:20,5,Web Indexing,4.2,"In the case of Map,",00:15:18,2,In case Map
4440,00:15:26,5,Web Indexing,4.2,"it's going to count the occurrences of a word using an associative array,",00:15:20,2,going count occurrences word using associative array
4441,00:15:32,5,Web Indexing,4.2,and will output all the counts together with the document ID here.,00:15:26,2,output counts together document ID
4442,00:15:36,5,Web Indexing,4.2,"Right? So this,",00:15:32,2,Right So
4443,00:15:42,5,Web Indexing,4.2,"the reduce function, on the other hand simply concatenates",00:15:37,2,reduce function hand simply concatenates
4444,00:15:47,5,Web Indexing,4.2,all the input that it has been given and,00:15:42,2,input given
4445,00:15:53,5,Web Indexing,4.2,then put them together as one single entry for this key.,00:15:47,2,put together one single entry key
4446,00:15:58,5,Web Indexing,4.2,"So this is a very simple MapReduce function, yet",00:15:53,2,So simple MapReduce function yet
4447,00:16:03,5,Web Indexing,4.2,"it would allow us to construct an inverted index at a very large scale, and",00:15:58,2,would allow us construct inverted index large scale
4448,00:16:07,5,Web Indexing,4.2,data can be processed by different machines.,00:16:03,2,data processed different machines
4449,00:16:11,5,Web Indexing,4.2,The program doesn't have to take care of the details.,00:16:07,2,The program take care details
4450,00:16:18,5,Web Indexing,4.2,So this is how we can do parallel index construction for web search.,00:16:12,2,So parallel index construction web search
4451,00:16:24,5,Web Indexing,4.2,"So to summarize, web scale indexing requires some new techniques that",00:16:20,2,So summarize web scale indexing requires new techniques
4452,00:16:28,5,Web Indexing,4.2,go beyond the standard traditional indexing techniques.,00:16:24,2,go beyond standard traditional indexing techniques
4453,00:16:34,5,Web Indexing,4.2,"Mainly, we have to store index on multiple machines, and this is usually",00:16:28,2,Mainly store index multiple machines usually
4454,00:16:40,5,Web Indexing,4.2,"done by using a file system like Google File System, a distributed file system.",00:16:34,2,done using file system like Google File System distributed file system
4455,00:16:45,5,Web Indexing,4.2,"And secondly, it requires creating the index in parallel, because it's so",00:16:40,2,And secondly requires creating index parallel
4456,00:16:49,5,Web Indexing,4.2,"large, it takes a long time to create an index for all the documents.",00:16:45,2,large takes long time create index documents
4457,00:16:53,5,Web Indexing,4.2,"So if we can do it in parallel, it would be much faster, and",00:16:49,2,So parallel would much faster
4458,00:16:56,5,Web Indexing,4.2,this is done by using the MapReduce framework.,00:16:53,2,done using MapReduce framework
4459,00:17:02,5,Web Indexing,4.2,"Note that the both the GFS and MapReduce frameworks are very general, so",00:16:57,2,Note GFS MapReduce frameworks general
4460,00:17:04,5,Web Indexing,4.2,they can also support many other applications.,00:17:02,2,also support many applications
4461,00:00:14,2,Doc Length Normalization,1.9,This lecture is about document length normalization in the vector space model.,00:00:08,9,This lecture document length normalization vector space model
4462,00:00:19,2,Doc Length Normalization,1.9,In this lecture we are going to continue the discussion of the vector space model,00:00:14,9,In lecture going continue discussion vector space model
4463,00:00:21,2,Doc Length Normalization,1.9,in particular we are going to discuss.,00:00:19,9,particular going discuss
4464,00:00:23,2,Doc Length Normalization,1.9,The issue of document length normalization.,00:00:21,9,The issue document length normalization
4465,00:00:28,2,Doc Length Normalization,1.9,"So far in the lectures about the vector space model,",00:00:25,9,So far lectures vector space model
4466,00:00:33,2,Doc Length Normalization,1.9,we have used the various signals from the document to,00:00:28,9,used various signals document
4467,00:00:37,2,Doc Length Normalization,1.9,assess the matching of the document though with a preorder.,00:00:33,9,assess matching document though preorder
4468,00:00:40,2,Doc Length Normalization,1.9,"In particular we have considered the term frequency,",00:00:37,9,In particular considered term frequency
4469,00:00:42,2,Doc Length Normalization,1.9,the count of a term in a document.,00:00:40,9,count term document
4470,00:00:44,2,Doc Length Normalization,1.9,"We have also considered a,",00:00:42,9,We also considered
4471,00:00:50,2,Doc Length Normalization,1.9,it's global statistics such as IDF in words document frequency.,00:00:44,9,global statistics IDF words document frequency
4472,00:00:53,2,Doc Length Normalization,1.9,But we have not considered a document length.,00:00:50,9,But considered document length
4473,00:00:58,2,Doc Length Normalization,1.9,"So, here I show two example documents.",00:00:54,9,So I show two example documents
4474,00:01:01,2,Doc Length Normalization,1.9,D4 is much shorter with only 100 words.,00:00:58,9,D4 much shorter 100 words
4475,00:01:05,2,Doc Length Normalization,1.9,"D6 on the other hand has 5,000 words.",00:01:01,9,D6 hand 5 000 words
4476,00:01:10,2,Doc Length Normalization,1.9,If you look at the matching of these query words we see that in D6 there,00:01:05,9,If look matching query words see D6
4477,00:01:14,2,Doc Length Normalization,1.9,are more matchings of the query words but,00:01:10,9,matchings query words
4478,00:01:20,2,Doc Length Normalization,1.9,one might reason that D6 may have matched these query words.,00:01:14,9,one might reason D6 may matched query words
4479,00:01:23,2,Doc Length Normalization,1.9,In a scattered manner.,00:01:22,9,In scattered manner
4480,00:01:30,2,Doc Length Normalization,1.9,So maybe the topic of d6 is not really about the topic of the query.,00:01:24,9,So maybe topic d6 really topic query
4481,00:01:34,2,Doc Length Normalization,1.9,So the discussion of a campaign at the beginning of the document,00:01:31,9,So discussion campaign beginning document
4482,00:01:38,2,Doc Length Normalization,1.9,may have nothing to do with the mention of presidential at the end.,00:01:34,9,may nothing mention presidential end
4483,00:01:44,2,Doc Length Normalization,1.9,"In general, if you think about the long documents,",00:01:40,9,In general think long documents
4484,00:01:47,2,Doc Length Normalization,1.9,they would have a higher chance to match any query.,00:01:44,9,would higher chance match query
4485,00:01:52,2,Doc Length Normalization,1.9,"In fact, if you generate a, a long document that randomly sampling,",00:01:47,9,In fact generate long document randomly sampling
4486,00:01:56,2,Doc Length Normalization,1.9,"sampling words from the distribution of words,",00:01:52,9,sampling words distribution words
4487,00:01:59,2,Doc Length Normalization,1.9,then eventually you probably will match any query.,00:01:56,9,eventually probably match query
4488,00:02:04,2,Doc Length Normalization,1.9,So in this sense we should penalize no documents because they,00:02:00,9,So sense penalize documents
4489,00:02:07,2,Doc Length Normalization,1.9,just naturally have better chances to match any query.,00:02:04,9,naturally better chances match query
4490,00:02:12,2,Doc Length Normalization,1.9,And this is our idea of document answer.,00:02:07,9,And idea document answer
4491,00:02:18,2,Doc Length Normalization,1.9,We also need to be careful in avoiding to overpenalize small documents.,00:02:12,9,We also need careful avoiding overpenalize small documents
4492,00:02:22,2,Doc Length Normalization,1.9,"On the one hand, we want to penalize a long document.",00:02:19,9,On one hand want penalize long document
4493,00:02:27,2,Doc Length Normalization,1.9,"But on the other hand, we also don't want to over-penalize them.",00:02:22,9,But hand also want penalize
4494,00:02:31,2,Doc Length Normalization,1.9,And the reason is because a document that may be long because of different reason.,00:02:27,9,And reason document may long different reason
4495,00:02:36,2,Doc Length Normalization,1.9,In one case the document may be more long because it uses more words.,00:02:32,9,In one case document may long uses words
4496,00:02:44,2,Doc Length Normalization,1.9,So for example think about the article of a research paper.,00:02:38,9,So example think article research paper
4497,00:02:47,2,Doc Length Normalization,1.9,It would use more words than the corresponding abstract.,00:02:44,9,It would use words corresponding abstract
4498,00:02:53,2,Doc Length Normalization,1.9,So this is the case where we probably should penalize the matching of,00:02:49,9,So case probably penalize matching
4499,00:02:57,2,Doc Length Normalization,1.9,"a long document such as, full paper.",00:02:53,9,long document full paper
4500,00:03:01,2,Doc Length Normalization,1.9,When we compare the matching of words in such,00:02:58,9,When compare matching words
4501,00:03:06,2,Doc Length Normalization,1.9,long document with matching of the words in the short abstract.,00:03:02,9,long document matching words short abstract
4502,00:03:13,2,Doc Length Normalization,1.9,Then long papers generally have a higher chance of matching query words.,00:03:07,9,Then long papers generally higher chance matching query words
4503,00:03:15,2,Doc Length Normalization,1.9,Therefore we should penalize them.,00:03:13,9,Therefore penalize
4504,00:03:18,2,Doc Length Normalization,1.9,"However, there is another case when the document is long and",00:03:15,9,However another case document long
4505,00:03:21,2,Doc Length Normalization,1.9,that is when the document simply has more content.,00:03:18,9,document simply content
4506,00:03:24,2,Doc Length Normalization,1.9,"Now consider another case of a long document,",00:03:21,9,Now consider another case long document
4507,00:03:29,2,Doc Length Normalization,1.9,where we simply concatenated a lot of abstracts of different papers.,00:03:24,9,simply concatenated lot abstracts different papers
4508,00:03:34,2,Doc Length Normalization,1.9,"In such a case, obviously, we don't want to penalize such a long document.",00:03:29,9,In case obviously want penalize long document
4509,00:03:38,2,Doc Length Normalization,1.9,"Indeed, we probably don't want to penalize such a document because it's long.",00:03:34,9,Indeed probably want penalize document long
4510,00:03:42,2,Doc Length Normalization,1.9,So that's why we need to be careful.,00:03:39,9,So need careful
4511,00:03:46,2,Doc Length Normalization,1.9,About using the right degree of penalization.,00:03:42,9,About using right degree penalization
4512,00:03:53,2,Doc Length Normalization,1.9,"A method that has been working well based on recent research is called,",00:03:48,9,A method working well based recent research called
4513,00:03:54,2,Doc Length Normalization,1.9,pivot length normalization.,00:03:53,9,pivot length normalization
4514,00:03:57,2,Doc Length Normalization,1.9,And in this case the idea is to use.,00:03:54,9,And case idea use
4515,00:04:01,2,Doc Length Normalization,1.9,"The average document length as a P word, as a reference point.",00:03:57,9,The average document length P word reference point
4516,00:04:05,2,Doc Length Normalization,1.9,"That means we will assume that for the average length documents,",00:04:01,9,That means assume average length documents
4517,00:04:07,2,Doc Length Normalization,1.9,the score is about right.,00:04:05,9,score right
4518,00:04:10,2,Doc Length Normalization,1.9,"So, the normalizer would be 1.",00:04:07,9,So normalizer would 1
4519,00:04:14,2,Doc Length Normalization,1.9,But if a document is longer than the average document length,00:04:10,9,But document longer average document length
4520,00:04:16,2,Doc Length Normalization,1.9,then there will be some penalization.,00:04:14,9,penalization
4521,00:04:20,2,Doc Length Normalization,1.9,Where as if it's shorter than there's even some reward.,00:04:16,9,Where shorter even reward
4522,00:04:23,2,Doc Length Normalization,1.9,So this is an illustrator that using this slide.,00:04:20,9,So illustrator using slide
4523,00:04:28,2,Doc Length Normalization,1.9,"On the axis, s axis you can see the length of document.",00:04:25,9,On axis axis see length document
4524,00:04:33,2,Doc Length Normalization,1.9,"On the y-axis we show the normalizer,",00:04:28,9,On axis show normalizer
4525,00:04:39,2,Doc Length Normalization,1.9,in the case pivoted length normalization formula for the normalizer is,00:04:33,9,case pivoted length normalization formula normalizer
4526,00:04:45,2,Doc Length Normalization,1.9,is seem to be interpolation of one and,00:04:41,9,seem interpolation one
4527,00:04:50,2,Doc Length Normalization,1.9,"the normalize the document lengths, controlled by a parameter b here.",00:04:45,9,normalize document lengths controlled parameter b
4528,00:04:55,2,Doc Length Normalization,1.9,"So, you can see here,",00:04:53,9,So see
4529,00:05:00,2,Doc Length Normalization,1.9,when we first divide the lengths of the document by the average document length.,00:04:55,9,first divide lengths document average document length
4530,00:05:04,2,Doc Length Normalization,1.9,"This not only gives us some sense about the,",00:05:01,9,This gives us sense
4531,00:05:07,2,Doc Length Normalization,1.9,"how this document is compared with the average document length, but",00:05:04,9,document compared average document length
4532,00:05:13,2,Doc Length Normalization,1.9,also gives us a benefit of not worrying about the unit of,00:05:07,9,also gives us benefit worrying unit
4533,00:05:18,2,Doc Length Normalization,1.9,"length, we can measure the length by words or by characters.",00:05:15,9,length measure length words characters
4534,00:05:24,2,Doc Length Normalization,1.9,Anyway this normalizer has an interesting property.,00:05:20,9,Anyway normalizer interesting property
4535,00:05:29,2,Doc Length Normalization,1.9,"First we see that if we set the parameter b to 0 then the value would be 1,",00:05:24,9,First see set parameter b 0 value would 1
4536,00:05:33,2,Doc Length Normalization,1.9,"so there's no pair, length normalization at all.",00:05:29,9,pair length normalization
4537,00:05:40,2,Doc Length Normalization,1.9,"So b in this sense controls the length normalization, where as if we set",00:05:33,9,So b sense controls length normalization set
4538,00:05:45,2,Doc Length Normalization,1.9,"d to a non-zero value, then the normalizer will look like this, right.",00:05:40,9,non zero value normalizer look like right
4539,00:05:48,2,Doc Length Normalization,1.9,So the value would be higher for,00:05:45,9,So value would higher
4540,00:05:53,2,Doc Length Normalization,1.9,documents that are longer than the average document length.,00:05:48,9,documents longer average document length
4541,00:05:57,2,Doc Length Normalization,1.9,Where as the value of the normalizer will be short- will be smaller for,00:05:53,9,Where value normalizer short smaller
4542,00:05:59,2,Doc Length Normalization,1.9,shorter documents.,00:05:57,9,shorter documents
4543,00:06:03,2,Doc Length Normalization,1.9,So in this sense we see there's a penalization for long documents.,00:05:59,9,So sense see penalization long documents
4544,00:06:07,2,Doc Length Normalization,1.9,And there's a reward for short documents.,00:06:05,9,And reward short documents
4545,00:06:11,2,Doc Length Normalization,1.9,The degree of penalization is conjured by b.,00:06:09,9,The degree penalization conjured b
4546,00:06:15,2,Doc Length Normalization,1.9,Because if we set b to a larger value then the normalizer.,00:06:11,9,Because set b larger value normalizer
4547,00:06:16,2,Doc Length Normalization,1.9,What looked like this.,00:06:15,9,What looked like
4548,00:06:20,2,Doc Length Normalization,1.9,There's even more penalization for long documents and more reward for,00:06:16,9,There even penalization long documents reward
4549,00:06:22,2,Doc Length Normalization,1.9,the short documents.,00:06:20,9,short documents
4550,00:06:26,2,Doc Length Normalization,1.9,By adjusting b which varies from zero to one,00:06:22,9,By adjusting b varies zero one
4551,00:06:29,2,Doc Length Normalization,1.9,we can control the degree of length normalization.,00:06:26,9,control degree length normalization
4552,00:06:35,2,Doc Length Normalization,1.9,So if we're plucking this length normalization factor into,00:06:29,9,So plucking length normalization factor
4553,00:06:40,2,Doc Length Normalization,1.9,the vector space model ranking functions that we have already examined.,00:06:35,9,vector space model ranking functions already examined
4554,00:06:45,2,Doc Length Normalization,1.9,"Then we will end up heading with formulas, and",00:06:41,9,Then end heading formulas
4555,00:06:49,2,Doc Length Normalization,1.9,these are in fact the state of the are vector space models.,00:06:45,9,fact state vector space models
4556,00:06:50,2,Doc Length Normalization,1.9,Formulas.,00:06:49,9,Formulas
4557,00:06:55,2,Doc Length Normalization,1.9,"So, let's talk an that, let's take a look at the each of them.",00:06:52,9,So let talk let take look
4558,00:06:58,2,Doc Length Normalization,1.9,The first one's called a pivoted length normalization vector space model.,00:06:55,9,The first one called pivoted length normalization vector space model
4559,00:07:05,2,Doc Length Normalization,1.9,"And, a reference in the end has detail about the derivation of this model.",00:07:00,9,And reference end detail derivation model
4560,00:07:10,2,Doc Length Normalization,1.9,"And, here, we see that it's basically the TFIDF weighting model that we have",00:07:05,9,And see basically TFIDF weighting model
4561,00:07:12,2,Doc Length Normalization,1.9,discussed.,00:07:10,9,discussed
4562,00:07:16,2,Doc Length Normalization,1.9,The IDF component should be very familiar now to you.,00:07:12,9,The IDF component familiar
4563,00:07:21,2,Doc Length Normalization,1.9,"There is also a query term frequency component, here.",00:07:18,9,There also query term frequency component
4564,00:07:27,2,Doc Length Normalization,1.9,"And, and then in the middle there is.",00:07:24,9,And middle
4565,00:07:29,2,Doc Length Normalization,1.9,And normalize the TF.,00:07:27,9,And normalize TF
4566,00:07:34,2,Doc Length Normalization,1.9,"And in this case, we see we use the double algorithm,",00:07:30,9,And case see use double algorithm
4567,00:07:40,2,Doc Length Normalization,1.9,"as we discussed before, and this is to achieve a sublinear transformation.",00:07:35,9,discussed achieve sublinear transformation
4568,00:07:46,2,Doc Length Normalization,1.9,"But we also put document length normalizer in the bottom, all right so",00:07:40,9,But also put document length normalizer bottom right
4569,00:07:50,2,Doc Length Normalization,1.9,"this would cause penalty for a long document, because the larger",00:07:46,9,would cause penalty long document larger
4570,00:07:55,2,Doc Length Normalization,1.9,"the denominator is, the denominator is then the smaller the shift weight is.",00:07:50,9,denominator denominator smaller shift weight
4571,00:07:59,2,Doc Length Normalization,1.9,And this is of course controlled by the parameter b here.,00:07:56,9,And course controlled parameter b
4572,00:08:06,2,Doc Length Normalization,1.9,"And you can see again, b is set to 0, and there, there is no length normalization.",00:08:01,9,And see b set 0 length normalization
4573,00:08:13,2,Doc Length Normalization,1.9,Okay. So this is one of the two most effective.,00:08:08,9,Okay So one two effective
4574,00:08:16,2,Doc Length Normalization,1.9,Not this base model of formulas.,00:08:13,9,Not base model formulas
4575,00:08:23,2,Doc Length Normalization,1.9,"The next one called a BM25, or Okapi, is, also similar.",00:08:16,9,The next one called BM25 Okapi also similar
4576,00:08:30,2,Doc Length Normalization,1.9,"In that, it also has a i, df component here, and a query df component here.",00:08:23,9,In also df component query df component
4577,00:08:36,2,Doc Length Normalization,1.9,"But in the middle, the normalization's a little bit different.",00:08:33,9,But middle normalization little bit different
4578,00:08:41,2,Doc Length Normalization,1.9,As we expand there is this or copied here for transformation here.,00:08:36,9,As expand copied transformation
4579,00:08:46,2,Doc Length Normalization,1.9,"And that does, sublinear transformation with an upper bound.",00:08:41,9,And sublinear transformation upper bound
4580,00:08:53,2,Doc Length Normalization,1.9,In this case we have put the length normalization factor here.,00:08:48,9,In case put length normalization factor
4581,00:08:58,2,Doc Length Normalization,1.9,"We are adjusting k, but it achieves a similar factor",00:08:53,9,We adjusting k achieves similar factor
4582,00:09:02,2,Doc Length Normalization,1.9,because we put a normalizer in the denominator.,00:08:58,9,put normalizer denominator
4583,00:09:08,2,Doc Length Normalization,1.9,"Therefore again, if a document is longer, then the term weight will be smaller.",00:09:02,9,Therefore document longer term weight smaller
4584,00:09:14,2,Doc Length Normalization,1.9,"So, you can see, after we have gone through all the instances that we talked",00:09:10,9,So see gone instances talked
4585,00:09:19,2,Doc Length Normalization,1.9,"about, and we have, in the end, reached the,",00:09:14,9,end reached
4586,00:09:25,2,Doc Length Normalization,1.9,basically the state of the art mutual function.,00:09:19,9,basically state art mutual function
4587,00:09:30,2,Doc Length Normalization,1.9,"So, so far we have talked about mainly how to place",00:09:25,9,So far talked mainly place
4588,00:09:33,2,Doc Length Normalization,1.9,the document matter in the matter space.,00:09:30,9,document matter matter space
4589,00:09:40,2,Doc Length Normalization,1.9,"And this has played an important role in uh,determining the factors of",00:09:35,9,And played important role uh determining factors
4590,00:09:41,2,Doc Length Normalization,1.9,the function.,00:09:40,9,function
4591,00:09:45,2,Doc Length Normalization,1.9,But there are also other dimensions where we did not really examine detail.,00:09:41,9,But also dimensions really examine detail
4592,00:09:48,2,Doc Length Normalization,1.9,For example can we further,00:09:45,9,For example
4593,00:09:53,2,Doc Length Normalization,1.9,improve the instantiation of the dimension of the vector space model.,00:09:48,9,improve instantiation dimension vector space model
4594,00:09:56,2,Doc Length Normalization,1.9,Now we've just assumed that the back of words.,00:09:53,9,Now assumed back words
4595,00:09:57,2,Doc Length Normalization,1.9,So each dimension is a word.,00:09:56,9,So dimension word
4596,00:10:01,2,Doc Length Normalization,1.9,But obviously we can see there are many other choices.,00:09:57,9,But obviously see many choices
4597,00:10:06,2,Doc Length Normalization,1.9,"For example, stemmed words, those are the words that have been transformed",00:10:01,9,For example stemmed words words transformed
4598,00:10:10,2,Doc Length Normalization,1.9,into the same rule form.,00:10:06,9,rule form
4599,00:10:16,2,Doc Length Normalization,1.9,So that computation and computing will all become the same and they can be matched.,00:10:10,9,So computation computing become matched
4600,00:10:18,2,Doc Length Normalization,1.9,We need to stop water removal.,00:10:16,9,We need stop water removal
4601,00:10:22,2,Doc Length Normalization,1.9,This is removes on very common words that don't carry any content.,00:10:18,9,This removes common words carry content
4602,00:10:29,2,Doc Length Normalization,1.9,"Like the or of, we use the phrases to define that [SOUND].",00:10:22,9,Like use phrases define SOUND
4603,00:10:34,2,Doc Length Normalization,1.9,"We can even use late in the semantica, an answer sort of find in the sum cluster.",00:10:29,9,We even use late semantica answer sort find sum cluster
4604,00:10:39,2,Doc Length Normalization,1.9,So words that represent a legend of concept as one.,00:10:34,9,So words represent legend concept one
4605,00:10:43,2,Doc Length Normalization,1.9,"We can also use smaller units, like a character in grams.",00:10:39,9,We also use smaller units like character grams
4606,00:10:48,2,Doc Length Normalization,1.9,Those are sequences of n characters for dimensions.,00:10:43,9,Those sequences n characters dimensions
4607,00:10:55,2,Doc Length Normalization,1.9,"However, in practice people have found that the bag-of-words representation",00:10:50,9,However practice people found bag words representation
4608,00:10:58,2,Doc Length Normalization,1.9,with the phrases is where the the most effective one.,00:10:55,9,phrases effective one
4609,00:11:03,2,Doc Length Normalization,1.9,And it's also efficient so this is still so,00:10:58,9,And also efficient still
4610,00:11:08,2,Doc Length Normalization,1.9,far the most popular dimension instantiation method and,00:11:03,9,far popular dimension instantiation method
4611,00:11:12,2,Doc Length Normalization,1.9,it's used in all the major search engines.,00:11:08,9,used major search engines
4612,00:11:18,2,Doc Length Normalization,1.9,I should also mention that sometimes we did to do language specific and,00:11:13,9,I also mention sometimes language specific
4613,00:11:21,2,Doc Length Normalization,1.9,domain specific organization.,00:11:18,9,domain specific organization
4614,00:11:26,2,Doc Length Normalization,1.9,And this is actually very important as we might have variations of the terms.,00:11:21,9,And actually important might variations terms
4615,00:11:31,2,Doc Length Normalization,1.9,That might prevent us from matching them with each other.,00:11:28,9,That might prevent us matching
4616,00:11:33,2,Doc Length Normalization,1.9,Even though they mean the same thing.,00:11:31,9,Even though mean thing
4617,00:11:38,2,Doc Length Normalization,1.9,"And some of them, which is like Chinese, the results of the.",00:11:33,9,And like Chinese results
4618,00:11:44,2,Doc Length Normalization,1.9,Segmenting text to obtain word boundaries.,00:11:38,9,Segmenting text obtain word boundaries
4619,00:11:47,2,Doc Length Normalization,1.9,Because it's just a sequence of characters.,00:11:44,9,Because sequence characters
4620,00:11:51,2,Doc Length Normalization,1.9,"A word might, might correspond to one character or two characters or",00:11:47,9,A word might might correspond one character two characters
4621,00:11:53,2,Doc Length Normalization,1.9,even three characters.,00:11:51,9,even three characters
4622,00:11:58,2,Doc Length Normalization,1.9,So it's easier in English when we have a space to separate the words.,00:11:53,9,So easier English space separate words
4623,00:12:02,2,Doc Length Normalization,1.9,But in some other languages we may need to do some natural language processing,00:11:58,9,But languages may need natural language processing
4624,00:12:05,2,Doc Length Normalization,1.9,"to figure out the, where are the boundaries for words.",00:12:02,9,figure boundaries words
4625,00:12:10,2,Doc Length Normalization,1.9,There is also possibility to improve this in narrative function.,00:12:06,9,There also possibility improve narrative function
4626,00:12:13,2,Doc Length Normalization,1.9,"And so far we have used the about product, but",00:12:10,9,And far used product
4627,00:12:15,2,Doc Length Normalization,1.9,one can imagine there are other matches.,00:12:13,9,one imagine matches
4628,00:12:20,2,Doc Length Normalization,1.9,"For example we can match the cosine of the angle between two vectors, or",00:12:15,9,For example match cosine angle two vectors
4629,00:12:23,2,Doc Length Normalization,1.9,we can use Euclidean distance measure.,00:12:20,9,use Euclidean distance measure
4630,00:12:26,2,Doc Length Normalization,1.9,And these are all possible.,00:12:24,9,And possible
4631,00:12:30,2,Doc Length Normalization,1.9,The dot product seems still the best and,00:12:26,9,The dot product seems still best
4632,00:12:32,2,Doc Length Normalization,1.9,one of the reasons is because it's very general.,00:12:30,9,one reasons general
4633,00:12:36,2,Doc Length Normalization,1.9,"In fact, it's sufficiently general.",00:12:33,9,In fact sufficiently general
4634,00:12:43,2,Doc Length Normalization,1.9,If you consider the possibilities of doing weighting in different ways.,00:12:37,9,If consider possibilities weighting different ways
4635,00:12:45,2,Doc Length Normalization,1.9,"So, for example,",00:12:44,9,So example
4636,00:12:50,2,Doc Length Normalization,1.9,cosine measure can be regarded as the dot product of two normalized vectors.,00:12:45,9,cosine measure regarded dot product two normalized vectors
4637,00:12:54,2,Doc Length Normalization,1.9,"That means we first normalize each vector, and then we take the dot product.",00:12:50,9,That means first normalize vector take dot product
4638,00:12:57,2,Doc Length Normalization,1.9,That would be equivalent to the cosine measure.,00:12:54,9,That would equivalent cosine measure
4639,00:13:00,2,Doc Length Normalization,1.9,I just mentioned that the BM25.,00:12:57,9,I mentioned BM25
4640,00:13:03,2,Doc Length Normalization,1.9,Seems to be one of the most effective formulas.,00:13:00,9,Seems one effective formulas
4641,00:13:10,2,Doc Length Normalization,1.9,"But there has been also further development in, improving BM25, although",00:13:04,9,But also development improving BM25 although
4642,00:13:15,2,Doc Length Normalization,1.9,none of these works have changed the BM25 fundamentally.,00:13:10,9,none works changed BM25 fundamentally
4643,00:13:20,2,Doc Length Normalization,1.9,"So in one line of work, people have derived BM25 F.",00:13:15,9,So one line work people derived BM25 F
4644,00:13:24,2,Doc Length Normalization,1.9,"Here F stands for field, and this is a little use BM25 for",00:13:20,9,Here F stands field little use BM25
4645,00:13:26,2,Doc Length Normalization,1.9,documents with a structures.,00:13:24,9,documents structures
4646,00:13:31,2,Doc Length Normalization,1.9,"For example you might consider title field, the abstract, or",00:13:26,9,For example might consider title field abstract
4647,00:13:37,2,Doc Length Normalization,1.9,"body of the reasearch article, or even anchor text on the web pages.",00:13:31,9,body reasearch article even anchor text web pages
4648,00:13:41,2,Doc Length Normalization,1.9,Those are the text fields that describe links to other pages.,00:13:37,9,Those text fields describe links pages
4649,00:13:45,2,Doc Length Normalization,1.9,And these can all be combined with a appropriate,00:13:41,9,And combined appropriate
4650,00:13:50,2,Doc Length Normalization,1.9,weight on different fields to help improve scoring for document.,00:13:45,9,weight different fields help improve scoring document
4651,00:13:54,2,Doc Length Normalization,1.9,Use BM25 for such a document.,00:13:50,9,Use BM25 document
4652,00:13:58,2,Doc Length Normalization,1.9,"And the obvious choice is to apply BM25 for each field, and",00:13:54,9,And obvious choice apply BM25 field
4653,00:14:00,2,Doc Length Normalization,1.9,then combine the scores.,00:13:58,9,combine scores
4654,00:14:05,2,Doc Length Normalization,1.9,"Basically, the ideal of BM25F, is to first combine",00:14:00,9,Basically ideal BM25F first combine
4655,00:14:11,2,Doc Length Normalization,1.9,the frequency counts of tons in all the fields and then apply BM25.,00:14:05,9,frequency counts tons fields apply BM25
4656,00:14:19,2,Doc Length Normalization,1.9,Now this has advantage of avoiding over counting the first occurrence of the term.,00:14:11,9,Now advantage avoiding counting first occurrence term
4657,00:14:22,2,Doc Length Normalization,1.9,"Remember in the sublinear transformation of TF,",00:14:19,9,Remember sublinear transformation TF
4658,00:14:27,2,Doc Length Normalization,1.9,"the first recurrence is very important then, and contributes a large weight.",00:14:22,9,first recurrence important contributes large weight
4659,00:14:33,2,Doc Length Normalization,1.9,"And if we do that for all the fields, then the same term might have gained a, a lot",00:14:27,9,And fields term might gained lot
4660,00:14:38,2,Doc Length Normalization,1.9,"of advantage in every field, but when we combine these word frequencies together.",00:14:33,9,advantage every field combine word frequencies together
4661,00:14:42,2,Doc Length Normalization,1.9,"We just do the transformation one time, and",00:14:38,9,We transformation one time
4662,00:14:47,2,Doc Length Normalization,1.9,that time then the extra occurrences will not be counted as fresh first occurrences.,00:14:42,9,time extra occurrences counted fresh first occurrences
4663,00:14:54,2,Doc Length Normalization,1.9,And this method has been working very well for scoring structured documents.,00:14:48,9,And method working well scoring structured documents
4664,00:15:02,2,Doc Length Normalization,1.9,"The other line of extension is called a BM25 plus and this line, arresters",00:14:55,9,The line extension called BM25 plus line arresters
4665,00:15:06,2,Doc Length Normalization,1.9,have addressed the problem of over penalization of long documents by BM25.,00:15:02,9,addressed problem penalization long documents BM25
4666,00:15:14,2,Doc Length Normalization,1.9,"So to address this problem, the fix is actually quite simple.",00:15:08,9,So address problem fix actually quite simple
4667,00:15:19,2,Doc Length Normalization,1.9,We can simply add a small constant to the TF normalization formula.,00:15:14,9,We simply add small constant TF normalization formula
4668,00:15:24,2,Doc Length Normalization,1.9,But what's interesting is that we can analytically prove that by doing such,00:15:19,9,But interesting analytically prove
4669,00:15:29,2,Doc Length Normalization,1.9,"a small modification, we will fix the problem of a,",00:15:24,9,small modification fix problem
4670,00:15:33,2,Doc Length Normalization,1.9,over penalization of long documents by the original BM25.,00:15:29,9,penalization long documents original BM25
4671,00:15:38,2,Doc Length Normalization,1.9,So the new formula called BM25-plus is empirically and,00:15:33,9,So new formula called BM25 plus empirically
4672,00:15:40,2,Doc Length Normalization,1.9,analytically shown to be better than BM25.,00:15:38,9,analytically shown better BM25
4673,00:15:47,2,Doc Length Normalization,1.9,So to summarize all what we have said about the Vector Space Model.,00:15:42,9,So summarize said Vector Space Model
4674,00:15:51,2,Doc Length Normalization,1.9,Here are the major takeaway points.,00:15:49,9,Here major takeaway points
4675,00:15:57,2,Doc Length Normalization,1.9,"First, in such a model, we use the similarity notion of relevance,",00:15:51,9,First model use similarity notion relevance
4676,00:16:01,2,Doc Length Normalization,1.9,assuming that the relevance of a document with respect to a query is,00:15:57,9,assuming relevance document respect query
4677,00:16:08,2,Doc Length Normalization,1.9,basically proportional to the similarity between the query and the document.,00:16:02,9,basically proportional similarity query document
4678,00:16:10,2,Doc Length Normalization,1.9,"So, naturally, that implies that the query and",00:16:08,9,So naturally implies query
4679,00:16:14,2,Doc Length Normalization,1.9,"document must be represented in the same way, and in this case,",00:16:10,9,document must represented way case
4680,00:16:19,2,Doc Length Normalization,1.9,we represent them as vectors in high dimensional vector space.,00:16:14,9,represent vectors high dimensional vector space
4681,00:16:24,2,Doc Length Normalization,1.9,Where the dimensions are defined by words or concepts or terms in general.,00:16:19,9,Where dimensions defined words concepts terms general
4682,00:16:29,2,Doc Length Normalization,1.9,And we generally need to use a lot of heuristics to design a ranking function.,00:16:25,9,And generally need use lot heuristics design ranking function
4683,00:16:34,2,Doc Length Normalization,1.9,"We use some examples which show the need for several heuristics,",00:16:29,9,We use examples show need several heuristics
4684,00:16:37,2,Doc Length Normalization,1.9,including TF waiting and transformation.,00:16:34,9,including TF waiting transformation
4685,00:16:41,2,Doc Length Normalization,1.9,"And IDF weighting, and document length normalization.",00:16:38,9,And IDF weighting document length normalization
4686,00:16:47,2,Doc Length Normalization,1.9,These major heuristics are the most important heuristics to ensure such,00:16:41,9,These major heuristics important heuristics ensure
4687,00:16:51,2,Doc Length Normalization,1.9,a general ranking function to work well for all kinds of tasks.,00:16:47,9,general ranking function work well kinds tasks
4688,00:16:55,2,Doc Length Normalization,1.9,And finally BM25 and Pivoted normalization seem,00:16:51,9,And finally BM25 Pivoted normalization seem
4689,00:16:59,2,Doc Length Normalization,1.9,to be the most effective formulas out of that Space Model.,00:16:55,9,effective formulas Space Model
4690,00:17:05,2,Doc Length Normalization,1.9,"Now I have to say that, I've put BM25 in the category of Vector Space Model.",00:16:59,9,Now I say I put BM25 category Vector Space Model
4691,00:17:09,2,Doc Length Normalization,1.9,But in fact the BM25 has been derived using model.,00:17:05,9,But fact BM25 derived using model
4692,00:17:17,2,Doc Length Normalization,1.9,So the reason why I've put it in the vector space model is first,00:17:11,9,So reason I put vector space model first
4693,00:17:22,2,Doc Length Normalization,1.9,the ranking function actually has a nice interpretation in the vector space model.,00:17:17,9,ranking function actually nice interpretation vector space model
4694,00:17:26,2,Doc Length Normalization,1.9,We can easily see it looks very much like a vector space model,00:17:22,9,We easily see looks much like vector space model
4695,00:17:27,2,Doc Length Normalization,1.9,with a special weighting function.,00:17:26,9,special weighting function
4696,00:17:34,2,Doc Length Normalization,1.9,The second reason is because the original BM25 has a somewhat different from of IDF.,00:17:28,9,The second reason original BM25 somewhat different IDF
4697,00:17:39,2,Doc Length Normalization,1.9,And that form of IDF actually doesn't really work so,00:17:36,9,And form IDF actually really work
4698,00:17:44,2,Doc Length Normalization,1.9,well as the standard IDF that you have seen here.,00:17:39,9,well standard IDF seen
4699,00:17:50,2,Doc Length Normalization,1.9,So as a effective original function BM25 should probably use a heuristic,00:17:44,9,So effective original function BM25 probably use heuristic
4700,00:17:55,2,Doc Length Normalization,1.9,modification of the IDF to make that even more like a vector space model.,00:17:50,9,modification IDF make even like vector space model
4701,00:18:01,2,Doc Length Normalization,1.9,There are some additional readings.,00:17:59,9,There additional readings
4702,00:18:06,2,Doc Length Normalization,1.9,The first is a paper about the pivoted length normalization.,00:18:01,9,The first paper pivoted length normalization
4703,00:18:11,2,Doc Length Normalization,1.9,It's an excellent example of using empirical data enhances to suggest a need,00:18:06,9,It excellent example using empirical data enhances suggest need
4704,00:18:17,2,Doc Length Normalization,1.9,"for length normalization, and then further derived a length normalization formula.",00:18:11,9,length normalization derived length normalization formula
4705,00:18:24,2,Doc Length Normalization,1.9,The second is the original paper when the was proposed.,00:18:17,9,The second original paper proposed
4706,00:18:28,2,Doc Length Normalization,1.9,The third paper has a thorough discussion of and,00:18:24,9,The third paper thorough discussion
4707,00:18:32,2,Doc Length Normalization,1.9,"its extensions, particularly BM-25F.",00:18:28,9,extensions particularly BM 25F
4708,00:18:37,2,Doc Length Normalization,1.9,"And finally, the last paper has a discussion of improving",00:18:32,9,And finally last paper discussion improving
4709,00:18:43,2,Doc Length Normalization,1.9,BM-25 to correct the overpenalization of long documents.,00:18:37,9,BM 25 correct overpenalization long documents
4710,00:00:08,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,[NOISE].,00:00:00,13,NOISE
4711,00:00:14,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And here what will do is talk about basic strategy,",00:00:08,13,And talk basic strategy
4712,00:00:19,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,and that would be based on similarity of users and,00:00:14,13,would based similarity users
4713,00:00:24,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"then predicting the rating of an object by a, a,",00:00:19,13,predicting rating object
4714,00:00:32,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,active user using the ratings of similar users to this active user.,00:00:24,13,active user using ratings similar users active user
4715,00:00:38,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,This is called a memory-based approach because it's a little bit similar to,00:00:32,13,This called memory based approach little bit similar
4716,00:00:44,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,storing all the user information.,00:00:40,13,storing user information
4717,00:00:49,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And when we are considering a particular user, we're going to try to",00:00:44,13,And considering particular user going try
4718,00:00:56,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"kind of retrieve the relevant users, or the similar users through this user case.",00:00:49,13,kind retrieve relevant users similar users user case
4719,00:00:57,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And then try to use that,00:00:56,13,And try use
4720,00:01:03,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,user's information about those users to predict the preference of this user.,00:00:58,13,user information users predict preference user
4721,00:01:11,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So here's the general idea, and we use some notations here, so.",00:01:05,13,So general idea use notations
4722,00:01:16,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,X sub i j denotes the rating of object o j by user u i.,00:01:11,13,X sub j denotes rating object j user u
4723,00:01:23,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And n sub i is average rating of all objects by this user.,00:01:17,13,And n sub average rating objects user
4724,00:01:30,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So this n i is needed.,00:01:26,13,So n needed
4725,00:01:35,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Because we would like to normalize the ratings of objects by this user.,00:01:30,13,Because would like normalize ratings objects user
4726,00:01:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So how do you do normalization?,00:01:35,13,So normalization
4727,00:01:40,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Well, where do you adjust that?",00:01:39,13,Well adjust
4728,00:01:46,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Subtract the, the average rating from all the ratings.",00:01:43,13,Subtract average rating ratings
4729,00:01:49,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Now this is the normalized ratings so,00:01:47,13,Now normalized ratings
4730,00:01:53,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,that the ratings from different users will be comparable.,00:01:49,13,ratings different users comparable
4731,00:02:01,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Because some users might be more generous and they generally give more high ratings.,00:01:55,13,Because users might generous generally give high ratings
4732,00:02:04,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"But, some others might be more critical.",00:02:01,13,But others might critical
4733,00:02:08,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So, their ratings can not be directly compared with each other or",00:02:04,13,So ratings directly compared
4734,00:02:10,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,aggregated them together.,00:02:08,13,aggregated together
4735,00:02:13,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So, we need to do this normalization.",00:02:10,13,So need normalization
4736,00:02:17,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Now, the prediction of the rating.",00:02:13,13,Now prediction rating
4737,00:02:22,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"On the item by another user or active user, u sub a here",00:02:17,13,On item another user active user u sub
4738,00:02:29,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,can be based on the average ratings of similar users.,00:02:24,13,based average ratings similar users
4739,00:02:36,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So the user u sub a is the user that we are interested in recommending items to.,00:02:30,13,So user u sub user interested recommending items
4740,00:02:42,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And we now are interested in recommending this o sub j.,00:02:36,13,And interested recommending sub j
4741,00:02:47,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So we're interested in knowing how likely this user will like this object.,00:02:42,13,So interested knowing likely user like object
4742,00:02:50,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,How do we know that?,00:02:47,13,How know
4743,00:02:55,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Well the idea here is to look at the how whether similar users to this user,00:02:50,13,Well idea look whether similar users user
4744,00:02:57,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,have liked this object.,00:02:55,13,liked object
4745,00:03:01,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So mathematically, this is, as you say,",00:02:59,13,So mathematically say
4746,00:03:07,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"the predict the rating of this user on this app, object.",00:03:01,13,predict rating user app object
4747,00:03:13,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,User A on object Oj is basically combination of,00:03:07,13,User A object Oj basically combination
4748,00:03:18,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,the normalized ratings of different users.,00:03:13,13,normalized ratings different users
4749,00:03:23,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And in fact, here, we're picking a sum of all the users.",00:03:18,13,And fact picking sum users
4750,00:03:29,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But not all users contribute equally to the average.,00:03:23,13,But users contribute equally average
4751,00:03:32,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And this is controlled by the weights.,00:03:29,13,And controlled weights
4752,00:03:33,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So this.,00:03:32,13,So
4753,00:03:40,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Weight controls the inference of a user on the prediction.,00:03:34,13,Weight controls inference user prediction
4754,00:03:47,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And of course, naturally this weight should be related to the similarity",00:03:41,13,And course naturally weight related similarity
4755,00:03:51,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"between ua and this particular user, ui.",00:03:47,13,ua particular user ui
4756,00:03:57,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,The more similar they are then the more contribution we would like,00:03:51,13,The similar contribution would like
4757,00:04:02,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,user u i to make in predicting the preference of u a.,00:03:57,13,user u make predicting preference u
4758,00:04:05,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So the formula is extremely simple.,00:04:03,13,So formula extremely simple
4759,00:04:10,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,You're going to see it's a sum of all the possible users.,00:04:05,13,You going see sum possible users
4760,00:04:13,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And inside the sum, we have their ratings,",00:04:10,13,And inside sum ratings
4761,00:04:17,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,well their normalized ratings as I just explained.,00:04:13,13,well normalized ratings I explained
4762,00:04:21,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,The ratings need to be normalized in order to be comfortable with each other.,00:04:17,13,The ratings need normalized order comfortable
4763,00:04:26,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And then these ratings are rated by their similarity.,00:04:22,13,And ratings rated similarity
4764,00:04:33,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So we can imagine a W of A and I is just a similarity of user A user I.,00:04:26,13,So imagine W A I similarity user A user I
4765,00:04:35,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Now, what's k here?",00:04:34,13,Now k
4766,00:04:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Well, k is a simpler normalizer.",00:04:35,13,Well k simpler normalizer
4767,00:04:45,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"It's just it's just one over the sum of all the weights, over all the users.",00:04:39,13,It one sum weights users
4768,00:04:53,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And so this means, basically, if you consider the weight here together with k.",00:04:47,13,And means basically consider weight together k
4769,00:04:59,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And we have coefficients or weights that would sum to one for all the users.,00:04:53,13,And coefficients weights would sum one users
4770,00:05:05,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And it's just a normalization strategy, so that you get this predicted rating",00:04:59,13,And normalization strategy get predicted rating
4771,00:05:12,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,in the same range as the these ratings that we use to make the prediction.,00:05:05,13,range ratings use make prediction
4772,00:05:14,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Right?,00:05:13,13,Right
4773,00:05:20,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So, this is basically the main idea of memory-based approaches for",00:05:14,13,So basically main idea memory based approaches
4774,00:05:21,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"collaborative filtering, okay?",00:05:20,13,collaborative filtering okay
4775,00:05:29,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Once we make this prediction, we also would like to map back",00:05:22,13,Once make prediction also would like map back
4776,00:05:34,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,to the rating that the user.,00:05:29,13,rating user
4777,00:05:38,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,The user would actually make.,00:05:34,13,The user would actually make
4778,00:05:44,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And this is to further add the, mean rating or",00:05:38,13,And add mean rating
4779,00:05:49,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,average rating of this user u sub a to the predicted value.,00:05:44,13,average rating user u sub predicted value
4780,00:05:51,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,This would recover.,00:05:49,13,This would recover
4781,00:05:54,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,A meaningful rating for this user.,00:05:51,13,A meaningful rating user
4782,00:05:59,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So if this user is generous, then the average would be somewhat high,",00:05:54,13,So user generous average would somewhat high
4783,00:06:04,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"and when we added that, the rating will be adjusted to a relatively high rating.",00:05:59,13,added rating adjusted relatively high rating
4784,00:06:07,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Now, when you recommend an item to a user,",00:06:04,13,Now recommend item user
4785,00:06:12,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,this actually doesn't really matter because you are interested in basically,00:06:07,13,actually really matter interested basically
4786,00:06:16,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,the normalized rating that's more meaningful.,00:06:12,13,normalized rating meaningful
4787,00:06:22,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But when they evaluate these collaborative filtering approach is typically,00:06:17,13,But evaluate collaborative filtering approach typically
4788,00:06:31,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,assumed that actual ratings of user on these objects to be unknown.,00:06:22,13,assumed actual ratings user objects unknown
4789,00:06:33,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And then you do the prediction and,00:06:31,13,And prediction
4790,00:06:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,then you compare the predicted ratings with their actual ratings.,00:06:33,13,compare predicted ratings actual ratings
4791,00:06:42,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So they, you do have access to the actual ratings.",00:06:39,13,So access actual ratings
4792,00:06:44,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But then you pretend you don't know.,00:06:42,13,But pretend know
4793,00:06:48,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And then you compare real systems predictions with the actual ratings.,00:06:44,13,And compare real systems predictions actual ratings
4794,00:06:53,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"In that case, obviously the system's prediction would have to be adjusted to",00:06:48,13,In case obviously system prediction would adjusted
4795,00:06:59,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"match the actual result the user, and this is not what's happening here, basically.",00:06:53,13,match actual result user happening basically
4796,00:07:01,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Okay?,00:07:01,13,Okay
4797,00:07:05,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So this is the memory-based approach.,00:07:01,13,So memory based approach
4798,00:07:06,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Now of course if you look at the formula,",00:07:05,13,Now course look formula
4799,00:07:09,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,if you want to write the program to implement it.,00:07:06,13,want write program implement
4800,00:07:15,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"You still face the problem of determining what is this w function, right?",00:07:09,13,You still face problem determining w function right
4801,00:07:21,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Once you know the w function, then the formula is very easy to implement.",00:07:15,13,Once know w function formula easy implement
4802,00:07:28,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So indeed there are many different ways to compute this function or this weight, w.",00:07:22,13,So indeed many different ways compute function weight w
4803,00:07:33,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And, specific approaches generally differ in how this is computed.",00:07:28,13,And specific approaches generally differ computed
4804,00:07:37,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So, here are some possibilities.",00:07:35,13,So possibilities
4805,00:07:42,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And, you can imagine, there are many pro, other possibilities.",00:07:37,13,And imagine many pro possibilities
4806,00:07:46,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,One popular approach is we use the Pearson Correlation Coefficient.,00:07:42,13,One popular approach use Pearson Correlation Coefficient
4807,00:07:53,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"This would be a sum of a common range of items, and the formula",00:07:48,13,This would sum common range items formula
4808,00:07:58,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"is a standard Pearson correlation coefficient formula, as shown here.",00:07:53,13,standard Pearson correlation coefficient formula shown
4809,00:08:05,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So, this basically measures weather the two users tended",00:07:59,13,So basically measures weather two users tended
4810,00:08:11,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"to all give higher ratings to similar items, or lower ratings to similar items.",00:08:05,13,give higher ratings similar items lower ratings similar items
4811,00:08:17,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Another measure is the cosine measure and this is the retreat the rating vectors as,00:08:11,13,Another measure cosine measure retreat rating vectors
4812,00:08:23,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"vectors in the vector space, and then we're going to measure the the angel and",00:08:17,13,vectors vector space going measure angel
4813,00:08:27,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,compute the cosign of the angle of the two vectors.,00:08:23,13,compute cosign angle two vectors
4814,00:08:32,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And this measure has been used in the vector space more for retrieval as well.,00:08:27,13,And measure used vector space retrieval well
4815,00:08:36,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So as you can imagine, there are so many different ways of doing that.",00:08:32,13,So imagine many different ways
4816,00:08:41,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"In all these cases, note that the user similarity is based on their preferences",00:08:36,13,In cases note user similarity based preferences
4817,00:08:47,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"on items, and we did not actually use any content information of these items.",00:08:41,13,items actually use content information items
4818,00:08:49,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,It didn't matter what these items are.,00:08:47,13,It matter items
4819,00:08:52,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"They can be movies, they can be books,",00:08:49,13,They movies books
4820,00:08:55,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"they can be products, they can be tax documents.",00:08:52,13,products tax documents
4821,00:08:57,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,We just didn't care about the content.,00:08:55,13,We care content
4822,00:09:07,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And so this allows such approach to be applied to a wide range of problems.,00:08:57,13,And allows approach applied wide range problems
4823,00:09:08,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Now in some newer approaches of course,",00:09:07,13,Now newer approaches course
4824,00:09:11,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,we would like to use more information about the user.,00:09:08,13,would like use information user
4825,00:09:18,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Clearly, we know more about the user, not just a, these preferences on these items.",00:09:11,13,Clearly know user preferences items
4826,00:09:24,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And so in a actual filtering system, using collaborative filtering, we could also",00:09:18,13,And actual filtering system using collaborative filtering could also
4827,00:09:30,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"combine that with content-based filtering, we could use context information.",00:09:24,13,combine content based filtering could use context information
4828,00:09:36,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And those are all interesting approaches that people are still studying.,00:09:30,13,And interesting approaches people still studying
4829,00:09:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,There are newer approaches proposed.,00:09:36,13,There newer approaches proposed
4830,00:09:43,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But this approach has been shown to work reasonably well and,00:09:39,13,But approach shown work reasonably well
4831,00:09:45,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,it's easy to implement.,00:09:43,13,easy implement
4832,00:09:49,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And practical applications could be a starting point to,00:09:45,13,And practical applications could starting point
4833,00:09:53,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,see if the strand here works well for your application.,00:09:49,13,see strand works well application
4834,00:10:01,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So there are some obvious ways to also improve this approach.,00:09:56,13,So obvious ways also improve approach
4835,00:10:07,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And mainly would like to improve the user similarity measure.,00:10:01,13,And mainly would like improve user similarity measure
4836,00:10:09,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And there are some practical issues to deal with here as well.,00:10:07,13,And practical issues deal well
4837,00:10:11,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So for example, there will be a lot of missing values.",00:10:09,13,So example lot missing values
4838,00:10:12,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,What do you do with them?,00:10:11,13,What
4839,00:10:18,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"Well, you can set them to default values or the average ratings of the user.",00:10:12,13,Well set default values average ratings user
4840,00:10:20,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,And that will be a simple solution.,00:10:18,13,And simple solution
4841,00:10:26,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But there are advantages to approaches that can actually try to predict those,00:10:20,13,But advantages approaches actually try predict
4842,00:10:33,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,missing values and then use the predicted values to improve the similarity.,00:10:26,13,missing values use predicted values improve similarity
4843,00:10:35,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So in fact, the memory database approach,",00:10:33,13,So fact memory database approach
4844,00:10:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"you can predict those with missing values, right?",00:10:35,13,predict missing values right
4845,00:10:39,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"So you can imagine,",00:10:39,13,So imagine
4846,00:10:44,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,you have iterative approach where you first do some preliminary prediction and,00:10:39,13,iterative approach first preliminary prediction
4847,00:10:48,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,then you can use the predictor values to further improve the similarity function.,00:10:44,13,use predictor values improve similarity function
4848,00:10:54,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Right so this is here is a way to solve the problem.,00:10:48,13,Right way solve problem
4849,00:10:58,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And the strategy of this in the effect of the performance of clarity filtering,",00:10:54,13,And strategy effect performance clarity filtering
4850,00:11:05,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"just like in the other heuristics, we improve the similarity function.",00:11:00,13,like heuristics improve similarity function
4851,00:11:10,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Another idea which is actually very similar to the idea of IDF that we,00:11:06,13,Another idea actually similar idea IDF
4852,00:11:15,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"have seen in text research, is called the inverse user frequency or IUF.",00:11:10,13,seen text research called inverse user frequency IUF
4853,00:11:23,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,Now here the idea is to look at the where the two users share similar ratings.,00:11:15,13,Now idea look two users share similar ratings
4854,00:11:30,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"If the item is a popular item that has been aah, viewed by many people and",00:11:24,13,If item popular item aah viewed many people
4855,00:11:35,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,seemingly leads to people interested in this item may not be so interesting.,00:11:30,13,seemingly leads people interested item may interesting
4856,00:11:40,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,But if it's a rare item and has not been viewed by many users.,00:11:36,13,But rare item viewed many users
4857,00:11:42,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"But, these two users [INAUDIBLE] to this item.",00:11:40,13,But two users INAUDIBLE item
4858,00:11:47,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,"And they give similar ratings, and it says more about their similarity, right?",00:11:42,13,And give similar ratings says similarity right
4859,00:11:52,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,So it's kind of to emphasize more on similarity,00:11:47,13,So kind emphasize similarity
4860,00:11:56,5,Recommender Systems- Collaborative Filtering - Part 2,4.7,on items that are not viewed by many users.,00:11:52,13,items viewed many users
4861,00:00:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,[MUSIC],00:00:00,6,MUSIC
4862,00:00:11,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"This lecture is about, how we can evaluate a ranked list?",00:00:07,6,This lecture evaluate ranked list
4863,00:00:17,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In this lecture, we will continue the discussion of evaluation.",00:00:13,6,In lecture continue discussion evaluation
4864,00:00:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In particular,",00:00:17,6,In particular
4865,00:00:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we are going to look at, how we can evaluate a ranked list of results.",00:00:18,6,going look evaluate ranked list results
4866,00:00:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In the previous lecture, we talked about, precision-recall.",00:00:24,6,In previous lecture talked precision recall
4867,00:00:33,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"These are the two basic measures for,",00:00:30,6,These two basic measures
4868,00:00:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,quantitatively measuring the performance of a search result.,00:00:33,6,quantitatively measuring performance search result
4869,00:00:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But, as we talked about, ranking, before,",00:00:40,6,But talked ranking
4870,00:00:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we framed that the text of retrieval problem, as a ranking problem.",00:00:44,6,framed text retrieval problem ranking problem
4871,00:00:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, we also need to evaluate the, the quality of a ranked list.",00:00:50,6,So also need evaluate quality ranked list
4872,00:01:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"How can we use precision-recall to evaluate, a ranked list?",00:00:56,6,How use precision recall evaluate ranked list
4873,00:01:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, naturally, we have to look after the precision-recall at different, cut-offs.",00:01:01,6,Well naturally look precision recall different cut offs
4874,00:01:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Because in the end, the approximation of relevant documents, set,",00:01:07,6,Because end approximation relevant documents set
4875,00:01:17,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"given by a ranked list, is determined by where the user stops browsing.",00:01:12,6,given ranked list determined user stops browsing
4876,00:01:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Right? If we assume the user, securely browses,",00:01:17,6,Right If assume user securely browses
4877,00:01:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"the list of results, the user would, stop at some point, and",00:01:21,6,list results user would stop point
4878,00:01:27,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,that point would determine the set.,00:01:25,6,point would determine set
4879,00:01:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And then, that's the most important, cut-off,",00:01:27,6,And important cut
4880,00:01:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"that we have to consider, when we compute the precision-recall.",00:01:31,6,consider compute precision recall
4881,00:01:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Without knowing where exactly user would stop,",00:01:35,6,Without knowing exactly user would stop
4882,00:01:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"then we have to consider, all the positions where the user could stop.",00:01:37,6,consider positions user could stop
4883,00:01:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, let's look at these positions.",00:01:42,6,So let look positions
4884,00:01:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Look at this slide, and then, let's look at the,",00:01:44,6,Look slide let look
4885,00:01:51,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"what if the user stops at the, the first document?",00:01:49,6,user stops first document
4886,00:01:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What's the precision-recall at this point?,00:01:51,6,What precision recall point
4887,00:01:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What do you think?,00:01:55,6,What think
4888,00:02:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, it's easy to see, that this document is So, the precision is one out of one.",00:01:56,6,Well easy see document So precision one one
4889,00:02:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"We have, got one document, and that's relevent.",00:02:02,6,We got one document relevent
4890,00:02:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What about the recall?,00:02:05,6,What recall
4891,00:02:11,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, note that, we're assuming that, there are ten relevant documents, for",00:02:07,6,Well note assuming ten relevant documents
4892,00:02:14,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"this query in the collection, so, it's one out of ten.",00:02:11,6,query collection one ten
4893,00:02:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What if the user stops at the second position?,00:02:16,6,What user stops second position
4894,00:02:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,Top two.,00:02:19,6,Top two
4895,00:02:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, the precision is the same, 100%, two out of two.",00:02:21,6,Well precision 100 two two
4896,00:02:27,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, the record is two out of ten.",00:02:25,6,And record two ten
4897,00:02:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What if the user stops at the third position?,00:02:28,6,What user stops third position
4898,00:02:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, this is interesting, because in this case, we have not got any,",00:02:31,6,Well interesting case got
4899,00:02:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"additional relevant document, so, the record does not change.",00:02:35,6,additional relevant document record change
4900,00:02:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But the precision is lower, because we've got number [INAUDIBLE] so,",00:02:41,6,But precision lower got number INAUDIBLE
4901,00:02:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,what's exactly the precision?,00:02:45,6,exactly precision
4902,00:02:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, it's two out of three, right?",00:02:49,6,Well two three right
4903,00:02:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, recall is the same, two out of ten.",00:02:52,6,And recall two ten
4904,00:02:58,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, when would see another point, where the recall would be different?",00:02:54,6,So would see another point recall would different
4905,00:03:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, if you look down the list, well, it won't happen until,",00:02:58,6,Now look list well happen
4906,00:03:06,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we have, seeing another relevant document.",00:03:02,6,seeing another relevant document
4907,00:03:10,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In this case D5, at that point, the, the recall is increased through",00:03:06,6,In case D5 point recall increased
4908,00:03:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"three out of ten, and, the precision is three out of five.",00:03:10,6,three ten precision three five
4909,00:03:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, you can see, if we keep doing this, we can also get to D8.",00:03:15,6,So see keep also get D8
4910,00:03:23,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And then, we will have a precision of four out of eight,",00:03:20,6,And precision four eight
4911,00:03:26,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"because there are eight documents, and four of them are relevant.",00:03:23,6,eight documents four relevant
4912,00:03:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, the recall is a four out of ten.",00:03:26,6,And recall four ten
4913,00:03:33,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, when can we get, a recall of five out of ten?",00:03:29,6,Now get recall five ten
4914,00:03:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, in this list, we don't have it, so, we have to go down on the list.",00:03:33,6,Well list go list
4915,00:03:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"We don't know, where it is?",00:03:39,6,We know
4916,00:03:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But, as convenience, we often assume that, the precision is zero,",00:03:40,6,But convenience often assume precision zero
4917,00:03:51,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"at all the, the othe, the precision are zero at",00:03:47,6,othe precision zero
4918,00:03:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"all the other levels of recall, that are beyond the search results.",00:03:51,6,levels recall beyond search results
4919,00:03:59,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, of course, this is a pessimistic assumption,",00:03:56,6,So course pessimistic assumption
4920,00:04:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"the actual position would be higher, but we make, make this assumption,",00:03:59,6,actual position would higher make make assumption
4921,00:04:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"in order to, have an easy way to,",00:04:05,6,order easy way
4922,00:04:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"compute another measure called Average Precision, that we will discuss later.",00:04:09,6,compute another measure called Average Precision discuss later
4923,00:04:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, I should also say, now, here you see,",00:04:14,6,Now I also say see
4924,00:04:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we make these assumptions that are clearly not, accurate.",00:04:16,6,make assumptions clearly accurate
4925,00:04:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But, this is okay, for the purpose of comparing to, text methods.",00:04:22,6,But okay purpose comparing text methods
4926,00:04:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this is for the relative comparison, so, it's okay, if the actual measure,",00:04:28,6,And relative comparison okay actual measure
4927,00:04:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"or actual, actual number deviates a little bit, from the true number.",00:04:34,6,actual actual number deviates little bit true number
4928,00:04:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"As long as the deviation,",00:04:39,6,As long deviation
4929,00:04:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"is not biased toward any particular retrieval method, we are okay.",00:04:41,6,biased toward particular retrieval method okay
4930,00:04:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"We can still, accurately tell which method works better.",00:04:46,6,We still accurately tell method works better
4931,00:04:53,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this is important point, to keep in mind.",00:04:50,6,And important point keep mind
4932,00:04:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"When you compare different algorithms,",00:04:53,6,When compare different algorithms
4933,00:04:58,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,the key's to avoid any bias toward each method.,00:04:55,6,key avoid bias toward method
4934,00:05:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, as long as, you can avoid that.",00:04:58,6,And long avoid
4935,00:05:06,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"It's okay, for you to do transformation of these measures anyway, so,",00:05:02,6,It okay transformation measures anyway
4936,00:05:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,you can preserve the order.,00:05:06,6,preserve order
4937,00:05:11,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Okay, so, we'll just talk about,",00:05:09,6,Okay talk
4938,00:05:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,we can get a lot of precision-recall numbers at different positions.,00:05:11,6,get lot precision recall numbers different positions
4939,00:05:19,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, now, you can imagine, we can plot a curve.",00:05:16,6,So imagine plot curve
4940,00:05:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this just shows on the, x-axis, we show the recalls.",00:05:19,6,And shows x axis show recalls
4941,00:05:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, on the y-axis, we show the precision.",00:05:23,6,And axis show precision
4942,00:05:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, the precision line was marked as .1, .2, .3, and, 1.0.",00:05:30,6,So precision line marked 1 2 3 1 0
4943,00:05:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Right? So,",00:05:35,6,Right So
4944,00:05:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"this is, the different, levels of recall.",00:05:35,6,different levels recall
4945,00:05:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And,, the y-axis also has, different amounts, that's for precision.",00:05:38,6,And axis also different amounts precision
4946,00:05:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, we plot the, these, precision-recall numbers, that we have got,",00:05:45,6,So plot precision recall numbers got
4947,00:05:51,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,as points on this picture.,00:05:49,6,points picture
4948,00:05:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, we can further, and link these points to form a curve.",00:05:51,6,Now link points form curve
4949,00:05:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"As you'll see,",00:05:56,6,As see
4950,00:06:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we assumed all the other, precision as the high-level recalls, be zero.",00:05:57,6,assumed precision high level recalls zero
4951,00:06:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, that's why, they are down here, so, they are all zero.",00:06:02,6,And zero
4952,00:06:14,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And this, the actual curve probably will be something like this, but, as we just",00:06:08,6,And actual curve probably something like
4953,00:06:19,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"discussed, it, it doesn't matter that much, for comparing two methods.",00:06:14,6,discussed matter much comparing two methods
4954,00:06:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"because this would be, underestimated, for all the method.",00:06:20,6,would underestimated method
4955,00:06:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Okay, so, now that we, have this precision-recall curve,",00:06:25,6,Okay precision recall curve
4956,00:06:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,how can we compare ranked to back list?,00:06:31,6,compare ranked back list
4957,00:06:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"All right, so, that means, we have to compare two PR curves.",00:06:34,6,All right means compare two PR curves
4958,00:06:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And here, we show, two cases.",00:06:38,6,And show two cases
4959,00:06:47,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Where system A is showing red, system B is showing blue, there's crosses.",00:06:40,6,Where system A showing red system B showing blue crosses
4960,00:06:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"All right, so, which one is better?",00:06:48,6,All right one better
4961,00:06:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"I hope you can see, where system A is clearly better.",00:06:50,6,I hope see system A clearly better
4962,00:06:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Why? Because, for the same level of recall,",00:06:54,6,Why Because level recall
4963,00:07:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"see same level of recall here, and you can see,",00:06:58,6,see level recall see
4964,00:07:06,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"the precision point by system A is better, system B.",00:07:01,6,precision point system A better system B
4965,00:07:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, there's no question.",00:07:06,6,So question
4966,00:07:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In here, you can imagine, what does the code look like, for ideal search system?",00:07:08,6,In imagine code look like ideal search system
4967,00:07:17,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, it has to have perfect, precision at all the recall points, so,",00:07:13,6,Well perfect precision recall points
4968,00:07:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,it has to be this line.,00:07:17,6,line
4969,00:07:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,That would be the ideal system.,00:07:18,6,That would ideal system
4970,00:07:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In general, the higher the curve is, the better, right?",00:07:21,6,In general higher curve better right
4971,00:07:27,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"The problem is that, we might see a case like this.",00:07:24,6,The problem might see case like
4972,00:07:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,This actually happens often.,00:07:27,6,This actually happens often
4973,00:07:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Like, the two curves cross each other.",00:07:29,6,Like two curves cross
4974,00:07:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, in this case, which one is better?",00:07:32,6,Now case one better
4975,00:07:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What do you think?,00:07:35,6,What think
4976,00:07:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, this is a real problem, that you actually, might have face.",00:07:38,6,Now real problem actually might face
4977,00:07:47,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Suppose, you build a search engine, and you have a old algorithm,",00:07:41,6,Suppose build search engine old algorithm
4978,00:07:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"that's shown here in blue, or system B.",00:07:47,6,shown blue system B
4979,00:07:53,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, you have come up with a new idea.",00:07:50,6,And come new idea
4980,00:07:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, you test it.",00:07:53,6,And test
4981,00:07:58,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, the results are shown in red, curve A.",00:07:54,6,And results shown red curve A
4982,00:08:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, your question is, is your new method better than the old method?",00:07:59,6,Now question new method better old method
4983,00:08:10,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Or more, practically, do you have to replace the algorithm that",00:08:05,6,Or practically replace algorithm
4984,00:08:15,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"you're already using, your, in your search engine, with another, new algorithm?",00:08:10,6,already using search engine another new algorithm
4985,00:08:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, should we use system, method A, to replace method B?",00:08:15,6,So use system method A replace method B
4986,00:08:23,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"This is going to be a real decision, that you to have to make.",00:08:20,6,This going real decision make
4987,00:08:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"If you make the replacement, the search engine would behave like system A here,",00:08:23,6,If make replacement search engine would behave like system A
4988,00:08:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"whereas, if you don't do that, it will be like a system B.",00:08:29,6,whereas like system B
4989,00:08:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, what do you do?",00:08:34,6,So
4990,00:08:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, if you want to spend more time to think about this, pause the video.",00:08:36,6,Now want spend time think pause video
4991,00:08:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, it's actually very useful to think about that.",00:08:40,6,And actually useful think
4992,00:08:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"As I said, it's a real decision that you have to make, if you are building your own",00:08:42,6,As I said real decision make building
4993,00:08:51,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"search engine, or if you're working, for a company that, cares about the search.",00:08:46,6,search engine working company cares search
4994,00:08:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, if you have thought about this for",00:08:52,6,Now thought
4995,00:08:59,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"a moment, you might realize that, well, in this case, it's hard to say.",00:08:54,6,moment might realize well case hard say
4996,00:09:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, some users might like a system A, some users might like, like system B.",00:08:59,6,Now users might like system A users might like like system B
4997,00:09:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, what's the difference here?",00:09:04,6,So difference
4998,00:09:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, the difference is just that, you know,",00:09:05,6,Well difference know
4999,00:09:14,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"in the, low level of recall, in this region, system B is better.",00:09:08,6,low level recall region system B better
5000,00:09:15,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,There's a higher precision.,00:09:14,6,There higher precision
5001,00:09:19,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But in high recall region, system A is better.",00:09:15,6,But high recall region system A better
5002,00:09:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, so, that also means, it depends on whether the user",00:09:20,6,Now also means depends whether user
5003,00:09:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"cares about the high recall, or low recall, but high precision.",00:09:24,6,cares high recall low recall high precision
5004,00:09:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"You can imagine, if someone is just going to check out, what's happening today, and",00:09:28,6,You imagine someone going check happening today
5005,00:09:33,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,want to find out something relevant in the news.,00:09:32,6,want find something relevant news
5006,00:09:36,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, which one is better?",00:09:34,6,Well one better
5007,00:09:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,What do you think?,00:09:36,6,What think
5008,00:09:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In this case, clearly, system B is better,",00:09:38,6,In case clearly system B better
5009,00:09:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,because the user is unlikely examining a lot of results.,00:09:41,6,user unlikely examining lot results
5010,00:09:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,The user doesn't care about high recall.,00:09:44,6,The user care high recall
5011,00:09:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"On the other hand, if you think about a case,",00:09:47,6,On hand think case
5012,00:09:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"where a user is doing you are, starting a problem.",00:09:50,6,user starting problem
5013,00:10:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"You want to find, whether your idea ha, has been started before.",00:09:54,6,You want find whether idea ha started
5014,00:10:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In that case, you emphasize high recall.",00:10:00,6,In case emphasize high recall
5015,00:10:06,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, you want to see, as many relevant documents as possible.",00:10:03,6,So want see many relevant documents possible
5016,00:10:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Therefore, you might, favor, system A.",00:10:06,6,Therefore might favor system A
5017,00:10:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, that means, which one is better?",00:10:09,6,So means one better
5018,00:10:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"That actually depends on users, and more precisely, users task.",00:10:12,6,That actually depends users precisely users task
5019,00:10:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, this means, you may not necessarily be able to come up with one number,",00:10:19,6,So means may necessarily able come one number
5020,00:10:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,that would accurately depict the performance.,00:10:25,6,would accurately depict performance
5021,00:10:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,You have to look at the overall picture.,00:10:29,6,You look overall picture
5022,00:10:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Yet, as I said, when you have a practical decision to make,",00:10:31,6,Yet I said practical decision make
5023,00:10:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"whether you replace ours with another,",00:10:35,6,whether replace another
5024,00:10:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"then you may have to actually come up with a single number, to quantify each, method.",00:10:38,6,may actually come single number quantify method
5025,00:10:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Or, when we compare many different methods in research, ideally, we have",00:10:44,6,Or compare many different methods research ideally
5026,00:10:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"one number to compare, them with, so, that we can easily make a lot of comparisons.",00:10:49,6,one number compare easily make lot comparisons
5027,00:11:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, for all these reasons, it is desirable to have one, single number to match it up.",00:10:54,6,So reasons desirable one single number match
5028,00:11:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, how do we do that?",00:11:00,6,So
5029,00:11:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, that, needs a number to summarize the range.",00:11:01,6,And needs number summarize range
5030,00:11:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, here again it's the precision-recall curve, right?",00:11:05,6,So precision recall curve right
5031,00:11:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, one way to summarize this whole ranked, list, for",00:11:09,6,And one way summarize whole ranked list
5032,00:11:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"this whole curve, is look at the area underneath the curve.",00:11:13,6,whole curve look area underneath curve
5033,00:11:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Right? So, this is one way to measure that.",00:11:19,6,Right So one way measure
5034,00:11:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"There are other ways to measure that, but, it just turns out that,,",00:11:21,6,There ways measure turns
5035,00:11:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"this particular way of matching it has been very, popular, and",00:11:26,6,particular way matching popular
5036,00:11:36,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"has been used, since a long time ago for text And, this is,",00:11:31,6,used since long time ago text And
5037,00:11:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"basically, in this way, and it's called the average precision.",00:11:36,6,basically way called average precision
5038,00:11:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Basically, we're going to take a, a look at the, every different, recall point.",00:11:41,6,Basically going take look every different recall point
5039,00:11:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And then, look out for the precision.",00:11:47,6,And look precision
5040,00:11:51,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, we know, you know, this is one precision.",00:11:49,6,So know know one precision
5041,00:11:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this is another, with, different recall.",00:11:51,6,And another different recall
5042,00:11:59,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, this, we don't count to this one,",00:11:56,6,Now count one
5043,00:12:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"because the recall level is the same, and we're going to, look at the,",00:11:59,6,recall level going look
5044,00:12:10,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"this number, and that's precision at a different recall level et cetera.",00:12:04,6,number precision different recall level et cetera
5045,00:12:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, we have all these, you know, added up.",00:12:10,6,So know added
5046,00:12:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"These are the precisions at the different points,",00:12:13,6,These precisions different points
5047,00:12:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"corresponding to retrieving the first relevant document, the second, and",00:12:16,6,corresponding retrieving first relevant document second
5048,00:12:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"then, the third, that follows, et cetera.",00:12:21,6,third follows et cetera
5049,00:12:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, we missed the many relevant documents, so, in all of those cases,",00:12:25,6,Now missed many relevant documents cases
5050,00:12:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we just, assume, that they have zero precisions.",00:12:29,6,assume zero precisions
5051,00:12:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And then, finally, we take the average.",00:12:33,6,And finally take average
5052,00:12:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, we divide it by ten, and",00:12:35,6,So divide ten
5053,00:12:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,which is the total number of relevant documents in the collection.,00:12:37,6,total number relevant documents collection
5054,00:12:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Note that here, we're not dividing this sum by four.",00:12:41,6,Note dividing sum four
5055,00:12:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,Which is a number retrieved relevant documents.,00:12:46,6,Which number retrieved relevant documents
5056,00:12:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, imagine, if I divide by four, what would happen?",00:12:49,6,Now imagine I divide four would happen
5057,00:12:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Now, think about this, for a moment.",00:12:54,6,Now think moment
5058,00:13:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"It's a common mistake that people, sometimes, overlook.",00:12:57,6,It common mistake people sometimes overlook
5059,00:13:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Right, so, if we, we divide this by four, it's actually not very good.",00:13:02,6,Right divide four actually good
5060,00:13:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In fact, that you are favoring a system, that would retrieve very few random",00:13:08,6,In fact favoring system would retrieve random
5061,00:13:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"documents, as in that case, the denominator would be very small.",00:13:13,6,documents case denominator would small
5062,00:13:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, this would be, not a good matching.",00:13:18,6,So would good matching
5063,00:13:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, note that this denomina, denominator is ten,",00:13:22,6,So note denomina denominator ten
5064,00:13:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,the total number of relevant documents.,00:13:25,6,total number relevant documents
5065,00:13:33,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this will basically ,compute the area, and the needs occur.",00:13:29,6,And basically compute area needs occur
5066,00:13:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, this is the standard method, used for evaluating a ranked list.",00:13:33,6,And standard method used evaluating ranked list
5067,00:13:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Note that, it actually combines recall and, precision.",00:13:41,6,Note actually combines recall precision
5068,00:13:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But first, you know, we have precision numbers here, but secondly,",00:13:44,6,But first know precision numbers secondly
5069,00:13:53,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"we also consider recall, because if missed many, there would be many zeros here.",00:13:49,6,also consider recall missed many would many zeros
5070,00:13:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"All right, so, it combines precision and recall.",00:13:53,6,All right combines precision recall
5071,00:14:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And furthermore, you can see this measure is sensitive to a small change",00:13:57,6,And furthermore see measure sensitive small change
5072,00:14:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,of a position of a relevant document.,00:14:02,6,position relevant document
5073,00:14:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Let's say, if I move this relevant document up a little bit, now,",00:14:04,6,Let say I move relevant document little bit
5074,00:14:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"it would increase this means, this average precision.",00:14:09,6,would increase means average precision
5075,00:14:17,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Whereas, if I move any relevant document, down, let's say, I move this relevant",00:14:12,6,Whereas I move relevant document let say I move relevant
5076,00:14:23,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"document down, then it would decrease, uh,the average precision.",00:14:17,6,document would decrease uh average precision
5077,00:14:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, this is a very good,",00:14:23,6,So good
5078,00:14:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,because it's a very sensitive to the ranking of every relevant document.,00:14:25,6,sensitive ranking every relevant document
5079,00:14:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"It can tell, small differences between two ranked lists.",00:14:30,6,It tell small differences two ranked lists
5080,00:14:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, that is what we want,",00:14:34,6,And want
5081,00:14:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,sometimes one algorithm only works slightly better than another.,00:14:35,6,sometimes one algorithm works slightly better another
5082,00:14:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"And, we want to see this difference.",00:14:40,6,And want see difference
5083,00:14:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In contrast, if we look at the precision at the ten documents.",00:14:42,6,In contrast look precision ten documents
5084,00:14:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"If we look at this, this whole set, well,",00:14:46,6,If look whole set well
5085,00:14:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"what, what's the precision, what do you think?",00:14:49,6,precision think
5086,00:14:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Well, it's easy to see, that's a four out of ten, right?",00:14:52,6,Well easy see four ten right
5087,00:15:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, that precision is very meaningful, because it tells us, what user would see?",00:14:54,6,So precision meaningful tells us user would see
5088,00:15:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, that's pretty useful, right?",00:15:02,6,So pretty useful right
5089,00:15:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"So, it's a meaningful measure, from a users perspective.",00:15:04,6,So meaningful measure users perspective
5090,00:15:11,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"But, if we use this measure to compare systems, it wouldn't be good,",00:15:07,6,But use measure compare systems good
5091,00:15:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,because it wouldn't be sensitive to where these four relevant documents are ranked.,00:15:11,6,sensitive four relevant documents ranked
5092,00:15:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"If I move them around the precision at ten, still, the same.",00:15:16,6,If I move around precision ten still
5093,00:15:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"Right. So,",00:15:21,6,Right So
5094,00:15:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,this is not a good measure for comparing different algorithms.,00:15:22,6,good measure comparing different algorithms
5095,00:15:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"In contrast, the average precision is a much better measure.",00:15:25,6,In contrast average precision much better measure
5096,00:15:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"It can tell the difference of, different,",00:15:29,6,It tell difference different
5097,00:15:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 1,2.6,"a difference in ranked list in, subtle ways.",00:15:34,6,difference ranked list subtle ways
5098,00:00:04,3,Implementation of TR Systems,2.1,[SOUND] This lecture is about,00:00:00,1,SOUND This lecture
5099,00:00:12,3,Implementation of TR Systems,2.1,the implementation of text retrieval systems.,00:00:04,1,implementation text retrieval systems
5100,00:00:17,3,Implementation of TR Systems,2.1,"In this lecture, we will discuss how we can implement a text retrieval method",00:00:12,1,In lecture discuss implement text retrieval method
5101,00:00:19,3,Implementation of TR Systems,2.1,to build a search engine.,00:00:17,1,build search engine
5102,00:00:23,3,Implementation of TR Systems,2.1,The main challenge is to manage a lot of text data and,00:00:19,1,The main challenge manage lot text data
5103,00:00:29,3,Implementation of TR Systems,2.1,to enable a query to be answered very quickly and to respond to many queries.,00:00:23,1,enable query answered quickly respond many queries
5104,00:00:35,3,Implementation of TR Systems,2.1,This is a typical text retrieval system architecture.,00:00:29,1,This typical text retrieval system architecture
5105,00:00:39,3,Implementation of TR Systems,2.1,We can see the documents are first processed by a tokenizer,00:00:35,1,We see documents first processed tokenizer
5106,00:00:43,3,Implementation of TR Systems,2.1,"to get tokenizer units, for example words.",00:00:39,1,get tokenizer units example words
5107,00:00:45,3,Implementation of TR Systems,2.1,And then these words or,00:00:43,1,And words
5108,00:00:51,3,Implementation of TR Systems,2.1,"tokens would be processed by an indexer that would create an index,",00:00:45,1,tokens would processed indexer would create index
5109,00:00:56,3,Implementation of TR Systems,2.1,which is a data structure for the search engine to use to quickly answer a query.,00:00:51,1,data structure search engine use quickly answer query
5110,00:01:02,3,Implementation of TR Systems,2.1,And the query will be going through a similar processing step.,00:00:57,1,And query going similar processing step
5111,00:01:05,3,Implementation of TR Systems,2.1,"So, the tokenizer will be apprised to query as well so",00:01:02,1,So tokenizer apprised query well
5112,00:01:09,3,Implementation of TR Systems,2.1,that the text can be processed in the same way.,00:01:05,1,text processed way
5113,00:01:11,3,Implementation of TR Systems,2.1,The same units will be matched with each other.,00:01:09,1,The units matched
5114,00:01:17,3,Implementation of TR Systems,2.1,And the query's representation will then be given to the scorer.,00:01:11,1,And query representation given scorer
5115,00:01:21,3,Implementation of TR Systems,2.1,Which would use a index to quickly answer a user's query by,00:01:17,1,Which would use index quickly answer user query
5116,00:01:25,3,Implementation of TR Systems,2.1,scoring the documents and then ranking them.,00:01:21,1,scoring documents ranking
5117,00:01:27,3,Implementation of TR Systems,2.1,The results will be given to the user.,00:01:25,1,The results given user
5118,00:01:32,3,Implementation of TR Systems,2.1,And then the user can look at the results and and provide some feedback that can be,00:01:27,1,And user look results provide feedback
5119,00:01:37,3,Implementation of TR Systems,2.1,"expressed judgements about which documents are good, which documents are bad,",00:01:32,1,expressed judgements documents good documents bad
5120,00:01:43,3,Implementation of TR Systems,2.1,"or implicit feedback such as pixels so the user doesn't have to any, anything extra.",00:01:37,1,implicit feedback pixels user anything extra
5121,00:01:46,3,Implementation of TR Systems,2.1,The user will just look at the results and skip some and,00:01:43,1,The user look results skip
5122,00:01:49,3,Implementation of TR Systems,2.1,click on some results to view.,00:01:46,1,click results view
5123,00:01:55,3,Implementation of TR Systems,2.1,So these interaction signals can be used by the system to improve the ranking,00:01:49,1,So interaction signals used system improve ranking
5124,00:02:01,3,Implementation of TR Systems,2.1,accuracy by assuming that viewed documents are better than the skipped ones.,00:01:55,1,accuracy assuming viewed documents better skipped ones
5125,00:02:05,3,Implementation of TR Systems,2.1,"So, a search engine system then can be divided into three parts.",00:02:01,1,So search engine system divided three parts
5126,00:02:10,3,Implementation of TR Systems,2.1,"The first part is the indexer, and the second part is the scorer,",00:02:05,1,The first part indexer second part scorer
5127,00:02:13,3,Implementation of TR Systems,2.1,that responds to the user's query.,00:02:10,1,responds user query
5128,00:02:15,3,Implementation of TR Systems,2.1,And the third part is the feedback mechanism.,00:02:13,1,And third part feedback mechanism
5129,00:02:22,3,Implementation of TR Systems,2.1,"Now typically, the indexer is done in the offline manner so you can pre-process",00:02:16,1,Now typically indexer done offline manner pre process
5130,00:02:28,3,Implementation of TR Systems,2.1,the correct data and to build the inverter index which we will introduce in a moment.,00:02:22,1,correct data build inverter index introduce moment
5131,00:02:35,3,Implementation of TR Systems,2.1,And this data structure can then be used by the online module which is a scorer,00:02:29,1,And data structure used online module scorer
5132,00:02:40,3,Implementation of TR Systems,2.1,to process a user's query dynamically and quickly generate search results.,00:02:35,1,process user query dynamically quickly generate search results
5133,00:02:44,3,Implementation of TR Systems,2.1,The feedback mechanism can be done online or offline depending on the method.,00:02:40,1,The feedback mechanism done online offline depending method
5134,00:02:50,3,Implementation of TR Systems,2.1,"The implementation of the index and the, the scorer is fairly standard,",00:02:44,1,The implementation index scorer fairly standard
5135,00:02:55,3,Implementation of TR Systems,2.1,and this is the main topic of this lecture and the next few lectures.,00:02:50,1,main topic lecture next lectures
5136,00:02:58,3,Implementation of TR Systems,2.1,"The feedback mechanism, on the other hand has variations.",00:02:55,1,The feedback mechanism hand variations
5137,00:03:01,3,Implementation of TR Systems,2.1,It depends on what method is used.,00:02:58,1,It depends method used
5138,00:03:07,3,Implementation of TR Systems,2.1,So that is usually done in a algorithm-specific way.,00:03:01,1,So usually done algorithm specific way
5139,00:03:10,3,Implementation of TR Systems,2.1,Let's first talk about the tokenize.,00:03:09,1,Let first talk tokenize
5140,00:03:15,3,Implementation of TR Systems,2.1,Tokenization is a normalize lexical units into the same form so,00:03:11,1,Tokenization normalize lexical units form
5141,00:03:19,3,Implementation of TR Systems,2.1,that semantically similar words can be matched with each other.,00:03:15,1,semantically similar words matched
5142,00:03:24,3,Implementation of TR Systems,2.1,Now in the language of English stemming is often used and,00:03:19,1,Now language English stemming often used
5143,00:03:29,3,Implementation of TR Systems,2.1,this what map all the inflectional forms of words into the same root form.,00:03:24,1,map inflectional forms words root form
5144,00:03:32,3,Implementation of TR Systems,2.1,"So for example, computer computation and",00:03:29,1,So example computer computation
5145,00:03:37,3,Implementation of TR Systems,2.1,computing can all be matched to the root form compute.,00:03:32,1,computing matched root form compute
5146,00:03:42,3,Implementation of TR Systems,2.1,"This way, all these different forms of computing can be matched with each other.",00:03:37,1,This way different forms computing matched
5147,00:03:47,3,Implementation of TR Systems,2.1,Normally this is a good idea to increase,00:03:43,1,Normally good idea increase
5148,00:03:52,3,Implementation of TR Systems,2.1,the coverage of documents that are matched with this query.,00:03:47,1,coverage documents matched query
5149,00:03:56,3,Implementation of TR Systems,2.1,But it's also not always beneficial,00:03:52,1,But also always beneficial
5150,00:04:01,3,Implementation of TR Systems,2.1,because sometimes the subtlest difference between computer and,00:03:56,1,sometimes subtlest difference computer
5151,00:04:07,3,Implementation of TR Systems,2.1,computation might still suggest the difference in the coverage of the content.,00:04:01,1,computation might still suggest difference coverage content
5152,00:04:13,3,Implementation of TR Systems,2.1,"But in most cases, stemming seems to be beneficial.",00:04:07,1,But cases stemming seems beneficial
5153,00:04:20,3,Implementation of TR Systems,2.1,"When we tokenize the text in some other languages, for example Chinese, we might",00:04:13,1,When tokenize text languages example Chinese might
5154,00:04:25,3,Implementation of TR Systems,2.1,face some special challenges in segmenting the text to find the word boundaries.,00:04:20,1,face special challenges segmenting text find word boundaries
5155,00:04:27,3,Implementation of TR Systems,2.1,"Because it's not ob,",00:04:25,1,Because ob
5156,00:04:33,3,Implementation of TR Systems,2.1,obvious where the boundary is as there's no space separating them.,00:04:27,1,obvious boundary space separating
5157,00:04:38,3,Implementation of TR Systems,2.1,"So, here, of course, we have to use some language-specific",00:04:33,1,So course use language specific
5158,00:04:41,3,Implementation of TR Systems,2.1,natural language processing techniques.,00:04:38,1,natural language processing techniques
5159,00:04:47,3,Implementation of TR Systems,2.1,"Once we do tokenization, then we would index the text documents, and that it",00:04:41,1,Once tokenization would index text documents
5160,00:04:52,3,Implementation of TR Systems,2.1,will convert the documents into some data structure that can enable fast search.,00:04:47,1,convert documents data structure enable fast search
5161,00:04:56,3,Implementation of TR Systems,2.1,"The basic idea is to precompute as much as we can, basically.",00:04:52,1,The basic idea precompute much basically
5162,00:05:03,3,Implementation of TR Systems,2.1,So the most commonly used index is called a inverted index.,00:04:58,1,So commonly used index called inverted index
5163,00:05:05,3,Implementation of TR Systems,2.1,"And this has been used, to,",00:05:03,1,And used
5164,00:05:09,3,Implementation of TR Systems,2.1,in many search engines to support basic search algorithms.,00:05:05,1,many search engines support basic search algorithms
5165,00:05:13,3,Implementation of TR Systems,2.1,"Sometimes other indices, for example a document index,",00:05:09,1,Sometimes indices example document index
5166,00:05:16,3,Implementation of TR Systems,2.1,"might be needed in order to support a, a feedback.",00:05:13,1,might needed order support feedback
5167,00:05:24,3,Implementation of TR Systems,2.1,"Like I said, this, this kind of techniques are not really standard",00:05:18,1,Like I said kind techniques really standard
5168,00:05:27,3,Implementation of TR Systems,2.1,in that they vary a lot according to the feedback methods.,00:05:24,1,vary lot according feedback methods
5169,00:05:32,3,Implementation of TR Systems,2.1,To understand why we are using inverted index.,00:05:28,1,To understand using inverted index
5170,00:05:37,3,Implementation of TR Systems,2.1,It will be useful for you to think about how you would respond to,00:05:32,1,It useful think would respond
5171,00:05:39,3,Implementation of TR Systems,2.1,a single term query quickly.,00:05:37,1,single term query quickly
5172,00:05:43,3,Implementation of TR Systems,2.1,"So if you want to use more time to think about that, pause the video.",00:05:40,1,So want use time think pause video
5173,00:05:50,3,Implementation of TR Systems,2.1,So think about how you can preprocess the text data so,00:05:45,1,So think preprocess text data
5174,00:05:54,3,Implementation of TR Systems,2.1,that you can quickly respond to a query with just one word.,00:05:50,1,quickly respond query one word
5175,00:05:59,3,Implementation of TR Systems,2.1,"Well, if you have thought about question, you might realize that where the best is",00:05:54,1,Well thought question might realize best
5176,00:06:06,3,Implementation of TR Systems,2.1,to simply create a list of documents that match every term in the vocabulary.,00:05:59,1,simply create list documents match every term vocabulary
5177,00:06:11,3,Implementation of TR Systems,2.1,"In this way, you can basically pre-construct the answers.",00:06:07,1,In way basically pre construct answers
5178,00:06:15,3,Implementation of TR Systems,2.1,"So when you see a term, you can simply just fetch",00:06:11,1,So see term simply fetch
5179,00:06:20,3,Implementation of TR Systems,2.1,the ranked list of documents for that term and return the list to the user.,00:06:15,1,ranked list documents term return list user
5180,00:06:25,3,Implementation of TR Systems,2.1,So that's the fastest way to respond to single term query.,00:06:20,1,So fastest way respond single term query
5181,00:06:30,3,Implementation of TR Systems,2.1,Now the idea of invert index is actually basically like that.,00:06:25,1,Now idea invert index actually basically like
5182,00:06:35,3,Implementation of TR Systems,2.1,"We can do, pre-construct such a index.",00:06:30,1,We pre construct index
5183,00:06:38,3,Implementation of TR Systems,2.1,"That would allow us to quickly find the,",00:06:35,1,That would allow us quickly find
5184,00:06:41,3,Implementation of TR Systems,2.1,all the documents that match a particular term.,00:06:38,1,documents match particular term
5185,00:06:43,3,Implementation of TR Systems,2.1,So let's take a look at this example.,00:06:41,1,So let take look example
5186,00:06:45,3,Implementation of TR Systems,2.1,"We have three documents here, and",00:06:43,1,We three documents
5187,00:06:49,3,Implementation of TR Systems,2.1,these are the documents that you have seen in some previous lectures.,00:06:45,1,documents seen previous lectures
5188,00:06:52,3,Implementation of TR Systems,2.1,Suppose we want to create invert index for,00:06:49,1,Suppose want create invert index
5189,00:06:55,3,Implementation of TR Systems,2.1,"these documents, then we will need to maintain a dictionary.",00:06:52,1,documents need maintain dictionary
5190,00:06:58,3,Implementation of TR Systems,2.1,In the dictionary we'll have one entry for each term.,00:06:55,1,In dictionary one entry term
5191,00:07:01,3,Implementation of TR Systems,2.1,And we're going to store some basic statistics about the term.,00:06:58,1,And going store basic statistics term
5192,00:07:05,3,Implementation of TR Systems,2.1,"For example, the number of documents that match the term or",00:07:01,1,For example number documents match term
5193,00:07:09,3,Implementation of TR Systems,2.1,"the total number of, fre, total frequency of the term,",00:07:05,1,total number fre total frequency term
5194,00:07:12,3,Implementation of TR Systems,2.1,which means we would encounter duplicated occurrences of the term.,00:07:09,1,means would encounter duplicated occurrences term
5195,00:07:16,3,Implementation of TR Systems,2.1,"And so, for example, news.",00:07:14,1,And example news
5196,00:07:21,3,Implementation of TR Systems,2.1,This term occurred in all the three documents.,00:07:18,1,This term occurred three documents
5197,00:07:24,3,Implementation of TR Systems,2.1,So the count of documents is three.,00:07:21,1,So count documents three
5198,00:07:31,3,Implementation of TR Systems,2.1,And you might also realize we needed this count of documents or document,00:07:26,1,And might also realize needed count documents document
5199,00:07:37,3,Implementation of TR Systems,2.1,frequency for computing some statistics to be used in the vector space model.,00:07:31,1,frequency computing statistics used vector space model
5200,00:07:42,3,Implementation of TR Systems,2.1,Can you think of that?,00:07:37,1,Can think
5201,00:07:50,3,Implementation of TR Systems,2.1,"So, what waiting heuristic would need this count?",00:07:42,1,So waiting heuristic would need count
5202,00:07:53,3,Implementation of TR Systems,2.1,"Well, that's the IDF, right, inverse document frequency.",00:07:50,1,Well IDF right inverse document frequency
5203,00:07:58,3,Implementation of TR Systems,2.1,"So IDF is a property of the term, and we can compute it right here.",00:07:53,1,So IDF property term compute right
5204,00:08:00,3,Implementation of TR Systems,2.1,"So with the document account here,",00:07:58,1,So document account
5205,00:08:06,3,Implementation of TR Systems,2.1,it's easy to compute the IDF either at this time or when we build an index or.,00:08:00,1,easy compute IDF either time build index
5206,00:08:10,3,Implementation of TR Systems,2.1,At running time when we see a query.,00:08:06,1,At running time see query
5207,00:08:14,3,Implementation of TR Systems,2.1,Now in addition to these basic statistics we also,00:08:10,1,Now addition basic statistics also
5208,00:08:18,3,Implementation of TR Systems,2.1,saw all the documents that matched news.,00:08:14,1,saw documents matched news
5209,00:08:23,3,Implementation of TR Systems,2.1,And these entries are stored in a file called a Postings.,00:08:18,1,And entries stored file called Postings
5210,00:08:27,3,Implementation of TR Systems,2.1,So in this case it matched 3 documents and,00:08:24,1,So case matched 3 documents
5211,00:08:31,3,Implementation of TR Systems,2.1,we store Information about these 3 documents here.,00:08:27,1,store Information 3 documents
5212,00:08:36,3,Implementation of TR Systems,2.1,"This is the document id, document 1, and the frequency is 1.",00:08:31,1,This document id document 1 frequency 1
5213,00:08:41,3,Implementation of TR Systems,2.1,The TF is 1 for news.,00:08:38,1,The TF 1 news
5214,00:08:45,3,Implementation of TR Systems,2.1,"In the second document it's also 1, etc.",00:08:41,1,In second document also 1 etc
5215,00:08:50,3,Implementation of TR Systems,2.1,So from this list that we can get all the documents that match the term news.,00:08:45,1,So list get documents match term news
5216,00:08:55,3,Implementation of TR Systems,2.1,And we can also know the frequency of news in these documents.,00:08:50,1,And also know frequency news documents
5217,00:08:59,3,Implementation of TR Systems,2.1,"So, if the query has just one word, news, and",00:08:55,1,So query one word news
5218,00:09:03,3,Implementation of TR Systems,2.1,we can easily look up in this table to find the entry and,00:08:59,1,easily look table find entry
5219,00:09:06,3,Implementation of TR Systems,2.1,go quickly to the postings to fetch all the documents that match news.,00:09:03,1,go quickly postings fetch documents match news
5220,00:09:09,3,Implementation of TR Systems,2.1,"So, let's take a look at another term.",00:09:06,1,So let take look another term
5221,00:09:12,3,Implementation of TR Systems,2.1,Now this time let's take a look at the word presidential.,00:09:09,1,Now time let take look word presidential
5222,00:09:18,3,Implementation of TR Systems,2.1,"All right, this word occurred in only 1 document, document 3.",00:09:13,1,All right word occurred 1 document document 3
5223,00:09:23,3,Implementation of TR Systems,2.1,"So, the document frequency is 1, but it occurred twice in this document.",00:09:18,1,So document frequency 1 occurred twice document
5224,00:09:28,3,Implementation of TR Systems,2.1,"And so the frequency count is 2, and the frequency count is used for,",00:09:23,1,And frequency count 2 frequency count used
5225,00:09:33,3,Implementation of TR Systems,2.1,in some other retrieval method where we might use the frequency,00:09:28,1,retrieval method might use frequency
5226,00:09:38,3,Implementation of TR Systems,2.1,"to assess the popularity of a, a term in the collection.",00:09:33,1,assess popularity term collection
5227,00:09:42,3,Implementation of TR Systems,2.1,"And similarly, we'll have a pointer to the postings, right here.",00:09:38,1,And similarly pointer postings right
5228,00:09:47,3,Implementation of TR Systems,2.1,And in this case there is only one entry here because,00:09:42,1,And case one entry
5229,00:09:50,3,Implementation of TR Systems,2.1,the term occurred in just one document.,00:09:48,1,term occurred one document
5230,00:09:51,3,Implementation of TR Systems,2.1,And that's here.,00:09:50,1,And
5231,00:09:57,3,Implementation of TR Systems,2.1,"The document id is 3, and it occurred twice.",00:09:53,1,The document id 3 occurred twice
5232,00:10:02,3,Implementation of TR Systems,2.1,So this is the basic idea of inverted index.,00:09:59,1,So basic idea inverted index
5233,00:10:04,3,Implementation of TR Systems,2.1,"It's actually pretty simple, right?",00:10:02,1,It actually pretty simple right
5234,00:10:12,3,Implementation of TR Systems,2.1,With this structure we can easily fetch all the documents that match a term.,00:10:06,1,With structure easily fetch documents match term
5235,00:10:15,3,Implementation of TR Systems,2.1,And this will be the basis for storing documents for our query.,00:10:12,1,And basis storing documents query
5236,00:10:23,3,Implementation of TR Systems,2.1,Now sometimes we also want to store the positions of these terms.,00:10:15,1,Now sometimes also want store positions terms
5237,00:10:30,3,Implementation of TR Systems,2.1,"So, in many of these cases the term occurred",00:10:25,1,So many cases term occurred
5238,00:10:34,3,Implementation of TR Systems,2.1,"just once in the document so there's only one position, for example in this case.",00:10:30,1,document one position example case
5239,00:10:40,3,Implementation of TR Systems,2.1,But in this case the term occurred twice so it would store two positions.,00:10:35,1,But case term occurred twice would store two positions
5240,00:10:44,3,Implementation of TR Systems,2.1,Now the position information is very useful for checking whether,00:10:40,1,Now position information useful checking whether
5241,00:10:49,3,Implementation of TR Systems,2.1,"the matching of query terms is actually within a small window of, let's say,",00:10:44,1,matching query terms actually within small window let say
5242,00:10:53,3,Implementation of TR Systems,2.1,"five words, or ten words, or whether the matching of,",00:10:49,1,five words ten words whether matching
5243,00:11:00,3,Implementation of TR Systems,2.1,"the two query terms, is in fact a phrase of two words.",00:10:55,1,two query terms fact phrase two words
5244,00:11:04,3,Implementation of TR Systems,2.1,This can all be checked quickly by using the position information.,00:11:00,1,This checked quickly using position information
5245,00:11:10,3,Implementation of TR Systems,2.1,So why is inverted index good for faster search?,00:11:05,1,So inverted index good faster search
5246,00:11:13,3,Implementation of TR Systems,2.1,Well we just talked about the possibility,00:11:10,1,Well talked possibility
5247,00:11:16,3,Implementation of TR Systems,2.1,of using the two ends of a single-word query.,00:11:13,1,using two ends single word query
5248,00:11:17,3,Implementation of TR Systems,2.1,And that's very easy.,00:11:16,1,And easy
5249,00:11:19,3,Implementation of TR Systems,2.1,What about a multiple-term queries?,00:11:17,1,What multiple term queries
5250,00:11:23,3,Implementation of TR Systems,2.1,"Well, let's look at the, some special cases of the Boolean query.",00:11:19,1,Well let look special cases Boolean query
5251,00:11:27,3,Implementation of TR Systems,2.1,"A Boolean query is basically a Boolean expression, like this.",00:11:23,1,A Boolean query basically Boolean expression like
5252,00:11:35,3,Implementation of TR Systems,2.1,So I want the relevant document to match both term A AND term B.,00:11:27,1,So I want relevant document match term A AND term B
5253,00:11:38,3,Implementation of TR Systems,2.1,"All right, so that's one conjunctive query.",00:11:35,1,All right one conjunctive query
5254,00:11:45,3,Implementation of TR Systems,2.1,"Or, I want the relevant documents to match term A OR term B.",00:11:38,1,Or I want relevant documents match term A OR term B
5255,00:11:46,3,Implementation of TR Systems,2.1,That's a disjunctive query.,00:11:45,1,That disjunctive query
5256,00:11:51,3,Implementation of TR Systems,2.1,Now how can we answer such a query by using inverted index?,00:11:46,1,Now answer query using inverted index
5257,00:11:55,3,Implementation of TR Systems,2.1,"Well if you think a, a bit about it, it would be obvious.",00:11:52,1,Well think bit would obvious
5258,00:12:00,3,Implementation of TR Systems,2.1,Because we have simply to fetch all the documents that match term A and,00:11:55,1,Because simply fetch documents match term A
5259,00:12:03,3,Implementation of TR Systems,2.1,also fetch all the documents that match term B.,00:12:00,1,also fetch documents match term B
5260,00:12:08,3,Implementation of TR Systems,2.1,And then just take the intersection to answer a query like A and B.,00:12:03,1,And take intersection answer query like A B
5261,00:12:12,3,Implementation of TR Systems,2.1,Or to take the union to answer the query A or B.,00:12:08,1,Or take union answer query A B
5262,00:12:16,3,Implementation of TR Systems,2.1,So this is all very easy to answer.,00:12:12,1,So easy answer
5263,00:12:17,3,Implementation of TR Systems,2.1,It's going to be very quick.,00:12:16,1,It going quick
5264,00:12:20,3,Implementation of TR Systems,2.1,Now what about the multi-term keyword query?,00:12:17,1,Now multi term keyword query
5265,00:12:24,3,Implementation of TR Systems,2.1,We talked about the vector space model for example.,00:12:20,1,We talked vector space model example
5266,00:12:28,3,Implementation of TR Systems,2.1,And we would match such a query with a document and generate a score.,00:12:24,1,And would match query document generate score
5267,00:12:32,3,Implementation of TR Systems,2.1,And the score is based on aggregated term weights.,00:12:28,1,And score based aggregated term weights
5268,00:12:35,3,Implementation of TR Systems,2.1,"So in this case it's not a Boolean query, but",00:12:32,1,So case Boolean query
5269,00:12:38,3,Implementation of TR Systems,2.1,the scoring can be actually done in a similar way.,00:12:35,1,scoring actually done similar way
5270,00:12:42,3,Implementation of TR Systems,2.1,Basically it's similar to disjunctive Boolean query.,00:12:38,1,Basically similar disjunctive Boolean query
5271,00:12:44,3,Implementation of TR Systems,2.1,Basically It's like A OR B.,00:12:42,1,Basically It like A OR B
5272,00:12:49,3,Implementation of TR Systems,2.1,"We take the union of all the, documents that matched at least one query term,",00:12:44,1,We take union documents matched least one query term
5273,00:12:53,3,Implementation of TR Systems,2.1,and then we would aggregate the term weights.,00:12:49,1,would aggregate term weights
5274,00:12:57,3,Implementation of TR Systems,2.1,"So this is a, a, a basic idea of",00:12:53,1,So basic idea
5275,00:13:01,3,Implementation of TR Systems,2.1,using inverted index for scoring documents in general.,00:12:57,1,using inverted index scoring documents general
5276,00:13:05,3,Implementation of TR Systems,2.1,And we're going to talk about this in more detail later.,00:13:01,1,And going talk detail later
5277,00:13:07,3,Implementation of TR Systems,2.1,"But for now, let's just look at the question,",00:13:05,1,But let look question
5278,00:13:12,3,Implementation of TR Systems,2.1,"why is inverted index, a good idea?",00:13:07,1,inverted index good idea
5279,00:13:16,3,Implementation of TR Systems,2.1,"Basically, why is it more efficient than sequentially just scanning documents?",00:13:12,1,Basically efficient sequentially scanning documents
5280,00:13:17,3,Implementation of TR Systems,2.1,Right?,00:13:16,1,Right
5281,00:13:20,3,Implementation of TR Systems,2.1,"This is, the obvious approach.",00:13:17,1,This obvious approach
5282,00:13:23,3,Implementation of TR Systems,2.1,"You can just compute the score for each document, and",00:13:20,1,You compute score document
5283,00:13:26,3,Implementation of TR Systems,2.1,"then you can score them, sorry, you can then sort them.",00:13:23,1,score sorry sort
5284,00:13:29,3,Implementation of TR Systems,2.1,"This is a, a straightforward method.",00:13:27,1,This straightforward method
5285,00:13:31,3,Implementation of TR Systems,2.1,But this is going to be very slow.,00:13:29,1,But going slow
5286,00:13:32,3,Implementation of TR Systems,2.1,Imagine the web.,00:13:31,1,Imagine web
5287,00:13:34,3,Implementation of TR Systems,2.1,It has a lot of documents.,00:13:32,1,It lot documents
5288,00:13:39,3,Implementation of TR Systems,2.1,"If you do this, then it will take a long time to answer your query.",00:13:34,1,If take long time answer query
5289,00:13:45,3,Implementation of TR Systems,2.1,"So the question now is, why would the in, the inverted index be much faster?",00:13:39,1,So question would inverted index much faster
5290,00:13:48,3,Implementation of TR Systems,2.1,Well it has to do with the word distribution in text.,00:13:45,1,Well word distribution text
5291,00:13:54,3,Implementation of TR Systems,2.1,"So, here's some common phenomenon of word distribution in text.",00:13:48,1,So common phenomenon word distribution text
5292,00:13:58,3,Implementation of TR Systems,2.1,"There are some language-in, independent patterns that seem to be stable.",00:13:54,1,There language independent patterns seem stable
5293,00:14:07,3,Implementation of TR Systems,2.1,And these patterns are basically characterized by the following pattern.,00:14:00,1,And patterns basically characterized following pattern
5294,00:14:12,3,Implementation of TR Systems,2.1,"A few words like the common words like the a, or we, occur very,",00:14:07,1,A words like common words like occur
5295,00:14:14,3,Implementation of TR Systems,2.1,very frequently in text.,00:14:12,1,frequently text
5296,00:14:18,3,Implementation of TR Systems,2.1,So they account for a large percent of occurrences of words.,00:14:14,1,So account large percent occurrences words
5297,00:14:22,3,Implementation of TR Systems,2.1,But most word would occur just rarely.,00:14:19,1,But word would occur rarely
5298,00:14:25,3,Implementation of TR Systems,2.1,"There are many words that occur just once,",00:14:22,1,There many words occur
5299,00:14:29,3,Implementation of TR Systems,2.1,"let's say, in a document, or once in the collection.",00:14:25,1,let say document collection
5300,00:14:32,3,Implementation of TR Systems,2.1,And there are many such single terms.,00:14:29,1,And many single terms
5301,00:14:39,3,Implementation of TR Systems,2.1,It's also true that the most frequent words in one corpus,00:14:34,1,It also true frequent words one corpus
5302,00:14:40,3,Implementation of TR Systems,2.1,may actually be rare in another.,00:14:39,1,may actually rare another
5303,00:14:45,3,Implementation of TR Systems,2.1,"That means, although the general phenomenon is applicable or",00:14:40,1,That means although general phenomenon applicable
5304,00:14:47,3,Implementation of TR Systems,2.1,"is observed in many cases,",00:14:45,1,observed many cases
5305,00:14:54,3,Implementation of TR Systems,2.1,the exact words that are common may vary from context to context.,00:14:48,1,exact words common may vary context context
5306,00:14:59,3,Implementation of TR Systems,2.1,So this phenomena is characterized by what's called a Zipf's Law.,00:14:54,1,So phenomena characterized called Zipf Law
5307,00:15:03,3,Implementation of TR Systems,2.1,"This law says that the rank of a word multiplied by,",00:14:59,1,This law says rank word multiplied
5308,00:15:06,3,Implementation of TR Systems,2.1,the frequency of the word is roughly constant.,00:15:03,1,frequency word roughly constant
5309,00:15:12,3,Implementation of TR Systems,2.1,"So formally if we use F of w to denote the, frequency,",00:15:07,1,So formally use F w denote frequency
5310,00:15:17,3,Implementation of TR Systems,2.1,"r of w to denote the rank of a word, then this is the formula.",00:15:12,1,r w denote rank word formula
5311,00:15:22,3,Implementation of TR Systems,2.1,"It basically says the same thing, just mathematical term, where C is,",00:15:17,1,It basically says thing mathematical term C
5312,00:15:27,3,Implementation of TR Systems,2.1,"basically a constant, right, so as, so.",00:15:22,1,basically constant right
5313,00:15:30,3,Implementation of TR Systems,2.1,"And there is also parameter alpha that might,",00:15:27,1,And also parameter alpha might
5314,00:15:34,3,Implementation of TR Systems,2.1,be adjusted to better fit any empirical observations.,00:15:30,1,adjusted better fit empirical observations
5315,00:15:38,3,Implementation of TR Systems,2.1,"So if I plot the word frequencies in sorted order,",00:15:34,1,So I plot word frequencies sorted order
5316,00:15:41,3,Implementation of TR Systems,2.1,then you can see this more easily.,00:15:38,1,see easily
5317,00:15:43,3,Implementation of TR Systems,2.1,The x-axis is basically the word rank.,00:15:41,1,The x axis basically word rank
5318,00:15:45,3,Implementation of TR Systems,2.1,And this is r of w.,00:15:43,1,And r w
5319,00:15:50,3,Implementation of TR Systems,2.1,"And the y-axis is the word frequency, or F of w.",00:15:45,1,And axis word frequency F w
5320,00:15:55,3,Implementation of TR Systems,2.1,"Now, this curve basically shows that the product of the two",00:15:50,1,Now curve basically shows product two
5321,00:15:57,3,Implementation of TR Systems,2.1,is roughly the constant.,00:15:55,1,roughly constant
5322,00:15:59,3,Implementation of TR Systems,2.1,"Now, if you look these words, we can see.",00:15:57,1,Now look words see
5323,00:16:02,3,Implementation of TR Systems,2.1,They can be separated into three group2s.,00:15:59,1,They separated three group2s
5324,00:16:06,3,Implementation of TR Systems,2.1,In the middle it's the immediate frequency words.,00:16:02,1,In middle immediate frequency words
5325,00:16:11,3,Implementation of TR Systems,2.1,"These words tend to occur in quite a few documents, right?",00:16:06,1,These words tend occur quite documents right
5326,00:16:14,3,Implementation of TR Systems,2.1,But they're not like those most frequent words.,00:16:11,1,But like frequent words
5327,00:16:17,3,Implementation of TR Systems,2.1,And they are also not very rare.,00:16:14,1,And also rare
5328,00:16:23,3,Implementation of TR Systems,2.1,"So they tend to be often used in in, in queries.",00:16:18,1,So tend often used queries
5329,00:16:28,3,Implementation of TR Systems,2.1,And they also tend to have high TFI diff weights in these intermediate,00:16:23,1,And also tend high TFI diff weights intermediate
5330,00:16:31,3,Implementation of TR Systems,2.1,frequency words.,00:16:28,1,frequency words
5331,00:16:35,3,Implementation of TR Systems,2.1,But if you look at the left part of the curve.,00:16:31,1,But look left part curve
5332,00:16:37,3,Implementation of TR Systems,2.1,These are the highest frequency words.,00:16:35,1,These highest frequency words
5333,00:16:39,3,Implementation of TR Systems,2.1,They occur very frequently.,00:16:37,1,They occur frequently
5334,00:16:45,3,Implementation of TR Systems,2.1,"They are usually stopper words, the, we, of, et cetera.",00:16:39,1,They usually stopper words et cetera
5335,00:16:47,3,Implementation of TR Systems,2.1,"Those words are very, very frequently.",00:16:45,1,Those words frequently
5336,00:16:50,3,Implementation of TR Systems,2.1,"They are, in fact, a too frequently to be discriminated.",00:16:47,1,They fact frequently discriminated
5337,00:16:55,3,Implementation of TR Systems,2.1,"And they generally are not very useful for, for retrieval.",00:16:50,1,And generally useful retrieval
5338,00:17:01,3,Implementation of TR Systems,2.1,"So, they are often removed, and this is called a stop words removal.",00:16:55,1,So often removed called stop words removal
5339,00:17:06,3,Implementation of TR Systems,2.1,So you can use pretty much just the count of words in the collection to kind,00:17:01,1,So use pretty much count words collection kind
5340,00:17:09,3,Implementation of TR Systems,2.1,of infer what words might be stop words.,00:17:06,1,infer words might stop words
5341,00:17:12,3,Implementation of TR Systems,2.1,Those are basically the highest frequency words.,00:17:09,1,Those basically highest frequency words
5342,00:17:17,3,Implementation of TR Systems,2.1,And they also occupy a lot of space in the invert index.,00:17:13,1,And also occupy lot space invert index
5343,00:17:22,3,Implementation of TR Systems,2.1,You can imagine the posting entries for such a word would be very long.,00:17:17,1,You imagine posting entries word would long
5344,00:17:26,3,Implementation of TR Systems,2.1,"And then therefore, if you can remove such words,",00:17:22,1,And therefore remove words
5345,00:17:29,3,Implementation of TR Systems,2.1,you can save a lot of space in the invert index.,00:17:26,1,save lot space invert index
5346,00:17:35,3,Implementation of TR Systems,2.1,"We also show the tail part, which is, has a lot of rare words.",00:17:29,1,We also show tail part lot rare words
5347,00:17:38,3,Implementation of TR Systems,2.1,"Those words don't occur very frequently, and there are many such words.",00:17:35,1,Those words occur frequently many words
5348,00:17:41,3,Implementation of TR Systems,2.1,"Those words are actually very useful for search,",00:17:39,1,Those words actually useful search
5349,00:17:45,3,Implementation of TR Systems,2.1,"also, if a user happens to be interested in such a topic.",00:17:41,1,also user happens interested topic
5350,00:17:49,3,Implementation of TR Systems,2.1,"But because they're rare it's often true that users are,",00:17:45,1,But rare often true users
5351,00:17:53,3,Implementation of TR Systems,2.1,aren't the necessary interest in those words.,00:17:49,1,necessary interest words
5352,00:17:58,3,Implementation of TR Systems,2.1,"But retain them would allow us to match such a document accurately,",00:17:53,1,But retain would allow us match document accurately
5353,00:18:01,3,Implementation of TR Systems,2.1,and they generally have very high IDFs.,00:17:58,1,generally high IDFs
5354,00:18:10,3,Implementation of TR Systems,2.1,So what kind of data structures should we use to to store inverted index?,00:18:05,1,So kind data structures use store inverted index
5355,00:18:11,3,Implementation of TR Systems,2.1,"Well, it has two parts, right?",00:18:10,1,Well two parts right
5356,00:18:17,3,Implementation of TR Systems,2.1,"If you recall we have a dictionary, and we also have postings.",00:18:11,1,If recall dictionary also postings
5357,00:18:19,3,Implementation of TR Systems,2.1,"The dictionary has modest size, although for",00:18:17,1,The dictionary modest size although
5358,00:18:22,3,Implementation of TR Systems,2.1,"the web, it still wouldn't be very large.",00:18:19,1,web still large
5359,00:18:24,3,Implementation of TR Systems,2.1,"But compared with postings, it's modest.",00:18:22,1,But compared postings modest
5360,00:18:29,3,Implementation of TR Systems,2.1,"And we also need to have fast, random access to the entries",00:18:26,1,And also need fast random access entries
5361,00:18:32,3,Implementation of TR Systems,2.1,because we want to look up the query term very quickly.,00:18:29,1,want look query term quickly
5362,00:18:39,3,Implementation of TR Systems,2.1,"So, therefore, we prefer to keep such a dictionary in memory if it's possible.",00:18:32,1,So therefore prefer keep dictionary memory possible
5363,00:18:43,3,Implementation of TR Systems,2.1,"Or, or, or if the connection is not very large, and this is visible.",00:18:39,1,Or connection large visible
5364,00:18:47,3,Implementation of TR Systems,2.1,"But if the connection is very large, then it's in general not possible.",00:18:43,1,But connection large general possible
5365,00:18:52,3,Implementation of TR Systems,2.1,"If the vocabulary size is very large, obviously we can't do that.",00:18:47,1,If vocabulary size large obviously
5366,00:18:55,3,Implementation of TR Systems,2.1,"So, but in general, that's our goal.",00:18:52,1,So general goal
5367,00:18:58,3,Implementation of TR Systems,2.1,So the data structures that we often use for,00:18:55,1,So data structures often use
5368,00:19:03,3,Implementation of TR Systems,2.1,"storing dictionary would be direct access data structures, like a hash table or",00:18:58,1,storing dictionary would direct access data structures like hash table
5369,00:19:08,3,Implementation of TR Systems,2.1,B-tree if we can't store everything in memory of the newest disk.,00:19:03,1,B tree store everything memory newest disk
5370,00:19:12,3,Implementation of TR Systems,2.1,And but to try to build a structure that would allow it to quickly look up our,00:19:08,1,And try build structure would allow quickly look
5371,00:19:12,3,Implementation of TR Systems,2.1,entries.,00:19:12,1,entries
5372,00:19:13,3,Implementation of TR Systems,2.1,Right.,00:19:12,1,Right
5373,00:19:18,3,Implementation of TR Systems,2.1,"For postings, they're huge, you can see.",00:19:13,1,For postings huge see
5374,00:19:24,3,Implementation of TR Systems,2.1,"And in general, we don't have to have direct access to a specific engine.",00:19:18,1,And general direct access specific engine
5375,00:19:29,3,Implementation of TR Systems,2.1,"We generally would just look up a, a sequence of document IDs and",00:19:24,1,We generally would look sequence document IDs
5376,00:19:32,3,Implementation of TR Systems,2.1,frequencies for all of the documents that match a query term.,00:19:29,1,frequencies documents match query term
5377,00:19:36,3,Implementation of TR Systems,2.1,So we would read those entries sequentially.,00:19:33,1,So would read entries sequentially
5378,00:19:41,3,Implementation of TR Systems,2.1,"And therefore, because it's large and we generate,",00:19:37,1,And therefore large generate
5379,00:19:45,3,Implementation of TR Systems,2.1,"have store postings on disk, so they have to stay on disk.",00:19:41,1,store postings disk stay disk
5380,00:19:51,3,Implementation of TR Systems,2.1,"And they would contain information such as document IDs, term frequencies, or",00:19:46,1,And would contain information document IDs term frequencies
5381,00:19:53,3,Implementation of TR Systems,2.1,"term positions, et cetera.",00:19:51,1,term positions et cetera
5382,00:19:58,3,Implementation of TR Systems,2.1,"Now because they're very large, compression is often desirable.",00:19:53,1,Now large compression often desirable
5383,00:20:01,3,Implementation of TR Systems,2.1,Now this is not only to save disk space and,00:19:58,1,Now save disk space
5384,00:20:06,3,Implementation of TR Systems,2.1,"this is of course, one benefit of compression.",00:20:01,1,course one benefit compression
5385,00:20:09,3,Implementation of TR Systems,2.1,It's not going to occupy that much space.,00:20:06,1,It going occupy much space
5386,00:20:11,3,Implementation of TR Systems,2.1,But it's also to help improving speed.,00:20:09,1,But also help improving speed
5387,00:20:15,3,Implementation of TR Systems,2.1,Can you see why?,00:20:13,1,Can see
5388,00:20:20,3,Implementation of TR Systems,2.1,"Well, we know that input and",00:20:15,1,Well know input
5389,00:20:28,3,Implementation of TR Systems,2.1,output will cost a lot of time in comparison with the time taken by CPU.,00:20:20,1,output cost lot time comparison time taken CPU
5390,00:20:29,3,Implementation of TR Systems,2.1,So CPU is much faster.,00:20:28,1,So CPU much faster
5391,00:20:32,3,Implementation of TR Systems,2.1,But IO takes time.,00:20:29,1,But IO takes time
5392,00:20:38,3,Implementation of TR Systems,2.1,"And so by compressing the inverted index, the posting files will become smaller.",00:20:32,1,And compressing inverted index posting files become smaller
5393,00:20:42,3,Implementation of TR Systems,2.1,And the entries that we have to read into memory,00:20:38,1,And entries read memory
5394,00:20:47,3,Implementation of TR Systems,2.1,"to process a query done, would would be smaller.",00:20:42,1,process query done would would smaller
5395,00:20:52,3,Implementation of TR Systems,2.1,"And then so we, we can reduce the amount of traffic and IO.",00:20:47,1,And reduce amount traffic IO
5396,00:20:54,3,Implementation of TR Systems,2.1,And that can save a lot of time.,00:20:52,1,And save lot time
5397,00:20:59,3,Implementation of TR Systems,2.1,"Of course, we have to then do more processing of the data",00:20:54,1,Of course processing data
5398,00:21:03,3,Implementation of TR Systems,2.1,"when we uncompress the, the data in the memory.",00:20:59,1,uncompress data memory
5399,00:21:07,3,Implementation of TR Systems,2.1,"But as I said, CPU is fast, so overall, we can still save time.",00:21:03,1,But I said CPU fast overall still save time
5400,00:21:10,3,Implementation of TR Systems,2.1,So compression here is both to save disk space and,00:21:07,1,So compression save disk space
5401,00:21:14,3,Implementation of TR Systems,2.1,to speed up the loading of the inverted index.,00:21:10,1,speed loading inverted index
5402,00:00:14,2,Text Access,1.2,"[SOUND] In this lecture, we're going to talk about text access.",00:00:08,2,SOUND In lecture going talk text access
5403,00:00:17,2,Text Access,1.2,"In the previously lecture, we talked about natural language content analysis.",00:00:14,2,In previously lecture talked natural language content analysis
5404,00:00:23,2,Text Access,1.2,We explained that the state of the art natural language processing techniques,00:00:19,2,We explained state art natural language processing techniques
5405,00:00:28,2,Text Access,1.2,are still not good enough to process a lot of unrestricted text data,00:00:23,2,still good enough process lot unrestricted text data
5406,00:00:30,2,Text Access,1.2,in a robust manner.,00:00:28,2,robust manner
5407,00:00:33,2,Text Access,1.2,"As a result, bag of words representation",00:00:30,2,As result bag words representation
5408,00:00:37,2,Text Access,1.2,remains very popular in applications like search engines.,00:00:33,2,remains popular applications like search engines
5409,00:00:43,2,Text Access,1.2,In this lecture we're going to talk about some high level strategies,00:00:39,2,In lecture going talk high level strategies
5410,00:00:48,2,Text Access,1.2,to help users get access to the text data.,00:00:43,2,help users get access text data
5411,00:00:53,2,Text Access,1.2,"This is also important step to convert raw, big text data into small",00:00:48,2,This also important step convert raw big text data small
5412,00:00:59,2,Text Access,1.2,relevant data that are actually needed in a specific application.,00:00:53,2,relevant data actually needed specific application
5413,00:01:04,2,Text Access,1.2,"So the main question we address here is, how can a text information system",00:00:59,2,So main question address text information system
5414,00:01:07,2,Text Access,1.2,help users get access to the relevant text data?,00:01:04,2,help users get access relevant text data
5415,00:01:11,2,Text Access,1.2,"We're going to cover two complementary strategies, push vs pull.",00:01:07,2,We going cover two complementary strategies push vs pull
5416,00:01:17,2,Text Access,1.2,"And then we're going to talk about two ways to implement the pull mode,",00:01:12,2,And going talk two ways implement pull mode
5417,00:01:19,2,Text Access,1.2,querying vs browsing.,00:01:17,2,querying vs browsing
5418,00:01:22,2,Text Access,1.2,"So first, push vs pull.",00:01:20,2,So first push vs pull
5419,00:01:29,2,Text Access,1.2,These are two different ways to connect users with the right information,00:01:24,2,These two different ways connect users right information
5420,00:01:29,2,Text Access,1.2,at the right time.,00:01:29,2,right time
5421,00:01:35,2,Text Access,1.2,"The difference is which takes the initiative,",00:01:31,2,The difference takes initiative
5422,00:01:38,2,Text Access,1.2,which party it takes in the initiative.,00:01:37,2,party takes initiative
5423,00:01:41,2,Text Access,1.2,"In the pull mode,",00:01:40,2,In pull mode
5424,00:01:46,2,Text Access,1.2,the users would take the initiative to start the information access process.,00:01:41,2,users would take initiative start information access process
5425,00:01:53,2,Text Access,1.2,"And in this case, a user typically would use a search engine to fulfill the goal.",00:01:47,2,And case user typically would use search engine fulfill goal
5426,00:01:57,2,Text Access,1.2,"For example, the user may type in a query, and",00:01:53,2,For example user may type query
5427,00:02:01,2,Text Access,1.2,then browse results to find the relevant information.,00:01:57,2,browse results find relevant information
5428,00:02:05,2,Text Access,1.2,So this is usually appropriate for,00:02:02,2,So usually appropriate
5429,00:02:09,2,Text Access,1.2,satisfying a user's ad hoc information need.,00:02:05,2,satisfying user ad hoc information need
5430,00:02:14,2,Text Access,1.2,An ad hoc information need is a temporary information need.,00:02:09,2,An ad hoc information need temporary information need
5431,00:02:17,2,Text Access,1.2,"For example, you want to buy a product so",00:02:14,2,For example want buy product
5432,00:02:22,2,Text Access,1.2,you suddenly have a need to read reviews about related products.,00:02:17,2,suddenly need read reviews related products
5433,00:02:24,2,Text Access,1.2,"But after you have collected information,",00:02:22,2,But collected information
5434,00:02:28,2,Text Access,1.2,"you have purchased your product, you generally no longer need such information.",00:02:24,2,purchased product generally longer need information
5435,00:02:30,2,Text Access,1.2,So it's a temporary information need.,00:02:28,2,So temporary information need
5436,00:02:35,2,Text Access,1.2,"In such a case, it's very hard for a system to predict your need, and",00:02:31,2,In case hard system predict need
5437,00:02:39,2,Text Access,1.2,it's more appropriate for the users to take the initiative.,00:02:35,2,appropriate users take initiative
5438,00:02:42,2,Text Access,1.2,"And that's why search engines are very useful today,",00:02:39,2,And search engines useful today
5439,00:02:48,2,Text Access,1.2,because many people have many ad hoc information needs all the time.,00:02:42,2,many people many ad hoc information needs time
5440,00:02:52,2,Text Access,1.2,"So as we are speaking Google probably is processing many queries from this, and",00:02:48,2,So speaking Google probably processing many queries
5441,00:02:56,2,Text Access,1.2,"those are all, or mostly ad hoc information needs.",00:02:52,2,mostly ad hoc information needs
5442,00:02:59,2,Text Access,1.2,So this is a pull mode.,00:02:57,2,So pull mode
5443,00:03:03,2,Text Access,1.2,"In contrast, in the push mode the system will take the initiative",00:02:59,2,In contrast push mode system take initiative
5444,00:03:07,2,Text Access,1.2,to push the information to the user or to recommend the information to the user.,00:03:03,2,push information user recommend information user
5445,00:03:11,2,Text Access,1.2,"So in this case, this is usually supported by a recommender system.",00:03:07,2,So case usually supported recommender system
5446,00:03:17,2,Text Access,1.2,Now this would be appropriate if the user has a stable information need.,00:03:13,2,Now would appropriate user stable information need
5447,00:03:22,2,Text Access,1.2,"For example, you may have a research interest in some topic, and",00:03:17,2,For example may research interest topic
5448,00:03:26,2,Text Access,1.2,"that interest tends to stay for a while, so it's relatively stable.",00:03:22,2,interest tends stay relatively stable
5449,00:03:31,2,Text Access,1.2,Your hobby is another example of a stable information need.,00:03:26,2,Your hobby another example stable information need
5450,00:03:36,2,Text Access,1.2,"In such a case, the system can interact with you and can learn your interest, and",00:03:31,2,In case system interact learn interest
5451,00:03:38,2,Text Access,1.2,then can monitor the information stream.,00:03:36,2,monitor information stream
5452,00:03:43,2,Text Access,1.2,"If it is, the system hasn't seen any relevant items to your interest,",00:03:38,2,If system seen relevant items interest
5453,00:03:47,2,Text Access,1.2,the system could then take the initiative to recommend information to you.,00:03:43,2,system could take initiative recommend information
5454,00:03:49,2,Text Access,1.2,"So for example, a news filter or",00:03:47,2,So example news filter
5455,00:03:52,2,Text Access,1.2,news recommender system could monitor the news stream and,00:03:49,2,news recommender system could monitor news stream
5456,00:03:56,2,Text Access,1.2,"identify interest in news to you, and simply push the news articles to you.",00:03:52,2,identify interest news simply push news articles
5457,00:04:03,2,Text Access,1.2,This mode of information access may be also appropriate when,00:03:59,2,This mode information access may also appropriate
5458,00:04:06,2,Text Access,1.2,the system has a good knowledge about the user's need.,00:04:03,2,system good knowledge user need
5459,00:04:08,2,Text Access,1.2,And this happens in the search context.,00:04:06,2,And happens search context
5460,00:04:12,2,Text Access,1.2,"So for example, when you search for information on the web a search",00:04:08,2,So example search information web search
5461,00:04:17,2,Text Access,1.2,engine might infer you might be also interested in some related information.,00:04:12,2,engine might infer might also interested related information
5462,00:04:19,2,Text Access,1.2,And they would recommend the information to you.,00:04:17,2,And would recommend information
5463,00:04:24,2,Text Access,1.2,"So that should remind you for example, advertisement placed on a search page.",00:04:19,2,So remind example advertisement placed search page
5464,00:04:34,2,Text Access,1.2,"So this is about the, the two high level strategies or two modes of text access.",00:04:27,2,So two high level strategies two modes text access
5465,00:04:38,2,Text Access,1.2,Now let's look at the pull mode in more detail.,00:04:35,2,Now let look pull mode detail
5466,00:04:43,2,Text Access,1.2,"In the pull mode, we can further this in usually two ways to help users,",00:04:39,2,In pull mode usually two ways help users
5467,00:04:46,2,Text Access,1.2,querying vs browsing.,00:04:43,2,querying vs browsing
5468,00:04:50,2,Text Access,1.2,"In querying, a user would just enter a query, typically a keyword query, and",00:04:46,2,In querying user would enter query typically keyword query
5469,00:04:54,2,Text Access,1.2,the search engine system would return relevant documents to users.,00:04:50,2,search engine system would return relevant documents users
5470,00:04:58,2,Text Access,1.2,"And this works well when the user knows what exactly key,",00:04:54,2,And works well user knows exactly key
5471,00:05:00,2,Text Access,1.2,are the keywords to be used.,00:04:58,2,keywords used
5472,00:05:02,2,Text Access,1.2,So if you know exactly what you're looking for,00:05:00,2,So know exactly looking
5473,00:05:06,2,Text Access,1.2,"you tend to know the right keywords, and then query would work very well.",00:05:02,2,tend know right keywords query would work well
5474,00:05:07,2,Text Access,1.2,And we do that all the time.,00:05:06,2,And time
5475,00:05:12,2,Text Access,1.2,But we also know that sometimes it doesn't work so,00:05:09,2,But also know sometimes work
5476,00:05:16,2,Text Access,1.2,"well, when you don't know the right keywords to use in the query or",00:05:12,2,well know right keywords use query
5477,00:05:21,2,Text Access,1.2,you want to browse information in some topic area.,00:05:16,2,want browse information topic area
5478,00:05:24,2,Text Access,1.2,In this case browsing would be more useful.,00:05:21,2,In case browsing would useful
5479,00:05:29,2,Text Access,1.2,So in this case in the case of browsing the users would simply navigate,00:05:24,2,So case case browsing users would simply navigate
5480,00:05:33,2,Text Access,1.2,into the relevant information by following the path that's,00:05:29,2,relevant information following path
5481,00:05:39,2,Text Access,1.2,supported by the structures on the documents.,00:05:34,2,supported structures documents
5482,00:05:42,2,Text Access,1.2,"So the system would maintain some kind of structures, and",00:05:39,2,So system would maintain kind structures
5483,00:05:47,2,Text Access,1.2,then the user could follow these structures to navigate.,00:05:42,2,user could follow structures navigate
5484,00:05:53,2,Text Access,1.2,So this strategy works well when the user wants to explore information space or,00:05:47,2,So strategy works well user wants explore information space
5485,00:05:59,2,Text Access,1.2,the user doesn't know what are the keywords to use in the query.,00:05:53,2,user know keywords use query
5486,00:06:05,2,Text Access,1.2,"Or simply because the user, finds it inconvenient to type in the query.",00:05:59,2,Or simply user finds inconvenient type query
5487,00:06:10,2,Text Access,1.2,"So even if a user knows what query to type in, if the user is using a cell phone",00:06:05,2,So even user knows query type user using cell phone
5488,00:06:14,2,Text Access,1.2,"to search for information, then it's still hard to enter the query.",00:06:10,2,search information still hard enter query
5489,00:06:18,2,Text Access,1.2,"In such a case again, browsing tends to be more convenient.",00:06:14,2,In case browsing tends convenient
5490,00:06:21,2,Text Access,1.2,The relationship between browsing and,00:06:18,2,The relationship browsing
5491,00:06:25,2,Text Access,1.2,the query is best understood by making an analogy to sightseeing.,00:06:21,2,query best understood making analogy sightseeing
5492,00:06:26,2,Text Access,1.2,Imagine if you are touring a city.,00:06:25,2,Imagine touring city
5493,00:06:31,2,Text Access,1.2,"Now if you know the exact address of a attraction, then taking a taxi",00:06:26,2,Now know exact address attraction taking taxi
5494,00:06:36,2,Text Access,1.2,"there is perhaps the fastest way, you can go directly to the site.",00:06:31,2,perhaps fastest way go directly site
5495,00:06:40,2,Text Access,1.2,"But if you don't know the exact address, you may need to walk around, or",00:06:36,2,But know exact address may need walk around
5496,00:06:43,2,Text Access,1.2,"you can take a taxi to a nearby place, and then walk around.",00:06:40,2,take taxi nearby place walk around
5497,00:06:48,2,Text Access,1.2,It turns out that we do exactly the same in the information space.,00:06:44,2,It turns exactly information space
5498,00:06:51,2,Text Access,1.2,"If you know exactly what you are looking for, then you can",00:06:48,2,If know exactly looking
5499,00:06:55,2,Text Access,1.2,use the right keywords in your query to find the information directly.,00:06:51,2,use right keywords query find information directly
5500,00:06:58,2,Text Access,1.2,That's usually the fastest way to do find information.,00:06:55,2,That usually fastest way find information
5501,00:07:02,2,Text Access,1.2,But what if you don't know the exact keywords to use?,00:06:59,2,But know exact keywords use
5502,00:07:06,2,Text Access,1.2,"Well, your query probably won't work so well, and you will land on some related",00:07:02,2,Well query probably work well land related
5503,00:07:09,2,Text Access,1.2,"pages, and then you need to also walk around in the information space.",00:07:06,2,pages need also walk around information space
5504,00:07:13,2,Text Access,1.2,"Meaning by following the links or by browsing,",00:07:09,2,Meaning following links browsing
5505,00:07:16,2,Text Access,1.2,you can then finally get into the relevant page.,00:07:13,2,finally get relevant page
5506,00:07:23,2,Text Access,1.2,"If you want to learn about a topic again, you you will likely do a lot of browsing.",00:07:17,2,If want learn topic likely lot browsing
5507,00:07:28,2,Text Access,1.2,So just like you are looking around in some area and,00:07:24,2,So like looking around area
5508,00:07:33,2,Text Access,1.2,you want to see some interesting attractions in a related,00:07:28,2,want see interesting attractions related
5509,00:07:39,2,Text Access,1.2,in the same region.,00:07:35,2,region
5510,00:07:45,2,Text Access,1.2,So this analogy also tells us that today we have very good support for,00:07:39,2,So analogy also tells us today good support
5511,00:07:49,2,Text Access,1.2,"querying, but we don't really have good support for browsing.",00:07:45,2,querying really good support browsing
5512,00:07:56,2,Text Access,1.2,"And this is because in order to browse effectively, we need a a map to guide us,",00:07:50,2,And order browse effectively need map guide us
5513,00:08:00,2,Text Access,1.2,just like you need a map of Chicago to tour the city of Chicago.,00:07:56,2,like need map Chicago tour city Chicago
5514,00:08:04,2,Text Access,1.2,You need a topical map to tour the information space.,00:08:00,2,You need topical map tour information space
5515,00:08:08,2,Text Access,1.2,So how to construct such a topical map is in fact a very interesting,00:08:04,2,So construct topical map fact interesting
5516,00:08:12,2,Text Access,1.2,research question that likely will bring us,00:08:08,2,research question likely bring us
5517,00:08:16,2,Text Access,1.2,more interesting browsing experience on the web or in other applications.,00:08:12,2,interesting browsing experience web applications
5518,00:08:23,2,Text Access,1.2,"So to summarize this lecture, we have talked about two high level strategies for",00:08:19,2,So summarize lecture talked two high level strategies
5519,00:08:26,2,Text Access,1.2,"text access, push and pull.",00:08:23,2,text access push pull
5520,00:08:29,2,Text Access,1.2,Push tends to be supported by a recommender system and,00:08:26,2,Push tends supported recommender system
5521,00:08:31,2,Text Access,1.2,pull tends to be supported by a search engine.,00:08:29,2,pull tends supported search engine
5522,00:08:35,2,Text Access,1.2,"Of course in the sophisticated intent in the information system,",00:08:31,2,Of course sophisticated intent information system
5523,00:08:36,2,Text Access,1.2,we should combine the two.,00:08:35,2,combine two
5524,00:08:41,2,Text Access,1.2,In the pull mode we have further distinguished querying and browsing.,00:08:38,2,In pull mode distinguished querying browsing
5525,00:08:46,2,Text Access,1.2,"Again, we generally want to combine the two ways to help users so",00:08:41,2,Again generally want combine two ways help users
5526,00:08:50,2,Text Access,1.2,that you can support both querying and browsing.,00:08:46,2,support querying browsing
5527,00:08:55,2,Text Access,1.2,If you want to know more about the relationship between pull and,00:08:51,2,If want know relationship pull
5528,00:08:58,2,Text Access,1.2,"push, you can read this article.",00:08:55,2,push read article
5529,00:09:03,2,Text Access,1.2,This gives a excellent discussion of the relationship between information filtering,00:08:58,2,This gives excellent discussion relationship information filtering
5530,00:09:05,2,Text Access,1.2,and information retrieval.,00:09:03,2,information retrieval
5531,00:09:10,2,Text Access,1.2,"Here information filtering is similar to information recommendation,",00:09:05,2,Here information filtering similar information recommendation
5532,00:09:12,2,Text Access,1.2,or the push mode of information access.,00:09:10,2,push mode information access
5533,00:00:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,[SOUND],00:00:00,7,SOUND
5534,00:00:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So average precision is computer for,00:00:11,7,So average precision computer
5535,00:00:14,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,just one.,00:00:13,7,one
5536,00:00:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,one query. But we generally experiment with many,00:00:14,7,one query But generally experiment many
5537,00:00:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,different queries and this is to avoid the variance across queries.,00:00:18,7,different queries avoid variance across queries
5538,00:00:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Depending on the queries you use you might make different conclusions.,00:00:24,7,Depending queries use might make different conclusions
5539,00:00:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Right, so it's better then using more queries.",00:00:29,7,Right better using queries
5540,00:00:36,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"If you use more queries then, you will also have to",00:00:33,7,If use queries also
5541,00:00:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,take the average of the average precision over all these queries.,00:00:36,7,take average average precision queries
5542,00:00:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So how can we do that?,00:00:41,7,So
5543,00:00:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Well, you can naturally.",00:00:43,7,Well naturally
5544,00:00:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Think of just doing arithmetic mean as we,00:00:46,7,Think arithmetic mean
5545,00:00:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"always tend to, to think in, in this way.",00:00:50,7,always tend think way
5546,00:01:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So, this would give us what's called a ""Mean Average Position"", or MAP.",00:00:56,7,So would give us called Mean Average Position MAP
5547,00:01:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"In this case,",00:01:02,7,In case
5548,00:01:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,we take arithmetic mean of all the average precisions over several queries or topics.,00:01:02,7,take arithmetic mean average precisions several queries topics
5549,00:01:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"But as I just mentioned in another lecture, is this good?",00:01:09,7,But I mentioned another lecture good
5550,00:01:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,We call that.,00:01:15,7,We call
5551,00:01:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,We talked about the different ways of combining precision and recall.,00:01:16,7,We talked different ways combining precision recall
5552,00:01:27,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And we conclude that the arithmetic mean is not as good as the MAP measure.,00:01:21,7,And conclude arithmetic mean good MAP measure
5553,00:01:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,But here it's the same.,00:01:27,7,But
5554,00:01:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,We can also think about the alternative ways of aggregating the numbers.,00:01:28,7,We also think alternative ways aggregating numbers
5555,00:01:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Don't just automatically assume that, though.",00:01:32,7,Don automatically assume though
5556,00:01:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Let's just also take the arithmetic mean of the average position over,00:01:34,7,Let also take arithmetic mean average position
5557,00:01:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,these queries.,00:01:37,7,queries
5558,00:01:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Let's think about what's the best way of aggregating them.,00:01:38,7,Let think best way aggregating
5559,00:01:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"If you think about the different ways, naturally you will,",00:01:42,7,If think different ways naturally
5560,00:01:49,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"probably be able to think about another way, which is geometric mean.",00:01:46,7,probably able think another way geometric mean
5561,00:01:53,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And we call this kind of average a gMAP.,00:01:51,7,And call kind average gMAP
5562,00:01:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,This is another way.,00:01:55,7,This another way
5563,00:01:59,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So now, once you think about the two different ways.",00:01:56,7,So think two different ways
5564,00:02:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Of doing the same thing.,00:01:59,7,Of thing
5565,00:02:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"The natural question to ask is, which one is better?",00:02:00,7,The natural question ask one better
5566,00:02:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So.,00:02:03,7,So
5567,00:02:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So, do you use MAP or gMAP?",00:02:05,7,So use MAP gMAP
5568,00:02:11,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Again, that's important question.",00:02:09,7,Again important question
5569,00:02:14,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Imagine you are again testing a new algorithm in,",00:02:11,7,Imagine testing new algorithm
5570,00:02:17,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,by comparing the ways your old algorithms made the search engine.,00:02:14,7,comparing ways old algorithms made search engine
5571,00:02:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Now you tested multiple topics.,00:02:18,7,Now tested multiple topics
5572,00:02:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Now you've got the average precision for these topics.,00:02:22,7,Now got average precision topics
5573,00:02:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Now you are thinking of looking at the overall performance.,00:02:25,7,Now thinking looking overall performance
5574,00:02:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,You have to take the average.,00:02:28,7,You take average
5575,00:02:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"But which, which strategy would you use?",00:02:30,7,But strategy would use
5576,00:02:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Now first, you should also think about the question, well did it make a difference?",00:02:34,7,Now first also think question well make difference
5577,00:02:43,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Can you think of scenarios where using one of them would make a difference?,00:02:38,7,Can think scenarios using one would make difference
5578,00:02:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,That is they would give different rankings of those methods.,00:02:43,7,That would give different rankings methods
5579,00:02:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And that also means depending on the way you average or detect the.,00:02:45,7,And also means depending way average detect
5580,00:02:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Average of these average positions.,00:02:52,7,Average average positions
5581,00:02:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,You will get different conclusions.,00:02:55,7,You get different conclusions
5582,00:03:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,This makes the question becoming even more important.,00:02:57,7,This makes question becoming even important
5583,00:03:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Right? So, which one would you use?",00:03:01,7,Right So one would use
5584,00:03:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Well again, if you look at the difference between these.",00:03:05,7,Well look difference
5585,00:03:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Different ways of aggregating the average position.,00:03:08,7,Different ways aggregating average position
5586,00:03:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"You'll realize in arithmetic mean, the sum is dominating by large values.",00:03:12,7,You realize arithmetic mean sum dominating large values
5587,00:03:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So what does large value here mean?,00:03:18,7,So large value mean
5588,00:03:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,It means the query is relatively easy.,00:03:20,7,It means query relatively easy
5589,00:03:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"You can have a high pres, average position.",00:03:22,7,You high pres average position
5590,00:03:29,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Whereas gMAP tends to be affected more by low values.,00:03:25,7,Whereas gMAP tends affected low values
5591,00:03:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And those are the queries that don't have good performance.,00:03:30,7,And queries good performance
5592,00:03:36,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,The average precision is low.,00:03:34,7,The average precision low
5593,00:03:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So if you think about the, improving the search engine for",00:03:37,7,So think improving search engine
5594,00:03:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"those difficult queries, then gMAP would be preferred, right?",00:03:41,7,difficult queries gMAP would preferred right
5595,00:03:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"On the other hand, if you just want to.",00:03:47,7,On hand want
5596,00:03:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Have improved a lot.,00:03:50,7,Have improved lot
5597,00:03:55,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Over all the kinds of queries or particular popular queries that might be,00:03:52,7,Over kinds queries particular popular queries might
5598,00:04:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,easy and you want to make the perfect and maybe MAP would be then preferred.,00:03:55,7,easy want make perfect maybe MAP would preferred
5599,00:04:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So again, the answer depends on your users, your users tasks and",00:04:00,7,So answer depends users users tasks
5600,00:04:06,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"their pref, their preferences.",00:04:05,7,pref preferences
5601,00:04:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So the point that here is to think about the multiple ways to solve,00:04:08,7,So point think multiple ways solve
5602,00:04:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"the same problem, and then compare them, and think carefully about the differences.",00:04:13,7,problem compare think carefully differences
5603,00:04:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And which one makes more sense.,00:04:18,7,And one makes sense
5604,00:04:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Often, when one of them might make sense in one situation and",00:04:20,7,Often one might make sense one situation
5605,00:04:27,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,another might make more sense in a different situation.,00:04:24,7,another might make sense different situation
5606,00:04:31,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So it's important to pick out under what situations one is preferred.,00:04:27,7,So important pick situations one preferred
5607,00:04:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"As a special case of the mean average position, we can also think about",00:04:35,7,As special case mean average position also think
5608,00:04:43,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,the case where there was precisely one rank in the document.,00:04:38,7,case precisely one rank document
5609,00:04:47,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"And this happens often, for example, in what's called a known item search.",00:04:43,7,And happens often example called known item search
5610,00:04:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Where you know a target page, let's say you have to find Amazon, homepage.",00:04:47,7,Where know target page let say find Amazon homepage
5611,00:04:56,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"You have one relevant document there, and you hope to find it.",00:04:52,7,You one relevant document hope find
5612,00:04:58,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"That's call a ""known item search"".",00:04:56,7,That call known item search
5613,00:05:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"In that case, there's precisely one relevant document.",00:04:58,7,In case precisely one relevant document
5614,00:05:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Or in another application, like a question and answering,",00:05:01,7,Or another application like question answering
5615,00:05:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,maybe there's only one answer.,00:05:03,7,maybe one answer
5616,00:05:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Are there.,00:05:04,7,Are
5617,00:05:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So if you rank the answers,",00:05:05,7,So rank answers
5618,00:05:12,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"then your goal is to rank that one particular answer on top, right?",00:05:07,7,goal rank one particular answer top right
5619,00:05:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So in this case, you can easily verify the average position,",00:05:12,7,So case easily verify average position
5620,00:05:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,will basically boil down to reciprocal rank.,00:05:16,7,basically boil reciprocal rank
5621,00:05:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"That is, 1 over r where r is the rank position of that single relevant document.",00:05:21,7,That 1 r r rank position single relevant document
5622,00:05:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So if that document is ranked on the very top or is 1, and",00:05:28,7,So document ranked top 1
5623,00:05:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,then it's 1 for reciprocal rank.,00:05:32,7,1 reciprocal rank
5624,00:05:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"If it's ranked at the, the second, then it's 1 over 2.",00:05:35,7,If ranked second 1 2
5625,00:05:40,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Et cetera.,00:05:39,7,Et cetera
5626,00:05:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"And then we can also take a, a average of all these average precision or",00:05:41,7,And also take average average precision
5627,00:05:48,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"reciprocal rank over a set of topics, and",00:05:45,7,reciprocal rank set topics
5628,00:05:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,that would give us something called a mean reciprocal rank.,00:05:48,7,would give us something called mean reciprocal rank
5629,00:05:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,It's a very popular measure.,00:05:52,7,It popular measure
5630,00:05:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"For no item search or, you know,",00:05:54,7,For item search know
5631,00:06:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,an problem where you have just one relevant item.,00:05:57,7,problem one relevant item
5632,00:06:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Now again here, you can see this r actually is meaningful here.",00:06:03,7,Now see r actually meaningful
5633,00:06:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And this r is basically indicating how much effort,00:06:09,7,And r basically indicating much effort
5634,00:06:18,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,a user would have to make in order to find that relevant document.,00:06:13,7,user would make order find relevant document
5635,00:06:23,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"If it's ranked on the top it's low effort that you have to make, or little effort.",00:06:18,7,If ranked top low effort make little effort
5636,00:06:26,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"But if it's ranked at 100 then you actually have to,",00:06:23,7,But ranked 100 actually
5637,00:06:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,read presumably 100 documents in order to find it.,00:06:27,7,read presumably 100 documents order find
5638,00:06:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So, in this sense r is also a meaningful measure and the reciprocal rank will",00:06:32,7,So sense r also meaningful measure reciprocal rank
5639,00:06:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"take the reciprocal of r, instead of using r directly.",00:06:37,7,take reciprocal r instead using r directly
5640,00:06:45,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So my natural question here is why not simply using r?,00:06:42,7,So natural question simply using r
5641,00:06:50,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"I imagine if you were to design a ratio to, measure the performance",00:06:45,7,I imagine design ratio measure performance
5642,00:06:54,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"of a random system, when there is only one relevant item.",00:06:50,7,random system one relevant item
5643,00:07:00,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,You might have thought about using r directly as the measure.,00:06:55,7,You might thought using r directly measure
5644,00:07:02,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"After all, that measures the user's effort, right?",00:07:00,7,After measures user effort right
5645,00:07:10,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"But, think about if you take a average of this over a large number of topics.",00:07:02,7,But think take average large number topics
5646,00:07:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Again it would make a difference.,00:07:12,7,Again would make difference
5647,00:07:16,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Right, for one single topic, using r or",00:07:13,7,Right one single topic using r
5648,00:07:19,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,using 1 over r wouldn't make any difference.,00:07:16,7,using 1 r make difference
5649,00:07:21,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,It's the same.,00:07:19,7,It
5650,00:07:24,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Larger r with corresponds to a small 1 over r, right?",00:07:21,7,Larger r corresponds small 1 r right
5651,00:07:32,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"But the difference would only show when, show up when you have many topics.",00:07:26,7,But difference would show show many topics
5652,00:07:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So again, think about the average of Mean Reciprocal Rank versus average of just r.",00:07:32,7,So think average Mean Reciprocal Rank versus average r
5653,00:07:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,What's the difference?,00:07:39,7,What difference
5654,00:07:41,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Do you see any difference?,00:07:39,7,Do see difference
5655,00:07:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"And would, would this difference change the oath of systems.",00:07:41,7,And would would difference change oath systems
5656,00:07:46,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,In our conclusion.,00:07:46,7,In conclusion
5657,00:07:53,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"And this, it turns out that, there is actually a big difference, and",00:07:49,7,And turns actually big difference
5658,00:07:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"if you think about it, if you want to think about it and then, yourself,",00:07:53,7,think want think
5659,00:07:58,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,then pause the video.,00:07:57,7,pause video
5660,00:08:04,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Basically, the difference is, if you take some of our directory, then.",00:07:59,7,Basically difference take directory
5661,00:08:07,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Again it will be dominated by large values of r.,00:08:04,7,Again dominated large values r
5662,00:08:08,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So what are those values?,00:08:07,7,So values
5663,00:08:15,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Those are basically large values that indicate that lower ranked results.,00:08:08,7,Those basically large values indicate lower ranked results
5664,00:08:20,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,That means the relevant items rank very low down on the list.,00:08:15,7,That means relevant items rank low list
5665,00:08:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And the sum that's also the average that would then be dominated by.,00:08:20,7,And sum also average would dominated
5666,00:08:28,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"Where those relevant documents are ranked in, in ,in,",00:08:25,7,Where relevant documents ranked
5667,00:08:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,in the lower portion of the ranked.,00:08:28,7,lower portion ranked
5668,00:08:35,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,But from a users perspective we care more about the highly ranked documents.,00:08:30,7,But users perspective care highly ranked documents
5669,00:08:39,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So by taking this transformation by using reciprocal rank.,00:08:35,7,So taking transformation using reciprocal rank
5670,00:08:43,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Here we emphasize more on the difference on the top.,00:08:40,7,Here emphasize difference top
5671,00:08:48,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"You know, think about the difference between 1 and the 2,",00:08:43,7,You know think difference 1 2
5672,00:08:52,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"it would make a big difference, in 1 over r, but think about the 100, and 1, and",00:08:48,7,would make big difference 1 r think 100 1
5673,00:08:57,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,where and when won't make much difference if you use this.,00:08:52,7,make much difference use
5674,00:09:01,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,But if you use this there will be a big difference in 100 and,00:08:57,7,But use big difference 100
5675,00:09:03,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"let's say 1,000, right.",00:09:01,7,let say 1 000 right
5676,00:09:05,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So this is not the desirable.,00:09:03,7,So desirable
5677,00:09:09,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"On the other hand, a 1 and 2 won't make much difference.",00:09:06,7,On hand 1 2 make much difference
5678,00:09:13,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,So this is yet another case where there may be multiple choices of doing the same,00:09:09,7,So yet another case may multiple choices
5679,00:09:15,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,thing and then you need to figure out which one makes more sense.,00:09:13,7,thing need figure one makes sense
5680,00:09:22,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,"So to summarize, we showed that the precision-recall curve.",00:09:17,7,So summarize showed precision recall curve
5681,00:09:25,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Can characterize the overall accuracy of a ranked list.,00:09:22,7,Can characterize overall accuracy ranked list
5682,00:09:30,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,And we emphasized that the actual utility of a ranked list depends,00:09:25,7,And emphasized actual utility ranked list depends
5683,00:09:34,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,on how many top ranked results a user would actually examine.,00:09:30,7,many top ranked results user would actually examine
5684,00:09:37,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Some users will examine more.,00:09:34,7,Some users examine
5685,00:09:38,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,Than others.,00:09:37,7,Than others
5686,00:09:42,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,An average person uses a standard measure for comparing two ranking methods.,00:09:38,7,An average person uses standard measure comparing two ranking methods
5687,00:09:44,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,It combines precision and recall and,00:09:42,7,It combines precision recall
5688,00:09:48,3,Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01),2.6,it's sensitive to the rank of every random document.,00:09:44,7,sensitive rank every random document
5689,00:00:07,5,Link Analysis - Part 1,4.3,[SOUND].,00:00:00,3,SOUND
5690,00:00:11,5,Link Analysis - Part 1,4.3,This lecture is about link analysis for web search.,00:00:07,3,This lecture link analysis web search
5691,00:00:18,5,Link Analysis - Part 1,4.3,"In this lecture we're going to talk about web search, and particularly",00:00:13,3,In lecture going talk web search particularly
5692,00:00:23,5,Link Analysis - Part 1,4.3,focusing on how to do link analysis and use the results to improve search.,00:00:18,3,focusing link analysis use results improve search
5693,00:00:31,5,Link Analysis - Part 1,4.3,The main topic of this lecture is to look at the ranking algorithms for web search.,00:00:25,3,The main topic lecture look ranking algorithms web search
5694,00:00:35,5,Link Analysis - Part 1,4.3,"In the previous lecture, we talked about how to create index.",00:00:32,3,In previous lecture talked create index
5695,00:00:40,5,Link Analysis - Part 1,4.3,"Now that we have got index, we want to see how we can improve",00:00:35,3,Now got index want see improve
5696,00:00:45,5,Link Analysis - Part 1,4.3,ranking of pages on the web.,00:00:40,3,ranking pages web
5697,00:00:48,5,Link Analysis - Part 1,4.3,"Standard IR models can also be applied here,",00:00:45,3,Standard IR models also applied
5698,00:00:53,5,Link Analysis - Part 1,4.3,"in fact they are important building blocks for supporting web search,",00:00:48,3,fact important building blocks supporting web search
5699,00:00:58,5,Link Analysis - Part 1,4.3,"but they aren't sufficient, mainly for the following reasons.",00:00:53,3,sufficient mainly following reasons
5700,00:01:02,5,Link Analysis - Part 1,4.3,"First, on the web we tend to have very different information needs.",00:00:58,3,First web tend different information needs
5701,00:01:07,5,Link Analysis - Part 1,4.3,"For example, people might search for a web page or entry page, and",00:01:02,3,For example people might search web page entry page
5702,00:01:11,5,Link Analysis - Part 1,4.3,this is different from the traditional library search,00:01:07,3,different traditional library search
5703,00:01:15,5,Link Analysis - Part 1,4.3,where people are primarily interested in collecting literature information.,00:01:11,3,people primarily interested collecting literature information
5704,00:01:19,5,Link Analysis - Part 1,4.3,"So these kind of queries are often called navigational queries,",00:01:15,3,So kind queries often called navigational queries
5705,00:01:23,5,Link Analysis - Part 1,4.3,the purpose is to navigate into a particular targeted page.,00:01:19,3,purpose navigate particular targeted page
5706,00:01:28,5,Link Analysis - Part 1,4.3,"So for such queries, we might benefit from using link information.",00:01:23,3,So queries might benefit using link information
5707,00:01:31,5,Link Analysis - Part 1,4.3,"Secondly, documents have additional information.",00:01:28,3,Secondly documents additional information
5708,00:01:34,5,Link Analysis - Part 1,4.3,"And on the web, web pages are web format.",00:01:31,3,And web web pages web format
5709,00:01:37,5,Link Analysis - Part 1,4.3,"There are a lot of other groups,",00:01:34,3,There lot groups
5710,00:01:40,5,Link Analysis - Part 1,4.3,"such as the layout, the title, or link information again.",00:01:37,3,layout title link information
5711,00:01:45,5,Link Analysis - Part 1,4.3,So this has provided an opportunity to,00:01:40,3,So provided opportunity
5712,00:01:49,5,Link Analysis - Part 1,4.3,use extra context information of the document to improve scoring.,00:01:45,3,use extra context information document improve scoring
5713,00:01:52,5,Link Analysis - Part 1,4.3,"And finally, information quality varies a lot.",00:01:49,3,And finally information quality varies lot
5714,00:01:58,5,Link Analysis - Part 1,4.3,So that means we have to consider many factors to improve the ranking algorithm.,00:01:52,3,So means consider many factors improve ranking algorithm
5715,00:02:03,5,Link Analysis - Part 1,4.3,This would give us a more robust way to rank the pages making it the harder for,00:01:58,3,This would give us robust way rank pages making harder
5716,00:02:09,5,Link Analysis - Part 1,4.3,any spammer to just manipulate the one signal to improve the ranking of a page.,00:02:03,3,spammer manipulate one signal improve ranking page
5717,00:02:14,5,Link Analysis - Part 1,4.3,So as a result people have made a number of major extensions,00:02:10,3,So result people made number major extensions
5718,00:02:15,5,Link Analysis - Part 1,4.3,to the ranking algorithms.,00:02:14,3,ranking algorithms
5719,00:02:20,5,Link Analysis - Part 1,4.3,One line is to exploit links to,00:02:15,3,One line exploit links
5720,00:02:24,5,Link Analysis - Part 1,4.3,improve scoring and that's the main topic of this lecture.,00:02:20,3,improve scoring main topic lecture
5721,00:02:30,5,Link Analysis - Part 1,4.3,People have also proposed algorithms to exploit large scale,00:02:26,3,People also proposed algorithms exploit large scale
5722,00:02:34,5,Link Analysis - Part 1,4.3,implicit feedback information in the form of clickthroughs.,00:02:30,3,implicit feedback information form clickthroughs
5723,00:02:37,5,Link Analysis - Part 1,4.3,"That's of course in the category of feedback techniques, and",00:02:34,3,That course category feedback techniques
5724,00:02:40,5,Link Analysis - Part 1,4.3,machinery is often used there.,00:02:37,3,machinery often used
5725,00:02:45,5,Link Analysis - Part 1,4.3,"In general, in web search the ranking algorithms are based on machinery",00:02:40,3,In general web search ranking algorithms based machinery
5726,00:02:49,5,Link Analysis - Part 1,4.3,algorithms to combine all kinds of features.,00:02:45,3,algorithms combine kinds features
5727,00:02:54,5,Link Analysis - Part 1,4.3,And many of them are based on the standard original models such as BM25 that,00:02:49,3,And many based standard original models BM25
5728,00:02:57,5,Link Analysis - Part 1,4.3,"we talked about, or",00:02:54,3,talked
5729,00:03:02,5,Link Analysis - Part 1,4.3,queried iCode to score different parts of documents or,00:02:57,3,queried iCode score different parts documents
5730,00:03:07,5,Link Analysis - Part 1,4.3,"to, provide additional features based on content matching.",00:03:02,3,provide additional features based content matching
5731,00:03:11,5,Link Analysis - Part 1,4.3,But link information is also very useful so,00:03:07,3,But link information also useful
5732,00:03:17,5,Link Analysis - Part 1,4.3,they provide additional scoring signals.,00:03:11,3,provide additional scoring signals
5733,00:03:21,5,Link Analysis - Part 1,4.3,So let's look at links in more detail on the web.,00:03:17,3,So let look links detail web
5734,00:03:26,5,Link Analysis - Part 1,4.3,"So this is a snapshot of some part of the web, let's say.",00:03:21,3,So snapshot part web let say
5735,00:03:30,5,Link Analysis - Part 1,4.3,So we can see there are many links that link different pages together.,00:03:26,3,So see many links link different pages together
5736,00:03:35,5,Link Analysis - Part 1,4.3,"And in this case you can also look at the, the center here.",00:03:30,3,And case also look center
5737,00:03:39,5,Link Analysis - Part 1,4.3,There is a description of a link that's pointing to the document on,00:03:35,3,There description link pointing document
5738,00:03:40,5,Link Analysis - Part 1,4.3,the right side.,00:03:39,3,right side
5739,00:03:42,5,Link Analysis - Part 1,4.3,Now this description text is called anchor text.,00:03:40,3,Now description text called anchor text
5740,00:03:48,5,Link Analysis - Part 1,4.3,"If you think about this text, it's actually quite useful",00:03:44,3,If think text actually quite useful
5741,00:03:53,5,Link Analysis - Part 1,4.3,because it provides some extra description of that page being pointed to.,00:03:48,3,provides extra description page pointed
5742,00:03:59,5,Link Analysis - Part 1,4.3,"So, for example, if someone wants to bookmark Amazon.com front page,",00:03:53,3,So example someone wants bookmark Amazon com front page
5743,00:04:04,5,Link Analysis - Part 1,4.3,"the person might say, the big online bookstore, and",00:03:59,3,person might say big online bookstore
5744,00:04:07,5,Link Analysis - Part 1,4.3,"then with a link to Amazon, right?",00:04:04,3,link Amazon right
5745,00:04:11,5,Link Analysis - Part 1,4.3,So the description here is actually very similar to what the user would type in,00:04:07,3,So description actually similar user would type
5746,00:04:15,5,Link Analysis - Part 1,4.3,in the query box when they are looking for such a page.,00:04:11,3,query box looking page
5747,00:04:19,5,Link Analysis - Part 1,4.3,"That's why it's very useful for, for, ranking pages.",00:04:15,3,That useful ranking pages
5748,00:04:25,5,Link Analysis - Part 1,4.3,Suppose someone types in a query like online bookstore or,00:04:19,3,Suppose someone types query like online bookstore
5749,00:04:28,5,Link Analysis - Part 1,4.3,"big online bookstore, right.",00:04:25,3,big online bookstore right
5750,00:04:35,5,Link Analysis - Part 1,4.3,The query would match this anchor text in the page here.,00:04:28,3,The query would match anchor text page
5751,00:04:39,5,Link Analysis - Part 1,4.3,And then this actually provides evidence for,00:04:35,3,And actually provides evidence
5752,00:04:45,5,Link Analysis - Part 1,4.3,"matching the page that's been pointed to, that is the Amazon entry page.",00:04:39,3,matching page pointed Amazon entry page
5753,00:04:50,5,Link Analysis - Part 1,4.3,"So if you match the anchor text that describes the link to a page,",00:04:45,3,So match anchor text describes link page
5754,00:04:54,5,Link Analysis - Part 1,4.3,actually that provides good evidence for,00:04:50,3,actually provides good evidence
5755,00:04:58,5,Link Analysis - Part 1,4.3,the relevance of the page being pointing to.,00:04:54,3,relevance page pointing
5756,00:04:59,5,Link Analysis - Part 1,4.3,So anchor text is very useful.,00:04:58,3,So anchor text useful
5757,00:05:04,5,Link Analysis - Part 1,4.3,"If you look at the bottom part of this picture, you can also see there are some",00:05:00,3,If look bottom part picture also see
5758,00:05:08,5,Link Analysis - Part 1,4.3,"patterns of links, and these links might indicate the utility of a document.",00:05:04,3,patterns links links might indicate utility document
5759,00:05:09,5,Link Analysis - Part 1,4.3,"So for example,",00:05:08,3,So example
5760,00:05:14,5,Link Analysis - Part 1,4.3,"on the right side you can see this page has received many in, in links.",00:05:09,3,right side see page received many links
5761,00:05:16,5,Link Analysis - Part 1,4.3,That means many other pages are pointing to this page.,00:05:14,3,That means many pages pointing page
5762,00:05:20,5,Link Analysis - Part 1,4.3,And this shows that this page is quite useful.,00:05:16,3,And shows page quite useful
5763,00:05:25,5,Link Analysis - Part 1,4.3,"On the left side you can see, this is a page that points to many other pages.",00:05:21,3,On left side see page points many pages
5764,00:05:29,5,Link Analysis - Part 1,4.3,"So, this is a theater page that would allow you to",00:05:25,3,So theater page would allow
5765,00:05:31,5,Link Analysis - Part 1,4.3,actually see a lot of other pages.,00:05:29,3,actually see lot pages
5766,00:05:38,5,Link Analysis - Part 1,4.3,So we can call the first case authority page and the second case a hub page.,00:05:32,3,So call first case authority page second case hub page
5767,00:05:41,5,Link Analysis - Part 1,4.3,This means the link information can help in two ways.,00:05:38,3,This means link information help two ways
5768,00:05:44,5,Link Analysis - Part 1,4.3,One is to provide extra text for matching.,00:05:41,3,One provide extra text matching
5769,00:05:49,5,Link Analysis - Part 1,4.3,The other is to provide some additional scores for the web,00:05:44,3,The provide additional scores web
5770,00:05:54,5,Link Analysis - Part 1,4.3,"pages to characterize how likely a page is a hub, how likely a page is a authority.",00:05:49,3,pages characterize likely page hub likely page authority
5771,00:06:02,5,Link Analysis - Part 1,4.3,"So people then, of course, propose ideas to leverage this, this link information.",00:05:55,3,So people course propose ideas leverage link information
5772,00:06:04,5,Link Analysis - Part 1,4.3,"Google's PageRank,",00:06:02,3,Google PageRank
5773,00:06:09,5,Link Analysis - Part 1,4.3,"which was a main technique that they used in early days, is a good example.",00:06:04,3,main technique used early days good example
5774,00:06:14,5,Link Analysis - Part 1,4.3,"And that, that is the algorithm to capture page popularity,",00:06:09,3,And algorithm capture page popularity
5775,00:06:17,5,Link Analysis - Part 1,4.3,basically to score authority.,00:06:14,3,basically score authority
5776,00:06:21,5,Link Analysis - Part 1,4.3,"So the intuitions here are, links are just like citations in the literature.",00:06:17,3,So intuitions links like citations literature
5777,00:06:24,5,Link Analysis - Part 1,4.3,Think about one page pointing to another page.,00:06:21,3,Think one page pointing another page
5778,00:06:27,5,Link Analysis - Part 1,4.3,This is very similar to one paper citing another paper.,00:06:24,3,This similar one paper citing another paper
5779,00:06:30,5,Link Analysis - Part 1,4.3,"So, of course, then if a page is cited often,",00:06:27,3,So course page cited often
5780,00:06:35,5,Link Analysis - Part 1,4.3,"then we can assume this page to be more useful in general, right?",00:06:30,3,assume page useful general right
5781,00:06:36,5,Link Analysis - Part 1,4.3,So that's a very good intuition.,00:06:35,3,So good intuition
5782,00:06:42,5,Link Analysis - Part 1,4.3,"Now, page rank is essentially to take advantage of this",00:06:38,3,Now page rank essentially take advantage
5783,00:06:46,5,Link Analysis - Part 1,4.3,"intuition to implement the, with the principle approach.",00:06:42,3,intuition implement principle approach
5784,00:06:51,5,Link Analysis - Part 1,4.3,Intuitively it's essentially doing citation counting or in link counting.,00:06:46,3,Intuitively essentially citation counting link counting
5785,00:06:56,5,Link Analysis - Part 1,4.3,"It just improves this simple idea in, in two ways.",00:06:51,3,It improves simple idea two ways
5786,00:06:59,5,Link Analysis - Part 1,4.3,One is would consider indirect citations.,00:06:56,3,One would consider indirect citations
5787,00:07:03,5,Link Analysis - Part 1,4.3,"So that means you don't just look at the how many in links you have,",00:06:59,3,So means look many links
5788,00:07:08,5,Link Analysis - Part 1,4.3,you also look at the what are those pages that are pointing to you.,00:07:03,3,also look pages pointing
5789,00:07:13,5,Link Analysis - Part 1,4.3,"If those pages, themselves, have a lot of in links, well that means a lot.",00:07:08,3,If pages lot links well means lot
5790,00:07:15,5,Link Analysis - Part 1,4.3,In some sense you will get some credit from that.,00:07:13,3,In sense get credit
5791,00:07:20,5,Link Analysis - Part 1,4.3,But if those pages that are pointing to you are not are being pointed to,00:07:16,3,But pages pointing pointed
5792,00:07:25,5,Link Analysis - Part 1,4.3,"by other pages, they themselves don't have many in links, then, well,",00:07:20,3,pages many links well
5793,00:07:27,5,Link Analysis - Part 1,4.3,you don't get that much credit.,00:07:25,3,get much credit
5794,00:07:29,5,Link Analysis - Part 1,4.3,So that's the idea of getting indirect citation.,00:07:27,3,So idea getting indirect citation
5795,00:07:35,5,Link Analysis - Part 1,4.3,"Right, so you can also understand this idea by looking at, again,",00:07:31,3,Right also understand idea looking
5796,00:07:36,5,Link Analysis - Part 1,4.3,the research papers.,00:07:35,3,research papers
5797,00:07:42,5,Link Analysis - Part 1,4.3,"If you are cited by, let's say ten papers, and those ten papers are, just",00:07:36,3,If cited let say ten papers ten papers
5798,00:07:48,5,Link Analysis - Part 1,4.3,"workshop papers and that, or some papers that are not very influential, right,",00:07:42,3,workshop papers papers influential right
5799,00:07:54,5,Link Analysis - Part 1,4.3,"so although you got ten in links, that's not as good as if you have,",00:07:49,3,although got ten links good
5800,00:07:59,5,Link Analysis - Part 1,4.3,you're cited by ten papers that themselves have attracted a lot of other citations.,00:07:54,3,cited ten papers attracted lot citations
5801,00:08:04,5,Link Analysis - Part 1,4.3,So this is a case where,00:07:59,3,So case
5802,00:08:09,5,Link Analysis - Part 1,4.3,we would like to consider indirect links and PageRank does that.,00:08:04,3,would like consider indirect links PageRank
5803,00:08:13,5,Link Analysis - Part 1,4.3,"The other idea is, it's good to smooth the citations.",00:08:09,3,The idea good smooth citations
5804,00:08:17,5,Link Analysis - Part 1,4.3,"Or, or, or assume that basically every page is",00:08:13,3,Or assume basically every page
5805,00:08:21,5,Link Analysis - Part 1,4.3,having a non-zero pseudo citation count.,00:08:17,3,non zero pseudo citation count
5806,00:08:24,5,Link Analysis - Part 1,4.3,"Essentially, you are trying to imagine there are many virtual links that",00:08:21,3,Essentially trying imagine many virtual links
5807,00:08:28,5,Link Analysis - Part 1,4.3,"will link all the pages together so that you,",00:08:24,3,link pages together
5808,00:08:32,5,Link Analysis - Part 1,4.3,you actually get pseudo citations from everyone.,00:08:28,3,actually get pseudo citations everyone
5809,00:08:39,5,Link Analysis - Part 1,4.3,"The, the reason why they want to do that is this would allow them",00:08:34,3,The reason want would allow
5810,00:08:45,5,Link Analysis - Part 1,4.3,to solve the problem elegantly with linear algebra technique.,00:08:39,3,solve problem elegantly linear algebra technique
5811,00:08:52,5,Link Analysis - Part 1,4.3,So I think maybe the best way to understand the page,00:08:47,3,So I think maybe best way understand page
5812,00:08:57,5,Link Analysis - Part 1,4.3,"rank is through think of this as through computer,",00:08:52,3,rank think computer
5813,00:09:03,5,Link Analysis - Part 1,4.3,"the probability of a random surfer, visiting every web page, right.",00:08:57,3,probability random surfer visiting every web page right
5814,00:00:06,2,Overview of Text Retrieval Methods,1.4,[MUSIC],00:00:00,4,MUSIC
5815,00:00:10,2,Overview of Text Retrieval Methods,1.4,This lecture is a overview of text retrieval methods.,00:00:06,4,This lecture overview text retrieval methods
5816,00:00:17,2,Overview of Text Retrieval Methods,1.4,In the previous lecture we introduced you to the problem of text retrieval.,00:00:10,4,In previous lecture introduced problem text retrieval
5817,00:00:22,2,Overview of Text Retrieval Methods,1.4,We explained that the main problem is to design a ranking function to rank,00:00:17,4,We explained main problem design ranking function rank
5818,00:00:24,2,Overview of Text Retrieval Methods,1.4,documents for a query.,00:00:22,4,documents query
5819,00:00:25,2,Overview of Text Retrieval Methods,1.4,"In this lecture,",00:00:24,4,In lecture
5820,00:00:31,2,Overview of Text Retrieval Methods,1.4,we will give a overview of different ways of designing this ranking function.,00:00:25,4,give overview different ways designing ranking function
5821,00:00:35,2,Overview of Text Retrieval Methods,1.4,So the problem is the following.,00:00:33,4,So problem following
5822,00:00:39,2,Overview of Text Retrieval Methods,1.4,"We have a query that has a sequence of words, and",00:00:35,4,We query sequence words
5823,00:00:44,2,Overview of Text Retrieval Methods,1.4,"a document that, that's also a sequence of words, and we hope to define the function",00:00:39,4,document also sequence words hope define function
5824,00:00:49,2,Overview of Text Retrieval Methods,1.4,f that can compute a score based on the query and document.,00:00:44,4,f compute score based query document
5825,00:00:54,2,Overview of Text Retrieval Methods,1.4,So the main challenge you here is with designing a good ranking function that can,00:00:51,4,So main challenge designing good ranking function
5826,00:01:00,2,Overview of Text Retrieval Methods,1.4,"rank all the relevant documents, on top of all the non-relevant ones.",00:00:54,4,rank relevant documents top non relevant ones
5827,00:01:04,2,Overview of Text Retrieval Methods,1.4,Now clearly this means our function must be able to measure,00:01:00,4,Now clearly means function must able measure
5828,00:01:09,2,Overview of Text Retrieval Methods,1.4,the likelihood that a document d is relevant to a query q.,00:01:04,4,likelihood document relevant query q
5829,00:01:15,2,Overview of Text Retrieval Methods,1.4,That also means we have to have some way to define relevance.,00:01:09,4,That also means way define relevance
5830,00:01:20,2,Overview of Text Retrieval Methods,1.4,In particular in order to implement the program to do that we have to have,00:01:16,4,In particular order implement program
5831,00:01:25,2,Overview of Text Retrieval Methods,1.4,"a computational definition of relevance, and we achieve this goal by",00:01:20,4,computational definition relevance achieve goal
5832,00:01:30,2,Overview of Text Retrieval Methods,1.4,"designing a retrieval model, which gives us a formalization of relevance.",00:01:25,4,designing retrieval model gives us formalization relevance
5833,00:01:35,2,Overview of Text Retrieval Methods,1.4,"Now, over many decades, researchers have designed",00:01:32,4,Now many decades researchers designed
5834,00:01:40,2,Overview of Text Retrieval Methods,1.4,"many different kinds of retrieval models, and they fall into different categories.",00:01:35,4,many different kinds retrieval models fall different categories
5835,00:01:48,2,Overview of Text Retrieval Methods,1.4,"First, one fair many of the models are based on the similarity idea.",00:01:42,4,First one fair many models based similarity idea
5836,00:01:55,2,Overview of Text Retrieval Methods,1.4,"Basically, we assume that if a document is more similar to the query",00:01:50,4,Basically assume document similar query
5837,00:01:57,2,Overview of Text Retrieval Methods,1.4,"than another document is,",00:01:55,4,another document
5838,00:02:02,2,Overview of Text Retrieval Methods,1.4,then we would say the first document is more relevant than the second one.,00:01:57,4,would say first document relevant second one
5839,00:02:05,2,Overview of Text Retrieval Methods,1.4,"So in this case, the ranking function is defined as",00:02:02,4,So case ranking function defined
5840,00:02:08,2,Overview of Text Retrieval Methods,1.4,the similarity between the query and the document.,00:02:05,4,similarity query document
5841,00:02:13,2,Overview of Text Retrieval Methods,1.4,"One well known example in this case is vector space model,",00:02:10,4,One well known example case vector space model
5842,00:02:16,2,Overview of Text Retrieval Methods,1.4,which we will cover more in detail later in the lecture.,00:02:13,4,cover detail later lecture
5843,00:02:23,2,Overview of Text Retrieval Methods,1.4,The second kind of models are called probabilistic models.,00:02:19,4,The second kind models called probabilistic models
5844,00:02:28,2,Overview of Text Retrieval Methods,1.4,"In this family of models, we follow a very different strategy.",00:02:23,4,In family models follow different strategy
5845,00:02:31,2,Overview of Text Retrieval Methods,1.4,While we assume that queries and,00:02:28,4,While assume queries
5846,00:02:36,2,Overview of Text Retrieval Methods,1.4,"documents are all observations from random variables, and",00:02:31,4,documents observations random variables
5847,00:02:41,2,Overview of Text Retrieval Methods,1.4,"we assume there is a binary random variable called R here,",00:02:36,4,assume binary random variable called R
5848,00:02:45,2,Overview of Text Retrieval Methods,1.4,to indicate whether a document is relevant to a query.,00:02:42,4,indicate whether document relevant query
5849,00:02:52,2,Overview of Text Retrieval Methods,1.4,We then define the score of document with respect to a query as is a probability,00:02:46,4,We define score document respect query probability
5850,00:02:59,2,Overview of Text Retrieval Methods,1.4,"that this random variable R is equal to 1, given a particular document and query.",00:02:52,4,random variable R equal 1 given particular document query
5851,00:03:03,2,Overview of Text Retrieval Methods,1.4,There are different cases of such a general idea.,00:02:59,4,There different cases general idea
5852,00:03:08,2,Overview of Text Retrieval Methods,1.4,"One is classic probabilistic model, another is language model, yet",00:03:04,4,One classic probabilistic model another language model yet
5853,00:03:10,2,Overview of Text Retrieval Methods,1.4,another is divergence-from-randomness model.,00:03:08,4,another divergence randomness model
5854,00:03:15,2,Overview of Text Retrieval Methods,1.4,"In a later lecture, we will talk more about the, one case,",00:03:12,4,In later lecture talk one case
5855,00:03:16,2,Overview of Text Retrieval Methods,1.4,which is language model.,00:03:15,4,language model
5856,00:03:21,2,Overview of Text Retrieval Methods,1.4,The third kind of models of this is probabilistic inference.,00:03:18,4,The third kind models probabilistic inference
5857,00:03:27,2,Overview of Text Retrieval Methods,1.4,So here the idea is to associate uncertainty to inference rules.,00:03:21,4,So idea associate uncertainty inference rules
5858,00:03:32,2,Overview of Text Retrieval Methods,1.4,And we can then quantify the probability that we can show that the query,00:03:27,4,And quantify probability show query
5859,00:03:34,2,Overview of Text Retrieval Methods,1.4,follows from the document.,00:03:32,4,follows document
5860,00:03:44,2,Overview of Text Retrieval Methods,1.4,"Finally, there is also a family of models that are using axiomatic thinking.",00:03:35,4,Finally also family models using axiomatic thinking
5861,00:03:49,2,Overview of Text Retrieval Methods,1.4,Here the idea is to define a set of constraints that,00:03:44,4,Here idea define set constraints
5862,00:03:54,2,Overview of Text Retrieval Methods,1.4,we hope a good retrieval function to satisfy.,00:03:49,4,hope good retrieval function satisfy
5863,00:03:57,2,Overview of Text Retrieval Methods,1.4,So in this case the problem is you seek,00:03:55,4,So case problem seek
5864,00:04:04,2,Overview of Text Retrieval Methods,1.4,a good ranking function that can satisfy all the desired constraints.,00:03:58,4,good ranking function satisfy desired constraints
5865,00:04:09,2,Overview of Text Retrieval Methods,1.4,"Interestingly, although these different models are based on different thinking,",00:04:06,4,Interestingly although different models based different thinking
5866,00:04:17,2,Overview of Text Retrieval Methods,1.4,in the end the retrieval function tends to be very similar.,00:04:11,4,end retrieval function tends similar
5867,00:04:22,2,Overview of Text Retrieval Methods,1.4,And these functions tend to also involve similar variables.,00:04:17,4,And functions tend also involve similar variables
5868,00:04:26,2,Overview of Text Retrieval Methods,1.4,"So now let's take a look at the, the common form of a state of that retrieval",00:04:22,4,So let take look common form state retrieval
5869,00:04:32,2,Overview of Text Retrieval Methods,1.4,model and examine some of the common ideas used in all these models.,00:04:26,4,model examine common ideas used models
5870,00:04:38,2,Overview of Text Retrieval Methods,1.4,"First, these models are all based on the assumption",00:04:33,4,First models based assumption
5871,00:04:42,2,Overview of Text Retrieval Methods,1.4,of using bag of words for representing text.,00:04:38,4,using bag words representing text
5872,00:04:47,2,Overview of Text Retrieval Methods,1.4,And we explained this in the natural language processing lecture.,00:04:42,4,And explained natural language processing lecture
5873,00:04:51,2,Overview of Text Retrieval Methods,1.4,Bag of words representation remains the main representation used in all,00:04:47,4,Bag words representation remains main representation used
5874,00:04:52,2,Overview of Text Retrieval Methods,1.4,the search engines.,00:04:51,4,search engines
5875,00:04:58,2,Overview of Text Retrieval Methods,1.4,"So, with this assumption, the score of a query like a presidential",00:04:53,4,So assumption score query like presidential
5876,00:05:03,2,Overview of Text Retrieval Methods,1.4,"campaign news, with respect to a document d here,",00:04:58,4,campaign news respect document
5877,00:05:08,2,Overview of Text Retrieval Methods,1.4,"would be based on scores computed at, based on each individual word.",00:05:03,4,would based scores computed based individual word
5878,00:05:15,2,Overview of Text Retrieval Methods,1.4,"And that means the score would depend on the score of each word,",00:05:09,4,And means score would depend score word
5879,00:05:19,2,Overview of Text Retrieval Methods,1.4,"such as presidential, campaign, and news.",00:05:15,4,presidential campaign news
5880,00:05:23,2,Overview of Text Retrieval Methods,1.4,"Here we can see there are three different components,",00:05:19,4,Here see three different components
5881,00:05:29,2,Overview of Text Retrieval Methods,1.4,each corresponding to how well the document matches each of the query words.,00:05:23,4,corresponding well document matches query words
5882,00:05:36,2,Overview of Text Retrieval Methods,1.4,"Inside of these functions, we see a number of heuristics views.",00:05:31,4,Inside functions see number heuristics views
5883,00:05:43,2,Overview of Text Retrieval Methods,1.4,"So for example, one factor that affects the function g here is how",00:05:37,4,So example one factor affects function g
5884,00:05:48,2,Overview of Text Retrieval Methods,1.4,many times does the word presidential occur in the document?,00:05:43,4,many times word presidential occur document
5885,00:05:50,2,Overview of Text Retrieval Methods,1.4,This is called a Term Frequency or TF.,00:05:48,4,This called Term Frequency TF
5886,00:05:55,2,Overview of Text Retrieval Methods,1.4,We might also denote as c of presidential and d.,00:05:50,4,We might also denote c presidential
5887,00:06:00,2,Overview of Text Retrieval Methods,1.4,In general if the word occurs more frequently in,00:05:55,4,In general word occurs frequently
5888,00:06:07,2,Overview of Text Retrieval Methods,1.4,the document then the value of this function would be larger.,00:06:00,4,document value function would larger
5889,00:06:12,2,Overview of Text Retrieval Methods,1.4,"Another factor is how long is the document, and",00:06:07,4,Another factor long document
5890,00:06:17,2,Overview of Text Retrieval Methods,1.4,this is so to use the document length for score.,00:06:12,4,use document length score
5891,00:06:22,2,Overview of Text Retrieval Methods,1.4,"In general, if a term occurs in a long",00:06:17,4,In general term occurs long
5892,00:06:28,2,Overview of Text Retrieval Methods,1.4,"document that many times, it's not as significant as",00:06:22,4,document many times significant
5893,00:06:32,2,Overview of Text Retrieval Methods,1.4,if it occurred the same number of times in a short document.,00:06:28,4,occurred number times short document
5894,00:06:39,2,Overview of Text Retrieval Methods,1.4,Because in the long document any term is expected to occur more frequently.,00:06:32,4,Because long document term expected occur frequently
5895,00:06:44,2,Overview of Text Retrieval Methods,1.4,"Finally, there is this factor called a document frequency, and that is we also",00:06:39,4,Finally factor called document frequency also
5896,00:06:49,2,Overview of Text Retrieval Methods,1.4,want to look at how often presidential occurs in the entire collection.,00:06:44,4,want look often presidential occurs entire collection
5897,00:06:54,2,Overview of Text Retrieval Methods,1.4,"And we call this Document Frequency, or DF, of presidential.",00:06:49,4,And call Document Frequency DF presidential
5898,00:07:00,2,Overview of Text Retrieval Methods,1.4,And in some other models we might also use a probability,00:06:54,4,And models might also use probability
5899,00:07:04,2,Overview of Text Retrieval Methods,1.4,to characterize this information.,00:07:00,4,characterize information
5900,00:07:09,2,Overview of Text Retrieval Methods,1.4,"So here, I show the probability of presidential in the collection.",00:07:04,4,So I show probability presidential collection
5901,00:07:13,2,Overview of Text Retrieval Methods,1.4,So all these are trying to characterize the popularity of,00:07:10,4,So trying characterize popularity
5902,00:07:15,2,Overview of Text Retrieval Methods,1.4,the term in the collection.,00:07:13,4,term collection
5903,00:07:18,2,Overview of Text Retrieval Methods,1.4,"In general, matching a rare term in the collection",00:07:15,4,In general matching rare term collection
5904,00:07:23,2,Overview of Text Retrieval Methods,1.4,is contributing more to the overall score then matching a common term.,00:07:19,4,contributing overall score matching common term
5905,00:07:30,2,Overview of Text Retrieval Methods,1.4,So this captures some of the main ideas used in pretty much all the state of,00:07:25,4,So captures main ideas used pretty much state
5906,00:07:32,2,Overview of Text Retrieval Methods,1.4,the art retrieval models.,00:07:30,4,art retrieval models
5907,00:07:38,2,Overview of Text Retrieval Methods,1.4,"So now, a natural question is which model works the best?",00:07:34,4,So natural question model works best
5908,00:07:46,2,Overview of Text Retrieval Methods,1.4,"Now, it turns out that many models work equally well, so here I listed the four",00:07:40,4,Now turns many models work equally well I listed four
5909,00:07:52,2,Overview of Text Retrieval Methods,1.4,major models that are generally regarded as a state of the art retrieval models.,00:07:46,4,major models generally regarded state art retrieval models
5910,00:07:56,2,Overview of Text Retrieval Methods,1.4,"Pivoted length normalization, BM25, query likelihood, PL2.",00:07:52,4,Pivoted length normalization BM25 query likelihood PL2
5911,00:08:02,2,Overview of Text Retrieval Methods,1.4,"When optimized these models tend to perform similarly and this was,",00:07:56,4,When optimized models tend perform similarly
5912,00:08:07,2,Overview of Text Retrieval Methods,1.4,discussed in detail in this reference at the end of this lecture.,00:08:02,4,discussed detail reference end lecture
5913,00:08:13,2,Overview of Text Retrieval Methods,1.4,"Among all these, BM25 is probably the most popular.",00:08:07,4,Among BM25 probably popular
5914,00:08:17,2,Overview of Text Retrieval Methods,1.4,"It's most likely that this has been used in virtually all the search engines,",00:08:13,4,It likely used virtually search engines
5915,00:08:21,2,Overview of Text Retrieval Methods,1.4,and you will also often see this method discussed in research papers.,00:08:17,4,also often see method discussed research papers
5916,00:08:27,2,Overview of Text Retrieval Methods,1.4,And we'll talk more about this method later in some other lectures.,00:08:22,4,And talk method later lectures
5917,00:08:36,2,Overview of Text Retrieval Methods,1.4,"So, to summarize, the main points made in this lecture are, first the design",00:08:30,4,So summarize main points made lecture first design
5918,00:08:41,2,Overview of Text Retrieval Methods,1.4,"of a good ranking function pre-requires a computational definition of relevance, and",00:08:36,4,good ranking function pre requires computational definition relevance
5919,00:08:45,2,Overview of Text Retrieval Methods,1.4,we achieve this goal by designing a proper retrieval model.,00:08:41,4,achieve goal designing proper retrieval model
5920,00:08:52,2,Overview of Text Retrieval Methods,1.4,"Second, many models are equally effective but we don't have a single winner here.",00:08:47,4,Second many models equally effective single winner
5921,00:08:55,2,Overview of Text Retrieval Methods,1.4,"Researchers are still actively working on this problem,",00:08:52,4,Researchers still actively working problem
5922,00:08:58,2,Overview of Text Retrieval Methods,1.4,trying to find a truly optimal retrieval model.,00:08:55,4,trying find truly optimal retrieval model
5923,00:09:04,2,Overview of Text Retrieval Methods,1.4,"Finally, the state of the art ranking functions tend to rely on",00:09:01,4,Finally state art ranking functions tend rely
5924,00:09:05,2,Overview of Text Retrieval Methods,1.4,the following ideas.,00:09:04,4,following ideas
5925,00:09:08,2,Overview of Text Retrieval Methods,1.4,"First, bag of words representation.",00:09:05,4,First bag words representation
5926,00:09:14,2,Overview of Text Retrieval Methods,1.4,"Second, TF and the document frequency of words.",00:09:08,4,Second TF document frequency words
5927,00:09:19,2,Overview of Text Retrieval Methods,1.4,Such information is used when ranking function to determine,00:09:14,4,Such information used ranking function determine
5928,00:09:24,2,Overview of Text Retrieval Methods,1.4,"the overall contribution of matching a word, and document length.",00:09:19,4,overall contribution matching word document length
5929,00:09:29,2,Overview of Text Retrieval Methods,1.4,These are often combined in interesting ways and we'll discuss,00:09:25,4,These often combined interesting ways discuss
5930,00:09:34,2,Overview of Text Retrieval Methods,1.4,how exactly they are combined to rank documents in the lectures later.,00:09:29,4,exactly combined rank documents lectures later
5931,00:09:40,2,Overview of Text Retrieval Methods,1.4,There are two suggested additional readings if you have time.,00:09:36,4,There two suggested additional readings time
5932,00:09:45,2,Overview of Text Retrieval Methods,1.4,The first is a paper where you can find a detailed discussion and,00:09:41,4,The first paper find detailed discussion
5933,00:09:48,2,Overview of Text Retrieval Methods,1.4,comparison of multiple state of the art models.,00:09:45,4,comparison multiple state art models
5934,00:09:53,2,Overview of Text Retrieval Methods,1.4,"The second, is a book with a chapter that gives a broad review of",00:09:49,4,The second book chapter gives broad review
5935,00:09:55,2,Overview of Text Retrieval Methods,1.4,different retrieval models.,00:09:53,4,different retrieval models
5936,00:00:03,4,Smoothing of Language Model - Part 2,3.4,[SOUND],00:00:00,5,SOUND
5937,00:00:16,4,Smoothing of Language Model - Part 2,3.4,"So, I showed you how we rewrite the into",00:00:12,5,So I showed rewrite
5938,00:00:21,4,Smoothing of Language Model - Part 2,3.4,"a form that looks like a, the formula on this slide.",00:00:16,5,form looks like formula slide
5939,00:00:26,4,Smoothing of Language Model - Part 2,3.4,After we make the assumption about smoothing the language model,00:00:21,5,After make assumption smoothing language model
5940,00:00:30,4,Smoothing of Language Model - Part 2,3.4,based on the collection of the language model.,00:00:27,5,based collection language model
5941,00:00:36,4,Smoothing of Language Model - Part 2,3.4,"Now, if we look at the, this rewriting, it actually would give us two benefits.",00:00:30,5,Now look rewriting actually would give us two benefits
5942,00:00:42,4,Smoothing of Language Model - Part 2,3.4,"The first benefit is, it helps us better understand that this ranking function.",00:00:36,5,The first benefit helps us better understand ranking function
5943,00:00:47,4,Smoothing of Language Model - Part 2,3.4,"In particular, we're going to show that from this formula we can see smoothing is",00:00:42,5,In particular going show formula see smoothing
5944,00:00:51,4,Smoothing of Language Model - Part 2,3.4,the correction that we model will give us something like a TF-IDF weighting and,00:00:47,5,correction model give us something like TF IDF weighting
5945,00:00:52,4,Smoothing of Language Model - Part 2,3.4,length normalization.,00:00:51,5,length normalization
5946,00:00:57,4,Smoothing of Language Model - Part 2,3.4,The second benefit is that it also allows us to,00:00:52,5,The second benefit also allows us
5947,00:01:01,4,Smoothing of Language Model - Part 2,3.4,compute the query likelihood more efficiently.,00:00:57,5,compute query likelihood efficiently
5948,00:01:03,4,Smoothing of Language Model - Part 2,3.4,"In particular,",00:01:02,5,In particular
5949,00:01:07,4,Smoothing of Language Model - Part 2,3.4,we see that the main part of the formula is a sum over the matching query terms.,00:01:03,5,see main part formula sum matching query terms
5950,00:01:14,4,Smoothing of Language Model - Part 2,3.4,"So, this is much better than if we take the sum over all the words.",00:01:09,5,So much better take sum words
5951,00:01:16,4,Smoothing of Language Model - Part 2,3.4,"After we smooth the document the language model,",00:01:14,5,After smooth document language model
5952,00:01:21,4,Smoothing of Language Model - Part 2,3.4,we essentially have nonzero probabilities for all the words.,00:01:16,5,essentially nonzero probabilities words
5953,00:01:25,4,Smoothing of Language Model - Part 2,3.4,"So, this new form of the formula is much easier to score, or to compute.",00:01:21,5,So new form formula much easier score compute
5954,00:01:31,4,Smoothing of Language Model - Part 2,3.4,It's also interesting to note that the last of term here,00:01:27,5,It also interesting note last term
5955,00:01:34,4,Smoothing of Language Model - Part 2,3.4,is actually independent of the document.,00:01:31,5,actually independent document
5956,00:01:36,4,Smoothing of Language Model - Part 2,3.4,Since our goal is to rank the documents for,00:01:34,5,Since goal rank documents
5957,00:01:40,4,Smoothing of Language Model - Part 2,3.4,"the same query, we can ignore this term for ranking.",00:01:36,5,query ignore term ranking
5958,00:01:43,4,Smoothing of Language Model - Part 2,3.4,Because it's going to be the same for all the documents.,00:01:40,5,Because going documents
5959,00:01:46,4,Smoothing of Language Model - Part 2,3.4,Ignoring it wouldn't effect the order of the documents.,00:01:43,5,Ignoring effect order documents
5960,00:01:54,4,Smoothing of Language Model - Part 2,3.4,"Inside the sum, we also see that each matched",00:01:49,5,Inside sum also see matched
5961,00:01:59,4,Smoothing of Language Model - Part 2,3.4,query term would contribute a weight.,00:01:54,5,query term would contribute weight
5962,00:02:01,4,Smoothing of Language Model - Part 2,3.4,"And this weight actually,",00:01:59,5,And weight actually
5963,00:02:07,4,Smoothing of Language Model - Part 2,3.4,is very interesting because it looks like TF-IDF weighting.,00:02:01,5,interesting looks like TF IDF weighting
5964,00:02:11,4,Smoothing of Language Model - Part 2,3.4,"First, we can already see it has a frequency of the word in the query,",00:02:07,5,First already see frequency word query
5965,00:02:14,4,Smoothing of Language Model - Part 2,3.4,just like in the vector space model.,00:02:11,5,like vector space model
5966,00:02:16,4,Smoothing of Language Model - Part 2,3.4,"When we take adult product,",00:02:14,5,When take adult product
5967,00:02:20,4,Smoothing of Language Model - Part 2,3.4,we see the word frequency in the query to show up in such a sum.,00:02:16,5,see word frequency query show sum
5968,00:02:25,4,Smoothing of Language Model - Part 2,3.4,"And so naturally, this part will correspond to",00:02:22,5,And naturally part correspond
5969,00:02:30,4,Smoothing of Language Model - Part 2,3.4,the vector element from the document vector.,00:02:25,5,vector element document vector
5970,00:02:34,4,Smoothing of Language Model - Part 2,3.4,"And here, indeed, we can see it actually",00:02:30,5,And indeed see actually
5971,00:02:39,4,Smoothing of Language Model - Part 2,3.4,encodes a weight that has similar factor to TF-IDF weighting.,00:02:35,5,encodes weight similar factor TF IDF weighting
5972,00:02:43,4,Smoothing of Language Model - Part 2,3.4,I let you examine it.,00:02:41,5,I let examine
5973,00:02:43,4,Smoothing of Language Model - Part 2,3.4,Can you see it?,00:02:43,5,Can see
5974,00:02:49,4,Smoothing of Language Model - Part 2,3.4,"Can you see which part is capturing TF, and which part is capturing IDF weighting?",00:02:43,5,Can see part capturing TF part capturing IDF weighting
5975,00:02:54,4,Smoothing of Language Model - Part 2,3.4,"So if you want, you can pause the video to think more about it.",00:02:49,5,So want pause video think
5976,00:03:02,4,Smoothing of Language Model - Part 2,3.4,"So, have you noticed that this p sub-seen is related to the term frequency",00:02:55,5,So noticed p sub seen related term frequency
5977,00:03:08,4,Smoothing of Language Model - Part 2,3.4,"in the sense that if a word occurs very frequently in the document,",00:03:02,5,sense word occurs frequently document
5978,00:03:10,4,Smoothing of Language Model - Part 2,3.4,then the S probability here will tend to be larger.,00:03:08,5,S probability tend larger
5979,00:03:11,4,Smoothing of Language Model - Part 2,3.4,Right?,00:03:10,5,Right
5980,00:03:16,4,Smoothing of Language Model - Part 2,3.4,"So, this means this term is really doing something like TF weighting.",00:03:11,5,So means term really something like TF weighting
5981,00:03:22,4,Smoothing of Language Model - Part 2,3.4,Have you also noticed that this time in the denominator,00:03:18,5,Have also noticed time denominator
5982,00:03:26,4,Smoothing of Language Model - Part 2,3.4,is actually achieving the factor of IDF?,00:03:23,5,actually achieving factor IDF
5983,00:03:29,4,Smoothing of Language Model - Part 2,3.4,Why? Because this is the popularity of the term,00:03:26,5,Why Because popularity term
5984,00:03:33,4,Smoothing of Language Model - Part 2,3.4,"in the collection, but it's in the denominator.",00:03:29,5,collection denominator
5985,00:03:37,4,Smoothing of Language Model - Part 2,3.4,"So, if the probability in the collection is larger",00:03:33,5,So probability collection larger
5986,00:03:39,4,Smoothing of Language Model - Part 2,3.4,than the weight is actually smaller.,00:03:37,5,weight actually smaller
5987,00:03:41,4,Smoothing of Language Model - Part 2,3.4,"And, this means a popular term.",00:03:39,5,And means popular term
5988,00:03:43,4,Smoothing of Language Model - Part 2,3.4,We actually have a smaller weight.,00:03:41,5,We actually smaller weight
5989,00:03:45,4,Smoothing of Language Model - Part 2,3.4,"And, this is precisely what IDF weighting is doing.",00:03:43,5,And precisely IDF weighting
5990,00:03:50,4,Smoothing of Language Model - Part 2,3.4,"Only not, we now have a different form of TF and IDF.",00:03:47,5,Only different form TF IDF
5991,00:03:56,4,Smoothing of Language Model - Part 2,3.4,"Remember, IDF has a log, logarithm of document frequency, but",00:03:51,5,Remember IDF log logarithm document frequency
5992,00:03:58,4,Smoothing of Language Model - Part 2,3.4,here we have something different.,00:03:56,5,something different
5993,00:04:02,4,Smoothing of Language Model - Part 2,3.4,"But intuitively, it achieves a similar effect.",00:03:58,5,But intuitively achieves similar effect
5994,00:04:05,4,Smoothing of Language Model - Part 2,3.4,"Interestingly, we also have something related to the length normalization.",00:04:02,5,Interestingly also something related length normalization
5995,00:04:14,4,Smoothing of Language Model - Part 2,3.4,"Again, can you see which factor is related to the length in this formula.",00:04:05,5,Again see factor related length formula
5996,00:04:18,4,Smoothing of Language Model - Part 2,3.4,"Well, I just say that, that this term is related to IDF weighting.",00:04:14,5,Well I say term related IDF weighting
5997,00:04:23,4,Smoothing of Language Model - Part 2,3.4,"This, this collection probability.",00:04:19,5,This collection probability
5998,00:04:26,4,Smoothing of Language Model - Part 2,3.4,"But, it turns out this term here",00:04:23,5,But turns term
5999,00:04:28,4,Smoothing of Language Model - Part 2,3.4,is actually related to a document length normalization.,00:04:26,5,actually related document length normalization
6000,00:04:35,4,Smoothing of Language Model - Part 2,3.4,"In particular, D might be related to document N, length.",00:04:28,5,In particular D might related document N length
6001,00:04:40,4,Smoothing of Language Model - Part 2,3.4,"So, it, it encodes how much probability mass we want to give to unseen words.",00:04:35,5,So encodes much probability mass want give unseen words
6002,00:04:43,4,Smoothing of Language Model - Part 2,3.4,How much smoothing you are allowed to do.,00:04:41,5,How much smoothing allowed
6003,00:04:48,4,Smoothing of Language Model - Part 2,3.4,"Intuitively, if a document is long, then we need to do less smoothing.",00:04:43,5,Intuitively document long need less smoothing
6004,00:04:50,4,Smoothing of Language Model - Part 2,3.4,"Because we can assume that it is large enough that,",00:04:48,5,Because assume large enough
6005,00:04:55,4,Smoothing of Language Model - Part 2,3.4,we have probably observed all of the words that the author could have written.,00:04:50,5,probably observed words author could written
6006,00:05:00,4,Smoothing of Language Model - Part 2,3.4,"But if the document is short, the unseen are expected to be to be large,",00:04:55,5,But document short unseen expected large
6007,00:05:02,4,Smoothing of Language Model - Part 2,3.4,and we need to do more smoothing.,00:05:00,5,need smoothing
6008,00:05:06,4,Smoothing of Language Model - Part 2,3.4,It's like that there are words that have not been retained yet by the author.,00:05:02,5,It like words retained yet author
6009,00:05:12,4,Smoothing of Language Model - Part 2,3.4,"So, this term appears to paralyze long documents",00:05:06,5,So term appears paralyze long documents
6010,00:05:18,4,Smoothing of Language Model - Part 2,3.4,"tend to be longer than, larger than for long document.",00:05:12,5,tend longer larger long document
6011,00:05:21,4,Smoothing of Language Model - Part 2,3.4,But note that the also occurs here.,00:05:18,5,But note also occurs
6012,00:05:26,4,Smoothing of Language Model - Part 2,3.4,"And so, this may not actually be necessary,",00:05:22,5,And may actually necessary
6013,00:05:30,4,Smoothing of Language Model - Part 2,3.4,"penalizing long documents, and in fact is not so clear here.",00:05:26,5,penalizing long documents fact clear
6014,00:05:36,4,Smoothing of Language Model - Part 2,3.4,"But as we will see later, when we consider some specific smoothing methods,",00:05:31,5,But see later consider specific smoothing methods
6015,00:05:40,4,Smoothing of Language Model - Part 2,3.4,it turns out that they do penalize long documents.,00:05:36,5,turns penalize long documents
6016,00:05:42,4,Smoothing of Language Model - Part 2,3.4,Just like in TF-IDF weighting and,00:05:40,5,Just like TF IDF weighting
6017,00:05:45,4,Smoothing of Language Model - Part 2,3.4,the document ends formulas in the vector space model.,00:05:42,5,document ends formulas vector space model
6018,00:05:50,4,Smoothing of Language Model - Part 2,3.4,"So, that's a very interesting observation because it means",00:05:47,5,So interesting observation means
6019,00:05:54,4,Smoothing of Language Model - Part 2,3.4,we don't even have to think about the specific way of doing smoothing.,00:05:50,5,even think specific way smoothing
6020,00:05:59,4,Smoothing of Language Model - Part 2,3.4,"We just need to assume that if we smooth with this language model,",00:05:54,5,We need assume smooth language model
6021,00:06:05,4,Smoothing of Language Model - Part 2,3.4,then we would have a formula that looks like a TF-IDF weighting and,00:05:59,5,would formula looks like TF IDF weighting
6022,00:06:06,4,Smoothing of Language Model - Part 2,3.4,document length normalization.,00:06:05,5,document length normalization
6023,00:06:13,4,Smoothing of Language Model - Part 2,3.4,What's also interesting that we have a very fixed form of the ranking function.,00:06:08,5,What also interesting fixed form ranking function
6024,00:06:17,4,Smoothing of Language Model - Part 2,3.4,"And see, we have not heuristically put a logarithm here.",00:06:14,5,And see heuristically put logarithm
6025,00:06:23,4,Smoothing of Language Model - Part 2,3.4,"In fact, if you can think about, why we would have a logarithm here?",00:06:19,5,In fact think would logarithm
6026,00:06:26,4,Smoothing of Language Model - Part 2,3.4,"If you look at the assumptions that we have made, it will be clear.",00:06:23,5,If look assumptions made clear
6027,00:06:32,4,Smoothing of Language Model - Part 2,3.4,It's because we have used a logarithm of query likelihood for scoring.,00:06:26,5,It used logarithm query likelihood scoring
6028,00:06:37,4,Smoothing of Language Model - Part 2,3.4,"And, we turned the product into a sum of logarithm of probability.",00:06:33,5,And turned product sum logarithm probability
6029,00:06:39,4,Smoothing of Language Model - Part 2,3.4,"And, that's why we have this logarithm.",00:06:37,5,And logarithm
6030,00:06:44,4,Smoothing of Language Model - Part 2,3.4,Note that if we only want to heuristically implement a TF weighting and,00:06:40,5,Note want heuristically implement TF weighting
6031,00:06:48,4,Smoothing of Language Model - Part 2,3.4,"IDF weighting, we don't necessarily have to have a logarithm here.",00:06:44,5,IDF weighting necessarily logarithm
6032,00:06:53,4,Smoothing of Language Model - Part 2,3.4,"Imagine if we drop this logarithm, we would still have TF and IDF weighting.",00:06:48,5,Imagine drop logarithm would still TF IDF weighting
6033,00:06:57,4,Smoothing of Language Model - Part 2,3.4,"But, what's nice with probabilistic modeling is that we",00:06:55,5,But nice probabilistic modeling
6034,00:07:01,4,Smoothing of Language Model - Part 2,3.4,are automatically given a logarithm function here.,00:06:57,5,automatically given logarithm function
6035,00:07:04,4,Smoothing of Language Model - Part 2,3.4,"And, that's basically,",00:07:01,5,And basically
6036,00:07:09,4,Smoothing of Language Model - Part 2,3.4,a fixed reform of the formula that we did not really have to hueristically line.,00:07:04,5,fixed reform formula really hueristically line
6037,00:07:14,4,Smoothing of Language Model - Part 2,3.4,"And in this case, if you try to drop this logarithm",00:07:09,5,And case try drop logarithm
6038,00:07:19,4,Smoothing of Language Model - Part 2,3.4,"the model probably won't, won't work as well as if you keep the logarithm.",00:07:14,5,model probably work well keep logarithm
6039,00:07:24,4,Smoothing of Language Model - Part 2,3.4,"So, a nice property of probabilistic modeling is that by following some",00:07:19,5,So nice property probabilistic modeling following
6040,00:07:28,4,Smoothing of Language Model - Part 2,3.4,"assumptions and the probability rules, we'll get a formula automatically.",00:07:24,5,assumptions probability rules get formula automatically
6041,00:07:33,4,Smoothing of Language Model - Part 2,3.4,"And, the formula would have a particular form, like in this case.",00:07:29,5,And formula would particular form like case
6042,00:07:36,4,Smoothing of Language Model - Part 2,3.4,"And, if we hueristically design the formula,",00:07:34,5,And hueristically design formula
6043,00:07:40,4,Smoothing of Language Model - Part 2,3.4,we may not necessarily end up having such a specific form.,00:07:36,5,may necessarily end specific form
6044,00:07:46,4,Smoothing of Language Model - Part 2,3.4,"So to summarize, we talked about the need for smoothing a document and model.",00:07:41,5,So summarize talked need smoothing document model
6045,00:07:51,4,Smoothing of Language Model - Part 2,3.4,"Otherwise, it would give zero probability for unseen words in the document.",00:07:46,5,Otherwise would give zero probability unseen words document
6046,00:07:57,4,Smoothing of Language Model - Part 2,3.4,"And, that's not good for scoring a query with such an unseen word.",00:07:52,5,And good scoring query unseen word
6047,00:08:02,4,Smoothing of Language Model - Part 2,3.4,"It's also necessary, in general, to improve the acc,",00:07:59,5,It also necessary general improve acc
6048,00:08:08,4,Smoothing of Language Model - Part 2,3.4,accuracy of estimating the model representing the topic of this document.,00:08:02,5,accuracy estimating model representing topic document
6049,00:08:16,4,Smoothing of Language Model - Part 2,3.4,The general idea of smoothing in retrieval is to use the connection language model,00:08:10,5,The general idea smoothing retrieval use connection language model
6050,00:08:22,4,Smoothing of Language Model - Part 2,3.4,to give us some clue about which unseen word would have a higher probability.,00:08:16,5,give us clue unseen word would higher probability
6051,00:08:26,4,Smoothing of Language Model - Part 2,3.4,That is the probability of the unseen word is assumed to be proportional,00:08:22,5,That probability unseen word assumed proportional
6052,00:08:28,4,Smoothing of Language Model - Part 2,3.4,to its probability in the collection.,00:08:26,5,probability collection
6053,00:08:34,4,Smoothing of Language Model - Part 2,3.4,"With this assumption, we've shown that we can derive a general ranking formula for",00:08:29,5,With assumption shown derive general ranking formula
6054,00:08:35,4,Smoothing of Language Model - Part 2,3.4,query likelihood.,00:08:34,5,query likelihood
6055,00:08:39,4,Smoothing of Language Model - Part 2,3.4,That has a fact of TF-IDF waiting and document length normalization.,00:08:35,5,That fact TF IDF waiting document length normalization
6056,00:08:44,4,Smoothing of Language Model - Part 2,3.4,"We also see that through some rewriting, the scoring of such ranking function,",00:08:39,5,We also see rewriting scoring ranking function
6057,00:08:48,4,Smoothing of Language Model - Part 2,3.4,"is primarily based on sum of weights on matched query terms,",00:08:44,5,primarily based sum weights matched query terms
6058,00:08:50,4,Smoothing of Language Model - Part 2,3.4,just like in the vector space model.,00:08:48,5,like vector space model
6059,00:08:55,4,Smoothing of Language Model - Part 2,3.4,"But, the actual ranking function is given us automatically by",00:08:50,5,But actual ranking function given us automatically
6060,00:08:59,4,Smoothing of Language Model - Part 2,3.4,the probability rules and assumptions we have made.,00:08:55,5,probability rules assumptions made
6061,00:09:00,4,Smoothing of Language Model - Part 2,3.4,"Unlike in the vector space model,",00:08:59,5,Unlike vector space model
6062,00:09:04,4,Smoothing of Language Model - Part 2,3.4,where we have to heuristically think about the form of the function.,00:09:00,5,heuristically think form function
6063,00:09:08,4,Smoothing of Language Model - Part 2,3.4,"However, we still need to address the question,",00:09:04,5,However still need address question
6064,00:09:11,4,Smoothing of Language Model - Part 2,3.4,how exactly we should we should smooth a document image model?,00:09:08,5,exactly smooth document image model
6065,00:09:15,4,Smoothing of Language Model - Part 2,3.4,How exactly we should use the reference language model based on,00:09:11,5,How exactly use reference language model based
6066,00:09:20,4,Smoothing of Language Model - Part 2,3.4,the connection to adjusting the probability of the maximum.,00:09:15,5,connection adjusting probability maximum
6067,00:09:22,4,Smoothing of Language Model - Part 2,3.4,"And, this is the topic of the next to that.",00:09:20,5,And topic next
6068,00:00:06,5,Learning to Rank Part 1,4.4,[SOUND] This lecture is about,00:00:00,6,SOUND This lecture
6069,00:00:10,5,Learning to Rank Part 1,4.4,learning to rank.,00:00:06,6,learning rank
6070,00:00:14,5,Learning to Rank Part 1,4.4,"In this lecture, we're going to continue talking about web search.",00:00:10,6,In lecture going continue talking web search
6071,00:00:19,5,Learning to Rank Part 1,4.4,"In particular, we're going to talk about using machine running to combine definite",00:00:14,6,In particular going talk using machine running combine definite
6072,00:00:21,5,Learning to Rank Part 1,4.4,features to improve ranking function.,00:00:19,6,features improve ranking function
6073,00:00:26,5,Learning to Rank Part 1,4.4,So the question that we address in this lecture is,00:00:21,6,So question address lecture
6074,00:00:30,5,Learning to Rank Part 1,4.4,"how we can combine many features to generate a,",00:00:26,6,combine many features generate
6075,00:00:35,5,Learning to Rank Part 1,4.4,a single ranking function to optimize search results.,00:00:30,6,single ranking function optimize search results
6076,00:00:39,5,Learning to Rank Part 1,4.4,"In the previous lectures, we have talked about the,",00:00:35,6,In previous lectures talked
6077,00:00:41,5,Learning to Rank Part 1,4.4,a number of ways to rank documents.,00:00:39,6,number ways rank documents
6078,00:00:47,5,Learning to Rank Part 1,4.4,"We have talked about some retrieval models, like a BM25 or clear light code.",00:00:41,6,We talked retrieval models like BM25 clear light code
6079,00:00:54,5,Learning to Rank Part 1,4.4,They can generate a content based scores for matching documents with a query.,00:00:47,6,They generate content based scores matching documents query
6080,00:00:58,5,Learning to Rank Part 1,4.4,"And we also talked about the link-based approaches,",00:00:54,6,And also talked link based approaches
6081,00:01:03,5,Learning to Rank Part 1,4.4,like page rank that can give additional scores to help us improve ranking.,00:00:58,6,like page rank give additional scores help us improve ranking
6082,00:01:07,5,Learning to Rank Part 1,4.4,Now the question now is how can we combine all these features and,00:01:03,6,Now question combine features
6083,00:01:10,5,Learning to Rank Part 1,4.4,potentially many other features to do ranking?,00:01:07,6,potentially many features ranking
6084,00:01:14,5,Learning to Rank Part 1,4.4,And this will be very useful for ranking web pages not only just to improve,00:01:10,6,And useful ranking web pages improve
6085,00:01:19,5,Learning to Rank Part 1,4.4,"accuracy, but also to improve the robustness of the ranking function.",00:01:14,6,accuracy also improve robustness ranking function
6086,00:01:24,5,Learning to Rank Part 1,4.4,So that's it not easy for a spammer to just perturb a one or,00:01:19,6,So easy spammer perturb one
6087,00:01:27,5,Learning to Rank Part 1,4.4,a few features to promote a page.,00:01:24,6,features promote page
6088,00:01:32,5,Learning to Rank Part 1,4.4,So the general idea of learning to rank is to use machine learning to,00:01:27,6,So general idea learning rank use machine learning
6089,00:01:36,5,Learning to Rank Part 1,4.4,combine these features to optimize the weight on different features to,00:01:32,6,combine features optimize weight different features
6090,00:01:39,5,Learning to Rank Part 1,4.4,generate the optimal ranking function.,00:01:36,6,generate optimal ranking function
6091,00:01:44,5,Learning to Rank Part 1,4.4,"So we would assume that the given a query document pair,",00:01:39,6,So would assume given query document pair
6092,00:01:49,5,Learning to Rank Part 1,4.4,"Q and D, we can define a number of features.",00:01:44,6,Q D define number features
6093,00:01:54,5,Learning to Rank Part 1,4.4,And these features can vary from content based features such as,00:01:49,6,And features vary content based features
6094,00:01:59,5,Learning to Rank Part 1,4.4,a score of the document it was respected to the query,00:01:54,6,score document respected query
6095,00:02:04,5,Learning to Rank Part 1,4.4,"according to a retrieval function, such as BM25 or",00:01:59,6,according retrieval function BM25
6096,00:02:10,5,Learning to Rank Part 1,4.4,"Query Light or pivot commands from a machine or PL2, et cetera.",00:02:04,6,Query Light pivot commands machine PL2 et cetera
6097,00:02:15,5,Learning to Rank Part 1,4.4,It can also be linked based score like PageRank score.,00:02:10,6,It also linked based score like PageRank score
6098,00:02:22,5,Learning to Rank Part 1,4.4,It can be also application of retrieval models to the anchor text of the page.,00:02:15,6,It also application retrieval models anchor text page
6099,00:02:23,5,Learning to Rank Part 1,4.4,Right?,00:02:22,6,Right
6100,00:02:29,5,Learning to Rank Part 1,4.4,Those are the types of descriptions of links that pointed to this page.,00:02:23,6,Those types descriptions links pointed page
6101,00:02:33,5,Learning to Rank Part 1,4.4,So these can all be clues about whether this document is relevant or not.,00:02:29,6,So clues whether document relevant
6102,00:02:41,5,Learning to Rank Part 1,4.4,"We can even include a, a feature such as whether the URL has a [INAUDIBLE],",00:02:33,6,We even include feature whether URL INAUDIBLE
6103,00:02:47,5,Learning to Rank Part 1,4.4,because this might be the indicator of home page or entry page.,00:02:41,6,might indicator home page entry page
6104,00:02:51,5,Learning to Rank Part 1,4.4,"So, all of these features can then be combined together to generate the ranking",00:02:47,6,So features combined together generate ranking
6105,00:02:51,5,Learning to Rank Part 1,4.4,functions.,00:02:51,6,functions
6106,00:02:54,5,Learning to Rank Part 1,4.4,"The question is of course, how can we combine them?",00:02:51,6,The question course combine
6107,00:03:00,5,Learning to Rank Part 1,4.4,"In this approach, we simply hypothesize that the probability",00:02:54,6,In approach simply hypothesize probability
6108,00:03:07,5,Learning to Rank Part 1,4.4,that this document is random to this query is a function of all these features.,00:03:00,6,document random query function features
6109,00:03:12,5,Learning to Rank Part 1,4.4,So we can hypothesize this that the probability of,00:03:07,6,So hypothesize probability
6110,00:03:17,5,Learning to Rank Part 1,4.4,relevance is related to these features through a particular,00:03:12,6,relevance related features particular
6111,00:03:21,5,Learning to Rank Part 1,4.4,form of the function that has some parameters.,00:03:17,6,form function parameters
6112,00:03:26,5,Learning to Rank Part 1,4.4,These parameters can control the influence of,00:03:21,6,These parameters control influence
6113,00:03:30,5,Learning to Rank Part 1,4.4,different features on the final relevance.,00:03:26,6,different features final relevance
6114,00:03:33,5,Learning to Rank Part 1,4.4,"This is of course, just a assumption.",00:03:30,6,This course assumption
6115,00:03:38,5,Learning to Rank Part 1,4.4,"Whether this assumption really makes sense is still a, a big question.",00:03:33,6,Whether assumption really makes sense still big question
6116,00:03:44,5,Learning to Rank Part 1,4.4,"However, you have to empirically evaluate the, the, the function.",00:03:38,6,However empirically evaluate function
6117,00:03:49,5,Learning to Rank Part 1,4.4,But by hypothesizing that the relevance is related to those features,00:03:44,6,But hypothesizing relevance related features
6118,00:03:54,5,Learning to Rank Part 1,4.4,"in the particular way, we can then combine these futures to generate",00:03:49,6,particular way combine futures generate
6119,00:04:00,5,Learning to Rank Part 1,4.4,"the potentially more powerful ranking function, a more robust ranking function.",00:03:54,6,potentially powerful ranking function robust ranking function
6120,00:04:04,5,Learning to Rank Part 1,4.4,"Naturally, the next question is how do we estimate loose parameters?",00:04:00,6,Naturally next question estimate loose parameters
6121,00:04:08,5,Learning to Rank Part 1,4.4,"You know, how do we know which features should have high weight and",00:04:04,6,You know know features high weight
6122,00:04:11,5,Learning to Rank Part 1,4.4,which features should have low weight?,00:04:08,6,features low weight
6123,00:04:14,5,Learning to Rank Part 1,4.4,So this is a task of training or learning.,00:04:11,6,So task training learning
6124,00:04:15,5,Learning to Rank Part 1,4.4,"All right. So,",00:04:14,6,All right So
6125,00:04:19,5,Learning to Rank Part 1,4.4,in this approach what we will do is use some training data.,00:04:15,6,approach use training data
6126,00:04:24,5,Learning to Rank Part 1,4.4,Those are the data that have been judged by users.,00:04:19,6,Those data judged users
6127,00:04:27,5,Learning to Rank Part 1,4.4,So that we already know the relevance judgments.,00:04:24,6,So already know relevance judgments
6128,00:04:32,5,Learning to Rank Part 1,4.4,We already know which documents should be rather high for which queries and,00:04:27,6,We already know documents rather high queries
6129,00:04:37,5,Learning to Rank Part 1,4.4,"this information can be based on real judgments by users or can,",00:04:32,6,information based real judgments users
6130,00:04:42,5,Learning to Rank Part 1,4.4,this can also be approximated by just using click through information.,00:04:37,6,also approximated using click information
6131,00:04:48,5,Learning to Rank Part 1,4.4,Where we can assume the clicked documents are better than the skipped documents or,00:04:42,6,Where assume clicked documents better skipped documents
6132,00:04:53,5,Learning to Rank Part 1,4.4,clicked documents are relevant and the skipped documents are not relevant.,00:04:48,6,clicked documents relevant skipped documents relevant
6133,00:05:00,5,Learning to Rank Part 1,4.4,"So, in general, the fit such hypothesize ranging function to the training day,",00:04:53,6,So general fit hypothesize ranging function training day
6134,00:05:06,5,Learning to Rank Part 1,4.4,meaning that we will try to optimize its retrieval accuracy on the training data.,00:05:00,6,meaning try optimize retrieval accuracy training data
6135,00:05:12,5,Learning to Rank Part 1,4.4,And we adjust these parameters to see how we can optimize the performance,00:05:06,6,And adjust parameters see optimize performance
6136,00:05:19,5,Learning to Rank Part 1,4.4,of the function on the training data in terms of some measure such as map or NDCG.,00:05:12,6,function training data terms measure map NDCG
6137,00:05:25,5,Learning to Rank Part 1,4.4,So the training data would look like a table of tuples.,00:05:19,6,So training data would look like table tuples
6138,00:05:32,5,Learning to Rank Part 1,4.4,"H-tuple it has three elements, the query, the document and the judgment.",00:05:25,6,H tuple three elements query document judgment
6139,00:05:37,5,Learning to Rank Part 1,4.4,"So, it looks very much like our relevance judgment that we",00:05:32,6,So looks much like relevance judgment
6140,00:05:41,5,Learning to Rank Part 1,4.4,talked about in evaluation of retrieval systems.,00:05:37,6,talked evaluation retrieval systems
6141,00:00:06,3,Evaluation of TR Systems- Practical Issues,2.8,[NOISE].,00:00:00,9,NOISE
6142,00:00:10,3,Evaluation of TR Systems- Practical Issues,2.8,>> This lecture is about some practical issues that you would have to address in,00:00:06,9,This lecture practical issues would address
6143,00:00:12,3,Evaluation of TR Systems- Practical Issues,2.8,evaluation of text retrieval systems.,00:00:10,9,evaluation text retrieval systems
6144,00:00:17,3,Evaluation of TR Systems- Practical Issues,2.8,In this lecture we will continue the discussion of evaluation.,00:00:14,9,In lecture continue discussion evaluation
6145,00:00:21,3,Evaluation of TR Systems- Practical Issues,2.8,We will cover some practical issues that you will have to solve,00:00:17,9,We cover practical issues solve
6146,00:00:24,3,Evaluation of TR Systems- Practical Issues,2.8,in actual evaluation of text retrieval systems.,00:00:21,9,actual evaluation text retrieval systems
6147,00:00:31,3,Evaluation of TR Systems- Practical Issues,2.8,"So, in order to create a test collection, we have to create a set of queries,",00:00:25,9,So order create test collection create set queries
6148,00:00:34,3,Evaluation of TR Systems- Practical Issues,2.8,a set of documents and a set of relevance judgments.,00:00:31,9,set documents set relevance judgments
6149,00:00:39,3,Evaluation of TR Systems- Practical Issues,2.8,It turns out that each is actually challenging to create.,00:00:35,9,It turns actually challenging create
6150,00:00:43,3,Evaluation of TR Systems- Practical Issues,2.8,"So first, the documents and queries must be representative.",00:00:39,9,So first documents queries must representative
6151,00:00:47,3,Evaluation of TR Systems- Practical Issues,2.8,"They must rep, represent the real queries and real documents that the users handle.",00:00:43,9,They must rep represent real queries real documents users handle
6152,00:00:50,3,Evaluation of TR Systems- Practical Issues,2.8,And we also have to use many queries and,00:00:48,9,And also use many queries
6153,00:00:55,3,Evaluation of TR Systems- Practical Issues,2.8,many documents in order to avoid biased conclusions.,00:00:50,9,many documents order avoid biased conclusions
6154,00:01:02,3,Evaluation of TR Systems- Practical Issues,2.8,"For the matching of relevant documents with the queries,",00:00:56,9,For matching relevant documents queries
6155,00:01:10,3,Evaluation of TR Systems- Practical Issues,2.8,we also need to ensure that there exists a lot of relevant documents for each query.,00:01:02,9,also need ensure exists lot relevant documents query
6156,00:01:14,3,Evaluation of TR Systems- Practical Issues,2.8,"If a query has only one that is a relevant document in the collection then, you know,",00:01:10,9,If query one relevant document collection know
6157,00:01:18,3,Evaluation of TR Systems- Practical Issues,2.8,it's not very informative to compare different methods,00:01:14,9,informative compare different methods
6158,00:01:23,3,Evaluation of TR Systems- Practical Issues,2.8,using such a query because there is not much room for us to see difference.,00:01:18,9,using query much room us see difference
6159,00:01:26,3,Evaluation of TR Systems- Practical Issues,2.8,"So, ideally there should be more relevant documents in the collection.",00:01:23,9,So ideally relevant documents collection
6160,00:01:30,3,Evaluation of TR Systems- Practical Issues,2.8,But yet the queries also should represent real queries that we care about.,00:01:26,9,But yet queries also represent real queries care
6161,00:01:36,3,Evaluation of TR Systems- Practical Issues,2.8,"In terms of relevance judgements, the challenge is to ensure complete judgements",00:01:31,9,In terms relevance judgements challenge ensure complete judgements
6162,00:01:41,3,Evaluation of TR Systems- Practical Issues,2.8,"of all the documents for all the queries, yet, minimizing human and fault.",00:01:36,9,documents queries yet minimizing human fault
6163,00:01:44,3,Evaluation of TR Systems- Practical Issues,2.8,Because we have to use the human labor to label these documents.,00:01:41,9,Because use human labor label documents
6164,00:01:47,3,Evaluation of TR Systems- Practical Issues,2.8,It's very labor intensive.,00:01:44,9,It labor intensive
6165,00:01:52,3,Evaluation of TR Systems- Practical Issues,2.8,"And as a result, it's impossible to actually label all of the documents for",00:01:47,9,And result impossible actually label documents
6166,00:01:57,3,Evaluation of TR Systems- Practical Issues,2.8,"all the queries, especially considering a joint, data set like the web.",00:01:52,9,queries especially considering joint data set like web
6167,00:02:01,3,Evaluation of TR Systems- Practical Issues,2.8,"So, this is actually a major challenge.",00:01:58,9,So actually major challenge
6168,00:02:03,3,Evaluation of TR Systems- Practical Issues,2.8,It's a very difficult challenge.,00:02:01,9,It difficult challenge
6169,00:02:06,3,Evaluation of TR Systems- Practical Issues,2.8,"For measures, it's also challenging because what we want with measures is that",00:02:03,9,For measures also challenging want measures
6170,00:02:11,3,Evaluation of TR Systems- Practical Issues,2.8,with accuracy reflected the perceived utility of users.,00:02:06,9,accuracy reflected perceived utility users
6171,00:02:15,3,Evaluation of TR Systems- Practical Issues,2.8,We have to consider carefully what the users care about and,00:02:11,9,We consider carefully users care
6172,00:02:17,3,Evaluation of TR Systems- Practical Issues,2.8,then design measures to measure that.,00:02:15,9,design measures measure
6173,00:02:21,3,Evaluation of TR Systems- Practical Issues,2.8,"If we, your measure is not measuring the right thing,",00:02:18,9,If measure measuring right thing
6174,00:02:23,3,Evaluation of TR Systems- Practical Issues,2.8,"then your conclusion would, would be misled.",00:02:21,9,conclusion would would misled
6175,00:02:25,3,Evaluation of TR Systems- Practical Issues,2.8,So it's very important.,00:02:23,9,So important
6176,00:02:29,3,Evaluation of TR Systems- Practical Issues,2.8,So we're going to talk about a couple issues here.,00:02:26,9,So going talk couple issues
6177,00:02:31,3,Evaluation of TR Systems- Practical Issues,2.8,"One is the statistical significance test, and",00:02:29,9,One statistical significance test
6178,00:02:36,3,Evaluation of TR Systems- Practical Issues,2.8,"this also is the reason why we have to use a lot of queries, and",00:02:31,9,also reason use lot queries
6179,00:02:41,3,Evaluation of TR Systems- Practical Issues,2.8,the question here is how sure can you be that I observed the difference?,00:02:36,9,question sure I observed difference
6180,00:02:44,3,Evaluation of TR Systems- Practical Issues,2.8,It doesn't simply result from the particular queries you choose.,00:02:41,9,It simply result particular queries choose
6181,00:02:49,3,Evaluation of TR Systems- Practical Issues,2.8,So here are some sample results of average precision for System A and,00:02:44,9,So sample results average precision System A
6182,00:02:53,3,Evaluation of TR Systems- Practical Issues,2.8,System B in two different experiments.,00:02:49,9,System B two different experiments
6183,00:02:57,3,Evaluation of TR Systems- Practical Issues,2.8,"And you can see in the bottom, we have mean average position, all right?",00:02:53,9,And see bottom mean average position right
6184,00:03:00,3,Evaluation of TR Systems- Practical Issues,2.8,"So the mean, if you look at the mean average position",00:02:57,9,So mean look mean average position
6185,00:03:07,3,Evaluation of TR Systems- Practical Issues,2.8,the mean average positions are exactly the same in both experiments.,00:03:02,9,mean average positions exactly experiments
6186,00:03:12,3,Evaluation of TR Systems- Practical Issues,2.8,"All right, so you can see this is 0.2, this is 0.4 for",00:03:08,9,All right see 0 2 0 4
6187,00:03:16,3,Evaluation of TR Systems- Practical Issues,2.8,system B and again here its also 0.2 and 0.4.,00:03:12,9,system B also 0 2 0 4
6188,00:03:18,3,Evaluation of TR Systems- Practical Issues,2.8,So they are identical.,00:03:16,9,So identical
6189,00:03:23,3,Evaluation of TR Systems- Practical Issues,2.8,"Yet if you look at the, these exact average positions for different queries,",00:03:18,9,Yet look exact average positions different queries
6190,00:03:30,3,Evaluation of TR Systems- Practical Issues,2.8,"if you look at these numbers in detail, you will realize that in one case",00:03:23,9,look numbers detail realize one case
6191,00:03:35,3,Evaluation of TR Systems- Practical Issues,2.8,you would feel that you can trust the conclusion here given by the average.,00:03:30,9,would feel trust conclusion given average
6192,00:03:41,3,Evaluation of TR Systems- Practical Issues,2.8,"In another case, in the other case, you will feel that, well, I'm not sure.",00:03:36,9,In another case case feel well I sure
6193,00:03:45,3,Evaluation of TR Systems- Practical Issues,2.8,"So, why don't you take a look at all these numbers for a moment.",00:03:41,9,So take look numbers moment
6194,00:03:46,3,Evaluation of TR Systems- Practical Issues,2.8,Pause the video.,00:03:45,9,Pause video
6195,00:03:52,3,Evaluation of TR Systems- Practical Issues,2.8,"So, if you at the average, the main average position,",00:03:48,9,So average main average position
6196,00:03:56,3,Evaluation of TR Systems- Practical Issues,2.8,"we can easily say that, well, System B is better, right?",00:03:52,9,easily say well System B better right
6197,00:04:03,3,Evaluation of TR Systems- Practical Issues,2.8,"So it's, after all, it's 0.4 and then this is twice as much as 0.2.",00:03:56,9,So 0 4 twice much 0 2
6198,00:04:05,3,Evaluation of TR Systems- Practical Issues,2.8,So that's a better performance.,00:04:03,9,So better performance
6199,00:04:11,3,Evaluation of TR Systems- Practical Issues,2.8,"But if you look at these two experiments and look at the detailed results,",00:04:05,9,But look two experiments look detailed results
6200,00:04:16,3,Evaluation of TR Systems- Practical Issues,2.8,you will see that we'll be more confident to say that in the case one.,00:04:11,9,see confident say case one
6201,00:04:17,3,Evaluation of TR Systems- Practical Issues,2.8,In experiment one.,00:04:16,9,In experiment one
6202,00:04:23,3,Evaluation of TR Systems- Practical Issues,2.8,In this case because these numbers seem to be consistently better than for system B.,00:04:17,9,In case numbers seem consistently better system B
6203,00:04:29,3,Evaluation of TR Systems- Practical Issues,2.8,"Where as in, experiment two, we're not sure.",00:04:25,9,Where experiment two sure
6204,00:04:35,3,Evaluation of TR Systems- Practical Issues,2.8,"because, looking at some results, like this, after system A is better.",00:04:29,9,looking results like system A better
6205,00:04:37,3,Evaluation of TR Systems- Practical Issues,2.8,And this is another case where system A is better.,00:04:35,9,And another case system A better
6206,00:04:43,3,Evaluation of TR Systems- Practical Issues,2.8,"But yet, if we look at on the average, System B is better.",00:04:39,9,But yet look average System B better
6207,00:04:47,3,Evaluation of TR Systems- Practical Issues,2.8,So what do you think?,00:04:45,9,So think
6208,00:04:54,3,Evaluation of TR Systems- Practical Issues,2.8,"You know, how reliable is our conclusion if we only look at the average?",00:04:47,9,You know reliable conclusion look average
6209,00:04:59,3,Evaluation of TR Systems- Practical Issues,2.8,"Now in this case, intuitively, we feel it's better than one, it's more reliable.",00:04:55,9,Now case intuitively feel better one reliable
6210,00:05:04,3,Evaluation of TR Systems- Practical Issues,2.8,But how can we quantitatively answer this question?,00:05:01,9,But quantitatively answer question
6211,00:05:09,3,Evaluation of TR Systems- Practical Issues,2.8,And this is why we need to do statistical significance test.,00:05:04,9,And need statistical significance test
6212,00:05:15,3,Evaluation of TR Systems- Practical Issues,2.8,"So the idea of a statistical significance test is basically to assess the vary,",00:05:09,9,So idea statistical significance test basically assess vary
6213,00:05:18,3,Evaluation of TR Systems- Practical Issues,2.8,variance across these different queries.,00:05:15,9,variance across different queries
6214,00:05:21,3,Evaluation of TR Systems- Practical Issues,2.8,"If there's a, a big variance that means",00:05:18,9,If big variance means
6215,00:05:25,3,Evaluation of TR Systems- Practical Issues,2.8,that the results could fluctuate a lot according to different queries.,00:05:21,9,results could fluctuate lot according different queries
6216,00:05:30,3,Evaluation of TR Systems- Practical Issues,2.8,Then we should believe that unless you have used a lot of,00:05:25,9,Then believe unless used lot
6217,00:05:34,3,Evaluation of TR Systems- Practical Issues,2.8,queries the results might change if we use another set of queries.,00:05:30,9,queries results might change use another set queries
6218,00:05:37,3,Evaluation of TR Systems- Practical Issues,2.8,"Right? So, this is then not so",00:05:34,9,Right So
6219,00:05:43,3,Evaluation of TR Systems- Practical Issues,2.8,if you have seen high variance then it's not very reliable.,00:05:37,9,seen high variance reliable
6220,00:05:49,3,Evaluation of TR Systems- Practical Issues,2.8,So let's look at these results again in the second case.,00:05:43,9,So let look results second case
6221,00:05:54,3,Evaluation of TR Systems- Practical Issues,2.8,"So here we show two, different ways to compare them.",00:05:49,9,So show two different ways compare
6222,00:05:55,3,Evaluation of TR Systems- Practical Issues,2.8,One is a Sign Test.,00:05:54,9,One Sign Test
6223,00:05:56,3,Evaluation of TR Systems- Practical Issues,2.8,"And we'll, we'll just look at the sign.",00:05:55,9,And look sign
6224,00:06:01,3,Evaluation of TR Systems- Practical Issues,2.8,"If System B is better than System A, then we have a plus sign.",00:05:57,9,If System B better System A plus sign
6225,00:06:04,3,Evaluation of TR Systems- Practical Issues,2.8,When System A is better we have a minus sign etc.,00:06:01,9,When System A better minus sign etc
6226,00:06:09,3,Evaluation of TR Systems- Practical Issues,2.8,"Using this case if you see this, well, there are seven cases.",00:06:05,9,Using case see well seven cases
6227,00:06:13,3,Evaluation of TR Systems- Practical Issues,2.8,We actually have four cases where System B is better.,00:06:09,9,We actually four cases System B better
6228,00:06:14,3,Evaluation of TR Systems- Practical Issues,2.8,But three cases System A is better.,00:06:13,9,But three cases System A better
6229,00:06:19,3,Evaluation of TR Systems- Practical Issues,2.8,"You know intuitively, this is almost like random results.",00:06:14,9,You know intuitively almost like random results
6230,00:06:24,3,Evaluation of TR Systems- Practical Issues,2.8,"Right, so if you just take a random sample of to, to flip seven coins,",00:06:19,9,Right take random sample flip seven coins
6231,00:06:29,3,Evaluation of TR Systems- Practical Issues,2.8,"and if you use plus to denote the head and then minus to denote the tail, and",00:06:24,9,use plus denote head minus denote tail
6232,00:06:34,3,Evaluation of TR Systems- Practical Issues,2.8,"that could easily be the results of just randomly flipping, these seven coins.",00:06:29,9,could easily results randomly flipping seven coins
6233,00:06:39,3,Evaluation of TR Systems- Practical Issues,2.8,"So, the fact that the, the average is larger doesn't tell us anything.",00:06:34,9,So fact average larger tell us anything
6234,00:06:41,3,Evaluation of TR Systems- Practical Issues,2.8,"You know, we can't reliably concur that.",00:06:39,9,You know reliably concur
6235,00:06:46,3,Evaluation of TR Systems- Practical Issues,2.8,"And this can be quantitative in the measure by, a p value.",00:06:41,9,And quantitative measure p value
6236,00:06:49,3,Evaluation of TR Systems- Practical Issues,2.8,"And that basically, means,",00:06:46,9,And basically means
6237,00:06:54,3,Evaluation of TR Systems- Practical Issues,2.8,the probability that this result is in fact from random fluctuation.,00:06:49,9,probability result fact random fluctuation
6238,00:06:56,3,Evaluation of TR Systems- Practical Issues,2.8,"In this case, probability is one.",00:06:54,9,In case probability one
6239,00:07:00,3,Evaluation of TR Systems- Practical Issues,2.8,It means it surely is a random fluctuation.,00:06:56,9,It means surely random fluctuation
6240,00:07:06,3,Evaluation of TR Systems- Practical Issues,2.8,"Now in Wilcoxon, test, it's a non parametrical test,",00:07:01,9,Now Wilcoxon test non parametrical test
6241,00:07:09,3,Evaluation of TR Systems- Practical Issues,2.8,and we would be not only looking at the signs,00:07:06,9,would looking signs
6242,00:07:12,3,Evaluation of TR Systems- Practical Issues,2.8,we'll be also looking at the magnitude of the difference.,00:07:09,9,also looking magnitude difference
6243,00:07:15,3,Evaluation of TR Systems- Practical Issues,2.8,"But, we, we, we can draw a similar conclusion where you say well it's",00:07:12,9,But draw similar conclusion say well
6244,00:07:18,3,Evaluation of TR Systems- Practical Issues,2.8,very likely to be from random.,00:07:15,9,likely random
6245,00:07:22,3,Evaluation of TR Systems- Practical Issues,2.8,So to illustrate this let's think about such a distribution.,00:07:18,9,So illustrate let think distribution
6246,00:07:23,3,Evaluation of TR Systems- Practical Issues,2.8,And this is called a normal distribution.,00:07:22,9,And called normal distribution
6247,00:07:26,3,Evaluation of TR Systems- Practical Issues,2.8,We assume that the mean is zero here.,00:07:23,9,We assume mean zero
6248,00:07:29,3,Evaluation of TR Systems- Practical Issues,2.8,"Let's say, well, we started with the assumption that there's no difference",00:07:26,9,Let say well started assumption difference
6249,00:07:31,3,Evaluation of TR Systems- Practical Issues,2.8,between the two systems.,00:07:29,9,two systems
6250,00:07:35,3,Evaluation of TR Systems- Practical Issues,2.8,But we assume that because of random fluctuations depending on the queries,00:07:31,9,But assume random fluctuations depending queries
6251,00:07:39,3,Evaluation of TR Systems- Practical Issues,2.8,"we might observe a difference, so the actual difference might be",00:07:35,9,might observe difference actual difference might
6252,00:07:42,3,Evaluation of TR Systems- Practical Issues,2.8,"on the left side here or on the right side here, right?",00:07:39,9,left side right side right
6253,00:07:47,3,Evaluation of TR Systems- Practical Issues,2.8,"And, and this curve kind of shows the probability",00:07:42,9,And curve kind shows probability
6254,00:07:52,3,Evaluation of TR Systems- Practical Issues,2.8,that we would actually observe values that are deviating from zero here.,00:07:47,9,would actually observe values deviating zero
6255,00:07:58,3,Evaluation of TR Systems- Practical Issues,2.8,"Now, so if we, look at this picture then",00:07:53,9,Now look picture
6256,00:08:03,3,Evaluation of TR Systems- Practical Issues,2.8,"we see that if a difference is observed here,",00:07:58,9,see difference observed
6257,00:08:07,3,Evaluation of TR Systems- Practical Issues,2.8,"then the chance is very high that this is in fact,",00:08:03,9,chance high fact
6258,00:08:10,3,Evaluation of TR Systems- Practical Issues,2.8,"a random observation, right.",00:08:07,9,random observation right
6259,00:08:17,3,Evaluation of TR Systems- Practical Issues,2.8,"We can define region of you know, likely observation because of random fluctuation.",00:08:10,9,We define region know likely observation random fluctuation
6260,00:08:21,3,Evaluation of TR Systems- Practical Issues,2.8,And this is 95% of all outcomes.,00:08:17,9,And 95 outcomes
6261,00:08:25,3,Evaluation of TR Systems- Practical Issues,2.8,And in this interval then the observed values,00:08:21,9,And interval observed values
6262,00:08:27,3,Evaluation of TR Systems- Practical Issues,2.8,may still be from random fluctuation.,00:08:25,9,may still random fluctuation
6263,00:08:34,3,Evaluation of TR Systems- Practical Issues,2.8,"But if you observe a value in this region or a difference on this side,",00:08:28,9,But observe value region difference side
6264,00:08:40,3,Evaluation of TR Systems- Practical Issues,2.8,then the difference is unlikely from random fluctuation.,00:08:34,9,difference unlikely random fluctuation
6265,00:08:44,3,Evaluation of TR Systems- Practical Issues,2.8,"Right, so there is a very small probability that you will observe",00:08:40,9,Right small probability observe
6266,00:08:48,3,Evaluation of TR Systems- Practical Issues,2.8,such a difference just because of random fluctuation.,00:08:44,9,difference random fluctuation
6267,00:08:52,3,Evaluation of TR Systems- Practical Issues,2.8,"So in that case, we can then conclude the difference must be real.",00:08:48,9,So case conclude difference must real
6268,00:08:54,3,Evaluation of TR Systems- Practical Issues,2.8,So System B is indeed better.,00:08:52,9,So System B indeed better
6269,00:08:58,3,Evaluation of TR Systems- Practical Issues,2.8,"So, this is the idea of the statistical significance test.",00:08:56,9,So idea statistical significance test
6270,00:09:03,3,Evaluation of TR Systems- Practical Issues,2.8,The takeaway message here is that you have used many queries to avoid,00:08:59,9,The takeaway message used many queries avoid
6271,00:09:08,3,Evaluation of TR Systems- Practical Issues,2.8,jumping into a conclusion as in this case to say System B is better.,00:09:03,9,jumping conclusion case say System B better
6272,00:09:13,3,Evaluation of TR Systems- Practical Issues,2.8,There are many different ways of doing this statistical significance test.,00:09:09,9,There many different ways statistical significance test
6273,00:09:20,3,Evaluation of TR Systems- Practical Issues,2.8,"So now, let's talk about the other problem of making judgements and",00:09:15,9,So let talk problem making judgements
6274,00:09:21,3,Evaluation of TR Systems- Practical Issues,2.8,"as we said earlier,",00:09:20,9,said earlier
6275,00:09:27,3,Evaluation of TR Systems- Practical Issues,2.8,it's very hard to judge all the documents completely unless it is a small data set.,00:09:21,9,hard judge documents completely unless small data set
6276,00:09:31,3,Evaluation of TR Systems- Practical Issues,2.8,"So the question is, if we can't afford judging all the documents",00:09:27,9,So question afford judging documents
6277,00:09:33,3,Evaluation of TR Systems- Practical Issues,2.8,"in the collection, which subset should we judge?",00:09:31,9,collection subset judge
6278,00:09:38,3,Evaluation of TR Systems- Practical Issues,2.8,And the solution here is pooling.,00:09:35,9,And solution pooling
6279,00:09:46,3,Evaluation of TR Systems- Practical Issues,2.8,And this is a strategy that has been used in many cases to solve this problem.,00:09:38,9,And strategy used many cases solve problem
6280,00:09:49,3,Evaluation of TR Systems- Practical Issues,2.8,So the idea of pulling is the following.,00:09:46,9,So idea pulling following
6281,00:09:54,3,Evaluation of TR Systems- Practical Issues,2.8,"We would first choose a diverse set of ranking methods,",00:09:49,9,We would first choose diverse set ranking methods
6282,00:09:56,3,Evaluation of TR Systems- Practical Issues,2.8,these are types of retrieval systems.,00:09:54,9,types retrieval systems
6283,00:10:00,3,Evaluation of TR Systems- Practical Issues,2.8,And we hope these methods can help us nominate,00:09:57,9,And hope methods help us nominate
6284,00:10:02,3,Evaluation of TR Systems- Practical Issues,2.8,likely relevance in the documents.,00:10:00,9,likely relevance documents
6285,00:10:05,3,Evaluation of TR Systems- Practical Issues,2.8,So the goal is to pick out the relevant documents..,00:10:02,9,So goal pick relevant documents
6286,00:10:08,3,Evaluation of TR Systems- Practical Issues,2.8,It means we are to make judgements on relevant documents because those,00:10:05,9,It means make judgements relevant documents
6287,00:10:12,3,Evaluation of TR Systems- Practical Issues,2.8,are the most useful documents from the users perspective.,00:10:08,9,useful documents users perspective
6288,00:10:17,3,Evaluation of TR Systems- Practical Issues,2.8,"So, that way we would have each to return top-K documents.",00:10:12,9,So way would return top K documents
6289,00:10:19,3,Evaluation of TR Systems- Practical Issues,2.8,"And the K can vary from systems, right.",00:10:17,9,And K vary systems right
6290,00:10:24,3,Evaluation of TR Systems- Practical Issues,2.8,But the point is to ask them to suggest the most likely relevant documents.,00:10:19,9,But point ask suggest likely relevant documents
6291,00:10:30,3,Evaluation of TR Systems- Practical Issues,2.8,And then we simply combine all these top-K sets to form,00:10:25,9,And simply combine top K sets form
6292,00:10:35,3,Evaluation of TR Systems- Practical Issues,2.8,a pool of documents for human assessors to judge.,00:10:30,9,pool documents human assessors judge
6293,00:10:38,3,Evaluation of TR Systems- Practical Issues,2.8,"So, imagine you have many systems.",00:10:35,9,So imagine many systems
6294,00:10:43,3,Evaluation of TR Systems- Practical Issues,2.8,"Each will return K documents, you know, take the top-K documents, and",00:10:38,9,Each return K documents know take top K documents
6295,00:10:45,3,Evaluation of TR Systems- Practical Issues,2.8,we form the unit.,00:10:43,9,form unit
6296,00:10:47,3,Evaluation of TR Systems- Practical Issues,2.8,"Now, of course there are many documents that are duplicated,",00:10:45,9,Now course many documents duplicated
6297,00:10:51,3,Evaluation of TR Systems- Practical Issues,2.8,because many systems might have retrieved the same random documents.,00:10:47,9,many systems might retrieved random documents
6298,00:10:56,3,Evaluation of TR Systems- Practical Issues,2.8,So there will be some duplicate documents.,00:10:51,9,So duplicate documents
6299,00:10:56,3,Evaluation of TR Systems- Practical Issues,2.8,"And there are,",00:10:56,9,And
6300,00:11:01,3,Evaluation of TR Systems- Practical Issues,2.8,"there are also unique documents that are only returned by one system, so the idea",00:10:56,9,also unique documents returned one system idea
6301,00:11:07,3,Evaluation of TR Systems- Practical Issues,2.8,of having diverse set of result ranking methods is to ensure the pool is broad.,00:11:01,9,diverse set result ranking methods ensure pool broad
6302,00:11:11,3,Evaluation of TR Systems- Practical Issues,2.8,And can include as many possible random documents as possible.,00:11:07,9,And include many possible random documents possible
6303,00:11:16,3,Evaluation of TR Systems- Practical Issues,2.8,And then the users with the human assessors would make,00:11:12,9,And users human assessors would make
6304,00:11:21,3,Evaluation of TR Systems- Practical Issues,2.8,"complete the judgements on this data set, this pool.",00:11:16,9,complete judgements data set pool
6305,00:11:26,3,Evaluation of TR Systems- Practical Issues,2.8,And the other unjudged documents are usually just a assumed to be non-relevant.,00:11:22,9,And unjudged documents usually assumed non relevant
6306,00:11:30,3,Evaluation of TR Systems- Practical Issues,2.8,"Now if the pool is large enough, this assumption is okay.",00:11:26,9,Now pool large enough assumption okay
6307,00:11:37,3,Evaluation of TR Systems- Practical Issues,2.8,"But the, if the pool is not very large, this actually has to be reconsidered,",00:11:32,9,But pool large actually reconsidered
6308,00:11:41,3,Evaluation of TR Systems- Practical Issues,2.8,and we might use other strategies to deal with them and,00:11:37,9,might use strategies deal
6309,00:11:45,3,Evaluation of TR Systems- Practical Issues,2.8,there are indeed other methods to handle such cases.,00:11:41,9,indeed methods handle cases
6310,00:11:50,3,Evaluation of TR Systems- Practical Issues,2.8,And such a strategy is generally okay for,00:11:45,9,And strategy generally okay
6311,00:11:54,3,Evaluation of TR Systems- Practical Issues,2.8,comparing systems that contribute to the pool.,00:11:50,9,comparing systems contribute pool
6312,00:11:57,3,Evaluation of TR Systems- Practical Issues,2.8,That means if you participate in contributing to the pool then it's,00:11:54,9,That means participate contributing pool
6313,00:12:01,3,Evaluation of TR Systems- Practical Issues,2.8,unlikely that it will penalize your system because the top,00:11:57,9,unlikely penalize system top
6314,00:12:03,3,Evaluation of TR Systems- Practical Issues,2.8,ranked documents have all been judged.,00:12:01,9,ranked documents judged
6315,00:12:06,3,Evaluation of TR Systems- Practical Issues,2.8,"However, this is problematic for",00:12:04,9,However problematic
6316,00:12:11,3,Evaluation of TR Systems- Practical Issues,2.8,even evaluating a new system that may not have contributed to the pool.,00:12:06,9,even evaluating new system may contributed pool
6317,00:12:16,3,Evaluation of TR Systems- Practical Issues,2.8,"In this case, you know, a new system might be penalized because it might have",00:12:11,9,In case know new system might penalized might
6318,00:12:20,3,Evaluation of TR Systems- Practical Issues,2.8,nominated some relevant documents that have not been judged.,00:12:16,9,nominated relevant documents judged
6319,00:12:24,3,Evaluation of TR Systems- Practical Issues,2.8,So those documents might be assumed to be non-relevant.,00:12:20,9,So documents might assumed non relevant
6320,00:12:26,3,Evaluation of TR Systems- Practical Issues,2.8,"And that, that's unfair.",00:12:24,9,And unfair
6321,00:12:30,3,Evaluation of TR Systems- Practical Issues,2.8,"So to summarize the whole part of text retrieval evaluation,",00:12:26,9,So summarize whole part text retrieval evaluation
6322,00:12:36,3,Evaluation of TR Systems- Practical Issues,2.8,it's extremely important because the problem is an empirically defined problem.,00:12:30,9,extremely important problem empirically defined problem
6323,00:12:42,3,Evaluation of TR Systems- Practical Issues,2.8,"If we don't rely on users, there's no way to tell whether one method works better.",00:12:36,9,If rely users way tell whether one method works better
6324,00:12:46,3,Evaluation of TR Systems- Practical Issues,2.8,"If we have inappropriate experiment design,",00:12:43,9,If inappropriate experiment design
6325,00:12:49,3,Evaluation of TR Systems- Practical Issues,2.8,we might misguide our research or applications.,00:12:46,9,might misguide research applications
6326,00:12:52,3,Evaluation of TR Systems- Practical Issues,2.8,And we might just draw wrong conclusions.,00:12:49,9,And might draw wrong conclusions
6327,00:12:55,3,Evaluation of TR Systems- Practical Issues,2.8,And we have seen this in some of our discussion.,00:12:52,9,And seen discussion
6328,00:12:59,3,Evaluation of TR Systems- Practical Issues,2.8,"So, make sure to get it right for your research or application.",00:12:55,9,So make sure get right research application
6329,00:13:03,3,Evaluation of TR Systems- Practical Issues,2.8,The main methodology is Cranfield evaluation methodology and,00:12:59,9,The main methodology Cranfield evaluation methodology
6330,00:13:07,3,Evaluation of TR Systems- Practical Issues,2.8,"this is near the main paradigm used in all kinds of empirical evaluation tasks,",00:13:03,9,near main paradigm used kinds empirical evaluation tasks
6331,00:13:10,3,Evaluation of TR Systems- Practical Issues,2.8,not just a search engine variation.,00:13:07,9,search engine variation
6332,00:13:15,3,Evaluation of TR Systems- Practical Issues,2.8,Map and nDCG are the two main measures that should definitely know about and,00:13:10,9,Map nDCG two main measures definitely know
6333,00:13:19,3,Evaluation of TR Systems- Practical Issues,2.8,they are appropriate for comparing ranking algorithms.,00:13:15,9,appropriate comparing ranking algorithms
6334,00:13:22,3,Evaluation of TR Systems- Practical Issues,2.8,You will see them often in research papers.,00:13:19,9,You see often research papers
6335,00:13:27,3,Evaluation of TR Systems- Practical Issues,2.8,Perceiving up to ten documents is easier to interpret from users perspective.,00:13:22,9,Perceiving ten documents easier interpret users perspective
6336,00:13:28,3,Evaluation of TR Systems- Practical Issues,2.8,"So, that's also often useful.",00:13:27,9,So also often useful
6337,00:13:37,3,Evaluation of TR Systems- Practical Issues,2.8,What's not covered is some other evaluation strategy like A-B test,00:13:30,9,What covered evaluation strategy like A B test
6338,00:13:43,3,Evaluation of TR Systems- Practical Issues,2.8,where the system would mix two of the results of two methods randomly.,00:13:37,9,system would mix two results two methods randomly
6339,00:13:46,3,Evaluation of TR Systems- Practical Issues,2.8,And then will show the mix of results to users.,00:13:43,9,And show mix results users
6340,00:13:49,3,Evaluation of TR Systems- Practical Issues,2.8,"Of course, the users don't see which result is from which method.",00:13:46,9,Of course users see result method
6341,00:13:52,3,Evaluation of TR Systems- Practical Issues,2.8,The users would judge those results or,00:13:49,9,The users would judge results
6342,00:13:58,3,Evaluation of TR Systems- Practical Issues,2.8,click on those those documents in in a search engine application.,00:13:52,9,click documents search engine application
6343,00:14:02,3,Evaluation of TR Systems- Practical Issues,2.8,"In this case, then, the search engine can keep track of the clicked documents, and",00:13:58,9,In case search engine keep track clicked documents
6344,00:14:07,3,Evaluation of TR Systems- Practical Issues,2.8,see if one method has contributed more to the clicked documents.,00:14:02,9,see one method contributed clicked documents
6345,00:14:11,3,Evaluation of TR Systems- Practical Issues,2.8,"If the user tends to click on one the results from one method,",00:14:07,9,If user tends click one results one method
6346,00:14:17,3,Evaluation of TR Systems- Practical Issues,2.8,"then it's just that method may, may be better.",00:14:13,9,method may may better
6347,00:14:21,3,Evaluation of TR Systems- Practical Issues,2.8,So this is what leverages a real users of a search engine to do evaluation.,00:14:17,9,So leverages real users search engine evaluation
6348,00:14:24,3,Evaluation of TR Systems- Practical Issues,2.8,"It's called A-B Test, and it's a strategy that's often used by",00:14:21,9,It called A B Test strategy often used
6349,00:14:28,3,Evaluation of TR Systems- Practical Issues,2.8,"the modern search engines, the commercial search engines.",00:14:25,9,modern search engines commercial search engines
6350,00:14:32,3,Evaluation of TR Systems- Practical Issues,2.8,Another way to evaluate IR or,00:14:29,9,Another way evaluate IR
6351,00:14:35,3,Evaluation of TR Systems- Practical Issues,2.8,"text retrieval is user studies, and we haven't covered that.",00:14:32,9,text retrieval user studies covered
6352,00:14:38,3,Evaluation of TR Systems- Practical Issues,2.8,I've put some references here that you can,00:14:35,9,I put references
6353,00:14:40,3,Evaluation of TR Systems- Practical Issues,2.8,look at if you want to know more about that.,00:14:38,9,look want know
6354,00:14:44,3,Evaluation of TR Systems- Practical Issues,2.8,"So there are three additional readings here,",00:14:41,9,So three additional readings
6355,00:14:47,3,Evaluation of TR Systems- Practical Issues,2.8,these are three mini books about evaluation.,00:14:44,9,three mini books evaluation
6356,00:14:53,3,Evaluation of TR Systems- Practical Issues,2.8,And they are all excellent in covering a broad review of information retrieval and,00:14:47,9,And excellent covering broad review information retrieval
6357,00:14:54,3,Evaluation of TR Systems- Practical Issues,2.8,evaluation.,00:14:53,9,evaluation
6358,00:14:58,3,Evaluation of TR Systems- Practical Issues,2.8,And this covered some of the things that we discussed.,00:14:54,9,And covered things discussed
6359,00:15:02,3,Evaluation of TR Systems- Practical Issues,2.8,But they also have a lot of others to offer.,00:14:58,9,But also lot others offer
6360,00:00:04,5,Future of Web Search,4.5,[SOUND].,00:00:00,9,SOUND
6361,00:00:10,5,Future of Web Search,4.5,This lecture is about the future of web search.,00:00:07,9,This lecture future web search
6362,00:00:17,5,Future of Web Search,4.5,"In this lecture, we're going to talk about some possible future trends",00:00:12,9,In lecture going talk possible future trends
6363,00:00:22,5,Future of Web Search,4.5,of web search and intelligent information retrieval systems in general.,00:00:17,9,web search intelligent information retrieval systems general
6364,00:00:28,5,Future of Web Search,4.5,"In order to further improve the accuracy of a search engine,",00:00:24,9,In order improve accuracy search engine
6365,00:00:33,5,Future of Web Search,4.5,it's important that to consider special cases of information need.,00:00:28,9,important consider special cases information need
6366,00:00:39,5,Future of Web Search,4.5,So one particular trend could be to have more and more specialized than,00:00:33,9,So one particular trend could specialized
6367,00:00:44,5,Future of Web Search,4.5,"customized search engines, and they can be called vertical search engines.",00:00:39,9,customized search engines called vertical search engines
6368,00:00:50,5,Future of Web Search,4.5,These vertical search engines can be expected to be more effective than,00:00:46,9,These vertical search engines expected effective
6369,00:00:55,5,Future of Web Search,4.5,the current general search engines because they could assume that,00:00:50,9,current general search engines could assume
6370,00:01:02,5,Future of Web Search,4.5,"users are a special group of users that might have a common information need,",00:00:55,9,users special group users might common information need
6371,00:01:06,5,Future of Web Search,4.5,"and then the search engine can be customized with this ser, so, such users.",00:01:02,9,search engine customized ser users
6372,00:01:12,5,Future of Web Search,4.5,"And because of the customization, it's also possible to do personalization.",00:01:07,9,And customization also possible personalization
6373,00:01:14,5,Future of Web Search,4.5,"So the search can be personalized,",00:01:12,9,So search personalized
6374,00:01:18,5,Future of Web Search,4.5,because we have a better understanding of the users.,00:01:15,9,better understanding users
6375,00:01:25,5,Future of Web Search,4.5,"Because of the restrictions with domain, we also have some advantages",00:01:20,9,Because restrictions domain also advantages
6376,00:01:29,5,Future of Web Search,4.5,"in handling the documents, because we can have better understanding of documents.",00:01:25,9,handling documents better understanding documents
6377,00:01:33,5,Future of Web Search,4.5,"For example, particular words may not be ambiguous in such a domain.",00:01:29,9,For example particular words may ambiguous domain
6378,00:01:36,5,Future of Web Search,4.5,So we can bypass the problem of ambiguity.,00:01:33,9,So bypass problem ambiguity
6379,00:01:41,5,Future of Web Search,4.5,"Another trend we can expect to see,",00:01:38,9,Another trend expect see
6380,00:01:45,5,Future of Web Search,4.5,is the search engine will be able to learn over time.,00:01:41,9,search engine able learn time
6381,00:01:52,5,Future of Web Search,4.5,"It's like a lifetime learning or lifelong learning, and this is, of course,",00:01:45,9,It like lifetime learning lifelong learning course
6382,00:01:57,5,Future of Web Search,4.5,very attractive because that means the search engine will self-improve itself.,00:01:52,9,attractive means search engine self improve
6383,00:02:01,5,Future of Web Search,4.5,"As more people are using it, the search engine will become better and better, and",00:01:57,9,As people using search engine become better better
6384,00:02:03,5,Future of Web Search,4.5,"this is already happening,",00:02:01,9,already happening
6385,00:02:06,5,Future of Web Search,4.5,because the search engines can learn from the [INAUDIBLE] of feedback.,00:02:03,9,search engines learn INAUDIBLE feedback
6386,00:02:10,5,Future of Web Search,4.5,"More users use it, and the quality of the search engine allows for",00:02:06,9,More users use quality search engine allows
6387,00:02:15,5,Future of Web Search,4.5,"the popular queries that are typed in by many users allow it to become better,",00:02:10,9,popular queries typed many users allow become better
6388,00:02:19,5,Future of Web Search,4.5,so this is sort of another feature that we will see.,00:02:15,9,sort another feature see
6389,00:02:24,5,Future of Web Search,4.5,The third trend might be to the integration of,00:02:21,9,The third trend might integration
6390,00:02:27,5,Future of Web Search,4.5,bottles of information access.,00:02:24,9,bottles information access
6391,00:02:32,5,Future of Web Search,4.5,"So search, navigation, and recommendation or filtering might be",00:02:27,9,So search navigation recommendation filtering might
6392,00:02:37,5,Future of Web Search,4.5,combined to form a full-fledged information management system.,00:02:32,9,combined form full fledged information management system
6393,00:02:42,5,Future of Web Search,4.5,"And in the beginning of this course, we talked about push versus pull.",00:02:37,9,And beginning course talked push versus pull
6394,00:02:47,5,Future of Web Search,4.5,"These are different modes of information access, but these modes can be combined.",00:02:42,9,These different modes information access modes combined
6395,00:02:53,5,Future of Web Search,4.5,"And similarly, in the pull mode, querying and the browsing could also be combined.",00:02:48,9,And similarly pull mode querying browsing could also combined
6396,00:02:58,5,Future of Web Search,4.5,"And in fact we're doing that basically, today, is the [INAUDIBLE] search endings.",00:02:53,9,And fact basically today INAUDIBLE search endings
6397,00:03:02,5,Future of Web Search,4.5,"We are querying, sometimes browsing, clicking on links.",00:02:58,9,We querying sometimes browsing clicking links
6398,00:03:05,5,Future of Web Search,4.5,Sometimes we've got some information recommended.,00:03:02,9,Sometimes got information recommended
6399,00:03:11,5,Future of Web Search,4.5,Although most of the cases the information recommended is because of advertising.,00:03:05,9,Although cases information recommended advertising
6400,00:03:16,5,Future of Web Search,4.5,"But in the future, you can imagine seamlessly integrate the system with",00:03:11,9,But future imagine seamlessly integrate system
6401,00:03:21,5,Future of Web Search,4.5,"multi-mode for information access, and that would be convenient for people.",00:03:16,9,multi mode information access would convenient people
6402,00:03:27,5,Future of Web Search,4.5,Another trend is that we might see systems,00:03:23,9,Another trend might see systems
6403,00:03:30,5,Future of Web Search,4.5,that try to go beyond the searches to support the user tasks.,00:03:27,9,try go beyond searches support user tasks
6404,00:03:36,5,Future of Web Search,4.5,"After all, the reason why people want to search is to solve a problem or",00:03:30,9,After reason people want search solve problem
6405,00:03:39,5,Future of Web Search,4.5,to make a decision or perform a task.,00:03:36,9,make decision perform task
6406,00:03:42,5,Future of Web Search,4.5,For example consumers might search for,00:03:39,9,For example consumers might search
6407,00:03:45,5,Future of Web Search,4.5,"opinions about products in order to purchase a product,",00:03:42,9,opinions products order purchase product
6408,00:03:50,5,Future of Web Search,4.5,"choose a good product by, so in this case it would be beneficial to",00:03:45,9,choose good product case would beneficial
6409,00:03:55,5,Future of Web Search,4.5,"support the whole workflow of purchasing a product, or choosing a product.",00:03:50,9,support whole workflow purchasing product choosing product
6410,00:04:00,5,Future of Web Search,4.5,"In this era, after the common search engines already provide a good support.",00:03:56,9,In era common search engines already provide good support
6411,00:04:04,5,Future of Web Search,4.5,"For example, you can sometimes look at the reviews, and then if you want to buy it,",00:04:00,9,For example sometimes look reviews want buy
6412,00:04:09,5,Future of Web Search,4.5,you can just click on the button to go the shopping site and directly get it done.,00:04:04,9,click button go shopping site directly get done
6413,00:04:12,5,Future of Web Search,4.5,"But it does not provide a, a good task support for many other tasks.",00:04:09,9,But provide good task support many tasks
6414,00:04:14,5,Future of Web Search,4.5,"For example, for researchers,",00:04:12,9,For example researchers
6415,00:04:18,5,Future of Web Search,4.5,you might want to find the realm in the literature or site of the literature.,00:04:14,9,might want find realm literature site literature
6416,00:04:26,5,Future of Web Search,4.5,"And then, there's no, not much support for finishing a task such as writing a paper.",00:04:18,9,And much support finishing task writing paper
6417,00:04:31,5,Future of Web Search,4.5,"So, in general, I think, there are many opportunities in the wait.",00:04:26,9,So general I think many opportunities wait
6418,00:04:34,5,Future of Web Search,4.5,"So in the following few slides, I'll be talking a little bit more about some",00:04:31,9,So following slides I talking little bit
6419,00:04:39,5,Future of Web Search,4.5,"specific ideas or thoughts that hopefully,",00:04:34,9,specific ideas thoughts hopefully
6420,00:04:43,5,Future of Web Search,4.5,can help you in imagining new application possibilities.,00:04:39,9,help imagining new application possibilities
6421,00:04:51,5,Future of Web Search,4.5,Some of them might be already relevant to what you are currently working on.,00:04:43,9,Some might already relevant currently working
6422,00:04:55,5,Future of Web Search,4.5,"In general, we can think about any intelligent system, especially intelligent",00:04:51,9,In general think intelligent system especially intelligent
6423,00:05:02,5,Future of Web Search,4.5,"information system, as we specified by these these three nodes.",00:04:55,9,information system specified three nodes
6424,00:05:05,5,Future of Web Search,4.5,"And so if we connect these three into a triangle,",00:05:02,9,And connect three triangle
6425,00:05:09,5,Future of Web Search,4.5,then we'll able to specify an information system.,00:05:05,9,able specify information system
6426,00:05:12,5,Future of Web Search,4.5,And I call this Data-User-Service Triangle.,00:05:09,9,And I call Data User Service Triangle
6427,00:05:18,5,Future of Web Search,4.5,So basically the three questions you ask would be who are you serving and,00:05:12,9,So basically three questions ask would serving
6428,00:05:23,5,Future of Web Search,4.5,what kind of data are you are managing and what kind of service you provide.,00:05:18,9,kind data managing kind service provide
6429,00:05:29,5,Future of Web Search,4.5,"Right there, this would help us basically specify in your system.",00:05:24,9,Right would help us basically specify system
6430,00:05:33,5,Future of Web Search,4.5,And there are many different ways to connect them depending on,00:05:30,9,And many different ways connect depending
6431,00:05:36,5,Future of Web Search,4.5,"how you connect them, you will have a different kind of systems.",00:05:33,9,connect different kind systems
6432,00:05:37,5,Future of Web Search,4.5,So let me give you some examples.,00:05:36,9,So let give examples
6433,00:05:40,5,Future of Web Search,4.5,"On the top, you can see different kinds of users.",00:05:37,9,On top see different kinds users
6434,00:05:45,5,Future of Web Search,4.5,"On the left side, you can see different types of data or information, and",00:05:40,9,On left side see different types data information
6435,00:05:48,5,Future of Web Search,4.5,"on the bottom, you can see different service functions.",00:05:45,9,bottom see different service functions
6436,00:05:51,5,Future of Web Search,4.5,Now imagine you can connect all these in different ways.,00:05:48,9,Now imagine connect different ways
6437,00:05:55,5,Future of Web Search,4.5,"So, for example, you can connect everyone with web pages, and",00:05:51,9,So example connect everyone web pages
6438,00:05:59,5,Future of Web Search,4.5,"the support search and browsing, what do you get?",00:05:55,9,support search browsing get
6439,00:06:01,5,Future of Web Search,4.5,"Well, that's web search, right?",00:05:59,9,Well web search right
6440,00:06:07,5,Future of Web Search,4.5,What if we connect UIUC employees with organization documents or enterprise,00:06:02,9,What connect UIUC employees organization documents enterprise
6441,00:06:12,5,Future of Web Search,4.5,"documents to support the search and browsing, but that's enterprise search.",00:06:07,9,documents support search browsing enterprise search
6442,00:06:17,5,Future of Web Search,4.5,If you connect the scientist with literature information,00:06:12,9,If connect scientist literature information
6443,00:06:22,5,Future of Web Search,4.5,"to provide all kinds of service, including search, browsing, or",00:06:17,9,provide kinds service including search browsing
6444,00:06:28,5,Future of Web Search,4.5,"alert of new random documents or mining analyzing research trends,",00:06:22,9,alert new random documents mining analyzing research trends
6445,00:06:31,5,Future of Web Search,4.5,or provide the task with support or decision support.,00:06:28,9,provide task support decision support
6446,00:06:36,5,Future of Web Search,4.5,"For example, we might be, might be able to provide a support for",00:06:31,9,For example might might able provide support
6447,00:06:40,5,Future of Web Search,4.5,automatically generating related work section for,00:06:36,9,automatically generating related work section
6448,00:06:44,5,Future of Web Search,4.5,"a research paper, and this would be closer to task support.",00:06:40,9,research paper would closer task support
6449,00:06:45,5,Future of Web Search,4.5,Right? So then,00:06:44,9,Right So
6450,00:06:48,5,Future of Web Search,4.5,we can imagine this would be a literature assistant.,00:06:45,9,imagine would literature assistant
6451,00:06:52,5,Future of Web Search,4.5,If we connect the online shoppers with blog articles or product reviews,00:06:48,9,If connect online shoppers blog articles product reviews
6452,00:06:59,5,Future of Web Search,4.5,then we can help these people to improve shopping experience.,00:06:53,9,help people improve shopping experience
6453,00:07:05,5,Future of Web Search,4.5,"So we can provide, for example data mining capabilities to analyze the reviews,",00:06:59,9,So provide example data mining capabilities analyze reviews
6454,00:07:11,5,Future of Web Search,4.5,"to compare products, compare sentiment of products and to provide task support or",00:07:05,9,compare products compare sentiment products provide task support
6455,00:07:15,5,Future of Web Search,4.5,decision support to have them choose what product to buy.,00:07:11,9,decision support choose product buy
6456,00:07:21,5,Future of Web Search,4.5,"Or we can connect customer service people with emails from the customers,",00:07:15,9,Or connect customer service people emails customers
6457,00:07:27,5,Future of Web Search,4.5,"and, and we can imagine a system that can provide a analysis",00:07:22,9,imagine system provide analysis
6458,00:07:31,5,Future of Web Search,4.5,of these emails to find that the major complaints of the customers.,00:07:27,9,emails find major complaints customers
6459,00:07:35,5,Future of Web Search,4.5,We can imagine a system we could provide task support,00:07:31,9,We imagine system could provide task support
6460,00:07:39,5,Future of Web Search,4.5,by automatically generating a response to a customer email.,00:07:35,9,automatically generating response customer email
6461,00:07:45,5,Future of Web Search,4.5,Maybe intelligently attach also a promotion message,00:07:39,9,Maybe intelligently attach also promotion message
6462,00:07:49,5,Future of Web Search,4.5,"if appropriate, if they detect that that's a positive message, not a complaint, and",00:07:45,9,appropriate detect positive message complaint
6463,00:07:55,5,Future of Web Search,4.5,then you might take this opportunity to attach some promotion information.,00:07:49,9,might take opportunity attach promotion information
6464,00:07:57,5,Future of Web Search,4.5,"Whereas if it's a complaint, then you might be able to",00:07:55,9,Whereas complaint might able
6465,00:08:03,5,Future of Web Search,4.5,automatically generate some generic response first and,00:07:59,9,automatically generate generic response first
6466,00:08:08,5,Future of Web Search,4.5,"tell the customer that he or she can expect a detailed response later, etc.",00:08:03,9,tell customer expect detailed response later etc
6467,00:08:14,5,Future of Web Search,4.5,All of these are trying to help people to improve the productivity.,00:08:08,9,All trying help people improve productivity
6468,00:08:19,5,Future of Web Search,4.5,So this shows that the opportunities are really a lot.,00:08:15,9,So shows opportunities really lot
6469,00:08:22,5,Future of Web Search,4.5,It's just only restricted by our imagination.,00:08:19,9,It restricted imagination
6470,00:08:27,5,Future of Web Search,4.5,"So this picture shows the trend of the technology, and also,",00:08:22,9,So picture shows trend technology also
6471,00:08:33,5,Future of Web Search,4.5,"it characterizes the, intelligent information system in three angles.",00:08:27,9,characterizes intelligent information system three angles
6472,00:08:39,5,Future of Web Search,4.5,"You can see in the center, there's a triangle that connects keyword queries",00:08:33,9,You see center triangle connects keyword queries
6473,00:08:41,5,Future of Web Search,4.5,to search a bag of words representation.,00:08:39,9,search bag words representation
6474,00:08:46,5,Future of Web Search,4.5,That means the current search engines basically provides search support,00:08:41,9,That means current search engines basically provides search support
6475,00:08:54,5,Future of Web Search,4.5,to users and mostly model users based on keyword queries,00:08:46,9,users mostly model users based keyword queries
6476,00:08:59,5,Future of Web Search,4.5,and sees the data through bag of words representation.,00:08:54,9,sees data bag words representation
6477,00:09:06,5,Future of Web Search,4.5,So it's a very simple approximation of the actual information in the documents.,00:08:59,9,So simple approximation actual information documents
6478,00:09:08,5,Future of Web Search,4.5,But that's what the current system does.,00:09:06,9,But current system
6479,00:09:12,5,Future of Web Search,4.5,"It connects these three nodes in such a simple way, or",00:09:08,9,It connects three nodes simple way
6480,00:09:17,5,Future of Web Search,4.5,"it only provides a basic search function and doesn't really understand the user,",00:09:12,9,provides basic search function really understand user
6481,00:09:24,5,Future of Web Search,4.5,and it doesn't really understand that much information in the documents.,00:09:17,9,really understand much information documents
6482,00:09:31,5,Future of Web Search,4.5,"Now, I showed some trends to push each node toward a more advanced function.",00:09:24,9,Now I showed trends push node toward advanced function
6483,00:09:35,5,Future of Web Search,4.5,"So think about the user node here, right?",00:09:31,9,So think user node right
6484,00:09:39,5,Future of Web Search,4.5,"So we can go beyond the keyword queries, look at the user search history,",00:09:35,9,So go beyond keyword queries look user search history
6485,00:09:43,5,Future of Web Search,4.5,"and then further model the user completely to understand the,",00:09:39,9,model user completely understand
6486,00:09:49,5,Future of Web Search,4.5,"the user's task environment, task need context or other information.",00:09:43,9,user task environment task need context information
6487,00:09:55,5,Future of Web Search,4.5,"Okay, so this is pushing for personalization and complete user model.",00:09:49,9,Okay pushing personalization complete user model
6488,00:09:58,5,Future of Web Search,4.5,"And this is a major direction in research in,",00:09:55,9,And major direction research
6489,00:10:01,5,Future of Web Search,4.5,in order to build intelligent information systems.,00:09:58,9,order build intelligent information systems
6490,00:10:05,5,Future of Web Search,4.5,"On the document side, we can also see, we can",00:10:01,9,On document side also see
6491,00:10:10,5,Future of Web Search,4.5,go beyond bag of words implementation to have entity relation representation.,00:10:05,9,go beyond bag words implementation entity relation representation
6492,00:10:16,5,Future of Web Search,4.5,"This means we'll recognize people's names, their relations, locations, etc.",00:10:10,9,This means recognize people names relations locations etc
6493,00:10:20,5,Future of Web Search,4.5,And this is already feasible with today's natural processing technique.,00:10:16,9,And already feasible today natural processing technique
6494,00:10:24,5,Future of Web Search,4.5,And Google is the reason the initiative on the knowledge graph.,00:10:20,9,And Google reason initiative knowledge graph
6495,00:10:28,5,Future of Web Search,4.5,"If you haven't heard of it, it is a good step toward this direction.",00:10:24,9,If heard good step toward direction
6496,00:10:33,5,Future of Web Search,4.5,"And once we can get to that level without initiating robust manner at larger scale,",00:10:28,9,And get level without initiating robust manner larger scale
6497,00:10:38,5,Future of Web Search,4.5,it can enable the search engine to provide a much better service.,00:10:33,9,enable search engine provide much better service
6498,00:10:41,5,Future of Web Search,4.5,In the future we would like to have,00:10:38,9,In future would like
6499,00:10:45,5,Future of Web Search,4.5,"knowledge representation where we can add perhaps inference rules, and",00:10:41,9,knowledge representation add perhaps inference rules
6500,00:10:47,5,Future of Web Search,4.5,then the search engine would become more intelligent.,00:10:45,9,search engine would become intelligent
6501,00:10:53,5,Future of Web Search,4.5,"So this calls for large-scale semantic analysis, and",00:10:49,9,So calls large scale semantic analysis
6502,00:10:57,5,Future of Web Search,4.5,perhaps this is more feasible for vertical search engines.,00:10:53,9,perhaps feasible vertical search engines
6503,00:10:59,5,Future of Web Search,4.5,It's easier to make progress in the particular domain.,00:10:57,9,It easier make progress particular domain
6504,00:11:01,5,Future of Web Search,4.5,"Now on the service side,",00:10:59,9,Now service side
6505,00:11:05,5,Future of Web Search,4.5,we see we need to go beyond the search of support information access in general.,00:11:01,9,see need go beyond search support information access general
6506,00:11:13,5,Future of Web Search,4.5,So search is only one way to get access to information as well recommender,00:11:07,9,So search one way get access information well recommender
6507,00:11:19,5,Future of Web Search,4.5,systems and push and pull so different ways to get access to random information.,00:11:13,9,systems push pull different ways get access random information
6508,00:11:21,5,Future of Web Search,4.5,"But going beyond access,",00:11:19,9,But going beyond access
6509,00:11:25,5,Future of Web Search,4.5,"we also need to help people digest the information once the information is found,",00:11:21,9,also need help people digest information information found
6510,00:11:30,5,Future of Web Search,4.5,and this step has to do with analysis of information or data mining.,00:11:25,9,step analysis information data mining
6511,00:11:35,5,Future of Web Search,4.5,We have to find patterns or convert the text information into,00:11:30,9,We find patterns convert text information
6512,00:11:38,5,Future of Web Search,4.5,real knowledge that can be used in application or,00:11:35,9,real knowledge used application
6513,00:11:43,5,Future of Web Search,4.5,actionable knowledge that can be used for decision making.,00:11:38,9,actionable knowledge used decision making
6514,00:11:47,5,Future of Web Search,4.5,And furthermore the knowledge will be used to help a user to,00:11:43,9,And furthermore knowledge used help user
6515,00:11:52,5,Future of Web Search,4.5,"improve productivity in finishing a task, for example, a decision-making task.",00:11:47,9,improve productivity finishing task example decision making task
6516,00:11:54,5,Future of Web Search,4.5,"Right, so this is a trend.",00:11:52,9,Right trend
6517,00:11:59,5,Future of Web Search,4.5,"And, and, and so basically, in this dimension, we anticipate",00:11:54,9,And basically dimension anticipate
6518,00:12:04,5,Future of Web Search,4.5,in the future intelligent information systems will provide intelligent and,00:11:59,9,future intelligent information systems provide intelligent
6519,00:12:06,5,Future of Web Search,4.5,interactive task support.,00:12:04,9,interactive task support
6520,00:12:11,5,Future of Web Search,4.5,"Now I should also emphasize interactive here, because it's important to optimize",00:12:06,9,Now I also emphasize interactive important optimize
6521,00:12:16,5,Future of Web Search,4.5,the combined intelligence of the users and the system.,00:12:11,9,combined intelligence users system
6522,00:12:22,5,Future of Web Search,4.5,"So we, we can get some help from users in some natural way.",00:12:16,9,So get help users natural way
6523,00:12:26,5,Future of Web Search,4.5,"And we don't have to assume the system has to do everything when the human,",00:12:22,9,And assume system everything human
6524,00:12:32,5,Future of Web Search,4.5,"user, and the machine can collaborate in an intelligent way, an efficient way,",00:12:26,9,user machine collaborate intelligent way efficient way
6525,00:12:37,5,Future of Web Search,4.5,"then the combined intelligence will be high and in general,",00:12:32,9,combined intelligence high general
6526,00:12:41,5,Future of Web Search,4.5,we can minimize the user's overall effort in solving problem.,00:12:37,9,minimize user overall effort solving problem
6527,00:12:47,5,Future of Web Search,4.5,"So this is the big picture of future intelligent information systems,",00:12:42,9,So big picture future intelligent information systems
6528,00:12:52,5,Future of Web Search,4.5,and this hopefully can provide us with some insights about,00:12:47,9,hopefully provide us insights
6529,00:12:57,5,Future of Web Search,4.5,how to make further innovations on top of what we handled today.,00:12:52,9,make innovations top handled today
6530,00:00:06,4,Feedback in Text Retrieval- Feedback in LM,3.8,[SOUND] This lecture is about the feedback,00:00:00,10,SOUND This lecture feedback
6531,00:00:12,4,Feedback in Text Retrieval- Feedback in LM,3.8,in the language modeling approach.,00:00:06,10,language modeling approach
6532,00:00:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,In this lecture we will continue the discussion of feedback in text retrieval.,00:00:12,10,In lecture continue discussion feedback text retrieval
6533,00:00:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,In particular we're going to talk about the feedback in language modeling,00:00:17,10,In particular going talk feedback language modeling
6534,00:00:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,approaches.,00:00:20,10,approaches
6535,00:00:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,So we derive the query likelihood ranking function by making various assumptions.,00:00:23,10,So derive query likelihood ranking function making various assumptions
6536,00:00:35,4,Feedback in Text Retrieval- Feedback in LM,3.8,"As a basic retrieval function, that formula, or those formulas worked well.",00:00:30,10,As basic retrieval function formula formulas worked well
6537,00:00:39,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But if we think about the feedback information, it's a little bit awkward to",00:00:35,10,But think feedback information little bit awkward
6538,00:00:44,4,Feedback in Text Retrieval- Feedback in LM,3.8,use query likelihood to perform feedback because,00:00:39,10,use query likelihood perform feedback
6539,00:00:49,4,Feedback in Text Retrieval- Feedback in LM,3.8,a lot of times the feedback information is additional information about the query.,00:00:44,10,lot times feedback information additional information query
6540,00:00:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,But we assume the query is generated by assembling words,00:00:49,10,But assume query generated assembling words
6541,00:00:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,from a language model in the query likelihood method.,00:00:53,10,language model query likelihood method
6542,00:01:03,4,Feedback in Text Retrieval- Feedback in LM,3.8,"It's kind of unnatural to sample, words that, form feedback documents.",00:00:56,10,It kind unnatural sample words form feedback documents
6543,00:01:06,4,Feedback in Text Retrieval- Feedback in LM,3.8,"As a result, then research is proposed,",00:01:03,10,As result research proposed
6544,00:01:10,4,Feedback in Text Retrieval- Feedback in LM,3.8,a way to generalize query likelihood function.,00:01:06,10,way generalize query likelihood function
6545,00:01:14,4,Feedback in Text Retrieval- Feedback in LM,3.8,It's called a Kullback-Leibler divergence retrieval model.,00:01:10,10,It called Kullback Leibler divergence retrieval model
6546,00:01:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And this model is actually, going to make the query likelihood,",00:01:15,10,And model actually going make query likelihood
6547,00:01:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,our retrieval function much closer to vector space model.,00:01:20,10,retrieval function much closer vector space model
6548,00:01:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Yet this, form of the language model can be, regarded as a generalization of query",00:01:25,10,Yet form language model regarded generalization query
6549,00:01:36,4,Feedback in Text Retrieval- Feedback in LM,3.8,likelihood in the sense that if it can cover query likelihood as a special case.,00:01:32,10,likelihood sense cover query likelihood special case
6550,00:01:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,And in this case the feedback can be achieved through,00:01:38,10,And case feedback achieved
6551,00:01:44,4,Feedback in Text Retrieval- Feedback in LM,3.8,simply query model estimation or updating.,00:01:41,10,simply query model estimation updating
6552,00:01:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,This is very similar to Rocchio which updates the query vector.,00:01:44,10,This similar Rocchio updates query vector
6553,00:01:55,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So let's see what the, is the scale of divergence, which we will model.",00:01:50,10,So let see scale divergence model
6554,00:01:59,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, on the top, what you see is query",00:01:55,10,So top see query
6555,00:02:05,4,Feedback in Text Retrieval- Feedback in LM,3.8,"likelihood retrieval function, all right, this one.",00:01:59,10,likelihood retrieval function right one
6556,00:02:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,And then KL-divergence or also called cross entropy retrieval,00:02:05,10,And KL divergence also called cross entropy retrieval
6557,00:02:16,4,Feedback in Text Retrieval- Feedback in LM,3.8,"model is basically to generalize the frequency part,",00:02:11,10,model basically generalize frequency part
6558,00:02:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,"here, into a layered model.",00:02:16,10,layered model
6559,00:02:24,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So basically it's the difference,",00:02:21,10,So basically difference
6560,00:02:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,given by the probabilistic model here,00:02:24,10,given probabilistic model
6561,00:02:34,4,Feedback in Text Retrieval- Feedback in LM,3.8,to characterize what the user's looking for versus the kind of query words there.,00:02:29,10,characterize user looking versus kind query words
6562,00:02:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,And this difference allows us to plotting various different ways to estimate this.,00:02:35,10,And difference allows us plotting various different ways estimate
6563,00:02:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,So this can be estimated in many different ways including using feedback information.,00:02:42,10,So estimated many different ways including using feedback information
6564,00:02:51,4,Feedback in Text Retrieval- Feedback in LM,3.8,Now this is called a KL-divergence because,00:02:48,10,Now called KL divergence
6565,00:02:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,this can be interpreted as measuring the KL-divergence of two distributions.,00:02:51,10,interpreted measuring KL divergence two distributions
6566,00:03:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,One is the query model denoted by this distribution.,00:02:56,10,One query model denoted distribution
6567,00:03:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,"One is the talking, the language model here.",00:03:02,10,One talking language model
6568,00:03:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And [INAUDIBLE] though is a [INAUDIBLE] language model, of course.",00:03:07,10,And INAUDIBLE though INAUDIBLE language model course
6569,00:03:15,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And we are not going to talk about the detail of that, and",00:03:11,10,And going talk detail
6570,00:03:18,4,Feedback in Text Retrieval- Feedback in LM,3.8,you'll find the things in references.,00:03:15,10,find things references
6571,00:03:22,4,Feedback in Text Retrieval- Feedback in LM,3.8,"It's also called cross entropy, because, in, in fact,",00:03:18,10,It also called cross entropy fact
6572,00:03:27,4,Feedback in Text Retrieval- Feedback in LM,3.8,we can ignore some terms in the KL-divergence function and we will end up,00:03:22,10,ignore terms KL divergence function end
6573,00:03:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,"having actually cross entropy, and that, both are terms in information theory.",00:03:27,10,actually cross entropy terms information theory
6574,00:03:35,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But, anyway for",00:03:34,10,But anyway
6575,00:03:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,"our purposes here you can just see the two formulas look almost identical,",00:03:36,10,purposes see two formulas look almost identical
6576,00:03:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,except that here we have a probability of a word given by a query language model.,00:03:42,10,except probability word given query language model
6577,00:03:52,4,Feedback in Text Retrieval- Feedback in LM,3.8,"This, and here,",00:03:49,10,This
6578,00:03:57,4,Feedback in Text Retrieval- Feedback in LM,3.8,"the sum is over all the words that are in the document,",00:03:52,10,sum words document
6579,00:04:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,and also with the non-zero probability for the query model.,00:03:57,10,also non zero probability query model
6580,00:04:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So it's kind of, again, a generalization of sum over all the matching query words.",00:04:02,10,So kind generalization sum matching query words
6581,00:04:14,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Now you can also, easy to see, we can recover the query likelihood,",00:04:09,10,Now also easy see recover query likelihood
6582,00:04:18,4,Feedback in Text Retrieval- Feedback in LM,3.8,which we will find here by as simple as setting this query model to,00:04:14,10,find simple setting query model
6583,00:04:23,4,Feedback in Text Retrieval- Feedback in LM,3.8,"the relative frequency of a word in the query, right?",00:04:18,10,relative frequency word query right
6584,00:04:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,This is very to easy see once you practice this.,00:04:23,10,This easy see practice
6585,00:04:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And to here, you can eliminate this query lens, that's a constant,",00:04:25,10,And eliminate query lens constant
6586,00:04:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,and then you get exactly like that.,00:04:29,10,get exactly like
6587,00:04:36,4,Feedback in Text Retrieval- Feedback in LM,3.8,So you can see the equivalence.,00:04:33,10,So see equivalence
6588,00:04:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,And that's also why this KL-divergence model can be regarded as a generalization,00:04:36,10,And also KL divergence model regarded generalization
6589,00:04:47,4,Feedback in Text Retrieval- Feedback in LM,3.8,"of query likelihood because we can cover query likelihood as a special case,",00:04:41,10,query likelihood cover query likelihood special case
6590,00:04:49,4,Feedback in Text Retrieval- Feedback in LM,3.8,but it would also allow it to do much more than that.,00:04:47,10,would also allow much
6591,00:04:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,So this is how we use the KL-divergence model to then do feedback.,00:04:50,10,So use KL divergence model feedback
6592,00:05:00,4,Feedback in Text Retrieval- Feedback in LM,3.8,"The picture shows that we first estimate a document language model,",00:04:56,10,The picture shows first estimate document language model
6593,00:05:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,then we estimate a query language model and,00:05:00,10,estimate query language model
6594,00:05:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,"we compute the KL-divergence, this is often denoted by a D here.",00:05:02,10,compute KL divergence often denoted D
6595,00:05:14,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But this basically means, this was exactly like in vector space",00:05:09,10,But basically means exactly like vector space
6596,00:05:18,4,Feedback in Text Retrieval- Feedback in LM,3.8,model because we compute the vector for the document in the computer and,00:05:14,10,model compute vector document computer
6597,00:05:22,4,Feedback in Text Retrieval- Feedback in LM,3.8,"not the vector for the query, and then we compute the distance.",00:05:18,10,vector query compute distance
6598,00:05:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Only that these vectors are of special forms,",00:05:22,10,Only vectors special forms
6599,00:05:26,4,Feedback in Text Retrieval- Feedback in LM,3.8,they have probability distributions.,00:05:25,10,probability distributions
6600,00:05:31,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And then we get the results, and we can find some feedback documents.",00:05:27,10,And get results find feedback documents
6601,00:05:37,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Let's assume they are more selective sorry, mostly positive documents.",00:05:31,10,Let assume selective sorry mostly positive documents
6602,00:05:40,4,Feedback in Text Retrieval- Feedback in LM,3.8,Although we could also consider both kinds of documents.,00:05:37,10,Although could also consider kinds documents
6603,00:05:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So what we could do is, like in Rocchio,",00:05:40,10,So could like Rocchio
6604,00:05:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,we can compute another language model called feedback language model here.,00:05:42,10,compute another language model called feedback language model
6605,00:05:52,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Again, this is going to be another vector just like a computing centroid vector in",00:05:48,10,Again going another vector like computing centroid vector
6606,00:05:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,Rocchio.,00:05:52,10,Rocchio
6607,00:05:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,And then this model can be combined with the original,00:05:53,10,And model combined original
6608,00:05:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,query model using a linear interpolation.,00:05:56,10,query model using linear interpolation
6609,00:06:04,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And this would then give us an updated model, just like again in Rocchio.",00:06:00,10,And would give us updated model like Rocchio
6610,00:06:10,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Right, so here, we can see the parameter of our controlling amount of feedback if",00:06:05,10,Right see parameter controlling amount feedback
6611,00:06:14,4,Feedback in Text Retrieval- Feedback in LM,3.8,"it's set to 0, then it says here there's no feedback.",00:06:10,10,set 0 says feedback
6612,00:06:19,4,Feedback in Text Retrieval- Feedback in LM,3.8,"After set to 1, we've got full feedback, we can ignore the original query.",00:06:14,10,After set 1 got full feedback ignore original query
6613,00:06:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And this is generally not desirable, right.",00:06:19,10,And generally desirable right
6614,00:06:27,4,Feedback in Text Retrieval- Feedback in LM,3.8,So this unless you are absolutely sure you have seen a lot of relevant documents and,00:06:21,10,So unless absolutely sure seen lot relevant documents
6615,00:06:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,the query terms are not important.,00:06:27,10,query terms important
6616,00:06:34,4,Feedback in Text Retrieval- Feedback in LM,3.8,So of course the main question here is how do you compute this theta F?,00:06:31,10,So course main question compute theta F
6617,00:06:36,4,Feedback in Text Retrieval- Feedback in LM,3.8,This is the big question here.,00:06:34,10,This big question
6618,00:06:39,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And once you can do that, the rest is easy.",00:06:36,10,And rest easy
6619,00:06:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,So here we'll talk about one of the approaches.,00:06:39,10,So talk one approaches
6620,00:06:43,4,Feedback in Text Retrieval- Feedback in LM,3.8,And there are many approaches of course.,00:06:41,10,And many approaches course
6621,00:06:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,This approach is based on generative model and I'm going to show you how it works.,00:06:43,10,This approach based generative model I going show works
6622,00:06:50,4,Feedback in Text Retrieval- Feedback in LM,3.8,This is a user generative mixture model.,00:06:48,10,This user generative mixture model
6623,00:06:55,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So this picture shows that the we have this model here,",00:06:50,10,So picture shows model
6624,00:06:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,the feedback model that we want to estimate.,00:06:55,10,feedback model want estimate
6625,00:07:00,4,Feedback in Text Retrieval- Feedback in LM,3.8,And we the basis is the feedback options.,00:06:58,10,And basis feedback options
6626,00:07:04,4,Feedback in Text Retrieval- Feedback in LM,3.8,Let's say we are observing the positive documents.,00:07:00,10,Let say observing positive documents
6627,00:07:08,4,Feedback in Text Retrieval- Feedback in LM,3.8,"These are the collected documents by users, or random documents judged by",00:07:04,10,These collected documents users random documents judged
6628,00:07:12,4,Feedback in Text Retrieval- Feedback in LM,3.8,"users, or simply top ranked documents that we assumed to be random.",00:07:08,10,users simply top ranked documents assumed random
6629,00:07:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,Now imagine how we can compute a centroid for,00:07:14,10,Now imagine compute centroid
6630,00:07:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,these documents by using language model.,00:07:17,10,documents using language model
6631,00:07:23,4,Feedback in Text Retrieval- Feedback in LM,3.8,One approach is simply to assume,00:07:20,10,One approach simply assume
6632,00:07:28,4,Feedback in Text Retrieval- Feedback in LM,3.8,these documents are generated from this language model as we did before.,00:07:23,10,documents generated language model
6633,00:07:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,"What we could do is do it, just normalize the word frequency here.",00:07:28,10,What could normalize word frequency
6634,00:07:34,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And then we, we'll get this word distribution.",00:07:32,10,And get word distribution
6635,00:07:39,4,Feedback in Text Retrieval- Feedback in LM,3.8,Now the question is whether this distribution is good for feedback.,00:07:36,10,Now question whether distribution good feedback
6636,00:07:44,4,Feedback in Text Retrieval- Feedback in LM,3.8,Well you can imagine well the top rank of the words would be what?,00:07:39,10,Well imagine well top rank words would
6637,00:07:46,4,Feedback in Text Retrieval- Feedback in LM,3.8,What do you think?,00:07:45,10,What think
6638,00:07:51,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Well those words would be common words, right?",00:07:48,10,Well words would common words right
6639,00:07:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,"As well we see in, in the language model,",00:07:51,10,As well see language model
6640,00:07:57,4,Feedback in Text Retrieval- Feedback in LM,3.8,"in the top right, the words are actually common words like, the, et cetera.",00:07:53,10,top right words actually common words like et cetera
6641,00:08:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, it's not very good for feedback, because we will be adding a lot of such",00:07:57,10,So good feedback adding lot
6642,00:08:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,"words to our query when we interpret, this was the original query model.",00:08:02,10,words query interpret original query model
6643,00:08:13,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, this is not good, so we need to do something, in particular,",00:08:08,10,So good need something particular
6644,00:08:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,we are trying to get rid of those common words.,00:08:13,10,trying get rid common words
6645,00:08:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And we all, we have seen actually one way to do that, by using background language",00:08:17,10,And seen actually one way using background language
6646,00:08:27,4,Feedback in Text Retrieval- Feedback in LM,3.8,"model in the case of learning the associations with of words, right.",00:08:21,10,model case learning associations words right
6647,00:08:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,The words that are related to the word computer.,00:08:27,10,The words related word computer
6648,00:08:33,4,Feedback in Text Retrieval- Feedback in LM,3.8,"We could do that, and that would be another way to do this.",00:08:30,10,We could would another way
6649,00:08:36,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But here, we're going to talk about another approach,",00:08:33,10,But going talk another approach
6650,00:08:39,4,Feedback in Text Retrieval- Feedback in LM,3.8,which is a more principled approach.,00:08:36,10,principled approach
6651,00:08:43,4,Feedback in Text Retrieval- Feedback in LM,3.8,"In this case, we're going to say, well, you, you said that there are common words",00:08:39,10,In case going say well said common words
6652,00:08:49,4,Feedback in Text Retrieval- Feedback in LM,3.8,"here in this, these documents that should not belong to this top model, right?",00:08:43,10,documents belong top model right
6653,00:08:54,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So now, what we can do is to assume that, well, those words are, generally,",00:08:50,10,So assume well words generally
6654,00:08:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,"from background language model, so they will generate a,",00:08:54,10,background language model generate
6655,00:09:01,4,Feedback in Text Retrieval- Feedback in LM,3.8,"those words like the, for example.",00:08:58,10,words like example
6656,00:09:05,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And if we use maximum likelihood estimated,",00:09:02,10,And use maximum likelihood estimated
6657,00:09:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,"note that if all the words here must be generated from this model,",00:09:05,10,note words must generated model
6658,00:09:16,4,Feedback in Text Retrieval- Feedback in LM,3.8,"then this model is forced to assign high probabilities to a word like the,",00:09:11,10,model forced assign high probabilities word like
6659,00:09:19,4,Feedback in Text Retrieval- Feedback in LM,3.8,because it occurs so frequently here.,00:09:16,10,occurs frequently
6660,00:09:24,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Note that in order to reduce its probability in this model, we have to",00:09:19,10,Note order reduce probability model
6661,00:09:31,4,Feedback in Text Retrieval- Feedback in LM,3.8,"have another model, which is this one to help explain the word, the, here.",00:09:24,10,another model one help explain word
6662,00:09:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And in this case,",00:09:31,10,And case
6663,00:09:37,4,Feedback in Text Retrieval- Feedback in LM,3.8,it's not appropriate to use the background language model to achieve this goal,00:09:32,10,appropriate use background language model achieve goal
6664,00:09:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,because this model will assign high probabilities to these common words.,00:09:37,10,model assign high probabilities common words
6665,00:09:46,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So in this approach then, we assume",00:09:43,10,So approach assume
6666,00:09:50,4,Feedback in Text Retrieval- Feedback in LM,3.8,this machine that which generated these words would work as follows.,00:09:46,10,machine generated words would work follows
6667,00:09:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,We have a source controller here.,00:09:50,10,We source controller
6668,00:09:59,4,Feedback in Text Retrieval- Feedback in LM,3.8,Imagine we flip a coin here to decide what distribution to use.,00:09:53,10,Imagine flip coin decide distribution use
6669,00:10:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,With the probability of lambda the coin shows up as head.,00:09:59,10,With probability lambda coin shows head
6670,00:10:05,4,Feedback in Text Retrieval- Feedback in LM,3.8,And then we're going to use the background language model.,00:10:02,10,And going use background language model
6671,00:10:08,4,Feedback in Text Retrieval- Feedback in LM,3.8,And we can do then sample word from that model.,00:10:05,10,And sample word model
6672,00:10:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,"With probability of 1 minus lambda now,",00:10:08,10,With probability 1 minus lambda
6673,00:10:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,we now decide to use a unknown topic model here that we will try to estimate.,00:10:11,10,decide use unknown topic model try estimate
6674,00:10:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,And we're going to then generate a word here.,00:10:17,10,And going generate word
6675,00:10:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,"If we make this assumption, and this whole thing will be just one model, and",00:10:20,10,If make assumption whole thing one model
6676,00:10:27,4,Feedback in Text Retrieval- Feedback in LM,3.8,"we call this a mixture model,",00:10:25,10,call mixture model
6677,00:10:30,4,Feedback in Text Retrieval- Feedback in LM,3.8,because there are two distributions that are mixed here together.,00:10:27,10,two distributions mixed together
6678,00:10:33,4,Feedback in Text Retrieval- Feedback in LM,3.8,And we actually don't know when each distribution is used.,00:10:30,10,And actually know distribution used
6679,00:10:40,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Right, so again think of this whole thing as one model.",00:10:35,10,Right think whole thing one model
6680,00:10:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And we can still ask it for words, and",00:10:40,10,And still ask words
6681,00:10:47,4,Feedback in Text Retrieval- Feedback in LM,3.8,"it will still give us a word in a random method, right?",00:10:42,10,still give us word random method right
6682,00:10:52,4,Feedback in Text Retrieval- Feedback in LM,3.8,And of course which word will show up will depend on both this distribution and,00:10:47,10,And course word show depend distribution
6683,00:10:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,that distribution.,00:10:52,10,distribution
6684,00:10:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,"In addition, it would also depend on this lambda,",00:10:53,10,In addition would also depend lambda
6685,00:10:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,"because if you say, lambda is very high and",00:10:56,10,say lambda high
6686,00:11:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,"it's going to always use the background distribution, you'll get different words.",00:10:58,10,going always use background distribution get different words
6687,00:11:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,"If you say, well our lambda is very small, we're going to use this, all right?",00:11:02,10,If say well lambda small going use right
6688,00:11:12,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So all these are parameters, in this model.",00:11:07,10,So parameters model
6689,00:11:15,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And then, if you're thinking this way,",00:11:12,10,And thinking way
6690,00:11:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,"basically we can do exactly the same as what we did before, we're going to use",00:11:15,10,basically exactly going use
6691,00:11:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,maximum likelihood estimator to adjust this model to estimate the parameters.,00:11:20,10,maximum likelihood estimator adjust model estimate parameters
6692,00:11:30,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Basically we're going to adjust, well, this parameter so",00:11:25,10,Basically going adjust well parameter
6693,00:11:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,that we can best explain all the data.,00:11:30,10,best explain data
6694,00:11:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,The difference now is that we are not asking this model alone to explain this.,00:11:32,10,The difference asking model alone explain
6695,00:11:45,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But rather we're going to ask this whole model, mixture model,",00:11:41,10,But rather going ask whole model mixture model
6696,00:11:50,4,Feedback in Text Retrieval- Feedback in LM,3.8,to explain the data because it has got some help from the background model.,00:11:45,10,explain data got help background model
6697,00:11:54,4,Feedback in Text Retrieval- Feedback in LM,3.8,"It doesn't have to assign high probabilities towards like the,",00:11:50,10,It assign high probabilities towards like
6698,00:11:55,4,Feedback in Text Retrieval- Feedback in LM,3.8,as a result.,00:11:54,10,result
6699,00:12:01,4,Feedback in Text Retrieval- Feedback in LM,3.8,It would then assign high probabilities to other words that are common here but,00:11:55,10,It would assign high probabilities words common
6700,00:12:04,4,Feedback in Text Retrieval- Feedback in LM,3.8,not having high probability here.,00:12:01,10,high probability
6701,00:12:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,So those would be common here.,00:12:04,10,So would common
6702,00:12:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,Right?,00:12:10,10,Right
6703,00:12:15,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And if they're common they would have to have high probabilities,",00:12:11,10,And common would high probabilities
6704,00:12:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,according to a maximum likelihood estimator.,00:12:15,10,according maximum likelihood estimator
6705,00:12:23,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And if they are rare here, all right, so if they are rare here,",00:12:17,10,And rare right rare
6706,00:12:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,then you don't get much help from this background model.,00:12:23,10,get much help background model
6707,00:12:33,4,Feedback in Text Retrieval- Feedback in LM,3.8,"As a result, this topic model must assign high probabilities.",00:12:29,10,As result topic model must assign high probabilities
6708,00:12:37,4,Feedback in Text Retrieval- Feedback in LM,3.8,So the higher probability words according to the topic model,00:12:33,10,So higher probability words according topic model
6709,00:12:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,"will be those that are common here, but rare in the background.",00:12:37,10,common rare background
6710,00:12:49,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Okay, so, this is basically a little bit like a idea for weighting here.",00:12:43,10,Okay basically little bit like idea weighting
6711,00:12:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,This would allow us to achieve the effect of removing these top words,00:12:49,10,This would allow us achieve effect removing top words
6712,00:12:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,that are meaningless in the feedback.,00:12:53,10,meaningless feedback
6713,00:13:01,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So mathematically what we have is to compute the likelihood again,",00:12:56,10,So mathematically compute likelihood
6714,00:13:05,4,Feedback in Text Retrieval- Feedback in LM,3.8,local likelihood of the feedback documents.,00:13:01,10,local likelihood feedback documents
6715,00:13:08,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And, and note that, we also have another parameter, lambda here.",00:13:05,10,And note also another parameter lambda
6716,00:13:13,4,Feedback in Text Retrieval- Feedback in LM,3.8,But we assume that lambda denotes noise in the feedback document.,00:13:08,10,But assume lambda denotes noise feedback document
6717,00:13:16,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So we are going to, let's say, set this to a parameter, let's say,",00:13:13,10,So going let say set parameter let say
6718,00:13:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,"say 50% of the words are noise, or 90% are noise.",00:13:16,10,say 50 words noise 90 noise
6719,00:13:24,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And this can then be, assume it will be fixed.",00:13:21,10,And assume fixed
6720,00:13:31,4,Feedback in Text Retrieval- Feedback in LM,3.8,"If we assume this is fixed, then we only have these probabilities as parameters",00:13:24,10,If assume fixed probabilities parameters
6721,00:13:37,4,Feedback in Text Retrieval- Feedback in LM,3.8,"just like in the simplest unigram language model, we have n parameters.",00:13:31,10,like simplest unigram language model n parameters
6722,00:13:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,"n is the number of words and, then, the likelihood function will look like this.",00:13:37,10,n number words likelihood function look like
6723,00:13:46,4,Feedback in Text Retrieval- Feedback in LM,3.8,"It's very similar to the likelihood function, normal likelihood",00:13:42,10,It similar likelihood function normal likelihood
6724,00:13:52,4,Feedback in Text Retrieval- Feedback in LM,3.8,function we see before except that inside the logarithm there's a sum in here.,00:13:46,10,function see except inside logarithm sum
6725,00:13:57,4,Feedback in Text Retrieval- Feedback in LM,3.8,And this sum is because we can see the two distributions.,00:13:52,10,And sum see two distributions
6726,00:14:01,4,Feedback in Text Retrieval- Feedback in LM,3.8,And which ones used would depend on lambda and that's why we have this form.,00:13:57,10,And ones used would depend lambda form
6727,00:14:08,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But mathematically this is the function with theta as unknown variables, right?",00:14:02,10,But mathematically function theta unknown variables right
6728,00:14:10,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, this is just a function.",00:14:08,10,So function
6729,00:14:13,4,Feedback in Text Retrieval- Feedback in LM,3.8,"All the other variables are known, except for this guy.",00:14:10,10,All variables known except guy
6730,00:14:19,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, we can then choose this probability distribution to",00:14:15,10,So choose probability distribution
6731,00:14:22,4,Feedback in Text Retrieval- Feedback in LM,3.8,maximize this log likelihood.,00:14:19,10,maximize log likelihood
6732,00:14:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,The same idea as the maximum likelihood estimator.,00:14:22,10,The idea maximum likelihood estimator
6733,00:14:27,4,Feedback in Text Retrieval- Feedback in LM,3.8,"As a mathematical problem which is to,",00:14:25,10,As mathematical problem
6734,00:14:30,4,Feedback in Text Retrieval- Feedback in LM,3.8,we just have to solve this optimization problem.,00:14:27,10,solve optimization problem
6735,00:14:33,4,Feedback in Text Retrieval- Feedback in LM,3.8,"We said we would try all of the theta values, and",00:14:30,10,We said would try theta values
6736,00:14:37,4,Feedback in Text Retrieval- Feedback in LM,3.8,here we find one that gives this whole thing the maximum probability.,00:14:33,10,find one gives whole thing maximum probability
6737,00:14:40,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, it's a well-defined math problem.",00:14:37,10,So well defined math problem
6738,00:14:43,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Once we have done that, we obtain this theta F,",00:14:40,10,Once done obtain theta F
6739,00:14:47,4,Feedback in Text Retrieval- Feedback in LM,3.8,that can be the interpreter with the original query model to do feedback.,00:14:43,10,interpreter original query model feedback
6740,00:14:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,So here are some examples of the feedback model learned from a web,00:14:50,10,So examples feedback model learned web
6741,00:14:59,4,Feedback in Text Retrieval- Feedback in LM,3.8,"document collection, and we do pseudo-feedback.",00:14:56,10,document collection pseudo feedback
6742,00:15:03,4,Feedback in Text Retrieval- Feedback in LM,3.8,"We just use the top 10 documents, and we use this mixture model.",00:14:59,10,We use top 10 documents use mixture model
6743,00:15:06,4,Feedback in Text Retrieval- Feedback in LM,3.8,So the query is airport security.,00:15:03,10,So query airport security
6744,00:15:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,What we do is we first retrieve ten documents from the web database.,00:15:06,10,What first retrieve ten documents web database
6745,00:15:13,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And this is of course pseudo-feedback, right?",00:15:11,10,And course pseudo feedback right
6746,00:15:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And then we're going to feed to that mixture model, to this ten document set.",00:15:13,10,And going feed mixture model ten document set
6747,00:15:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,And these are the words learned using this approach.,00:15:20,10,And words learned using approach
6748,00:15:30,4,Feedback in Text Retrieval- Feedback in LM,3.8,This is the probability of a word given by the feedback model in both cases.,00:15:25,10,This probability word given feedback model cases
6749,00:15:35,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, in both cases, you can see the highest probability of words",00:15:31,10,So cases see highest probability words
6750,00:15:38,4,Feedback in Text Retrieval- Feedback in LM,3.8,include very random words to the query.,00:15:35,10,include random words query
6751,00:15:40,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, airport security for example,",00:15:38,10,So airport security example
6752,00:15:43,4,Feedback in Text Retrieval- Feedback in LM,3.8,these query words still show up as high probabilities,00:15:40,10,query words still show high probabilities
6753,00:15:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,in each case naturally because they occur frequently in the top rank of documents.,00:15:43,10,case naturally occur frequently top rank documents
6754,00:15:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,"But we also see beverage, alcohol, bomb, terrorist, et cetera.",00:15:48,10,But also see beverage alcohol bomb terrorist et cetera
6755,00:15:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,"Right, so these are relevant to this topic, and they,",00:15:53,10,Right relevant topic
6756,00:16:05,4,Feedback in Text Retrieval- Feedback in LM,3.8,"if combined with original query can help us match more accurately, on documents.",00:15:58,10,combined original query help us match accurately documents
6757,00:16:10,4,Feedback in Text Retrieval- Feedback in LM,3.8,"And also they can help us bring up documents that only managing the,",00:16:05,10,And also help us bring documents managing
6758,00:16:12,4,Feedback in Text Retrieval- Feedback in LM,3.8,some of these other words.,00:16:10,10,words
6759,00:16:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,And maybe for example just airport and then bomb for example.,00:16:12,10,And maybe example airport bomb example
6760,00:16:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,"These so, this is how pseudo-feedback works.",00:16:17,10,These pseudo feedback works
6761,00:16:22,4,Feedback in Text Retrieval- Feedback in LM,3.8,It shows that this model really works and,00:16:20,10,It shows model really works
6762,00:16:26,4,Feedback in Text Retrieval- Feedback in LM,3.8,"picks up mm, some related words to the query.",00:16:22,10,picks mm related words query
6763,00:16:31,4,Feedback in Text Retrieval- Feedback in LM,3.8,"What's also interesting is that if you look at the two tables here, and",00:16:26,10,What also interesting look two tables
6764,00:16:36,4,Feedback in Text Retrieval- Feedback in LM,3.8,"you compare them, and you see in this case, when lambda is set to a small value,",00:16:31,10,compare see case lambda set small value
6765,00:16:41,4,Feedback in Text Retrieval- Feedback in LM,3.8,"and we'll still see some common words here, and that means.",00:16:36,10,still see common words means
6766,00:16:45,4,Feedback in Text Retrieval- Feedback in LM,3.8,"When we don't use the background model often, remember lambda can",00:16:41,10,When use background model often remember lambda
6767,00:16:50,4,Feedback in Text Retrieval- Feedback in LM,3.8,use the probability of using the background model to generate to the text.,00:16:45,10,use probability using background model generate text
6768,00:16:53,4,Feedback in Text Retrieval- Feedback in LM,3.8,"If we don't rely much on background model,",00:16:50,10,If rely much background model
6769,00:16:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,we still have to use this topped model to account for the common words.,00:16:53,10,still use topped model account common words
6770,00:17:03,4,Feedback in Text Retrieval- Feedback in LM,3.8,Whereas if we set lambda to a very high value we would use the background,00:16:58,10,Whereas set lambda high value would use background
6771,00:17:06,4,Feedback in Text Retrieval- Feedback in LM,3.8,"model very often to explain these words, then there is no burden on",00:17:03,10,model often explain words burden
6772,00:17:11,4,Feedback in Text Retrieval- Feedback in LM,3.8,expanding those common words in the feedback documents by the topping model.,00:17:06,10,expanding common words feedback documents topping model
6773,00:17:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, as a result, the top of the model here is very discriminative.",00:17:11,10,So result top model discriminative
6774,00:17:20,4,Feedback in Text Retrieval- Feedback in LM,3.8,It contains all the relevant words without common words.,00:17:17,10,It contains relevant words without common words
6775,00:17:26,4,Feedback in Text Retrieval- Feedback in LM,3.8,So this can be added to the original query to achieve feedback.,00:17:21,10,So added original query achieve feedback
6776,00:17:32,4,Feedback in Text Retrieval- Feedback in LM,3.8,So to summarize in this lecture we have talked about the feedback in,00:17:28,10,So summarize lecture talked feedback
6777,00:17:34,4,Feedback in Text Retrieval- Feedback in LM,3.8,language model approach.,00:17:32,10,language model approach
6778,00:17:38,4,Feedback in Text Retrieval- Feedback in LM,3.8,"In general, feedback is to learn from examples.",00:17:34,10,In general feedback learn examples
6779,00:17:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,"These examples can be assumed examples, can be pseudo-examples,",00:17:38,10,These examples assumed examples pseudo examples
6780,00:17:48,4,Feedback in Text Retrieval- Feedback in LM,3.8,"like assume the, the top ten documents are assumed to be random.",00:17:42,10,like assume top ten documents assumed random
6781,00:17:52,4,Feedback in Text Retrieval- Feedback in LM,3.8,"They could be based on using fractions like feedback,",00:17:48,10,They could based using fractions like feedback
6782,00:17:55,4,Feedback in Text Retrieval- Feedback in LM,3.8,based on quick sorts or implicit feedback.,00:17:52,10,based quick sorts implicit feedback
6783,00:17:58,4,Feedback in Text Retrieval- Feedback in LM,3.8,"We talked about the three major feedback scenarios, relevance feedback,",00:17:55,10,We talked three major feedback scenarios relevance feedback
6784,00:18:02,4,Feedback in Text Retrieval- Feedback in LM,3.8,"pseudo-feedback, and implicit feedback.",00:17:58,10,pseudo feedback implicit feedback
6785,00:18:07,4,Feedback in Text Retrieval- Feedback in LM,3.8,We talked about how to use Rocchio to do feedback in vector-space model and,00:18:02,10,We talked use Rocchio feedback vector space model
6786,00:18:14,4,Feedback in Text Retrieval- Feedback in LM,3.8,how to use query model estimation for feedback in language model.,00:18:07,10,use query model estimation feedback language model
6787,00:18:17,4,Feedback in Text Retrieval- Feedback in LM,3.8,And we briefly talked about the mixture model and,00:18:14,10,And briefly talked mixture model
6788,00:18:21,4,Feedback in Text Retrieval- Feedback in LM,3.8,the basic idea and there are many other methods.,00:18:17,10,basic idea many methods
6789,00:18:25,4,Feedback in Text Retrieval- Feedback in LM,3.8,For example the relevance model is a very effective model for,00:18:21,10,For example relevance model effective model
6790,00:18:26,4,Feedback in Text Retrieval- Feedback in LM,3.8,estimating query model.,00:18:25,10,estimating query model
6791,00:18:29,4,Feedback in Text Retrieval- Feedback in LM,3.8,"So, you can read more about the,",00:18:26,10,So read
6792,00:18:34,4,Feedback in Text Retrieval- Feedback in LM,3.8,these methods in the references that are listed at the end of this lecture.,00:18:29,10,methods references listed end lecture
6793,00:18:38,4,Feedback in Text Retrieval- Feedback in LM,3.8,So there are two additional readings here.,00:18:36,10,So two additional readings
6794,00:18:42,4,Feedback in Text Retrieval- Feedback in LM,3.8,"The first one is a book that has a systematic, review and",00:18:38,10,The first one book systematic review
6795,00:18:45,4,Feedback in Text Retrieval- Feedback in LM,3.8,discussion of language models of more information retrieval.,00:18:42,10,discussion language models information retrieval
6796,00:18:51,4,Feedback in Text Retrieval- Feedback in LM,3.8,And the second one is an important research paper that's about relevance,00:18:46,10,And second one important research paper relevance
6797,00:18:56,4,Feedback in Text Retrieval- Feedback in LM,3.8,based language models and it's a very effective way of computing query model.,00:18:51,10,based language models effective way computing query model
6798,00:00:03,3,System Implementation- Inverted Index Construction,2.2,[SOUND].,00:00:00,2,SOUND
6799,00:00:11,3,System Implementation- Inverted Index Construction,2.2,This lecture is about the Inverted Index Construction.,00:00:07,2,This lecture Inverted Index Construction
6800,00:00:18,3,System Implementation- Inverted Index Construction,2.2,"In this lecture, we will continue the discussion of system implementation.",00:00:13,2,In lecture continue discussion system implementation
6801,00:00:21,3,System Implementation- Inverted Index Construction,2.2,"In particular, we're going to discuss how to construct the inverted index.",00:00:18,2,In particular going discuss construct inverted index
6802,00:00:29,3,System Implementation- Inverted Index Construction,2.2,The construction of the inverted index is actually very easy if the data set is,00:00:25,2,The construction inverted index actually easy data set
6803,00:00:30,3,System Implementation- Inverted Index Construction,2.2,very small.,00:00:29,2,small
6804,00:00:35,3,System Implementation- Inverted Index Construction,2.2,It's very easy to construct a dictionary and then store the postings in a file.,00:00:30,2,It easy construct dictionary store postings file
6805,00:00:41,3,System Implementation- Inverted Index Construction,2.2,"The problem's that when our data is not able to fit to the memory,",00:00:36,2,The problem data able fit memory
6806,00:00:46,3,System Implementation- Inverted Index Construction,2.2,then we have to use some special method to deal with it.,00:00:41,2,use special method deal
6807,00:00:52,3,System Implementation- Inverted Index Construction,2.2,"And unfortunately, in most retrieval a petitions, the data set would be large and",00:00:46,2,And unfortunately retrieval petitions data set would large
6808,00:00:56,3,System Implementation- Inverted Index Construction,2.2,"they generally cannot be, loaded into the memory at once.",00:00:52,2,generally cannot loaded memory
6809,00:01:00,3,System Implementation- Inverted Index Construction,2.2,"And there are many approaches to solving that problem, and",00:00:56,2,And many approaches solving problem
6810,00:01:06,3,System Implementation- Inverted Index Construction,2.2,"sorting-based method, is quite common and works in four steps as shown here.",00:01:00,2,sorting based method quite common works four steps shown
6811,00:01:11,3,System Implementation- Inverted Index Construction,2.2,"First, we collect the the local termID, document ID, and frequency tuples.",00:01:06,2,First collect local termID document ID frequency tuples
6812,00:01:17,3,System Implementation- Inverted Index Construction,2.2,"Basically, you overlook kinds of terms in a small set of documents, and, and",00:01:11,2,Basically overlook kinds terms small set documents
6813,00:01:24,3,System Implementation- Inverted Index Construction,2.2,"then, once you collect those counts, you can sort those counts based on terms so",00:01:17,2,collect counts sort counts based terms
6814,00:01:28,3,System Implementation- Inverted Index Construction,2.2,"that you build a local, a partial inverted index.",00:01:24,2,build local partial inverted index
6815,00:01:31,3,System Implementation- Inverted Index Construction,2.2,"And these are called, runs.",00:01:28,2,And called runs
6816,00:01:36,3,System Implementation- Inverted Index Construction,2.2,"And then, you write them into a temporary file on the disk.",00:01:31,2,And write temporary file disk
6817,00:01:41,3,System Implementation- Inverted Index Construction,2.2,"And then, you merge in step three with do pair-wise merging of these runs, and here,",00:01:36,2,And merge step three pair wise merging runs
6818,00:01:45,3,System Implementation- Inverted Index Construction,2.2,"you eventually merge all the runs, we generate a single inverted index.",00:01:41,2,eventually merge runs generate single inverted index
6819,00:01:51,3,System Implementation- Inverted Index Construction,2.2,So this is an illustration of this method.,00:01:47,2,So illustration method
6820,00:01:52,3,System Implementation- Inverted Index Construction,2.2,"On the left, you see some documents.",00:01:51,2,On left see documents
6821,00:01:59,3,System Implementation- Inverted Index Construction,2.2,"And on the right, we have, show a term lexicon and a document ID lexicon.",00:01:53,2,And right show term lexicon document ID lexicon
6822,00:02:05,3,System Implementation- Inverted Index Construction,2.2,And these lexicon's are to map a stream based representations of document IDs or,00:01:59,2,And lexicon map stream based representations document IDs
6823,00:02:09,3,System Implementation- Inverted Index Construction,2.2,terms into integer representations.,00:02:05,2,terms integer representations
6824,00:02:16,3,System Implementation- Inverted Index Construction,2.2,"Or, and, map back from, integers to the screen representation.",00:02:09,2,Or map back integers screen representation
6825,00:02:23,3,System Implementation- Inverted Index Construction,2.2,"And the reason why we want, are interested in using integers represent these IDs,",00:02:17,2,And reason want interested using integers represent IDs
6826,00:02:26,3,System Implementation- Inverted Index Construction,2.2,"is because, integers are often easier to handle.",00:02:23,2,integers often easier handle
6827,00:02:29,3,System Implementation- Inverted Index Construction,2.2,"For example, integers can be used as index for",00:02:26,2,For example integers used index
6828,00:02:33,3,System Implementation- Inverted Index Construction,2.2,array and they are also easy to compress.,00:02:29,2,array also easy compress
6829,00:02:39,3,System Implementation- Inverted Index Construction,2.2,"So this is a, one reason why we, tend to map these streams",00:02:34,2,So one reason tend map streams
6830,00:02:46,3,System Implementation- Inverted Index Construction,2.2,"into integers so that so that we don't have to, carry these streams around.",00:02:39,2,integers carry streams around
6831,00:02:48,3,System Implementation- Inverted Index Construction,2.2,So how does this approach work?,00:02:46,2,So approach work
6832,00:02:49,3,System Implementation- Inverted Index Construction,2.2,"Well, it's very simple.",00:02:48,2,Well simple
6833,00:02:53,3,System Implementation- Inverted Index Construction,2.2,"We're going to scan these documents sequentially, and",00:02:49,2,We going scan documents sequentially
6834,00:02:58,3,System Implementation- Inverted Index Construction,2.2,then pause the documents and a count the frequencies of terms.,00:02:53,2,pause documents count frequencies terms
6835,00:03:02,3,System Implementation- Inverted Index Construction,2.2,"And in this, stage we generally sort",00:02:58,2,And stage generally sort
6836,00:03:07,3,System Implementation- Inverted Index Construction,2.2,the frequencies by document IDs because we process each document that sequentially.,00:03:02,2,frequencies document IDs process document sequentially
6837,00:03:11,3,System Implementation- Inverted Index Construction,2.2,"So, we first encounter all the terms in, the first document.",00:03:07,2,So first encounter terms first document
6838,00:03:16,3,System Implementation- Inverted Index Construction,2.2,"Therefore, the document IDs, are all once in this stage.",00:03:11,2,Therefore document IDs stage
6839,00:03:21,3,System Implementation- Inverted Index Construction,2.2,"And so, and, this would be",00:03:16,2,And would
6840,00:03:25,3,System Implementation- Inverted Index Construction,2.2,followed by document IDs 2.,00:03:21,2,followed document IDs 2
6841,00:03:29,3,System Implementation- Inverted Index Construction,2.2,"And, and they're naturally sort in this order just because we process the data in",00:03:25,2,And naturally sort order process data
6842,00:03:31,3,System Implementation- Inverted Index Construction,2.2,this order.,00:03:29,2,order
6843,00:03:34,3,System Implementation- Inverted Index Construction,2.2,"At some point, the, we will run out of memory and",00:03:31,2,At point run memory
6844,00:03:38,3,System Implementation- Inverted Index Construction,2.2,"that would have to, to write them into the disk.",00:03:34,2,would write disk
6845,00:03:44,3,System Implementation- Inverted Index Construction,2.2,"But before we do that, we're going to a sort them, just,",00:03:38,2,But going sort
6846,00:03:48,3,System Implementation- Inverted Index Construction,2.2,"use whatever memory we have, we can sort them, and",00:03:44,2,use whatever memory sort
6847,00:03:51,3,System Implementation- Inverted Index Construction,2.2,"then, this time, we're going to sort based on term IDs.",00:03:48,2,time going sort based term IDs
6848,00:03:59,3,System Implementation- Inverted Index Construction,2.2,"Note that here, we're using, this, the term IDs as a key to sort.",00:03:51,2,Note using term IDs key sort
6849,00:04:03,3,System Implementation- Inverted Index Construction,2.2,"So, all the entries that share the same term would be grouped together.",00:03:59,2,So entries share term would grouped together
6850,00:04:09,3,System Implementation- Inverted Index Construction,2.2,"In this case, we can see all the, all the IDs",00:04:03,2,In case see IDs
6851,00:04:14,3,System Implementation- Inverted Index Construction,2.2,of documents that match term one would be grouped together.,00:04:09,2,documents match term one would grouped together
6852,00:04:18,3,System Implementation- Inverted Index Construction,2.2,And we're going to write this into the disk as a temporary file.,00:04:14,2,And going write disk temporary file
6853,00:04:24,3,System Implementation- Inverted Index Construction,2.2,"And that would, allow us to use the memory to process the next batch of documents,",00:04:18,2,And would allow us use memory process next batch documents
6854,00:04:26,3,System Implementation- Inverted Index Construction,2.2,and we're going to do that for all the documents.,00:04:24,2,going documents
6855,00:04:31,3,System Implementation- Inverted Index Construction,2.2,So we're going to write a lot of temporary files into the disk.,00:04:26,2,So going write lot temporary files disk
6856,00:04:35,3,System Implementation- Inverted Index Construction,2.2,"And then, the next stage is to do merge sort.",00:04:32,2,And next stage merge sort
6857,00:04:38,3,System Implementation- Inverted Index Construction,2.2,"Basically, we're going to, merge them and the sort them.",00:04:35,2,Basically going merge sort
6858,00:04:42,3,System Implementation- Inverted Index Construction,2.2,"Eventually, we will get a single inverted index where the,",00:04:38,2,Eventually get single inverted index
6859,00:04:45,3,System Implementation- Inverted Index Construction,2.2,their entries are sorted based on term IDs.,00:04:42,2,entries sorted based term IDs
6860,00:04:50,3,System Implementation- Inverted Index Construction,2.2,"And on the top, we can see these are the order entries for",00:04:46,2,And top see order entries
6861,00:04:53,3,System Implementation- Inverted Index Construction,2.2,the documents that match term ID 1.,00:04:50,2,documents match term ID 1
6862,00:04:59,3,System Implementation- Inverted Index Construction,2.2,"So this is basically how we can do, the construction of inverted index,",00:04:53,2,So basically construction inverted index
6863,00:05:05,3,System Implementation- Inverted Index Construction,2.2,"even though that they're or cannot be, or loaded into the memory.",00:04:59,2,even though cannot loaded memory
6864,00:05:10,3,System Implementation- Inverted Index Construction,2.2,"Now, we mentioned earlier that",00:05:05,2,Now mentioned earlier
6865,00:05:16,3,System Implementation- Inverted Index Construction,2.2,"because the po, postings are very large, it's desirable to compress them.",00:05:11,2,po postings large desirable compress
6866,00:05:20,3,System Implementation- Inverted Index Construction,2.2,So let's now talk a little bit about how we compress inverted index.,00:05:16,2,So let talk little bit compress inverted index
6867,00:05:24,3,System Implementation- Inverted Index Construction,2.2,"Well, the idea of compression, in general,",00:05:21,2,Well idea compression general
6868,00:05:28,3,System Implementation- Inverted Index Construction,2.2,is you leverage skewed distributions of values.,00:05:24,2,leverage skewed distributions values
6869,00:05:32,3,System Implementation- Inverted Index Construction,2.2,And we generally have to use variable lengths in coding instead of the fixed,00:05:28,2,And generally use variable lengths coding instead fixed
6870,00:05:38,3,System Implementation- Inverted Index Construction,2.2,"lengths in coding as we', using, by defaulting a program language like C++.",00:05:32,2,lengths coding using defaulting program language like C
6871,00:05:46,3,System Implementation- Inverted Index Construction,2.2,"And so, how can we leverage the skewed distributions of values to,",00:05:41,2,And leverage skewed distributions values
6872,00:05:48,3,System Implementation- Inverted Index Construction,2.2,compress these values?,00:05:46,2,compress values
6873,00:05:54,3,System Implementation- Inverted Index Construction,2.2,"Well, in general, we would use fewer bits to encode those frequent words",00:05:48,2,Well general would use fewer bits encode frequent words
6874,00:06:00,3,System Implementation- Inverted Index Construction,2.2,"at a cost of using, longer bits from the code than those, rare values.",00:05:54,2,cost using longer bits code rare values
6875,00:06:04,3,System Implementation- Inverted Index Construction,2.2,"So in our case, let's think about how we can compress the tf, term frequency.",00:06:00,2,So case let think compress tf term frequency
6876,00:06:09,3,System Implementation- Inverted Index Construction,2.2,If you can picture what the inverted index would look like and,00:06:05,2,If picture inverted index would look like
6877,00:06:13,3,System Implementation- Inverted Index Construction,2.2,"you'll see in postings there are a lot of, term frequencies.",00:06:09,2,see postings lot term frequencies
6878,00:06:18,3,System Implementation- Inverted Index Construction,2.2,"Those are the frequencies of terms, in all those documents.",00:06:13,2,Those frequencies terms documents
6879,00:06:25,3,System Implementation- Inverted Index Construction,2.2,"Now, we, if you think about it, what kind of values are most frequent there?",00:06:18,2,Now think kind values frequent
6880,00:06:29,3,System Implementation- Inverted Index Construction,2.2,"You probably will, be able to guess that the small numbers tend to occur",00:06:25,2,You probably able guess small numbers tend occur
6881,00:06:32,3,System Implementation- Inverted Index Construction,2.2,far more frequently than large numbers.,00:06:29,2,far frequently large numbers
6882,00:06:33,3,System Implementation- Inverted Index Construction,2.2,Why?,00:06:32,2,Why
6883,00:06:38,3,System Implementation- Inverted Index Construction,2.2,"Well, think of about the distribution of words, and",00:06:33,2,Well think distribution words
6884,00:06:42,3,System Implementation- Inverted Index Construction,2.2,"this is due to Zipf's law and many words occur just, rarely.",00:06:38,2,due Zipf law many words occur rarely
6885,00:06:47,3,System Implementation- Inverted Index Construction,2.2,"So we see a lot of small numbers, therefore, we can use fewer bits for",00:06:42,2,So see lot small numbers therefore use fewer bits
6886,00:06:51,3,System Implementation- Inverted Index Construction,2.2,"the small, but highly frequent integers,",00:06:47,2,small highly frequent integers
6887,00:06:57,3,System Implementation- Inverted Index Construction,2.2,and at the cost of using more bits for large integers.,00:06:53,2,cost using bits large integers
6888,00:06:59,3,System Implementation- Inverted Index Construction,2.2,"This is a trade-off, of course.",00:06:58,2,This trade course
6889,00:07:05,3,System Implementation- Inverted Index Construction,2.2,"If the values are distributed uniformly and this won't save us any, spacing.",00:06:59,2,If values distributed uniformly save us spacing
6890,00:07:10,3,System Implementation- Inverted Index Construction,2.2,"But because we tend to see many small values, they're very frequent.",00:07:05,2,But tend see many small values frequent
6891,00:07:15,3,System Implementation- Inverted Index Construction,2.2,"We can save on average even though sometimes,",00:07:10,2,We save average even though sometimes
6892,00:07:17,3,System Implementation- Inverted Index Construction,2.2,when we see a large number we have to use a lot of bits.,00:07:15,2,see large number use lot bits
6893,00:07:23,3,System Implementation- Inverted Index Construction,2.2,What about the document IDs that we also saw in postings.,00:07:19,2,What document IDs also saw postings
6894,00:07:27,3,System Implementation- Inverted Index Construction,2.2,"Well, they are not, distributed in a skewed way, right?",00:07:23,2,Well distributed skewed way right
6895,00:07:31,3,System Implementation- Inverted Index Construction,2.2,"So, how can we deal with that?",00:07:27,2,So deal
6896,00:07:34,3,System Implementation- Inverted Index Construction,2.2,"Well, it turns out you can use a trick called the d-gap,",00:07:31,2,Well turns use trick called gap
6897,00:07:38,3,System Implementation- Inverted Index Construction,2.2,"and that, that is to store the difference of these term IDs.",00:07:34,2,store difference term IDs
6898,00:07:44,3,System Implementation- Inverted Index Construction,2.2,"And we can, imagine if a term has matched many documents,",00:07:38,2,And imagine term matched many documents
6899,00:07:46,3,System Implementation- Inverted Index Construction,2.2,then there will be a long list of document IDs.,00:07:44,2,long list document IDs
6900,00:07:52,3,System Implementation- Inverted Index Construction,2.2,"So when we take the gap, and when we take difference between adjacent document IDs,",00:07:46,2,So take gap take difference adjacent document IDs
6901,00:07:54,3,System Implementation- Inverted Index Construction,2.2,those gaps will be small.,00:07:52,2,gaps small
6902,00:07:58,3,System Implementation- Inverted Index Construction,2.2,"So we'll again see a lot of small numbers, whereas,",00:07:54,2,So see lot small numbers whereas
6903,00:08:02,3,System Implementation- Inverted Index Construction,2.2,"if a term occurred in only a few documents, then the gap would be large.",00:07:58,2,term occurred documents gap would large
6904,00:08:06,3,System Implementation- Inverted Index Construction,2.2,"The larger numbers will not be frequent, so this creates some skewed distribution",00:08:02,2,The larger numbers frequent creates skewed distribution
6905,00:08:10,3,System Implementation- Inverted Index Construction,2.2,"that would allow us to, to compress these values.",00:08:06,2,would allow us compress values
6906,00:08:18,3,System Implementation- Inverted Index Construction,2.2,"This is also possible because in order to uncover or uncompress these document IDs,",00:08:11,2,This also possible order uncover uncompress document IDs
6907,00:08:23,3,System Implementation- Inverted Index Construction,2.2,we have to sequentially process the data because we stored the difference.,00:08:18,2,sequentially process data stored difference
6908,00:08:26,3,System Implementation- Inverted Index Construction,2.2,"And in order to recover the, the exact document ID,",00:08:23,2,And order recover exact document ID
6909,00:08:30,3,System Implementation- Inverted Index Construction,2.2,"we have to first recover the previous document ID, and then, we can add",00:08:26,2,first recover previous document ID add
6910,00:08:35,3,System Implementation- Inverted Index Construction,2.2,"the difference to the previous document ID to restore the, the current document ID.",00:08:30,2,difference previous document ID restore current document ID
6911,00:08:40,3,System Implementation- Inverted Index Construction,2.2,"Now, this was possible because we only needed to have sequential",00:08:35,2,Now possible needed sequential
6912,00:08:42,3,System Implementation- Inverted Index Construction,2.2,access to those document IDs.,00:08:40,2,access document IDs
6913,00:08:46,3,System Implementation- Inverted Index Construction,2.2,"Once we look up a term we fetch all the document IDs that match the term,",00:08:42,2,Once look term fetch document IDs match term
6914,00:08:48,3,System Implementation- Inverted Index Construction,2.2,then we sequentially process them.,00:08:46,2,sequentially process
6915,00:08:52,3,System Implementation- Inverted Index Construction,2.2,"So it's very natural that's why this, trick actually works.",00:08:48,2,So natural trick actually works
6916,00:08:55,3,System Implementation- Inverted Index Construction,2.2,And there are many different methods for encoding.,00:08:53,2,And many different methods encoding
6917,00:09:02,3,System Implementation- Inverted Index Construction,2.2,"So binary code is a common used code in, in just any program.",00:08:55,2,So binary code common used code program
6918,00:09:05,3,System Implementation- Inverted Index Construction,2.2,Language that we use basically a fixed length in coding.,00:09:02,2,Language use basically fixed length coding
6919,00:09:08,3,System Implementation- Inverted Index Construction,2.2,"Unary code and gamma code, and delta code are all possible in this and",00:09:05,2,Unary code gamma code delta code possible
6920,00:09:11,3,System Implementation- Inverted Index Construction,2.2,there are many other possible in this.,00:09:08,2,many possible
6921,00:09:14,3,System Implementation- Inverted Index Construction,2.2,So let's look at some of them in more detail.,00:09:11,2,So let look detail
6922,00:09:16,3,System Implementation- Inverted Index Construction,2.2,Binary code is really equal-length in coding.,00:09:14,2,Binary code really equal length coding
6923,00:09:20,3,System Implementation- Inverted Index Construction,2.2,And that's a property for the randomly distributed values.,00:09:16,2,And property randomly distributed values
6924,00:09:24,3,System Implementation- Inverted Index Construction,2.2,The unary coding is is a variable and it's important [INAUDIBLE].,00:09:20,2,The unary coding variable important INAUDIBLE
6925,00:09:29,3,System Implementation- Inverted Index Construction,2.2,"In this case, integer that is, I've missed one or",00:09:24,2,In case integer I missed one
6926,00:09:33,3,System Implementation- Inverted Index Construction,2.2,"we encode that as x minus 1, 1 bit followed by 0.",00:09:29,2,encode x minus 1 1 bit followed 0
6927,00:09:39,3,System Implementation- Inverted Index Construction,2.2,"So for example, 3 would be encoded as two 1s followed by a 0,",00:09:33,2,So example 3 would encoded two 1s followed 0
6928,00:09:45,3,System Implementation- Inverted Index Construction,2.2,"whereas 5 would be encoded as four 1s followed by 0, et cetera.",00:09:39,2,whereas 5 would encoded four 1s followed 0 et cetera
6929,00:09:49,3,System Implementation- Inverted Index Construction,2.2,"So now, now you can imagine how many bits do we have to use for",00:09:45,2,So imagine many bits use
6930,00:09:52,3,System Implementation- Inverted Index Construction,2.2,a large number like 100.,00:09:49,2,large number like 100
6931,00:09:57,3,System Implementation- Inverted Index Construction,2.2,"So, how many bits do I have to use for exactly for a number like 100?",00:09:52,2,So many bits I use exactly number like 100
6932,00:10:02,3,System Implementation- Inverted Index Construction,2.2,"Well, exactly, we have to use 100 bits,",00:09:57,2,Well exactly use 100 bits
6933,00:10:07,3,System Implementation- Inverted Index Construction,2.2,"but so, it's the same number of bits as the value of this number.",00:10:02,2,number bits value number
6934,00:10:08,3,System Implementation- Inverted Index Construction,2.2,"So, this is very inefficient.",00:10:07,2,So inefficient
6935,00:10:12,3,System Implementation- Inverted Index Construction,2.2,"If you were likely to see some large numbers,",00:10:08,2,If likely see large numbers
6936,00:10:17,3,System Implementation- Inverted Index Construction,2.2,"imagine if you occasionally see a number like 1000, you have to use 1000 bits.",00:10:12,2,imagine occasionally see number like 1000 use 1000 bits
6937,00:10:21,3,System Implementation- Inverted Index Construction,2.2,"So, this only works where if you are absolutely sure that there would be no",00:10:17,2,So works absolutely sure would
6938,00:10:23,3,System Implementation- Inverted Index Construction,2.2,large numbers.,00:10:21,2,large numbers
6939,00:10:28,3,System Implementation- Inverted Index Construction,2.2,"Mostly very frequent, they're often using very small numbers.",00:10:23,2,Mostly frequent often using small numbers
6940,00:10:30,3,System Implementation- Inverted Index Construction,2.2,"Now, how do you decode this code?",00:10:28,2,Now decode code
6941,00:10:34,3,System Implementation- Inverted Index Construction,2.2,"Since these are variables lengths in coding methods, and",00:10:30,2,Since variables lengths coding methods
6942,00:10:37,3,System Implementation- Inverted Index Construction,2.2,you can't just count how many bits and then just stop.,00:10:34,2,count many bits stop
6943,00:10:38,3,System Implementation- Inverted Index Construction,2.2,Right?,00:10:37,2,Right
6944,00:10:43,3,System Implementation- Inverted Index Construction,2.2,"You can say eight bits or 32 bits, then you, you will start another code.",00:10:38,2,You say eight bits 32 bits start another code
6945,00:10:50,3,System Implementation- Inverted Index Construction,2.2,"There are variable lengths, so, you have to rely on some mechanism.",00:10:43,2,There variable lengths rely mechanism
6946,00:10:55,3,System Implementation- Inverted Index Construction,2.2,"In this case for unary, you can see it's very easy to see the boundary.",00:10:50,2,In case unary see easy see boundary
6947,00:10:59,3,System Implementation- Inverted Index Construction,2.2,Now you can easily see 0 would signal the end of encoding.,00:10:55,2,Now easily see 0 would signal end encoding
6948,00:11:03,3,System Implementation- Inverted Index Construction,2.2,"So you just count how many 1s you have seen, and then you hit the 0.",00:10:59,2,So count many 1s seen hit 0
6949,00:11:07,3,System Implementation- Inverted Index Construction,2.2,"You know you have finished one number, you start another number.",00:11:03,2,You know finished one number start another number
6950,00:11:15,3,System Implementation- Inverted Index Construction,2.2,Now which is to start at unary code is to aggressive in rewarding small numbers.,00:11:07,2,Now start unary code aggressive rewarding small numbers
6951,00:11:20,3,System Implementation- Inverted Index Construction,2.2,"And if you occasionally can see a very big number, it will be a disaster.",00:11:15,2,And occasionally see big number disaster
6952,00:11:24,3,System Implementation- Inverted Index Construction,2.2,So what about some other less aggressive method?,00:11:20,2,So less aggressive method
6953,00:11:27,3,System Implementation- Inverted Index Construction,2.2,"Well, gamma coding is one of them.",00:11:24,2,Well gamma coding one
6954,00:11:32,3,System Implementation- Inverted Index Construction,2.2,"And in this method, we can do, use unary coding for",00:11:27,2,And method use unary coding
6955,00:11:36,3,System Implementation- Inverted Index Construction,2.2,a transformed form of the value.,00:11:32,2,transformed form value
6956,00:11:41,3,System Implementation- Inverted Index Construction,2.2,So it's 1 plus the flow of log of x.,00:11:36,2,So 1 plus flow log x
6957,00:11:47,3,System Implementation- Inverted Index Construction,2.2,"So the magnitude of this value is much lower than the original, x.",00:11:41,2,So magnitude value much lower original x
6958,00:11:53,3,System Implementation- Inverted Index Construction,2.2,"So that's why we have four using urinary code for that so,",00:11:47,2,So four using urinary code
6959,00:11:58,3,System Implementation- Inverted Index Construction,2.2,"and so we, first we have the urinary code for coding this log of s.",00:11:54,2,first urinary code coding log
6960,00:12:02,3,System Implementation- Inverted Index Construction,2.2,"And this will be followed by a uniform code or binary code, and",00:11:58,2,And followed uniform code binary code
6961,00:12:08,3,System Implementation- Inverted Index Construction,2.2,this is basically the same uniform code and binary code are the same.,00:12:02,2,basically uniform code binary code
6962,00:12:16,3,System Implementation- Inverted Index Construction,2.2,And we're going to use this code to code the remaining part of the value of x.,00:12:08,2,And going use code code remaining part value x
6963,00:12:21,3,System Implementation- Inverted Index Construction,2.2,"And this is basically, precisely, x minus 1, 2 to the flow of log of x.",00:12:16,2,And basically precisely x minus 1 2 flow log x
6964,00:12:30,3,System Implementation- Inverted Index Construction,2.2,"So the unary code or basically code with a flow of log of x, well,",00:12:25,2,So unary code basically code flow log x well
6965,00:12:32,3,System Implementation- Inverted Index Construction,2.2,"I added one there, and here.",00:12:30,2,I added one
6966,00:12:39,3,System Implementation- Inverted Index Construction,2.2,"But the remaining part will, we using uniform",00:12:34,2,But remaining part using uniform
6967,00:12:44,3,System Implementation- Inverted Index Construction,2.2,code to actually code the difference between the x and,00:12:39,2,code actually code difference x
6968,00:12:48,3,System Implementation- Inverted Index Construction,2.2,"and this, 2 to the log of x.",00:12:44,2,2 log x
6969,00:12:56,3,System Implementation- Inverted Index Construction,2.2,"And, and it's easy to to show that for this this value, there's difference.",00:12:49,2,And easy show value difference
6970,00:12:59,3,System Implementation- Inverted Index Construction,2.2,"We only need to use up to,",00:12:56,2,We need use
6971,00:13:05,3,System Implementation- Inverted Index Construction,2.2,this many bits and in flow of log of x bits.,00:13:01,2,many bits flow log x bits
6972,00:13:08,3,System Implementation- Inverted Index Construction,2.2,"And this is easy to understand,",00:13:06,2,And easy understand
6973,00:13:12,3,System Implementation- Inverted Index Construction,2.2,if the difference is too large then we would have a higher flow of log of x.,00:13:08,2,difference large would higher flow log x
6974,00:13:15,3,System Implementation- Inverted Index Construction,2.2,"So, here are some examples.",00:13:14,2,So examples
6975,00:13:19,3,System Implementation- Inverted Index Construction,2.2,"For example, 3 is encoded as 101.",00:13:15,2,For example 3 encoded 101
6976,00:13:21,3,System Implementation- Inverted Index Construction,2.2,The first two digits are the unary code.,00:13:19,2,The first two digits unary code
6977,00:13:22,3,System Implementation- Inverted Index Construction,2.2,Right.,00:13:21,2,Right
6978,00:13:26,3,System Implementation- Inverted Index Construction,2.2,"So, this is for the value 2.",00:13:22,2,So value 2
6979,00:13:26,3,System Implementation- Inverted Index Construction,2.2,Right.,00:13:26,2,Right
6980,00:13:31,3,System Implementation- Inverted Index Construction,2.2,10 encodes 2 in unary coding.,00:13:26,2,10 encodes 2 unary coding
6981,00:13:37,3,System Implementation- Inverted Index Construction,2.2,"And so, that means log of x, the flow of log of x is 1,",00:13:32,2,And means log x flow log x 1
6982,00:13:45,3,System Implementation- Inverted Index Construction,2.2,because we will actually use unary code to encode 1 plus the flow of log of x.,00:13:37,2,actually use unary code encode 1 plus flow log x
6983,00:13:50,3,System Implementation- Inverted Index Construction,2.2,"Since this is 2, then we know that the floor of log of x is actually 1.",00:13:45,2,Since 2 know floor log x actually 1
6984,00:13:55,3,System Implementation- Inverted Index Construction,2.2,"So but, 3 is still larger than 2 to the 1, so",00:13:52,2,So 3 still larger 2 1
6985,00:14:00,3,System Implementation- Inverted Index Construction,2.2,"the difference is 1, and that 1 is encoded here at the end.",00:13:55,2,difference 1 1 encoded end
6986,00:14:03,3,System Implementation- Inverted Index Construction,2.2,So that's why we have 101 for 3.,00:14:01,2,So 101 3
6987,00:14:11,3,System Implementation- Inverted Index Construction,2.2,"Now, similarly 5 is encoded as 110 followed by 01.",00:14:05,2,Now similarly 5 encoded 110 followed 01
6988,00:14:18,3,System Implementation- Inverted Index Construction,2.2,"And in this case, the unary code encodes 3.",00:14:12,2,And case unary code encodes 3
6989,00:14:26,3,System Implementation- Inverted Index Construction,2.2,"So, this is the unary code for 110 and so the floor of log of x is 2.",00:14:18,2,So unary code 110 floor log x 2
6990,00:14:30,3,System Implementation- Inverted Index Construction,2.2,"And that means, we will compute the difference between 5 and",00:14:26,2,And means compute difference 5
6991,00:14:35,3,System Implementation- Inverted Index Construction,2.2,"the 2 to the 2, and that's 1, and so we now have again 1 at the end.",00:14:30,2,2 2 1 1 end
6992,00:14:40,3,System Implementation- Inverted Index Construction,2.2,"But this time, we're going to use two bits because with this",00:14:35,2,But time going use two bits
6993,00:14:47,3,System Implementation- Inverted Index Construction,2.2,"level of flow of log of x, we could have more numbers, 5, 6, 7.",00:14:40,2,level flow log x could numbers 5 6 7
6994,00:14:51,3,System Implementation- Inverted Index Construction,2.2,"They would all share the same prefix here, 110.",00:14:47,2,They would share prefix 110
6995,00:14:53,3,System Implementation- Inverted Index Construction,2.2,"So, in order to differentiate them,",00:14:51,2,So order differentiate
6996,00:14:57,3,System Implementation- Inverted Index Construction,2.2,"we have to use two bits, in the end to differentiate them.",00:14:53,2,use two bits end differentiate
6997,00:15:03,3,System Implementation- Inverted Index Construction,2.2,"So you can imagine 6 would be, 10 here in the end instead of 01, after 110.",00:14:57,2,So imagine 6 would 10 end instead 01 110
6998,00:15:08,3,System Implementation- Inverted Index Construction,2.2,"It's also true that the form of a gamma code is always,",00:15:03,2,It also true form gamma code always
6999,00:15:15,3,System Implementation- Inverted Index Construction,2.2,"the first odd number of bits, and in the center, there was a 0.",00:15:10,2,first odd number bits center 0
7000,00:15:17,3,System Implementation- Inverted Index Construction,2.2,That's the end of the unary code.,00:15:15,2,That end unary code
7001,00:15:24,3,System Implementation- Inverted Index Construction,2.2,"And before that, or to, on the left side of this 0, there will be all 1s.",00:15:18,2,And left side 0 1s
7002,00:15:30,3,System Implementation- Inverted Index Construction,2.2,"And on the right side of this 0, it's binary coding or uniform coding.",00:15:24,2,And right side 0 binary coding uniform coding
7003,00:15:36,3,System Implementation- Inverted Index Construction,2.2,So how can you decode such a code?,00:15:32,2,So decode code
7004,00:15:40,3,System Implementation- Inverted Index Construction,2.2,"Well, you again first do unary coding, right?",00:15:36,2,Well first unary coding right
7005,00:15:43,3,System Implementation- Inverted Index Construction,2.2,"Once you hit 0, you know you have got the unary code.",00:15:40,2,Once hit 0 know got unary code
7006,00:15:48,3,System Implementation- Inverted Index Construction,2.2,And this also will tell you how many bits you have to read further to,00:15:43,2,And also tell many bits read
7007,00:15:50,3,System Implementation- Inverted Index Construction,2.2,decode the uniform code.,00:15:48,2,decode uniform code
7008,00:15:53,3,System Implementation- Inverted Index Construction,2.2,So this is how you can decode a gamma code.,00:15:50,2,So decode gamma code
7009,00:15:57,3,System Implementation- Inverted Index Construction,2.2,"There is also delta code, but that's basically same as gamma code,",00:15:53,2,There also delta code basically gamma code
7010,00:16:01,3,System Implementation- Inverted Index Construction,2.2,except that you replace the unary prefix with the gamma code.,00:15:57,2,except replace unary prefix gamma code
7011,00:16:04,3,System Implementation- Inverted Index Construction,2.2,"So that's even less conservative than gamma code,",00:16:01,2,So even less conservative gamma code
7012,00:16:08,3,System Implementation- Inverted Index Construction,2.2,in terms of avoiding the small integers.,00:16:04,2,terms avoiding small integers
7013,00:16:12,3,System Implementation- Inverted Index Construction,2.2,So that means it's okay if you occasionally see a large number.,00:16:08,2,So means okay occasionally see large number
7014,00:16:16,3,System Implementation- Inverted Index Construction,2.2,"It's, it's, you know, it's okay with delta code.",00:16:12,2,It know okay delta code
7015,00:16:18,3,System Implementation- Inverted Index Construction,2.2,It's also fine with gamma code.,00:16:16,2,It also fine gamma code
7016,00:16:24,3,System Implementation- Inverted Index Construction,2.2,"It's really a big loss for unary code, and they are all operating,",00:16:18,2,It really big loss unary code operating
7017,00:16:32,3,System Implementation- Inverted Index Construction,2.2,"of course, at different degrees of favoring short favoring small integers.",00:16:24,2,course different degrees favoring short favoring small integers
7018,00:16:38,3,System Implementation- Inverted Index Construction,2.2,And that also means they would appropriate for sorting distribution.,00:16:32,2,And also means would appropriate sorting distribution
7019,00:16:41,3,System Implementation- Inverted Index Construction,2.2,But none of them is perfect for all distributions.,00:16:38,2,But none perfect distributions
7020,00:16:43,3,System Implementation- Inverted Index Construction,2.2,"And which method works,",00:16:41,2,And method works
7021,00:16:47,3,System Implementation- Inverted Index Construction,2.2,the best would have to depend on the actual distribution in your data set.,00:16:43,2,best would depend actual distribution data set
7022,00:16:49,3,System Implementation- Inverted Index Construction,2.2,"For inverted index, compression,",00:16:47,2,For inverted index compression
7023,00:16:52,3,System Implementation- Inverted Index Construction,2.2,people have found that gamma coding seems to work well.,00:16:49,2,people found gamma coding seems work well
7024,00:16:58,3,System Implementation- Inverted Index Construction,2.2,So how to uncompress inverted index?,00:16:55,2,So uncompress inverted index
7025,00:16:59,3,System Implementation- Inverted Index Construction,2.2,"We just, talked about this.",00:16:58,2,We talked
7026,00:17:02,3,System Implementation- Inverted Index Construction,2.2,"Firstly, you decode those encode integers.",00:16:59,2,Firstly decode encode integers
7027,00:17:07,3,System Implementation- Inverted Index Construction,2.2,"And we just, I think discussed how we decode unary coding and gamma coding.",00:17:02,2,And I think discussed decode unary coding gamma coding
7028,00:17:11,3,System Implementation- Inverted Index Construction,2.2,So I won't repeat.,00:17:09,2,So I repeat
7029,00:17:15,3,System Implementation- Inverted Index Construction,2.2,What about the document IDs that might be compressed using d-gap?,00:17:11,2,What document IDs might compressed using gap
7030,00:17:18,3,System Implementation- Inverted Index Construction,2.2,"Well, we're going to do sequential decoding.",00:17:15,2,Well going sequential decoding
7031,00:17:23,3,System Implementation- Inverted Index Construction,2.2,"So suppose the encoded idealist is x1, x2, x3 et cetera.",00:17:18,2,So suppose encoded idealist x1 x2 x3 et cetera
7032,00:17:27,3,System Implementation- Inverted Index Construction,2.2,"We first decode x1 to obtain the first document ID, ID1.",00:17:23,2,We first decode x1 obtain first document ID ID1
7033,00:17:30,3,System Implementation- Inverted Index Construction,2.2,"Then, we will decode x2,",00:17:27,2,Then decode x2
7034,00:17:34,3,System Implementation- Inverted Index Construction,2.2,which is actually the difference between the second ID and the first one.,00:17:30,2,actually difference second ID first one
7035,00:17:41,3,System Implementation- Inverted Index Construction,2.2,So we have to add the decoded value of x2 to ID1 to recover the value,00:17:34,2,So add decoded value x2 ID1 recover value
7036,00:17:46,3,System Implementation- Inverted Index Construction,2.2,"of the, the ID at this secondary position, right.",00:17:41,2,ID secondary position right
7037,00:17:52,3,System Implementation- Inverted Index Construction,2.2,"So this is where you can see the advantage of, converting document IDs into integers.",00:17:46,2,So see advantage converting document IDs integers
7038,00:17:55,3,System Implementation- Inverted Index Construction,2.2,"And that allows us to do this kind of compression, and",00:17:52,2,And allows us kind compression
7039,00:17:59,3,System Implementation- Inverted Index Construction,2.2,we just repeat until we decode all the documents.,00:17:55,2,repeat decode documents
7040,00:18:04,3,System Implementation- Inverted Index Construction,2.2,Every time we use the document ID in the previous position,00:17:59,2,Every time use document ID previous position
7041,00:18:08,3,System Implementation- Inverted Index Construction,2.2,to help recover the document ID in the next position.,00:18:04,2,help recover document ID next position
7042,00:00:03,4,Smoothing Methods Part - 2,3.5,[SOUND],00:00:00,7,SOUND
7043,00:00:15,4,Smoothing Methods Part - 2,3.5,So let's plug in these model masses,00:00:13,7,So let plug model masses
7044,00:00:18,4,Smoothing Methods Part - 2,3.5,"into the ranking function to see what we will get, okay?",00:00:15,7,ranking function see get okay
7045,00:00:20,4,Smoothing Methods Part - 2,3.5,This is a general smoothing.,00:00:18,7,This general smoothing
7046,00:00:24,4,Smoothing Methods Part - 2,3.5,So a general ranking function for smoothing with subtraction and,00:00:20,7,So general ranking function smoothing subtraction
7047,00:00:26,4,Smoothing Methods Part - 2,3.5,you have seen this before.,00:00:24,7,seen
7048,00:00:32,4,Smoothing Methods Part - 2,3.5,"And now we have a very specific smoothing method, the JM smoothing method.",00:00:28,7,And specific smoothing method JM smoothing method
7049,00:00:39,4,Smoothing Methods Part - 2,3.5,So now let's see what what's a value for office of D here.,00:00:33,7,So let see value office D
7050,00:00:42,4,Smoothing Methods Part - 2,3.5,And what's the value for p sub c here?,00:00:40,7,And value p sub c
7051,00:00:46,4,Smoothing Methods Part - 2,3.5,"Right, so we may need to decide this",00:00:42,7,Right may need decide
7052,00:00:50,4,Smoothing Methods Part - 2,3.5,in order to figure out the exact form of the ranking function.,00:00:46,7,order figure exact form ranking function
7053,00:00:52,4,Smoothing Methods Part - 2,3.5,And we also need to figure out of course alpha.,00:00:50,7,And also need figure course alpha
7054,00:00:55,4,Smoothing Methods Part - 2,3.5,So let's see.,00:00:52,7,So let see
7055,00:01:00,4,Smoothing Methods Part - 2,3.5,"Well this ratio is basically this, right, so,",00:00:55,7,Well ratio basically right
7056,00:01:05,4,Smoothing Methods Part - 2,3.5,"here, this is the probability of c board on the top,",00:01:00,7,probability c board top
7057,00:01:09,4,Smoothing Methods Part - 2,3.5,"and this is the probability of unseen war or,",00:01:05,7,probability unseen war
7058,00:01:14,4,Smoothing Methods Part - 2,3.5,"in other words basically 11 times basically the alpha here,",00:01:09,7,words basically 11 times basically alpha
7059,00:01:18,4,Smoothing Methods Part - 2,3.5,"this, so it's easy to see that.",00:01:14,7,easy see
7060,00:01:21,4,Smoothing Methods Part - 2,3.5,This can be then rewritten as this.,00:01:18,7,This rewritten
7061,00:01:24,4,Smoothing Methods Part - 2,3.5,Very simple.,00:01:21,7,Very simple
7062,00:01:26,4,Smoothing Methods Part - 2,3.5,So we can plug this into here.,00:01:24,7,So plug
7063,00:01:30,4,Smoothing Methods Part - 2,3.5,"And then here, what's the value for alpha?",00:01:28,7,And value alpha
7064,00:01:31,4,Smoothing Methods Part - 2,3.5,What do you think?,00:01:30,7,What think
7065,00:01:35,4,Smoothing Methods Part - 2,3.5,"So it would be just lambda, right?",00:01:31,7,So would lambda right
7066,00:01:43,4,Smoothing Methods Part - 2,3.5,"And what would happen if we plug in this value here, if this is lambda.",00:01:38,7,And would happen plug value lambda
7067,00:01:45,4,Smoothing Methods Part - 2,3.5,What can we say about this?,00:01:43,7,What say
7068,00:01:49,4,Smoothing Methods Part - 2,3.5,Does it depend on the document?,00:01:47,7,Does depend document
7069,00:01:52,4,Smoothing Methods Part - 2,3.5,"No, so it can be ignored.",00:01:50,7,No ignored
7070,00:01:55,4,Smoothing Methods Part - 2,3.5,Right?,00:01:53,7,Right
7071,00:01:58,4,Smoothing Methods Part - 2,3.5,So we'll end up having this ranking function shown here.,00:01:55,7,So end ranking function shown
7072,00:02:02,4,Smoothing Methods Part - 2,3.5,"And in this case you can easy to see,",00:02:00,7,And case easy see
7073,00:02:07,4,Smoothing Methods Part - 2,3.5,this a precisely a vector space model because this part is,00:02:02,7,precisely vector space model part
7074,00:02:13,4,Smoothing Methods Part - 2,3.5,"a sum over all the matched query terms, this is an element of the query map.",00:02:07,7,sum matched query terms element query map
7075,00:02:16,4,Smoothing Methods Part - 2,3.5,What do you think is a element of the document up there?,00:02:13,7,What think element document
7076,00:02:20,4,Smoothing Methods Part - 2,3.5,"Well it's this, right.",00:02:18,7,Well right
7077,00:02:23,4,Smoothing Methods Part - 2,3.5,So that's our document left element.,00:02:20,7,So document left element
7078,00:02:29,4,Smoothing Methods Part - 2,3.5,And let's further examine what's inside of this logarithm.,00:02:23,7,And let examine inside logarithm
7079,00:02:32,4,Smoothing Methods Part - 2,3.5,Well one plus this.,00:02:30,7,Well one plus
7080,00:02:36,4,Smoothing Methods Part - 2,3.5,"So it's going to be nonnegative, this log of this,",00:02:32,7,So going nonnegative log
7081,00:02:37,4,Smoothing Methods Part - 2,3.5,"it's going to be at least 1, right?",00:02:36,7,going least 1 right
7082,00:02:42,4,Smoothing Methods Part - 2,3.5,"And these, this is a parameter, so lambda is parameter.",00:02:39,7,And parameter lambda parameter
7083,00:02:44,4,Smoothing Methods Part - 2,3.5,And let's look at this.,00:02:42,7,And let look
7084,00:02:45,4,Smoothing Methods Part - 2,3.5,Now this is a TF.,00:02:44,7,Now TF
7085,00:02:48,4,Smoothing Methods Part - 2,3.5,Now we see very clearly this TF weighting here.,00:02:45,7,Now see clearly TF weighting
7086,00:02:54,4,Smoothing Methods Part - 2,3.5,"And the larger the count is, the higher the weighting will be.",00:02:49,7,And larger count higher weighting
7087,00:02:57,4,Smoothing Methods Part - 2,3.5,"We also see IDF weighting, which is given by this.",00:02:54,7,We also see IDF weighting given
7088,00:03:00,4,Smoothing Methods Part - 2,3.5,And we see docking the lan's relationship here.,00:02:58,7,And see docking lan relationship
7089,00:03:03,4,Smoothing Methods Part - 2,3.5,So all these heuristics are captured in this formula.,00:03:00,7,So heuristics captured formula
7090,00:03:08,4,Smoothing Methods Part - 2,3.5,What's interesting that we kind of have got this,00:03:04,7,What interesting kind got
7091,00:03:12,4,Smoothing Methods Part - 2,3.5,weighting function automatically by making various assumptions.,00:03:08,7,weighting function automatically making various assumptions
7092,00:03:14,4,Smoothing Methods Part - 2,3.5,"Whereas in the vector space model,",00:03:12,7,Whereas vector space model
7093,00:03:19,4,Smoothing Methods Part - 2,3.5,we had to go through those heuristic design in order to get this.,00:03:14,7,go heuristic design order get
7094,00:03:21,4,Smoothing Methods Part - 2,3.5,And in this case note that there's a specific form.,00:03:19,7,And case note specific form
7095,00:03:25,4,Smoothing Methods Part - 2,3.5,And when you see whether this form actually makes sense.,00:03:21,7,And see whether form actually makes sense
7096,00:03:31,4,Smoothing Methods Part - 2,3.5,"All right so what do you think is the denominator here, hm?",00:03:26,7,All right think denominator hm
7097,00:03:33,4,Smoothing Methods Part - 2,3.5,This is a math of document.,00:03:31,7,This math document
7098,00:03:37,4,Smoothing Methods Part - 2,3.5,"Total number of words, multiplied by the probability of the word",00:03:33,7,Total number words multiplied probability word
7099,00:03:42,4,Smoothing Methods Part - 2,3.5,"given by the collection, right?",00:03:38,7,given collection right
7100,00:03:48,4,Smoothing Methods Part - 2,3.5,So this actually can be interpreted as expected account over word.,00:03:42,7,So actually interpreted expected account word
7101,00:03:53,4,Smoothing Methods Part - 2,3.5,"If we're going to draw, a word, from the connection that we model.",00:03:48,7,If going draw word connection model
7102,00:03:57,4,Smoothing Methods Part - 2,3.5,"And, we're going to draw as many as the number of words in the document.",00:03:53,7,And going draw many number words document
7103,00:04:02,4,Smoothing Methods Part - 2,3.5,"If you do that, the expected account of a word, w,",00:03:59,7,If expected account word w
7104,00:04:06,4,Smoothing Methods Part - 2,3.5,would be precisely given by this denominator.,00:04:02,7,would precisely given denominator
7105,00:04:14,4,Smoothing Methods Part - 2,3.5,"So, this ratio basically, is comparing the actual count, here.",00:04:08,7,So ratio basically comparing actual count
7106,00:04:21,4,Smoothing Methods Part - 2,3.5,The actual count of the word in the document with expected count given by this,00:04:15,7,The actual count word document expected count given
7107,00:04:29,4,Smoothing Methods Part - 2,3.5,product if the word is in fact following the distribution in the clutch this.,00:04:21,7,product word fact following distribution clutch
7108,00:04:33,4,Smoothing Methods Part - 2,3.5,"And if this counter is larger than the expected counter in this part,",00:04:29,7,And counter larger expected counter part
7109,00:04:34,4,Smoothing Methods Part - 2,3.5,this ratio would be larger than one.,00:04:33,7,ratio would larger one
7110,00:04:40,4,Smoothing Methods Part - 2,3.5,"So that's actually a very interesting interpretation, right?",00:04:37,7,So actually interesting interpretation right
7111,00:04:43,4,Smoothing Methods Part - 2,3.5,"It's very natural and intuitive, it makes a lot of sense.",00:04:40,7,It natural intuitive makes lot sense
7112,00:04:49,4,Smoothing Methods Part - 2,3.5,And this is one advantage of using this kind of probabilistic reasoning,00:04:45,7,And one advantage using kind probabilistic reasoning
7113,00:04:53,4,Smoothing Methods Part - 2,3.5,where we have made explicit assumptions.,00:04:49,7,made explicit assumptions
7114,00:04:56,4,Smoothing Methods Part - 2,3.5,"And, we know precisely why we have a logarithm here.",00:04:53,7,And know precisely logarithm
7115,00:04:58,4,Smoothing Methods Part - 2,3.5,"And, why we have these probabilities here.",00:04:56,7,And probabilities
7116,00:05:04,4,Smoothing Methods Part - 2,3.5,"And, we also have a formula that intuitively makes a lot of sense and",00:05:00,7,And also formula intuitively makes lot sense
7117,00:05:07,4,Smoothing Methods Part - 2,3.5,does TF-IDF weighting and documenting and some others.,00:05:04,7,TF IDF weighting documenting others
7118,00:05:11,4,Smoothing Methods Part - 2,3.5,"Let's look at the, the Dirichlet Prior Smoothing.",00:05:09,7,Let look Dirichlet Prior Smoothing
7119,00:05:16,4,Smoothing Methods Part - 2,3.5,It's very similar to the case of JM smoothing.,00:05:11,7,It similar case JM smoothing
7120,00:05:21,4,Smoothing Methods Part - 2,3.5,"In this case, the smoothing parameter is mu and",00:05:16,7,In case smoothing parameter mu
7121,00:05:27,4,Smoothing Methods Part - 2,3.5,that's different from lambda that we saw before.,00:05:21,7,different lambda saw
7122,00:05:30,4,Smoothing Methods Part - 2,3.5,But the format looks very similar.,00:05:27,7,But format looks similar
7123,00:05:32,4,Smoothing Methods Part - 2,3.5,The form of the function looks very similar.,00:05:30,7,The form function looks similar
7124,00:05:36,4,Smoothing Methods Part - 2,3.5,So we still have linear operation here.,00:05:34,7,So still linear operation
7125,00:05:40,4,Smoothing Methods Part - 2,3.5,"And when we compute this ratio,",00:05:38,7,And compute ratio
7126,00:05:45,4,Smoothing Methods Part - 2,3.5,one will find that is that the ratio is equal to this.,00:05:40,7,one find ratio equal
7127,00:05:51,4,Smoothing Methods Part - 2,3.5,And what's interesting here is that we are doing another comparison here now.,00:05:46,7,And interesting another comparison
7128,00:05:54,4,Smoothing Methods Part - 2,3.5,We're comparing the actual count.,00:05:51,7,We comparing actual count
7129,00:05:59,4,Smoothing Methods Part - 2,3.5,Which is the expected account of the world if we sampled meal worlds according to,00:05:54,7,Which expected account world sampled meal worlds according
7130,00:06:02,4,Smoothing Methods Part - 2,3.5,the collection world probability.,00:05:59,7,collection world probability
7131,00:06:07,4,Smoothing Methods Part - 2,3.5,So note that it's interesting we don't even see docking the lens here and,00:06:02,7,So note interesting even see docking lens
7132,00:06:08,4,Smoothing Methods Part - 2,3.5,lighter in the JMs model.,00:06:07,7,lighter JMs model
7133,00:06:13,4,Smoothing Methods Part - 2,3.5,All right so this of course should be plugged into this part.,00:06:08,7,All right course plugged part
7134,00:06:18,4,Smoothing Methods Part - 2,3.5,"So you might wonder, so where is docking lens.",00:06:15,7,So might wonder docking lens
7135,00:06:23,4,Smoothing Methods Part - 2,3.5,Interestingly the docking lens is here in alpha sub d so,00:06:18,7,Interestingly docking lens alpha sub
7136,00:06:26,4,Smoothing Methods Part - 2,3.5,this would be plugged into this part.,00:06:23,7,would plugged part
7137,00:06:31,4,Smoothing Methods Part - 2,3.5,As a result what we get is the following function here and,00:06:26,7,As result get following function
7138,00:06:35,4,Smoothing Methods Part - 2,3.5,this is again a sum over all the match query words.,00:06:31,7,sum match query words
7139,00:06:40,4,Smoothing Methods Part - 2,3.5,"And we're against the queer, the query, time frequency here.",00:06:36,7,And queer query time frequency
7140,00:06:45,4,Smoothing Methods Part - 2,3.5,"And you can interpret this as the element of a document vector,",00:06:41,7,And interpret element document vector
7141,00:06:48,4,Smoothing Methods Part - 2,3.5,"but this is no longer a single dot product, right?",00:06:45,7,longer single dot product right
7142,00:06:55,4,Smoothing Methods Part - 2,3.5,"Because we have this part, I know that n is the name of the query,",00:06:50,7,Because part I know n name query
7143,00:06:57,4,Smoothing Methods Part - 2,3.5,right?,00:06:55,7,right
7144,00:07:01,4,Smoothing Methods Part - 2,3.5,"So that just means if we score this function,",00:06:57,7,So means score function
7145,00:07:05,4,Smoothing Methods Part - 2,3.5,"we have to take a sum over all the query words, and",00:07:01,7,take sum query words
7146,00:07:09,4,Smoothing Methods Part - 2,3.5,then do some adjustment of the score based on the document.,00:07:05,7,adjustment score based document
7147,00:07:15,4,Smoothing Methods Part - 2,3.5,"But it's still, it's still clear that it does documents lens",00:07:11,7,But still still clear documents lens
7148,00:07:19,4,Smoothing Methods Part - 2,3.5,modulation because this lens is in the denominator so,00:07:15,7,modulation lens denominator
7149,00:07:23,4,Smoothing Methods Part - 2,3.5,a longer document will have a lower weight here.,00:07:19,7,longer document lower weight
7150,00:07:27,4,Smoothing Methods Part - 2,3.5,And we can also see it has tf here and now idf.,00:07:23,7,And also see tf idf
7151,00:07:32,4,Smoothing Methods Part - 2,3.5,Only that this time the form of the formula is different from the previous one,00:07:27,7,Only time form formula different previous one
7152,00:07:34,4,Smoothing Methods Part - 2,3.5,in JMs one.,00:07:32,7,JMs one
7153,00:07:39,4,Smoothing Methods Part - 2,3.5,"But intuitively it still implements TFIDF waiting and document lens rendition again,",00:07:34,7,But intuitively still implements TFIDF waiting document lens rendition
7154,00:07:44,4,Smoothing Methods Part - 2,3.5,the form of the function is dictated by the probabilistic reasoning and,00:07:39,7,form function dictated probabilistic reasoning
7155,00:07:45,4,Smoothing Methods Part - 2,3.5,assumptions that we have made.,00:07:44,7,assumptions made
7156,00:07:50,4,Smoothing Methods Part - 2,3.5,Now there are also disadvantages of this approach.,00:07:45,7,Now also disadvantages approach
7157,00:07:53,4,Smoothing Methods Part - 2,3.5,"And that is, there's no guarantee that there's such a form",00:07:50,7,And guarantee form
7158,00:07:55,4,Smoothing Methods Part - 2,3.5,of the formula will actually work well.,00:07:53,7,formula actually work well
7159,00:08:01,4,Smoothing Methods Part - 2,3.5,"So if we look about at this geo function, all those TF-IDF waiting and document lens",00:07:55,7,So look geo function TF IDF waiting document lens
7160,00:08:06,4,Smoothing Methods Part - 2,3.5,rendition for example it's unclear whether we have sub-linear transformation.,00:08:01,7,rendition example unclear whether sub linear transformation
7161,00:08:13,4,Smoothing Methods Part - 2,3.5,Unfortunately we can see here there is a logarithm function here.,00:08:06,7,Unfortunately see logarithm function
7162,00:08:17,4,Smoothing Methods Part - 2,3.5,"So we do have also the, so it's here right?",00:08:13,7,So also right
7163,00:08:20,4,Smoothing Methods Part - 2,3.5,"So we do have the sublinear transformation, but",00:08:17,7,So sublinear transformation
7164,00:08:23,4,Smoothing Methods Part - 2,3.5,we do not intentionally do that.,00:08:20,7,intentionally
7165,00:08:27,4,Smoothing Methods Part - 2,3.5,"That means there's no guarantee that we will end up in this, in this way.",00:08:23,7,That means guarantee end way
7166,00:08:31,4,Smoothing Methods Part - 2,3.5,"Suppose we don't have logarithm, then there's no sub-linear transformation.",00:08:27,7,Suppose logarithm sub linear transformation
7167,00:08:35,4,Smoothing Methods Part - 2,3.5,"As we discussed before, perhaps the formula is not going to work so well.",00:08:31,7,As discussed perhaps formula going work well
7168,00:08:40,4,Smoothing Methods Part - 2,3.5,So that's an example of the gap between a formal model like this and,00:08:35,7,So example gap formal model like
7169,00:08:43,4,Smoothing Methods Part - 2,3.5,"the relevance that we have to model,",00:08:40,7,relevance model
7170,00:08:48,4,Smoothing Methods Part - 2,3.5,which is really a subject motion that is tied to users.,00:08:43,7,really subject motion tied users
7171,00:08:53,4,Smoothing Methods Part - 2,3.5,So it doesn't mean we cannot fix this.,00:08:50,7,So mean cannot fix
7172,00:08:57,4,Smoothing Methods Part - 2,3.5,"For example, imagine if we did not have this logarithm, right?",00:08:53,7,For example imagine logarithm right
7173,00:08:59,4,Smoothing Methods Part - 2,3.5,"So we can take a risk and we're going to add one,",00:08:57,7,So take risk going add one
7174,00:09:01,4,Smoothing Methods Part - 2,3.5,or we can even add double logarithm.,00:08:59,7,even add double logarithm
7175,00:09:06,4,Smoothing Methods Part - 2,3.5,"But then, it would mean that the function is no longer a proper risk model.",00:09:01,7,But would mean function longer proper risk model
7176,00:09:10,4,Smoothing Methods Part - 2,3.5,So the consequence of the modification is no,00:09:06,7,So consequence modification
7177,00:09:14,4,Smoothing Methods Part - 2,3.5,longer as predictable as what we have been doing now.,00:09:10,7,longer predictable
7178,00:09:21,4,Smoothing Methods Part - 2,3.5,"So, that's also why, for example, PM45 remains very competitive and",00:09:15,7,So also example PM45 remains competitive
7179,00:09:26,4,Smoothing Methods Part - 2,3.5,"still, open channel how to use public risk models as they arrive,",00:09:21,7,still open channel use public risk models arrive
7180,00:09:28,4,Smoothing Methods Part - 2,3.5,better model than the PM25.,00:09:26,7,better model PM25
7181,00:09:34,4,Smoothing Methods Part - 2,3.5,In particular how do we use query like how to derive a model and,00:09:30,7,In particular use query like derive model
7182,00:09:37,4,Smoothing Methods Part - 2,3.5,that would work consistently better than DM 25.,00:09:34,7,would work consistently better DM 25
7183,00:09:39,4,Smoothing Methods Part - 2,3.5,Currently we still cannot do that.,00:09:37,7,Currently still cannot
7184,00:09:41,4,Smoothing Methods Part - 2,3.5,Still interesting open question.,00:09:40,7,Still interesting open question
7185,00:09:46,4,Smoothing Methods Part - 2,3.5,"So to summarize this part, we've talked about the two smoothing methods.",00:09:43,7,So summarize part talked two smoothing methods
7186,00:09:52,4,Smoothing Methods Part - 2,3.5,Jelinek-Mercer which is doing the fixed coefficient linear interpolation.,00:09:46,7,Jelinek Mercer fixed coefficient linear interpolation
7187,00:09:58,4,Smoothing Methods Part - 2,3.5,Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive,00:09:52,7,Dirichlet Prior add pseudo counts every word adaptive
7188,00:10:04,4,Smoothing Methods Part - 2,3.5,interpolation in that the coefficient would be larger for shorter documents.,00:09:58,7,interpolation coefficient would larger shorter documents
7189,00:10:10,4,Smoothing Methods Part - 2,3.5,"In most cases we can see, by using these smoothing methods, we will be able to",00:10:05,7,In cases see using smoothing methods able
7190,00:10:16,4,Smoothing Methods Part - 2,3.5,reach a retrieval function where the assumptions are clearly articulate.,00:10:10,7,reach retrieval function assumptions clearly articulate
7191,00:10:17,4,Smoothing Methods Part - 2,3.5,So they are less heuristic.,00:10:16,7,So less heuristic
7192,00:10:23,4,Smoothing Methods Part - 2,3.5,"Explaining the results also show that these, retrieval functions.",00:10:19,7,Explaining results also show retrieval functions
7193,00:10:31,4,Smoothing Methods Part - 2,3.5,Also are very effective and they are comparable to BM 25 or pm lens adultation.,00:10:23,7,Also effective comparable BM 25 pm lens adultation
7194,00:10:36,4,Smoothing Methods Part - 2,3.5,So this is a major advantage of probably smaller,00:10:31,7,So major advantage probably smaller
7195,00:10:39,4,Smoothing Methods Part - 2,3.5,where we don't have to do a lot of heuristic design.,00:10:36,7,lot heuristic design
7196,00:10:44,4,Smoothing Methods Part - 2,3.5,Yet in the end that we naturally implemented TF-IDF weighting and,00:10:40,7,Yet end naturally implemented TF IDF weighting
7197,00:10:45,4,Smoothing Methods Part - 2,3.5,doc length normalization.,00:10:44,7,doc length normalization
7198,00:10:51,4,Smoothing Methods Part - 2,3.5,Each of these functions also has precise ones smoothing parameter.,00:10:46,7,Each functions also precise ones smoothing parameter
7199,00:10:54,4,Smoothing Methods Part - 2,3.5,In this case of course we still need to set this smoothing parameter.,00:10:51,7,In case course still need set smoothing parameter
7200,00:10:58,4,Smoothing Methods Part - 2,3.5,There are also methods that can be used to estimate these parameters.,00:10:54,7,There also methods used estimate parameters
7201,00:11:04,4,Smoothing Methods Part - 2,3.5,"So overall, this shows by using a probabilistic model,",00:10:59,7,So overall shows using probabilistic model
7202,00:11:08,4,Smoothing Methods Part - 2,3.5,we follow very different strategies then the vector space model.,00:11:04,7,follow different strategies vector space model
7203,00:11:12,4,Smoothing Methods Part - 2,3.5,"Yet, in the end, we end up uh,with some retrievable functions that",00:11:08,7,Yet end end uh retrievable functions
7204,00:11:15,4,Smoothing Methods Part - 2,3.5,look very similar to the vector space model.,00:11:12,7,look similar vector space model
7205,00:11:21,4,Smoothing Methods Part - 2,3.5,With some advantages in having assumptions clearly stated.,00:11:15,7,With advantages assumptions clearly stated
7206,00:11:24,4,Smoothing Methods Part - 2,3.5,"And then, the form dictated by a probabilistic model.",00:11:21,7,And form dictated probabilistic model
7207,00:11:29,4,Smoothing Methods Part - 2,3.5,"Now, this also concludes our discussion of the query likelihood probabilistic model.",00:11:24,7,Now also concludes discussion query likelihood probabilistic model
7208,00:11:34,4,Smoothing Methods Part - 2,3.5,And let's recall what assumptions we have made,00:11:29,7,And let recall assumptions made
7209,00:11:39,4,Smoothing Methods Part - 2,3.5,in order to derive the functions that we have seen in this lecture.,00:11:34,7,order derive functions seen lecture
7210,00:11:42,4,Smoothing Methods Part - 2,3.5,Well we basically have made four assumptions that I listed here.,00:11:39,7,Well basically made four assumptions I listed
7211,00:11:48,4,Smoothing Methods Part - 2,3.5,The first assumption is that the relevance can be modeled by the query likelihood.,00:11:42,7,The first assumption relevance modeled query likelihood
7212,00:11:53,4,Smoothing Methods Part - 2,3.5,"And the second assumption with med is, are query words are generated independently",00:11:49,7,And second assumption med query words generated independently
7213,00:11:57,4,Smoothing Methods Part - 2,3.5,that allows us to decompose the probability of the whole query,00:11:53,7,allows us decompose probability whole query
7214,00:12:01,4,Smoothing Methods Part - 2,3.5,into a product of probabilities of old words in the query.,00:11:57,7,product probabilities old words query
7215,00:12:07,4,Smoothing Methods Part - 2,3.5,"And then, the third assumption that we have made is,",00:12:03,7,And third assumption made
7216,00:12:10,4,Smoothing Methods Part - 2,3.5,"if a word is not seen, the document or in the late,",00:12:07,7,word seen document late
7217,00:12:14,4,Smoothing Methods Part - 2,3.5,its probability proportional to its probability in the collection.,00:12:10,7,probability proportional probability collection
7218,00:12:17,4,Smoothing Methods Part - 2,3.5,That's a smoothing with a collection ama model.,00:12:14,7,That smoothing collection ama model
7219,00:12:20,4,Smoothing Methods Part - 2,3.5,"And finally, we made one of these two assumptions about the smoothing.",00:12:17,7,And finally made one two assumptions smoothing
7220,00:12:24,4,Smoothing Methods Part - 2,3.5,So we either used JM smoothing or Dirichlet prior smoothing.,00:12:20,7,So either used JM smoothing Dirichlet prior smoothing
7221,00:12:28,4,Smoothing Methods Part - 2,3.5,If we make these four assumptions then we have no choice but,00:12:24,7,If make four assumptions choice
7222,00:12:33,4,Smoothing Methods Part - 2,3.5,to take the form of the retrieval function that we have seen earlier.,00:12:28,7,take form retrieval function seen earlier
7223,00:12:37,4,Smoothing Methods Part - 2,3.5,Fortunately the function has a nice property in that it implements TF-IDF,00:12:33,7,Fortunately function nice property implements TF IDF
7224,00:12:44,4,Smoothing Methods Part - 2,3.5,weighting and document machine and these functions also work very well.,00:12:37,7,weighting document machine functions also work well
7225,00:12:45,4,Smoothing Methods Part - 2,3.5,"So in that sense,",00:12:44,7,So sense
7226,00:12:48,4,Smoothing Methods Part - 2,3.5,these functions are less heuristic compared with the vector space model.,00:12:45,7,functions less heuristic compared vector space model
7227,00:12:54,4,Smoothing Methods Part - 2,3.5,"And there are many extensions of this, this basic model and",00:12:50,7,And many extensions basic model
7228,00:12:59,4,Smoothing Methods Part - 2,3.5,you can find the discussion of them in the reference at the end of this lecture.,00:12:54,7,find discussion reference end lecture
7229,00:00:06,3,Evaluation of TR Systems- Basic Measures,2.5,"[SOUND] This lecture is about the, the basic measures for",00:00:00,5,SOUND This lecture basic measures
7230,00:00:11,3,Evaluation of TR Systems- Basic Measures,2.5,evaluation of text original systems.,00:00:06,5,evaluation text original systems
7231,00:00:17,3,Evaluation of TR Systems- Basic Measures,2.5,"In this lecture, we're going to discuss how we design basic",00:00:11,5,In lecture going discuss design basic
7232,00:00:24,3,Evaluation of TR Systems- Basic Measures,2.5,"measures [SOUND] to quantitatively, compare two original [SOUND] systems.",00:00:17,5,measures SOUND quantitatively compare two original SOUND systems
7233,00:00:27,3,Evaluation of TR Systems- Basic Measures,2.5,"This is a slide that you have seen earlier in the lecture,",00:00:24,5,This slide seen earlier lecture
7234,00:00:32,3,Evaluation of TR Systems- Basic Measures,2.5,where we talk about the grand evaluation methodology.,00:00:27,5,talk grand evaluation methodology
7235,00:00:37,3,Evaluation of TR Systems- Basic Measures,2.5,"We can have a test collection that consists of queries, documents and",00:00:32,5,We test collection consists queries documents
7236,00:00:38,3,Evaluation of TR Systems- Basic Measures,2.5,relevance judgements.,00:00:37,5,relevance judgements
7237,00:00:44,3,Evaluation of TR Systems- Basic Measures,2.5,"We can then run two systems on these da, data sets to,",00:00:39,5,We run two systems da data sets
7238,00:00:49,3,Evaluation of TR Systems- Basic Measures,2.5,quantitatively evaluate your performance.,00:00:44,5,quantitatively evaluate performance
7239,00:00:52,3,Evaluation of TR Systems- Basic Measures,2.5,"And we raised to the question about,",00:00:49,5,And raised question
7240,00:00:57,3,Evaluation of TR Systems- Basic Measures,2.5,[SOUND] which settles results is better is System A better or System B better?,00:00:52,5,SOUND settles results better System A better System B better
7241,00:01:02,3,Evaluation of TR Systems- Basic Measures,2.5,[SOUND] So let's now talk about how to actually quantify their performance.,00:00:57,5,SOUND So let talk actually quantify performance
7242,00:01:04,3,Evaluation of TR Systems- Basic Measures,2.5,"Suppose we have a total of,",00:01:02,5,Suppose total
7243,00:01:09,3,Evaluation of TR Systems- Basic Measures,2.5,of 10 random documents in the current folder for this query.,00:01:04,5,10 random documents current folder query
7244,00:01:11,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, the relevance judgements shown on the right,",00:01:09,5,Now relevance judgements shown right
7245,00:01:15,3,Evaluation of TR Systems- Basic Measures,2.5,did not include all the ten obviously.,00:01:12,5,include ten obviously
7246,00:01:20,3,Evaluation of TR Systems- Basic Measures,2.5,And we have only seen three rendered documents there but,00:01:15,5,And seen three rendered documents
7247,00:01:26,3,Evaluation of TR Systems- Basic Measures,2.5,we can imagine there are other random documents in judging for this query.,00:01:20,5,imagine random documents judging query
7248,00:01:30,3,Evaluation of TR Systems- Basic Measures,2.5,"So now, intuitively we thought that",00:01:26,5,So intuitively thought
7249,00:01:35,3,Evaluation of TR Systems- Basic Measures,2.5,System A is better because it did not have much noise.,00:01:30,5,System A better much noise
7250,00:01:41,3,Evaluation of TR Systems- Basic Measures,2.5,"And in particular we have seen, amount of three results,",00:01:35,5,And particular seen amount three results
7251,00:01:45,3,Evaluation of TR Systems- Basic Measures,2.5,two of them are relevant but in System B we,00:01:41,5,two relevant System B
7252,00:01:51,3,Evaluation of TR Systems- Basic Measures,2.5,have five results and only three of them are relevant.,00:01:46,5,five results three relevant
7253,00:01:56,3,Evaluation of TR Systems- Basic Measures,2.5,"So intuitively, it looks like System A is more accurate.",00:01:52,5,So intuitively looks like System A accurate
7254,00:02:00,3,Evaluation of TR Systems- Basic Measures,2.5,And this can be captured by a matching order precision.,00:01:56,5,And captured matching order precision
7255,00:02:05,3,Evaluation of TR Systems- Basic Measures,2.5,Where we simply compute to what extent all the retrieval results are relevant.,00:02:00,5,Where simply compute extent retrieval results relevant
7256,00:02:11,3,Evaluation of TR Systems- Basic Measures,2.5,If you have 100% precision that would mean all the retrieval documents are relevant.,00:02:05,5,If 100 precision would mean retrieval documents relevant
7257,00:02:16,3,Evaluation of TR Systems- Basic Measures,2.5,"So, in this case the system A has a Precision of two out of three.",00:02:11,5,So case system A Precision two three
7258,00:02:19,3,Evaluation of TR Systems- Basic Measures,2.5,System B as three over five.,00:02:16,5,System B three five
7259,00:02:24,3,Evaluation of TR Systems- Basic Measures,2.5,And this shows that System A is better by Precision.,00:02:19,5,And shows System A better Precision
7260,00:02:29,3,Evaluation of TR Systems- Basic Measures,2.5,But we also talked about System B might be preferred by,00:02:25,5,But also talked System B might preferred
7261,00:02:35,3,Evaluation of TR Systems- Basic Measures,2.5,some other users hold like to retrieve as many relevant documents as possible.,00:02:29,5,users hold like retrieve many relevant documents possible
7262,00:02:38,3,Evaluation of TR Systems- Basic Measures,2.5,"So, in that case we have to compare",00:02:35,5,So case compare
7263,00:02:39,3,Evaluation of TR Systems- Basic Measures,2.5,the number of relevant documents that retrieve.,00:02:38,5,number relevant documents retrieve
7264,00:02:42,3,Evaluation of TR Systems- Basic Measures,2.5,And there is an other measure called a Recall.,00:02:39,5,And measure called Recall
7265,00:02:47,3,Evaluation of TR Systems- Basic Measures,2.5,This measures the completeness of coverage of relevant documents,00:02:42,5,This measures completeness coverage relevant documents
7266,00:02:51,3,Evaluation of TR Systems- Basic Measures,2.5,in your retriever result.,00:02:47,5,retriever result
7267,00:02:57,3,Evaluation of TR Systems- Basic Measures,2.5,"So, we just assume that there are ten relevant documents in the collection.",00:02:51,5,So assume ten relevant documents collection
7268,00:03:04,3,Evaluation of TR Systems- Basic Measures,2.5,"And here we've got two of them in System A, so the recall is two out of ten.",00:02:57,5,And got two System A recall two ten
7269,00:03:07,3,Evaluation of TR Systems- Basic Measures,2.5,"Where as system B has got a three, so it's a three out of ten.",00:03:04,5,Where system B got three three ten
7270,00:03:12,3,Evaluation of TR Systems- Basic Measures,2.5,"Now ,we can see by recall System B is better and these two",00:03:08,5,Now see recall System B better two
7271,00:03:17,3,Evaluation of TR Systems- Basic Measures,2.5,measures turned out to be the very basic measures for evaluating search engine.,00:03:12,5,measures turned basic measures evaluating search engine
7272,00:03:22,3,Evaluation of TR Systems- Basic Measures,2.5,And they are very important because they are also widely used in many other,00:03:17,5,And important also widely used many
7273,00:03:24,3,Evaluation of TR Systems- Basic Measures,2.5,testing variation problems.,00:03:22,5,testing variation problems
7274,00:03:29,3,Evaluation of TR Systems- Basic Measures,2.5,"For example, if you look at the applications of machine learning you tend",00:03:24,5,For example look applications machine learning tend
7275,00:03:34,3,Evaluation of TR Systems- Basic Measures,2.5,to see precision recall numbers being reported for all kinds of tasks.,00:03:29,5,see precision recall numbers reported kinds tasks
7276,00:03:38,3,Evaluation of TR Systems- Basic Measures,2.5,"Okay, so now, let's define these two measures more precisely and",00:03:35,5,Okay let define two measures precisely
7277,00:03:43,3,Evaluation of TR Systems- Basic Measures,2.5,these measures are to evaluate a set of retrieval documents.,00:03:38,5,measures evaluate set retrieval documents
7278,00:03:46,3,Evaluation of TR Systems- Basic Measures,2.5,So that means we are considering that approximation,00:03:43,5,So means considering approximation
7279,00:03:48,3,Evaluation of TR Systems- Basic Measures,2.5,of a set of relevant documents.,00:03:46,5,set relevant documents
7280,00:03:53,3,Evaluation of TR Systems- Basic Measures,2.5,"We can distinguish it four cases, depending on the situation of a document.",00:03:50,5,We distinguish four cases depending situation document
7281,00:03:59,3,Evaluation of TR Systems- Basic Measures,2.5,"A document that can be retrieved or not retrieved, right?",00:03:53,5,A document retrieved retrieved right
7282,00:04:01,3,Evaluation of TR Systems- Basic Measures,2.5,Because we're talking about the set of result.,00:03:59,5,Because talking set result
7283,00:04:05,3,Evaluation of TR Systems- Basic Measures,2.5,The document can be also relevant or,00:04:02,5,The document also relevant
7284,00:04:10,3,Evaluation of TR Systems- Basic Measures,2.5,"not relevant, depending on whether the user thinks this is a useful document.",00:04:05,5,relevant depending whether user thinks useful document
7285,00:04:18,3,Evaluation of TR Systems- Basic Measures,2.5,"So, we can now have counts of documents in each of the four categories.",00:04:11,5,So counts documents four categories
7286,00:04:23,3,Evaluation of TR Systems- Basic Measures,2.5,We can have a to represent the number of documents that are retrieved and,00:04:18,5,We represent number documents retrieved
7287,00:04:30,3,Evaluation of TR Systems- Basic Measures,2.5,"relevant, b for documents that are not retrieved but relevant, etc.",00:04:23,5,relevant b documents retrieved relevant etc
7288,00:04:35,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, with this table, then we have defined precision.",00:04:31,5,Now table defined precision
7289,00:04:42,3,Evaluation of TR Systems- Basic Measures,2.5,"As the, ratio of, the relevant",00:04:36,5,As ratio relevant
7290,00:04:47,3,Evaluation of TR Systems- Basic Measures,2.5,retriever documents A to the total number of retriever documents.,00:04:42,5,retriever documents A total number retriever documents
7291,00:04:53,3,Evaluation of TR Systems- Basic Measures,2.5,"So this is just you know, a divided by the sum of a and c.",00:04:48,5,So know divided sum c
7292,00:04:55,3,Evaluation of TR Systems- Basic Measures,2.5,The sum of this column.,00:04:53,5,The sum column
7293,00:05:04,3,Evaluation of TR Systems- Basic Measures,2.5,Signal recall is defined by dividing a by the sum of a and b.,00:04:56,5,Signal recall defined dividing sum b
7294,00:05:10,3,Evaluation of TR Systems- Basic Measures,2.5,"So that's, again, to divide a by the sum of the rule, instead of the column.",00:05:04,5,So divide sum rule instead column
7295,00:05:15,3,Evaluation of TR Systems- Basic Measures,2.5,"All right, so we going to see precision and recall is all focused on",00:05:10,5,All right going see precision recall focused
7296,00:05:19,3,Evaluation of TR Systems- Basic Measures,2.5,"looking at the a, that's the number of retrieval relevant documents, but",00:05:15,5,looking number retrieval relevant documents
7297,00:05:22,3,Evaluation of TR Systems- Basic Measures,2.5,we're going to use different denominators.,00:05:19,5,going use different denominators
7298,00:05:27,3,Evaluation of TR Systems- Basic Measures,2.5,"Okay, so what would be an ideal result?",00:05:23,5,Okay would ideal result
7299,00:05:32,3,Evaluation of TR Systems- Basic Measures,2.5,"Well, you can able to see in ideal case we have precision and recall, all",00:05:27,5,Well able see ideal case precision recall
7300,00:05:40,3,Evaluation of TR Systems- Basic Measures,2.5,to be 1.0 that means we have got 1% of all the random documents in our results.,00:05:34,5,1 0 means got 1 random documents results
7301,00:05:44,3,Evaluation of TR Systems- Basic Measures,2.5,And all the results that we return are relevant.,00:05:40,5,And results return relevant
7302,00:05:47,3,Evaluation of TR Systems- Basic Measures,2.5,[INAUDIBLE] There's no single not relevant document returned.,00:05:44,5,INAUDIBLE There single relevant document returned
7303,00:05:54,3,Evaluation of TR Systems- Basic Measures,2.5,"The reality however, high recall tends to be associated with low precision And",00:05:48,5,The reality however high recall tends associated low precision And
7304,00:05:56,3,Evaluation of TR Systems- Basic Measures,2.5,you can imagine why that is the case.,00:05:54,5,imagine case
7305,00:06:00,3,Evaluation of TR Systems- Basic Measures,2.5,As you go down the distant to try to get as many relevant actions as possible.,00:05:56,5,As go distant try get many relevant actions possible
7306,00:06:06,3,Evaluation of TR Systems- Basic Measures,2.5,"You tend to in time a lot of non relevant documents, so the precision goes down.",00:06:00,5,You tend time lot non relevant documents precision goes
7307,00:06:13,3,Evaluation of TR Systems- Basic Measures,2.5,"Look at this set, can also be defined by a cutoff in a ranked list.",00:06:06,5,Look set also defined cutoff ranked list
7308,00:06:16,3,Evaluation of TR Systems- Basic Measures,2.5,"That's why, although these two measures are defined for a set of retrieved",00:06:13,5,That although two measures defined set retrieved
7309,00:06:19,3,Evaluation of TR Systems- Basic Measures,2.5,"documents, they are actually very useful for evaluating a ranked list.",00:06:16,5,documents actually useful evaluating ranked list
7310,00:06:24,3,Evaluation of TR Systems- Basic Measures,2.5,They are the fundamental measures in tension retrieval and many other tasks.,00:06:19,5,They fundamental measures tension retrieval many tasks
7311,00:06:28,3,Evaluation of TR Systems- Basic Measures,2.5,We often are interested in to the precision up to ten documents for,00:06:24,5,We often interested precision ten documents
7312,00:06:30,3,Evaluation of TR Systems- Basic Measures,2.5,web search.,00:06:28,5,web search
7313,00:06:31,3,Evaluation of TR Systems- Basic Measures,2.5,"This means we look at the,",00:06:30,5,This means look
7314,00:06:36,3,Evaluation of TR Systems- Basic Measures,2.5,how many documents among the top results are actually relevant.,00:06:31,5,many documents among top results actually relevant
7315,00:06:38,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, this is a very meaningful measure,",00:06:36,5,Now meaningful measure
7316,00:06:43,3,Evaluation of TR Systems- Basic Measures,2.5,because it tells us how many relevant documents a user can expect to see.,00:06:38,5,tells us many relevant documents user expect see
7317,00:06:47,3,Evaluation of TR Systems- Basic Measures,2.5,"On the first page of search results, where they typically show ten results.",00:06:43,5,On first page search results typically show ten results
7318,00:06:54,3,Evaluation of TR Systems- Basic Measures,2.5,"So, precision and recall are, the basic measures and",00:06:50,5,So precision recall basic measures
7319,00:06:59,3,Evaluation of TR Systems- Basic Measures,2.5,we need to use them to further evaluate a search engine but,00:06:54,5,need use evaluate search engine
7320,00:07:02,3,Evaluation of TR Systems- Basic Measures,2.5,they are the building blocks really.,00:06:59,5,building blocks really
7321,00:07:07,3,Evaluation of TR Systems- Basic Measures,2.5,We just to say that there tends to be a trade off between precision and recall.,00:07:03,5,We say tends trade precision recall
7322,00:07:10,3,Evaluation of TR Systems- Basic Measures,2.5,"So, naturally it would be interesting to [SOUND] combine them and",00:07:07,5,So naturally would interesting SOUND combine
7323,00:07:14,3,Evaluation of TR Systems- Basic Measures,2.5,"here's one measure that's often used, called f measure.",00:07:10,5,one measure often used called f measure
7324,00:07:21,3,Evaluation of TR Systems- Basic Measures,2.5,"And it's harmonic mean of precision and recall, it's defined on this slide.",00:07:14,5,And harmonic mean precision recall defined slide
7325,00:07:26,3,Evaluation of TR Systems- Basic Measures,2.5,"So you can see it first computed,",00:07:21,5,So see first computed
7326,00:07:31,3,Evaluation of TR Systems- Basic Measures,2.5,inverse of R and P here and then it would be,00:07:26,5,inverse R P would
7327,00:07:38,3,Evaluation of TR Systems- Basic Measures,2.5,"interpreted to by using a co, coefficients.",00:07:31,5,interpreted using co coefficients
7328,00:07:41,3,Evaluation of TR Systems- Basic Measures,2.5,Depending on the parameter Beta and,00:07:38,5,Depending parameter Beta
7329,00:07:47,3,Evaluation of TR Systems- Basic Measures,2.5,after some transformation we can easily see it would be of this form.,00:07:41,5,transformation easily see would form
7330,00:07:52,3,Evaluation of TR Systems- Basic Measures,2.5,And in many cases it's just a combination of precision and recall.,00:07:49,5,And many cases combination precision recall
7331,00:07:57,3,Evaluation of TR Systems- Basic Measures,2.5,"And, and Beta is a parameter that's often set to one.",00:07:53,5,And Beta parameter often set one
7332,00:08:00,3,Evaluation of TR Systems- Basic Measures,2.5,It can control the emphasis on precision or recall.,00:07:57,5,It control emphasis precision recall
7333,00:08:02,3,Evaluation of TR Systems- Basic Measures,2.5,"When we set,",00:08:00,5,When set
7334,00:08:08,3,Evaluation of TR Systems- Basic Measures,2.5,"beta to one we end up by having a special case of F measure, often called F1.",00:08:02,5,beta one end special case F measure often called F1
7335,00:08:13,3,Evaluation of TR Systems- Basic Measures,2.5,"This is a popular measure, that is often used as a combined precision and recall.",00:08:08,5,This popular measure often used combined precision recall
7336,00:08:18,3,Evaluation of TR Systems- Basic Measures,2.5,"And the formula looks very simple it's just this, here.",00:08:13,5,And formula looks simple
7337,00:08:24,3,Evaluation of TR Systems- Basic Measures,2.5,"Now it's easy to see that if you have, a larger precision or",00:08:21,5,Now easy see larger precision
7338,00:08:28,3,Evaluation of TR Systems- Basic Measures,2.5,larger recall than F measure would be high.,00:08:24,5,larger recall F measure would high
7339,00:08:32,3,Evaluation of TR Systems- Basic Measures,2.5,"But what's interesting is that, the trade off between precision and",00:08:28,5,But interesting trade precision
7340,00:08:36,3,Evaluation of TR Systems- Basic Measures,2.5,"recall, is captured in an interesting way in F1.",00:08:32,5,recall captured interesting way F1
7341,00:08:45,3,Evaluation of TR Systems- Basic Measures,2.5,"So, in order to understand that, we, can first look at the natural question.",00:08:38,5,So order understand first look natural question
7342,00:08:47,3,Evaluation of TR Systems- Basic Measures,2.5,"Why not just the,",00:08:45,5,Why
7343,00:08:53,3,Evaluation of TR Systems- Basic Measures,2.5,combining them using a simple arithmetic mean as a [INAUDIBLE] here.,00:08:47,5,combining using simple arithmetic mean INAUDIBLE
7344,00:08:56,3,Evaluation of TR Systems- Basic Measures,2.5,That would be likely the most natural way of combining them.,00:08:53,5,That would likely natural way combining
7345,00:09:00,3,Evaluation of TR Systems- Basic Measures,2.5,"So, what do you think?",00:08:59,5,So think
7346,00:09:05,3,Evaluation of TR Systems- Basic Measures,2.5,"If you want to think more, you can pause the media.",00:09:01,5,If want think pause media
7347,00:09:10,3,Evaluation of TR Systems- Basic Measures,2.5,So why is this not as good as F1?,00:09:07,5,So good F1
7348,00:09:15,3,Evaluation of TR Systems- Basic Measures,2.5,Or what's the problem with this?,00:09:13,5,Or problem
7349,00:09:21,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, if you think about the arithmetic mean,",00:09:15,5,Now think arithmetic mean
7350,00:09:28,3,Evaluation of TR Systems- Basic Measures,2.5,"you can see that this is the sum of, of multiple terms.",00:09:21,5,see sum multiple terms
7351,00:09:31,3,Evaluation of TR Systems- Basic Measures,2.5,"In this case, this is the sum of precision and recall.",00:09:28,5,In case sum precision recall
7352,00:09:36,3,Evaluation of TR Systems- Basic Measures,2.5,"In the case of the sum, the total value tends to be dominated by the large values.",00:09:31,5,In case sum total value tends dominated large values
7353,00:09:42,3,Evaluation of TR Systems- Basic Measures,2.5,"That means if you have a very high P or a very high R,",00:09:36,5,That means high P high R
7354,00:09:46,3,Evaluation of TR Systems- Basic Measures,2.5,"then you really don't care about the, whether the other varies is low.",00:09:42,5,really care whether varies low
7355,00:09:47,3,Evaluation of TR Systems- Basic Measures,2.5,"So, the whole sum would be high.",00:09:46,5,So whole sum would high
7356,00:09:53,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, this is not the desirable because one can easily have a perfect recall.",00:09:47,5,Now desirable one easily perfect recall
7357,00:09:57,3,Evaluation of TR Systems- Basic Measures,2.5,We can have a perfect recall is it?,00:09:53,5,We perfect recall
7358,00:09:58,3,Evaluation of TR Systems- Basic Measures,2.5,Can you imagine how?,00:09:57,5,Can imagine
7359,00:10:02,3,Evaluation of TR Systems- Basic Measures,2.5,It's probably very easy to imagine that,00:09:59,5,It probably easy imagine
7360,00:10:05,3,Evaluation of TR Systems- Basic Measures,2.5,"we simply retrieve all the document in the collection,",00:10:02,5,simply retrieve document collection
7361,00:10:09,3,Evaluation of TR Systems- Basic Measures,2.5,then we have a perfect recall and this will give us 0.5 as the average.,00:10:05,5,perfect recall give us 0 5 average
7362,00:10:15,3,Evaluation of TR Systems- Basic Measures,2.5,"But search results are clearly not very useful for users,",00:10:09,5,But search results clearly useful users
7363,00:10:20,3,Evaluation of TR Systems- Basic Measures,2.5,"even though the, the average using this formula would be relatively high.",00:10:15,5,even though average using formula would relatively high
7364,00:10:25,3,Evaluation of TR Systems- Basic Measures,2.5,"Now, in contrast, you can see F1 will reward a case where precision and",00:10:20,5,Now contrast see F1 reward case precision
7365,00:10:28,3,Evaluation of TR Systems- Basic Measures,2.5,recall are roughly but similar.,00:10:25,5,recall roughly similar
7366,00:10:32,3,Evaluation of TR Systems- Basic Measures,2.5,"So, it would paralyze a case where you have extremely high",00:10:28,5,So would paralyze case extremely high
7367,00:10:33,3,Evaluation of TR Systems- Basic Measures,2.5,matter for one of them.,00:10:32,5,matter one
7368,00:10:39,3,Evaluation of TR Systems- Basic Measures,2.5,"So, this means F1 encodes a different trade off between that.",00:10:35,5,So means F1 encodes different trade
7369,00:10:44,3,Evaluation of TR Systems- Basic Measures,2.5,"Now this example shows actually, a very important methodology here.",00:10:39,5,Now example shows actually important methodology
7370,00:10:49,3,Evaluation of TR Systems- Basic Measures,2.5,"When we try to solve a problem, you might naturally think of one solution.",00:10:44,5,When try solve problem might naturally think one solution
7371,00:10:52,3,Evaluation of TR Systems- Basic Measures,2.5,"Let's say, in this case, it's this arithmetic mean.",00:10:49,5,Let say case arithmetic mean
7372,00:10:57,3,Evaluation of TR Systems- Basic Measures,2.5,But it's important that not to settle on this solution.,00:10:53,5,But important settle solution
7373,00:11:00,3,Evaluation of TR Systems- Basic Measures,2.5,It's important to think whether you have other ways to combine them.,00:10:57,5,It important think whether ways combine
7374,00:11:04,3,Evaluation of TR Systems- Basic Measures,2.5,And once you think about the multiple variance.,00:11:02,5,And think multiple variance
7375,00:11:07,3,Evaluation of TR Systems- Basic Measures,2.5,It's important to analyze their difference and,00:11:04,5,It important analyze difference
7376,00:11:10,3,Evaluation of TR Systems- Basic Measures,2.5,then think about which one makes more sense.,00:11:07,5,think one makes sense
7377,00:11:11,3,Evaluation of TR Systems- Basic Measures,2.5,"In this case,",00:11:10,5,In case
7378,00:11:15,3,Evaluation of TR Systems- Basic Measures,2.5,if you think more carefully you will feel that if one problem makes more sense.,00:11:11,5,think carefully feel one problem makes sense
7379,00:11:18,3,Evaluation of TR Systems- Basic Measures,2.5,Then the simple arithmetic mean.,00:11:15,5,Then simple arithmetic mean
7380,00:11:21,3,Evaluation of TR Systems- Basic Measures,2.5,"Although in other cases, there may be, different results.",00:11:18,5,Although cases may different results
7381,00:11:26,3,Evaluation of TR Systems- Basic Measures,2.5,"But in this case, the arithmetic mean, seems not reasonable.",00:11:21,5,But case arithmetic mean seems reasonable
7382,00:11:29,3,Evaluation of TR Systems- Basic Measures,2.5,"But if you don't pay attention to these subtle differences,",00:11:26,5,But pay attention subtle differences
7383,00:11:33,3,Evaluation of TR Systems- Basic Measures,2.5,"you might just, take an easy way to combine them and then go ahead with it.",00:11:29,5,might take easy way combine go ahead
7384,00:11:37,3,Evaluation of TR Systems- Basic Measures,2.5,"And here later you'll find that, hm, the measure doesn't seem to work well.",00:11:33,5,And later find hm measure seem work well
7385,00:11:41,3,Evaluation of TR Systems- Basic Measures,2.5,"Right so, at this methodology is actually very important in",00:11:37,5,Right methodology actually important
7386,00:11:46,3,Evaluation of TR Systems- Basic Measures,2.5,general in solving problem and try to think about the best solution.,00:11:41,5,general solving problem try think best solution
7387,00:11:50,3,Evaluation of TR Systems- Basic Measures,2.5,"Try to understand that the problem, very well and then know why",00:11:46,5,Try understand problem well know
7388,00:11:55,3,Evaluation of TR Systems- Basic Measures,2.5,"you needed this measure, and why you need to combine precision and recall.",00:11:50,5,needed measure need combine precision recall
7389,00:11:59,3,Evaluation of TR Systems- Basic Measures,2.5,And then use that to guide you in finding a good way to solve the problem.,00:11:55,5,And use guide finding good way solve problem
7390,00:12:07,3,Evaluation of TR Systems- Basic Measures,2.5,"To summarize, we talk about precision, which addresses the question,",00:12:03,5,To summarize talk precision addresses question
7391,00:12:11,3,Evaluation of TR Systems- Basic Measures,2.5,are the retrieval results all relevant?,00:12:07,5,retrieval results relevant
7392,00:12:15,3,Evaluation of TR Systems- Basic Measures,2.5,"We'll also talk about the recall, which addresses the question,",00:12:11,5,We also talk recall addresses question
7393,00:12:17,3,Evaluation of TR Systems- Basic Measures,2.5,have all the relevant documents been retrieved?,00:12:15,5,relevant documents retrieved
7394,00:12:21,3,Evaluation of TR Systems- Basic Measures,2.5,These two are the two basic measures in testing retrieval in variation.,00:12:17,5,These two two basic measures testing retrieval variation
7395,00:12:25,3,Evaluation of TR Systems- Basic Measures,2.5,"They are are used for, for many other tasks as well.",00:12:21,5,They used many tasks well
7396,00:12:28,3,Evaluation of TR Systems- Basic Measures,2.5,We'll talk about F measure as a way to combine precision and recall.,00:12:25,5,We talk F measure way combine precision recall
7397,00:12:33,3,Evaluation of TR Systems- Basic Measures,2.5,We also talked about the trade off between precision and recall.,00:12:29,5,We also talked trade precision recall
7398,00:12:37,3,Evaluation of TR Systems- Basic Measures,2.5,And this turns out to depend on the users search tasks and,00:12:33,5,And turns depend users search tasks
7399,00:12:42,3,Evaluation of TR Systems- Basic Measures,2.5,we'll discuss this point more in the later lecture.,00:12:37,5,discuss point later lecture
7400,00:00:05,5,Link Analysis - Part 2,4.3,[SOUND].,00:00:00,4,SOUND
7401,00:00:12,5,Link Analysis - Part 2,4.3,So let's take a look at this in detail.,00:00:09,4,So let take look detail
7402,00:00:13,5,Link Analysis - Part 2,4.3,So in this random surfing model.,00:00:12,4,So random surfing model
7403,00:00:22,5,Link Analysis - Part 2,4.3,And any page would assume random surfer would choose the next page to visit.,00:00:13,4,And page would assume random surfer would choose next page visit
7404,00:00:25,5,Link Analysis - Part 2,4.3,So this is a small graph here.,00:00:22,4,So small graph
7405,00:00:29,5,Link Analysis - Part 2,4.3,"That's, of course an oversimplification of the complicate it well.",00:00:25,4,That course oversimplification complicate well
7406,00:00:31,5,Link Analysis - Part 2,4.3,But let's say there are four documents here.,00:00:29,4,But let say four documents
7407,00:00:34,5,Link Analysis - Part 2,4.3,"Right, D1, D2, D3 and D4.",00:00:33,4,Right D1 D2 D3 D4
7408,00:00:40,5,Link Analysis - Part 2,4.3,And let's assume that a random surfer or random walker can be any of these pages.,00:00:34,4,And let assume random surfer random walker pages
7409,00:00:49,5,Link Analysis - Part 2,4.3,And then the random surfer could decide to just randomly jump into any page.,00:00:41,4,And random surfer could decide randomly jump page
7410,00:00:55,5,Link Analysis - Part 2,4.3,Or follow a link and then visit the next page.,00:00:50,4,Or follow link visit next page
7411,00:00:59,5,Link Analysis - Part 2,4.3,So if the random server is at d1.,00:00:56,4,So random server d1
7412,00:01:06,5,Link Analysis - Part 2,4.3,"Then, you know, with some probability that random surfer will follow the links.",00:01:01,4,Then know probability random surfer follow links
7413,00:01:08,5,Link Analysis - Part 2,4.3,Now there two outlinks here.,00:01:06,4,Now two outlinks
7414,00:01:09,5,Link Analysis - Part 2,4.3,One is pointing to this D3.,00:01:08,4,One pointing D3
7415,00:01:12,5,Link Analysis - Part 2,4.3,The other is pointing to D4.,00:01:09,4,The pointing D4
7416,00:01:17,5,Link Analysis - Part 2,4.3,So the random surfer could pick any of these two to reach e3 and d4.,00:01:12,4,So random surfer could pick two reach e3 d4
7417,00:01:25,5,Link Analysis - Part 2,4.3,"But it also assumes that the random surfer might, get bored sometimes.",00:01:17,4,But also assumes random surfer might get bored sometimes
7418,00:01:29,5,Link Analysis - Part 2,4.3,"So the random surfer would decide to ignore the actual links, and",00:01:25,4,So random surfer would decide ignore actual links
7419,00:01:33,5,Link Analysis - Part 2,4.3,simply randomly jump to any page on the web.,00:01:29,4,simply randomly jump page web
7420,00:01:38,5,Link Analysis - Part 2,4.3,"So, if it does that, eh, it would be able to reach",00:01:33,4,So eh would able reach
7421,00:01:45,5,Link Analysis - Part 2,4.3,any of the other pages even though there is no link directly from to that page.,00:01:38,4,pages even though link directly page
7422,00:01:49,5,Link Analysis - Part 2,4.3,So this is the assume the randoms of.,00:01:46,4,So assume randoms
7423,00:01:54,5,Link Analysis - Part 2,4.3,"Imagine a random server is really doing surfing like this,",00:01:49,4,Imagine random server really surfing like
7424,00:01:56,5,Link Analysis - Part 2,4.3,then we can ask the question.,00:01:54,4,ask question
7425,00:02:00,5,Link Analysis - Part 2,4.3,How likely on average the server would actually reach,00:01:56,4,How likely average server would actually reach
7426,00:02:06,5,Link Analysis - Part 2,4.3,"a particular page d1, or d2, or d3.",00:02:01,4,particular page d1 d2 d3
7427,00:02:09,5,Link Analysis - Part 2,4.3,That's the average probability of visiting a particular page.,00:02:06,4,That average probability visiting particular page
7428,00:02:13,5,Link Analysis - Part 2,4.3,And this probability is precisely what page rank computes.,00:02:10,4,And probability precisely page rank computes
7429,00:02:17,5,Link Analysis - Part 2,4.3,So the page rank score of the document is the average probability,00:02:13,4,So page rank score document average probability
7430,00:02:20,5,Link Analysis - Part 2,4.3,that the surfer visits a particular page.,00:02:17,4,surfer visits particular page
7431,00:02:26,5,Link Analysis - Part 2,4.3,"Now, intuitively this will basically kept you the [INAUDIBLE] link account.",00:02:21,4,Now intuitively basically kept INAUDIBLE link account
7432,00:02:27,5,Link Analysis - Part 2,4.3,Why?,00:02:26,4,Why
7433,00:02:31,5,Link Analysis - Part 2,4.3,Because if a page has a lot of in-links then,00:02:27,4,Because page lot links
7434,00:02:35,5,Link Analysis - Part 2,4.3,"it would have a higher chance of being visited, because there will be more",00:02:31,4,would higher chance visited
7435,00:02:39,5,Link Analysis - Part 2,4.3,opportunities of having the surfer to follow a link to come to this page.,00:02:35,4,opportunities surfer follow link come page
7436,00:02:42,5,Link Analysis - Part 2,4.3,And this is why,00:02:41,4,And
7437,00:02:48,5,Link Analysis - Part 2,4.3,the random surfing model actually captures the idea of counting the in links.,00:02:43,4,random surfing model actually captures idea counting links
7438,00:02:52,5,Link Analysis - Part 2,4.3,Note that is also considers the indirect in links.,00:02:48,4,Note also considers indirect links
7439,00:02:52,5,Link Analysis - Part 2,4.3,Why?,00:02:52,4,Why
7440,00:02:59,5,Link Analysis - Part 2,4.3,"Because if the pages that point to you have themselves a lot of in links,",00:02:52,4,Because pages point lot links
7441,00:03:04,5,Link Analysis - Part 2,4.3,that would mean the random server would very likely reach one of them.,00:02:59,4,would mean random server would likely reach one
7442,00:03:07,5,Link Analysis - Part 2,4.3,And therefore it increases the chance of visiting you.,00:03:04,4,And therefore increases chance visiting
7443,00:03:13,5,Link Analysis - Part 2,4.3,So this is a nice way to capture both indirect and direct links.,00:03:07,4,So nice way capture indirect direct links
7444,00:03:18,5,Link Analysis - Part 2,4.3,"So mathematically, how can we compute this problem enough to see that we need",00:03:13,4,So mathematically compute problem enough see need
7445,00:03:22,5,Link Analysis - Part 2,4.3,to take a look at how this problem [INAUDIBLE] in computing.,00:03:18,4,take look problem INAUDIBLE computing
7446,00:03:25,5,Link Analysis - Part 2,4.3,So first let's take a look at the transition matching sphere.,00:03:22,4,So first let take look transition matching sphere
7447,00:03:29,5,Link Analysis - Part 2,4.3,"And this is just a matrix with values indicating how likely a rand,",00:03:25,4,And matrix values indicating likely rand
7448,00:03:33,5,Link Analysis - Part 2,4.3,the random surfer will go from one page to another.,00:03:29,4,random surfer go one page another
7449,00:03:37,5,Link Analysis - Part 2,4.3,So each rule stands for a starting page.,00:03:33,4,So rule stands starting page
7450,00:03:37,5,Link Analysis - Part 2,4.3,"For example,",00:03:37,4,For example
7451,00:03:43,5,Link Analysis - Part 2,4.3,rule one would indicate the probability of going to any other four pages from e1.,00:03:37,4,rule one would indicate probability going four pages e1
7452,00:03:48,5,Link Analysis - Part 2,4.3,And here we see there are only non two non zero entries.,00:03:43,4,And see non two non zero entries
7453,00:03:53,5,Link Analysis - Part 2,4.3,"Each is 1 over 2, a half.",00:03:49,4,Each 1 2 half
7454,00:04:00,5,Link Analysis - Part 2,4.3,"So this is because if you look at the graph, d1 is pointing to d3 and d4.",00:03:53,4,So look graph d1 pointing d3 d4
7455,00:04:05,5,Link Analysis - Part 2,4.3,"There's no link from d1 to d1 server or d2,",00:04:00,4,There link d1 d1 server d2
7456,00:04:11,5,Link Analysis - Part 2,4.3,so we've got 0s for the first two columns and,00:04:05,4,got 0s first two columns
7457,00:04:14,5,Link Analysis - Part 2,4.3,0.5 for d3 and d4.,00:04:11,4,0 5 d3 d4
7458,00:04:19,5,Link Analysis - Part 2,4.3,"In general, the M in this matrix",00:04:14,4,In general M matrix
7459,00:04:24,5,Link Analysis - Part 2,4.3,"M sub i j is the probability of going from d, i, to d, j.",00:04:19,4,M sub j probability going j
7460,00:04:29,5,Link Analysis - Part 2,4.3,"And obviously for each rule, the values should sum to one,",00:04:24,4,And obviously rule values sum one
7461,00:04:36,5,Link Analysis - Part 2,4.3,because the surfer will have to go to precisely one of these other pages.,00:04:29,4,surfer go precisely one pages
7462,00:04:37,5,Link Analysis - Part 2,4.3,Right? So this is a transition matrix.,00:04:36,4,Right So transition matrix
7463,00:04:43,5,Link Analysis - Part 2,4.3,Now how can we compute the probability of a server visiting a page?,00:04:39,4,Now compute probability server visiting page
7464,00:04:49,5,Link Analysis - Part 2,4.3,"Well if you look at the, the server model, then basically",00:04:44,4,Well look server model basically
7465,00:04:56,5,Link Analysis - Part 2,4.3,we can compute the probability of reaching a page as follows.,00:04:50,4,compute probability reaching page follows
7466,00:05:02,5,Link Analysis - Part 2,4.3,"So, here on the left-hand side, you see it's the probability of",00:04:56,4,So left hand side see probability
7467,00:05:07,5,Link Analysis - Part 2,4.3,visiting page DJ at time t plus 1 because it's the next time cont.,00:05:02,4,visiting page DJ time plus 1 next time cont
7468,00:05:14,5,Link Analysis - Part 2,4.3,"On the right hand side, you can see the question involves the probability",00:05:08,4,On right hand side see question involves probability
7469,00:05:20,5,Link Analysis - Part 2,4.3,"of, at page ei at time t.",00:05:14,4,page ei time
7470,00:05:24,5,Link Analysis - Part 2,4.3,"So you can see the subsequent index t, here.",00:05:21,4,So see subsequent index
7471,00:05:27,5,Link Analysis - Part 2,4.3,And that indicates that's the probability,00:05:24,4,And indicates probability
7472,00:05:32,5,Link Analysis - Part 2,4.3,that the server was at a document at time t.,00:05:27,4,server document time
7473,00:05:38,5,Link Analysis - Part 2,4.3,So the equation basically captures,00:05:34,4,So equation basically captures
7474,00:05:43,5,Link Analysis - Part 2,4.3,the two possibilities of reaching at d j at time t plus 1.,00:05:38,4,two possibilities reaching j time plus 1
7475,00:05:45,5,Link Analysis - Part 2,4.3,What are these two possibilities?,00:05:43,4,What two possibilities
7476,00:05:48,5,Link Analysis - Part 2,4.3,"Well one is through random surfing, and",00:05:45,4,Well one random surfing
7477,00:05:51,5,Link Analysis - Part 2,4.3,one is through following a link as we just explained.,00:05:48,4,one following link explained
7478,00:05:57,5,Link Analysis - Part 2,4.3,So the first part captures the probability,00:05:53,4,So first part captures probability
7479,00:06:01,5,Link Analysis - Part 2,4.3,that the random server would reach this page by following a link.,00:05:57,4,random server would reach page following link
7480,00:06:05,5,Link Analysis - Part 2,4.3,"And you can see, and the random surfer chooses this",00:06:01,4,And see random surfer chooses
7481,00:06:10,5,Link Analysis - Part 2,4.3,strategy was probably the [INAUDIBLE] as we assumed.,00:06:05,4,strategy probably INAUDIBLE assumed
7482,00:06:13,5,Link Analysis - Part 2,4.3,And so there is a factor of one minus alpha here.,00:06:10,4,And factor one minus alpha
7483,00:06:18,5,Link Analysis - Part 2,4.3,But the main part is really sum over all the possible,00:06:13,4,But main part really sum possible
7484,00:06:23,5,Link Analysis - Part 2,4.3,"pages that the server could have been at time t, right?",00:06:18,4,pages server could time right
7485,00:06:27,5,Link Analysis - Part 2,4.3,"There were N pages, so it's a sum over all the possible N pages.",00:06:23,4,There N pages sum possible N pages
7486,00:06:31,5,Link Analysis - Part 2,4.3,Inside the sum is the product of two probabilities.,00:06:27,4,Inside sum product two probabilities
7487,00:06:38,5,Link Analysis - Part 2,4.3,One is the probability that the server was at d i at time t.,00:06:31,4,One probability server time
7488,00:06:41,5,Link Analysis - Part 2,4.3,That's p sub t of d i.,00:06:38,4,That p sub
7489,00:06:46,5,Link Analysis - Part 2,4.3,The other is the transition probability from di to dj.,00:06:42,4,The transition probability di dj
7490,00:06:52,5,Link Analysis - Part 2,4.3,"And so in order to reach this dj page,",00:06:46,4,And order reach dj page
7491,00:06:57,5,Link Analysis - Part 2,4.3,the surfer must first be at di at time t.,00:06:52,4,surfer must first di time
7492,00:07:03,5,Link Analysis - Part 2,4.3,"And then also would have to follow the link to go from di to dj,",00:06:57,4,And also would follow link go di dj
7493,00:07:09,5,Link Analysis - Part 2,4.3,"so the probability is the probability of being at di at time t, not divide by",00:07:03,4,probability probability di time divide
7494,00:07:14,5,Link Analysis - Part 2,4.3,"the probability of, going from that page to the top of the page dj here.",00:07:09,4,probability going page top page dj
7495,00:07:17,5,Link Analysis - Part 2,4.3,The second part is a similar sum.,00:07:15,4,The second part similar sum
7496,00:07:23,5,Link Analysis - Part 2,4.3,"The only difference is that now the transition probability is uniform,",00:07:17,4,The difference transition probability uniform
7497,00:07:24,5,Link Analysis - Part 2,4.3,transition probability.,00:07:23,4,transition probability
7498,00:07:27,5,Link Analysis - Part 2,4.3,1 over n. And this part captures the probability of,00:07:24,4,1 n And part captures probability
7499,00:07:31,5,Link Analysis - Part 2,4.3,"reaching this page, through random jumping.",00:07:27,4,reaching page random jumping
7500,00:07:35,5,Link Analysis - Part 2,4.3,"Right. So, the form is exactly the same.",00:07:32,4,Right So form exactly
7501,00:07:38,5,Link Analysis - Part 2,4.3,"And in, in, this also allows us to see",00:07:35,4,And also allows us see
7502,00:07:43,5,Link Analysis - Part 2,4.3,why PageRank essentially assumes smoothing of the transition matrix.,00:07:38,4,PageRank essentially assumes smoothing transition matrix
7503,00:07:49,5,Link Analysis - Part 2,4.3,If you think about this 1 over N as coming from another transition matrix,00:07:43,4,If think 1 N coming another transition matrix
7504,00:07:55,5,Link Analysis - Part 2,4.3,"that has all the elements being 1 over N, the uniform matrix.",00:07:49,4,elements 1 N uniform matrix
7505,00:07:59,5,Link Analysis - Part 2,4.3,Then you can see very clearly essentially we can merge the two parts.,00:07:55,4,Then see clearly essentially merge two parts
7506,00:08:01,5,Link Analysis - Part 2,4.3,"Because they are of the same form,",00:07:59,4,Because form
7507,00:08:07,5,Link Analysis - Part 2,4.3,we can imagine there's a difference of metrics that's a combination of this m and,00:08:01,4,imagine difference metrics combination
7508,00:08:11,5,Link Analysis - Part 2,4.3,that uniform matrix where every element is 1 over n.,00:08:07,4,uniform matrix every element 1 n
7509,00:08:16,5,Link Analysis - Part 2,4.3,"In this sense, page one uses this idea of smoothing and",00:08:11,4,In sense page one uses idea smoothing
7510,00:08:21,5,Link Analysis - Part 2,4.3,"ensuring that there's no 0, entry in such a transition matrix.",00:08:16,4,ensuring 0 entry transition matrix
7511,00:08:28,5,Link Analysis - Part 2,4.3,"Of course this is, time depend, calculation of probabilities.",00:08:22,4,Of course time depend calculation probabilities
7512,00:08:32,5,Link Analysis - Part 2,4.3,"Now, we can imagine if we want to compute average probabilities,",00:08:28,4,Now imagine want compute average probabilities
7513,00:08:36,5,Link Analysis - Part 2,4.3,the average probabilities probably would satisfy this equation,00:08:32,4,average probabilities probably would satisfy equation
7514,00:08:38,5,Link Analysis - Part 2,4.3,without considering the time index.,00:08:36,4,without considering time index
7515,00:08:41,5,Link Analysis - Part 2,4.3,So let's drop the time index and just assume that they would be equal.,00:08:38,4,So let drop time index assume would equal
7516,00:08:46,5,Link Analysis - Part 2,4.3,Now this would give us N equations.,00:08:42,4,Now would give us N equations
7517,00:08:49,5,Link Analysis - Part 2,4.3,Because for each page we have such a equation.,00:08:46,4,Because page equation
7518,00:08:52,5,Link Analysis - Part 2,4.3,"And if you look at the what variables we have in these equations,",00:08:49,4,And look variables equations
7519,00:08:56,5,Link Analysis - Part 2,4.3,"there are also precisely N variables, right?",00:08:52,4,also precisely N variables right
7520,00:09:03,5,Link Analysis - Part 2,4.3,So this basically means we now have a system of,00:08:58,4,So basically means system
7521,00:09:10,5,Link Analysis - Part 2,4.3,"n equations with n variables, and these are linear equations.",00:09:04,4,n equations n variables linear equations
7522,00:09:16,5,Link Analysis - Part 2,4.3,"So basically, now the problem boils down to solve this system of equations and",00:09:10,4,So basically problem boils solve system equations
7523,00:09:20,5,Link Analysis - Part 2,4.3,here I also show that the equations in the metric form.,00:09:16,4,I also show equations metric form
7524,00:09:29,5,Link Analysis - Part 2,4.3,It's the vector P here equals a metrics or the transports of the metrics here.,00:09:20,4,It vector P equals metrics transports metrics
7525,00:09:31,5,Link Analysis - Part 2,4.3,And multiply it by the vector again.,00:09:29,4,And multiply vector
7526,00:09:37,5,Link Analysis - Part 2,4.3,Now if you still remember some knowledge that you learned from linear algebra and,00:09:32,4,Now still remember knowledge learned linear algebra
7527,00:09:41,5,Link Analysis - Part 2,4.3,then you will realize this is precisely the equation for item vector.,00:09:37,4,realize precisely equation item vector
7528,00:09:44,5,Link Analysis - Part 2,4.3,Right? When [INAUDIBLE] metrics by this method,00:09:41,4,Right When INAUDIBLE metrics method
7529,00:09:47,5,Link Analysis - Part 2,4.3,you get the same value as this method.,00:09:44,4,get value method
7530,00:09:52,5,Link Analysis - Part 2,4.3,And this can solved by using an iterative algorithm.,00:09:47,4,And solved using iterative algorithm
7531,00:10:01,5,Link Analysis - Part 2,4.3,"So is it, because she's here, on the ball, easily taken from the previous, slide.",00:09:54,4,So ball easily taken previous slide
7532,00:10:09,5,Link Analysis - Part 2,4.3,"So you see the, relationship between the, the page source of different pages.",00:10:01,4,So see relationship page source different pages
7533,00:10:13,5,Link Analysis - Part 2,4.3,And in this iterative approach or power approach,00:10:09,4,And iterative approach power approach
7534,00:10:18,5,Link Analysis - Part 2,4.3,"we simply start with, randomly the p.",00:10:13,4,simply start randomly p
7535,00:10:25,5,Link Analysis - Part 2,4.3,And then we repeatedly just updated this p by multiplying.,00:10:18,4,And repeatedly updated p multiplying
7536,00:10:30,5,Link Analysis - Part 2,4.3,The metrics here by this P-Vector.,00:10:26,4,The metrics P Vector
7537,00:10:34,5,Link Analysis - Part 2,4.3,So I also show a concrete example here.,00:10:30,4,So I also show concrete example
7538,00:10:41,5,Link Analysis - Part 2,4.3,"So you can see this now, if we assume.",00:10:37,4,So see assume
7539,00:10:43,5,Link Analysis - Part 2,4.3,How far is point two.,00:10:41,4,How far point two
7540,00:10:48,5,Link Analysis - Part 2,4.3,Then with the example that we show here on this slide,00:10:43,4,Then example show slide
7541,00:10:51,5,Link Analysis - Part 2,4.3,we have the original transition metrics here.,00:10:48,4,original transition metrics
7542,00:10:55,5,Link Analysis - Part 2,4.3,"Right? That encodes, that encodes the graph.",00:10:53,4,Right That encodes encodes graph
7543,00:10:57,5,Link Analysis - Part 2,4.3,The actual links.,00:10:55,4,The actual links
7544,00:11:00,5,Link Analysis - Part 2,4.3,"And we have this smoothing transition metrics,",00:10:57,4,And smoothing transition metrics
7545,00:11:05,5,Link Analysis - Part 2,4.3,"uniform transition metrics, representing random jumping.",00:11:00,4,uniform transition metrics representing random jumping
7546,00:11:07,5,Link Analysis - Part 2,4.3,And we can combine them together with,00:11:05,4,And combine together
7547,00:11:12,5,Link Analysis - Part 2,4.3,interpolation to form another metrics that would be like this.,00:11:07,4,interpolation form another metrics would like
7548,00:11:15,5,Link Analysis - Part 2,4.3,So essentially we can imagine now the looks.,00:11:12,4,So essentially imagine looks
7549,00:11:18,5,Link Analysis - Part 2,4.3,Like this can be captured by that.,00:11:15,4,Like captured
7550,00:11:21,5,Link Analysis - Part 2,4.3,There are virtual links between all the pages now.,00:11:18,4,There virtual links pages
7551,00:11:27,5,Link Analysis - Part 2,4.3,"So the page rank algorithm will just initialize the p vector first,",00:11:21,4,So page rank algorithm initialize p vector first
7552,00:11:30,5,Link Analysis - Part 2,4.3,and then just computed the updating of this p vector,00:11:27,4,computed updating p vector
7553,00:11:34,5,Link Analysis - Part 2,4.3,"by using this, metrics multiplication.",00:11:32,4,using metrics multiplication
7554,00:11:39,5,Link Analysis - Part 2,4.3,"Now if you rewrite this metrics multi, multiplication",00:11:36,4,Now rewrite metrics multi multiplication
7555,00:11:45,5,Link Analysis - Part 2,4.3,"in terms of just a, an individual equations, you'll see this.",00:11:41,4,terms individual equations see
7556,00:11:51,5,Link Analysis - Part 2,4.3,"And this is a, basically, the updating formula for",00:11:46,4,And basically updating formula
7557,00:11:54,5,Link Analysis - Part 2,4.3,"this particular page is a, page ranking score.",00:11:51,4,particular page page ranking score
7558,00:11:57,5,Link Analysis - Part 2,4.3,"So you can also see, even if you want to compute the value of this",00:11:54,4,So also see even want compute value
7559,00:12:01,5,Link Analysis - Part 2,4.3,"updated score for d1, you basically multiple this rule.",00:11:57,4,updated score d1 basically multiple rule
7560,00:12:03,5,Link Analysis - Part 2,4.3,Right?,00:12:03,4,Right
7561,00:12:09,5,Link Analysis - Part 2,4.3,"By this column, I will take the total product of the two, right?",00:12:03,4,By column I take total product two right
7562,00:12:13,5,Link Analysis - Part 2,4.3,And that will give us the value for this value.,00:12:09,4,And give us value value
7563,00:12:18,5,Link Analysis - Part 2,4.3,So this is how we updated the vector.,00:12:15,4,So updated vector
7564,00:12:20,5,Link Analysis - Part 2,4.3,We started with some initial values for these guys.,00:12:18,4,We started initial values guys
7565,00:12:24,5,Link Analysis - Part 2,4.3,"For, for this, and then,",00:12:22,4,For
7566,00:12:29,5,Link Analysis - Part 2,4.3,we just revise the scores which generate a new set of scores.,00:12:24,4,revise scores generate new set scores
7567,00:12:31,5,Link Analysis - Part 2,4.3,And the updated formula is this one.,00:12:29,4,And updated formula one
7568,00:12:37,5,Link Analysis - Part 2,4.3,"So we just repeatedly apply this, and here it converges.",00:12:33,4,So repeatedly apply converges
7569,00:12:40,5,Link Analysis - Part 2,4.3,And when the metrics is like this.,00:12:37,4,And metrics like
7570,00:12:43,5,Link Analysis - Part 2,4.3,Where there is no zero values and it can be guaranteed to converge.,00:12:40,4,Where zero values guaranteed converge
7571,00:12:49,5,Link Analysis - Part 2,4.3,"And at that point we will just, have the PageRank scores for all the pages.",00:12:44,4,And point PageRank scores pages
7572,00:12:53,5,Link Analysis - Part 2,4.3,Now we typically set the initial values just to 1 over n.,00:12:49,4,Now typically set initial values 1 n
7573,00:12:59,5,Link Analysis - Part 2,4.3,"So interestingly, this update formula can be also interpreted as",00:12:55,4,So interestingly update formula also interpreted
7574,00:13:01,5,Link Analysis - Part 2,4.3,propagating scores on the graph.,00:12:59,4,propagating scores graph
7575,00:13:03,5,Link Analysis - Part 2,4.3,All right. Can you see why?,00:13:01,4,All right Can see
7576,00:13:08,5,Link Analysis - Part 2,4.3,"Well if you look at this formula and then compare that with this graph,",00:13:03,4,Well look formula compare graph
7577,00:13:11,5,Link Analysis - Part 2,4.3,and can you imagine how we might be able to interpret this,00:13:08,4,imagine might able interpret
7578,00:13:15,5,Link Analysis - Part 2,4.3,as essentially propagating scores over the graph.,00:13:11,4,essentially propagating scores graph
7579,00:13:21,5,Link Analysis - Part 2,4.3,I hope you will see that indeed we can imagine we have values,00:13:17,4,I hope see indeed imagine values
7580,00:13:24,5,Link Analysis - Part 2,4.3,initialized on each of these page.,00:13:21,4,initialized page
7581,00:13:30,5,Link Analysis - Part 2,4.3,"All right, so we can have values here that say, that's one over four for each.",00:13:24,4,All right values say one four
7582,00:13:35,5,Link Analysis - Part 2,4.3,"And then welcome to use these matrix to update this, the scores.",00:13:30,4,And welcome use matrix update scores
7583,00:13:38,5,Link Analysis - Part 2,4.3,"And if you look at the equation here,",00:13:35,4,And look equation
7584,00:13:43,5,Link Analysis - Part 2,4.3,"this one, basically we're going to combine the scores",00:13:38,4,one basically going combine scores
7585,00:13:48,5,Link Analysis - Part 2,4.3,"of the pages that possible would lead to, reaching this page.",00:13:43,4,pages possible would lead reaching page
7586,00:13:52,5,Link Analysis - Part 2,4.3,So we'll look at all the pages that are pointing to this page.,00:13:48,4,So look pages pointing page
7587,00:13:54,5,Link Analysis - Part 2,4.3,And then combine their scores and,00:13:52,4,And combine scores
7588,00:14:00,5,Link Analysis - Part 2,4.3,"the propagated score, the sum of the scores to this document D1.",00:13:54,4,propagated score sum scores document D1
7589,00:14:06,5,Link Analysis - Part 2,4.3,"We look after the, the scores that represented the probability",00:14:00,4,We look scores represented probability
7590,00:14:11,5,Link Analysis - Part 2,4.3,that the random server would be visiting the other pages before it reaches the D1.,00:14:06,4,random server would visiting pages reaches D1
7591,00:14:14,5,Link Analysis - Part 2,4.3,And then just do the propagation to simulate the probability,00:14:11,4,And propagation simulate probability
7592,00:14:20,5,Link Analysis - Part 2,4.3,"of reaching this, this page D 1.",00:14:16,4,reaching page D 1
7593,00:14:23,5,Link Analysis - Part 2,4.3,So there are two interpretations here.,00:14:20,4,So two interpretations
7594,00:14:26,5,Link Analysis - Part 2,4.3,One is just the matrix multiplication.,00:14:23,4,One matrix multiplication
7595,00:14:27,5,Link Analysis - Part 2,4.3,And we repeated that.,00:14:26,4,And repeated
7596,00:14:31,5,Link Analysis - Part 2,4.3,Multiply the vector by this metrics.,00:14:27,4,Multiply vector metrics
7597,00:14:36,5,Link Analysis - Part 2,4.3,The other is to just think of it as propagating the scores repeatedly,00:14:31,4,The think propagating scores repeatedly
7598,00:14:38,5,Link Analysis - Part 2,4.3,on the web.,00:14:36,4,web
7599,00:14:44,5,Link Analysis - Part 2,4.3,So in practice the composition of PageRank score is actually efficient because,00:14:38,4,So practice composition PageRank score actually efficient
7600,00:14:49,5,Link Analysis - Part 2,4.3,the metrices are sparse and there are some ways to transform the equation so,00:14:44,4,metrices sparse ways transform equation
7601,00:14:55,5,Link Analysis - Part 2,4.3,you avoid actually literally computing the values of all of those elements.,00:14:49,4,avoid actually literally computing values elements
7602,00:14:59,5,Link Analysis - Part 2,4.3,"Sometimes you may also normalize the equation, and",00:14:56,4,Sometimes may also normalize equation
7603,00:15:02,5,Link Analysis - Part 2,4.3,"that will give you a somewhat different form of the equation,",00:14:59,4,give somewhat different form equation
7604,00:15:05,5,Link Analysis - Part 2,4.3,but then the ranking of pages will not change.,00:15:02,4,ranking pages change
7605,00:15:09,5,Link Analysis - Part 2,4.3,The results of this potential problem of zero out link problem.,00:15:06,4,The results potential problem zero link problem
7606,00:15:17,5,Link Analysis - Part 2,4.3,"In that case if the page does not have any outlook, then the probability of",00:15:10,4,In case page outlook probability
7607,00:15:22,5,Link Analysis - Part 2,4.3,"these pages will, will not sum to 1.",00:15:17,4,pages sum 1
7608,00:15:25,5,Link Analysis - Part 2,4.3,"Basically, the probability of reaching the next page from this",00:15:22,4,Basically probability reaching next page
7609,00:15:26,5,Link Analysis - Part 2,4.3,page will not sum to 1.,00:15:25,4,page sum 1
7610,00:15:29,5,Link Analysis - Part 2,4.3,Mainly because we have lost some probability mass when we assume that,00:15:26,4,Mainly lost probability mass assume
7611,00:15:34,5,Link Analysis - Part 2,4.3,there's some probability that the server will try to follow links but,00:15:29,4,probability server try follow links
7612,00:15:37,5,Link Analysis - Part 2,4.3,"then there's no link to follow, right?",00:15:34,4,link follow right
7613,00:15:43,5,Link Analysis - Part 2,4.3,And one possible solution is simply to use page specific damping factor and,00:15:37,4,And one possible solution simply use page specific damping factor
7614,00:15:45,5,Link Analysis - Part 2,4.3,"that, that could easily fix this.",00:15:43,4,could easily fix
7615,00:15:50,5,Link Analysis - Part 2,4.3,"Basically that's to say, how far do we want from zero for a page with no outlink.",00:15:46,4,Basically say far want zero page outlink
7616,00:15:54,5,Link Analysis - Part 2,4.3,In that case the server would just have to,00:15:50,4,In case server would
7617,00:15:57,5,Link Analysis - Part 2,4.3,render them [INAUDIBLE] to another page instead of trying to follow the link.,00:15:54,4,render INAUDIBLE another page instead trying follow link
7618,00:16:01,5,Link Analysis - Part 2,4.3,So there are many extensions of page rank.,00:15:59,4,So many extensions page rank
7619,00:16:05,5,Link Analysis - Part 2,4.3,One extension is to do top-specific page rank.,00:16:01,4,One extension top specific page rank
7620,00:16:09,5,Link Analysis - Part 2,4.3,"Note that page rank doesn't really use the query format machine, right?",00:16:05,4,Note page rank really use query format machine right
7621,00:16:15,5,Link Analysis - Part 2,4.3,"So, [INAUDIBLE] so we can make page rank, appear specific, however.",00:16:09,4,So INAUDIBLE make page rank appear specific however
7622,00:16:18,5,Link Analysis - Part 2,4.3,"So, for example, in the topic specific page rank,",00:16:15,4,So example topic specific page rank
7623,00:16:22,5,Link Analysis - Part 2,4.3,"we can simply assume when the surfer, is bored.",00:16:18,4,simply assume surfer bored
7624,00:16:25,5,Link Analysis - Part 2,4.3,The surfer is not going to randomly jump into any page on the web.,00:16:22,4,The surfer going randomly jump page web
7625,00:16:32,5,Link Analysis - Part 2,4.3,"Instead, it's going to jump, to only those pages that are to a query.",00:16:25,4,Instead going jump pages query
7626,00:16:36,5,Link Analysis - Part 2,4.3,"For example, if the query is about sports then we could assume that when it's",00:16:32,4,For example query sports could assume
7627,00:16:40,5,Link Analysis - Part 2,4.3,"doing random jumping, it's going to randomly jump to a sports page.",00:16:36,4,random jumping going randomly jump sports page
7628,00:16:45,5,Link Analysis - Part 2,4.3,By doing this then we canbuy a PageRank to topic align with sports.,00:16:40,4,By canbuy PageRank topic align sports
7629,00:16:49,5,Link Analysis - Part 2,4.3,And then if you know the current query is about sports then you can use this,00:16:45,4,And know current query sports use
7630,00:16:53,5,Link Analysis - Part 2,4.3,specialized PageRank score to rank the options.,00:16:49,4,specialized PageRank score rank options
7631,00:16:56,5,Link Analysis - Part 2,4.3,That would be better than if you use a generic PageRank score.,00:16:53,4,That would better use generic PageRank score
7632,00:17:01,5,Link Analysis - Part 2,4.3,PageRank is also general algorithm that can be used in many other.,00:16:57,4,PageRank also general algorithm used many
7633,00:17:05,5,Link Analysis - Part 2,4.3,"Locations for network analysis, particular for example for social networks.",00:17:01,4,Locations network analysis particular example social networks
7634,00:17:10,5,Link Analysis - Part 2,4.3,"We can imagine if you compute their PageRank scores for social network,",00:17:05,4,We imagine compute PageRank scores social network
7635,00:17:13,5,Link Analysis - Part 2,4.3,"where a link might indicate friendship relation,",00:17:10,4,link might indicate friendship relation
7636,00:17:16,5,Link Analysis - Part 2,4.3,you'll get some meaningful scores for people.,00:17:13,4,get meaningful scores people
7637,00:00:04,5,Learning to Rank - Part 3,4.4,[NOISE].,00:00:00,8,NOISE
7638,00:00:10,5,Learning to Rank - Part 3,4.4,There are many more advanced learning algorithms than the regression based,00:00:06,8,There many advanced learning algorithms regression based
7639,00:00:11,5,Learning to Rank - Part 3,4.4,reproaches.,00:00:10,8,reproaches
7640,00:00:13,5,Learning to Rank - Part 3,4.4,And they generally account to theoretically,00:00:11,8,And generally account theoretically
7641,00:00:14,5,Learning to Rank - Part 3,4.4,optimize or retrieval method.,00:00:13,8,optimize retrieval method
7642,00:00:18,5,Learning to Rank - Part 3,4.4,Like map or nDCG.,00:00:16,8,Like map nDCG
7643,00:00:24,5,Learning to Rank - Part 3,4.4,Note that the optimization objecting function that we have seen,00:00:19,8,Note optimization objecting function seen
7644,00:00:29,5,Learning to Rank - Part 3,4.4,on the previous slide is not directly related to retrieval measure.,00:00:24,8,previous slide directly related retrieval measure
7645,00:00:31,5,Learning to Rank - Part 3,4.4,Right?,00:00:31,8,Right
7646,00:00:34,5,Learning to Rank - Part 3,4.4,By maximizing the prediction of one or zero.,00:00:31,8,By maximizing prediction one zero
7647,00:00:39,5,Learning to Rank - Part 3,4.4,Or we don't necessarily optimize the ranking of those documents.,00:00:34,8,Or necessarily optimize ranking documents
7648,00:00:44,5,Learning to Rank - Part 3,4.4,"One can imagine that why, our prediction may not be too bad and",00:00:39,8,One imagine prediction may bad
7649,00:00:45,5,Learning to Rank - Part 3,4.4,let's say both are around 0.5.,00:00:44,8,let say around 0 5
7650,00:00:49,5,Learning to Rank - Part 3,4.4,So it's kind of in the middle of zero and one for,00:00:45,8,So kind middle zero one
7651,00:00:53,5,Learning to Rank - Part 3,4.4,"the two documents, but the ranking can be wrong.",00:00:49,8,two documents ranking wrong
7652,00:00:56,5,Learning to Rank - Part 3,4.4,"So we might have the, a larger value for.",00:00:53,8,So might larger value
7653,00:00:58,5,Learning to Rank - Part 3,4.4,D2 and then e1.,00:00:56,8,D2 e1
7654,00:01:03,5,Learning to Rank - Part 3,4.4,"So that won't be good from retrieval perspective,",00:01:00,8,So good retrieval perspective
7655,00:01:07,5,Learning to Rank - Part 3,4.4,"even though by likelihood function, it's not bad.",00:01:03,8,even though likelihood function bad
7656,00:01:10,5,Learning to Rank - Part 3,4.4,"In contrast, we might have another case where we predicted values.",00:01:07,8,In contrast might another case predicted values
7657,00:01:14,5,Learning to Rank - Part 3,4.4,"Or around 0.9 let's say,",00:01:10,8,Or around 0 9 let say
7658,00:01:17,5,Learning to Rank - Part 3,4.4,"and by the objective function, the error will be larger, but if we",00:01:14,8,objective function error larger
7659,00:01:22,5,Learning to Rank - Part 3,4.4,"can get the order of the two documents correct, that's actually a better result.",00:01:17,8,get order two documents correct actually better result
7660,00:01:28,5,Learning to Rank - Part 3,4.4,So these new more advanced approaches will try to correct that problem.,00:01:22,8,So new advanced approaches try correct problem
7661,00:01:30,5,Learning to Rank - Part 3,4.4,Of course then the challenge is that.,00:01:28,8,Of course challenge
7662,00:01:33,5,Learning to Rank - Part 3,4.4,That the optimization problem will be harder to solve.,00:01:30,8,That optimization problem harder solve
7663,00:01:37,5,Learning to Rank - Part 3,4.4,And then researchers have proposed many solutions to the problem.,00:01:33,8,And researchers proposed many solutions problem
7664,00:01:42,5,Learning to Rank - Part 3,4.4,And you can read more of the references at the end.,00:01:37,8,And read references end
7665,00:01:46,5,Learning to Rank - Part 3,4.4,Know more about the these approaches.,00:01:42,8,Know approaches
7666,00:01:49,5,Learning to Rank - Part 3,4.4,Now these learning to random approaches.,00:01:46,8,Now learning random approaches
7667,00:01:53,5,Learning to Rank - Part 3,4.4,"Are actually general, so they can also be applied to many other ranking problems,",00:01:49,8,Are actually general also applied many ranking problems
7668,00:01:55,5,Learning to Rank - Part 3,4.4,not just retrieval problem.,00:01:53,8,retrieval problem
7669,00:01:58,5,Learning to Rank - Part 3,4.4,"So here I list some for example recommender systems,",00:01:55,8,So I list example recommender systems
7670,00:02:03,5,Learning to Rank - Part 3,4.4,"computational adv, advertising, or summarization, and",00:01:58,8,computational adv advertising summarization
7671,00:02:08,5,Learning to Rank - Part 3,4.4,there are many others that you can probably encounter in your applications.,00:02:03,8,many others probably encounter applications
7672,00:02:16,5,Learning to Rank - Part 3,4.4,"To summarize this lecture, we have talked about, using machine",00:02:11,8,To summarize lecture talked using machine
7673,00:02:21,5,Learning to Rank - Part 3,4.4,learning to combine much more features to incorporate a ranking without.,00:02:16,8,learning combine much features incorporate ranking without
7674,00:02:24,5,Learning to Rank - Part 3,4.4,"Actually the use of machine learning,",00:02:22,8,Actually use machine learning
7675,00:02:29,5,Learning to Rank - Part 3,4.4,in information retrieval has started since many decades ago.,00:02:25,8,information retrieval started since many decades ago
7676,00:02:35,5,Learning to Rank - Part 3,4.4,So for example on the Rocchio feedback approach that we talked about earlier,00:02:29,8,So example Rocchio feedback approach talked earlier
7677,00:02:40,5,Learning to Rank - Part 3,4.4,"was a machine learning approach applied to to learn this feedback, but",00:02:35,8,machine learning approach applied learn feedback
7678,00:02:47,5,Learning to Rank - Part 3,4.4,the most reasonable use of machine learning has been driven by some changes.,00:02:40,8,reasonable use machine learning driven changes
7679,00:02:51,5,Learning to Rank - Part 3,4.4,In the environment of applications of retrieval systems.,00:02:47,8,In environment applications retrieval systems
7680,00:02:54,5,Learning to Rank - Part 3,4.4,"And first it's, mostly,",00:02:51,8,And first mostly
7681,00:03:00,5,Learning to Rank - Part 3,4.4,driven by the availability of a lot of training data in the form of clicks rules.,00:02:55,8,driven availability lot training data form clicks rules
7682,00:03:04,5,Learning to Rank - Part 3,4.4,Such data weren't available before.,00:03:01,8,Such data available
7683,00:03:10,5,Learning to Rank - Part 3,4.4,So the data can provide a lot of useful knowledge about,00:03:04,8,So data provide lot useful knowledge
7684,00:03:15,5,Learning to Rank - Part 3,4.4,relevance and machine learning methods can be applied to leverage this.,00:03:10,8,relevance machine learning methods applied leverage
7685,00:03:20,5,Learning to Rank - Part 3,4.4,Secondly it's also due by the need of combining them.,00:03:17,8,Secondly also due need combining
7686,00:03:21,5,Learning to Rank - Part 3,4.4,In the features. And,00:03:20,8,In features And
7687,00:03:26,5,Learning to Rank - Part 3,4.4,this is not only just because there are more features available on the web,00:03:21,8,features available web
7688,00:03:29,5,Learning to Rank - Part 3,4.4,that can be naturally re-used with improved scoring.,00:03:26,8,naturally used improved scoring
7689,00:03:35,5,Learning to Rank - Part 3,4.4,"It's also because by combining them, we can improve the robustness of ranking.",00:03:29,8,It also combining improve robustness ranking
7690,00:03:39,5,Learning to Rank - Part 3,4.4,So this is designed for combating spams.,00:03:35,8,So designed combating spams
7691,00:03:45,5,Learning to Rank - Part 3,4.4,Modern search engines all use some kind of machine learning techniques to combine,00:03:41,8,Modern search engines use kind machine learning techniques combine
7692,00:03:48,5,Learning to Rank - Part 3,4.4,many features to optimize ranking and,00:03:45,8,many features optimize ranking
7693,00:03:53,5,Learning to Rank - Part 3,4.4,"this is a major feature of these current engines such as Google, Bing.",00:03:48,8,major feature current engines Google Bing
7694,00:03:59,5,Learning to Rank - Part 3,4.4,The topic of learning to rank is still active research.,00:03:56,8,The topic learning rank still active research
7695,00:04:04,5,Learning to Rank - Part 3,4.4,"Topic in the community, and so you can expect to see new results being developed,",00:03:59,8,Topic community expect see new results developed
7696,00:04:08,5,Learning to Rank - Part 3,4.4,"in the next, few years.",00:04:06,8,next years
7697,00:04:09,5,Learning to Rank - Part 3,4.4,Perhaps.,00:04:08,8,Perhaps
7698,00:04:16,5,Learning to Rank - Part 3,4.4,Here are some additional readings that can give you more information about.,00:04:12,8,Here additional readings give information
7699,00:04:21,5,Learning to Rank - Part 3,4.4,"About, how learning to rank books and",00:04:16,8,About learning rank books
7700,00:04:25,5,Learning to Rank - Part 3,4.4,also some advanced methods.,00:04:21,8,also advanced methods
