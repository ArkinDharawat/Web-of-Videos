end_time,lecture,lecture_name,lecture_no,sentence,start_time,sub_lecture,tokenized_sentence
00:00:03,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,[SOUND],00:00:00,14,SOUND
00:00:11,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,We can compute this maximum estimate,00:00:08,14,We compute maximum estimate
00:00:12,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,by using the EM algorithm.,00:00:11,14,using EM algorithm
00:00:16,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So in the e step, we now have to introduce more hidden",00:00:12,14,So e step introduce hidden
00:00:21,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"variables because we have more topics, so our hidden variable z now,",00:00:16,14,variables topics hidden variable z
00:00:25,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,which is a topic indicator can take more than two values.,00:00:21,14,topic indicator take two values
00:00:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So specifically will take a k plus one values,",00:00:25,14,So specifically take k plus one values
00:00:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,with b in the noting the background.,00:00:28,14,b noting background
00:00:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And once locate, to denote other k topics, right.",00:00:32,14,And locate denote k topics right
00:00:40,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So, now the e step, as you can recall is your augmented data, and",00:00:36,14,So e step recall augmented data
00:00:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,by predicting the values of the hidden variable.,00:00:40,14,predicting values hidden variable
00:00:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,So we're going to predict for,00:00:44,14,So going predict
00:00:52,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"a word, whether the word has come from one of these k plus one distributions.",00:00:47,14,word whether word come one k plus one distributions
00:00:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,This equation allows us to predict the probability,00:00:52,14,This equation allows us predict probability
00:01:01,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,that the word w in document d is generated from topic zero sub j.,00:00:57,14,word w document generated topic zero sub j
00:01:06,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And the bottom one is the predicted probability that this,00:01:03,14,And bottom one predicted probability
00:01:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,word has been generated from the background.,00:01:06,14,word generated background
00:01:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Note that we use document d here to index the word.,00:01:08,14,Note use document index word
00:01:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Why?,00:01:14,14,Why
00:01:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Because whether a word is from a particular topic,00:01:14,14,Because whether word particular topic
00:01:20,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,actually depends on the document.,00:01:18,14,actually depends document
00:01:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Can you see why?,00:01:20,14,Can see
00:01:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Well, it's through the pi's.",00:01:22,14,Well pi
00:01:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,The pi's are tied to each document.,00:01:24,14,The pi tied document
00:01:31,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Each document can have potentially different pi's, right.",00:01:26,14,Each document potentially different pi right
00:01:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,The pi's will then affect our prediction.,00:01:31,14,The pi affect prediction
00:01:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So, the pi's are here.",00:01:33,14,So pi
00:01:36,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And this depends on the document.,00:01:35,14,And depends document
00:01:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And that might give a different guess for,00:01:38,14,And might give different guess
00:01:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"a word in different documents, and that's desirable.",00:01:41,14,word different documents desirable
00:01:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In both cases we are using the Baye's Rule, as I explained, basically",00:01:46,14,In cases using Baye Rule I explained basically
00:01:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,assessing the likelihood of generating word from each of this division and,00:01:50,14,assessing likelihood generating word division
00:01:56,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,there's normalize.,00:01:55,14,normalize
00:01:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,What about the m step?,00:01:57,14,What step
00:02:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Well, we may recall the m step is we take advantage of the inferred z values.",00:01:59,14,Well may recall step take advantage inferred z values
00:02:05,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,To split the counts.,00:02:04,14,To split counts
00:02:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And then collected the right counts to re-estimate the parameters.,00:02:05,14,And collected right counts estimate parameters
00:02:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So in this case, we can re-estimate our coverage of probability.",00:02:09,14,So case estimate coverage probability
00:02:21,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And this is re-estimated based on collecting all the words in the document.,00:02:14,14,And estimated based collecting words document
00:02:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And that's why we have the count of the word in document.,00:02:22,14,And count word document
00:02:29,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And sum over all the words.,00:02:26,14,And sum words
00:02:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And then we're going to look at to what extent this word belongs to,00:02:29,14,And going look extent word belongs
00:02:36,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the topic theta sub j.,00:02:34,14,topic theta sub j
00:02:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And this part is our guess from each step.,00:02:36,14,And part guess step
00:02:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,This tells us how likely this word is actually from theta sub j.,00:02:40,14,This tells us likely word actually theta sub j
00:02:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And when we multiply them together,",00:02:44,14,And multiply together
00:02:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,we get the discounted count that's located for topic theta sub j.,00:02:47,14,get discounted count located topic theta sub j
00:02:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And when we normalize this over all the topics,",00:02:51,14,And normalize topics
00:02:58,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,we get the distribution of all the topics to indicate the coverage.,00:02:54,14,get distribution topics indicate coverage
00:03:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And similarly, the bottom one is the estimated probability of word for a topic.",00:02:58,14,And similarly bottom one estimated probability word topic
00:03:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And in this case we are using exact the same count, you can see this is",00:03:04,14,And case using exact count see
00:03:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"the same discounted account, ] it tells us to what extend we should",00:03:09,14,discounted account tells us extend
00:03:19,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,allocate this word [INAUDIBLE] but then normalization is different.,00:03:14,14,allocate word INAUDIBLE normalization different
00:03:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Because in this case we are interested in the word distribution, so",00:03:19,14,Because case interested word distribution
00:03:27,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,we simply normalize this over all the words.,00:03:24,14,simply normalize words
00:03:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"This is different, in contrast here we normalize the amount all the topics.",00:03:27,14,This different contrast normalize amount topics
00:03:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,It would be useful to take a comparison between the two.,00:03:33,14,It would useful take comparison two
00:03:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,This give us different distributions.,00:03:37,14,This give us different distributions
00:03:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And these tells us how to improve the parameters.,00:03:39,14,And tells us improve parameters
00:03:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And as I just explained, in both the formula is we have a maximum",00:03:48,14,And I explained formula maximum
00:04:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,estimate based on allocated word counts [INAUDIBLE].,00:03:55,14,estimate based allocated word counts INAUDIBLE
00:04:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Now this phenomena is actually general phenomena in all the EM algorithms.,00:04:00,14,Now phenomena actually general phenomena EM algorithms
00:04:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In the m-step, you general with the computer expect an account of",00:04:04,14,In step general computer expect account
00:04:15,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"the event based on the e-step result, and then you just and",00:04:09,14,event based e step result
00:04:20,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"then count to four, particular normalize it, typically.",00:04:15,14,count four particular normalize typically
00:04:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So, in terms of computation of this EM algorithm, we can",00:04:20,14,So terms computation EM algorithm
00:04:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,actually just keep accounting various events and then normalize them.,00:04:24,14,actually keep accounting various events normalize
00:04:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And when we thinking this way,",00:04:32,14,And thinking way
00:04:37,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,we also have a more concise way of presenting the EM Algorithm.,00:04:34,14,also concise way presenting EM Algorithm
00:04:42,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,It actually helps us better understand the formulas.,00:04:37,14,It actually helps us better understand formulas
00:04:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,So I'm going to go over this in some detail.,00:04:42,14,So I going go detail
00:04:48,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So as a algorithm we first initialize all the unknown perimeters randomly,",00:04:44,14,So algorithm first initialize unknown perimeters randomly
00:04:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,all right.,00:04:48,14,right
00:04:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So, in our case, we are interested in all of those coverage perimeters, pi's and",00:04:50,14,So case interested coverage perimeters pi
00:04:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"awarded distributions [INAUDIBLE], and we just randomly normalize them.",00:04:55,14,awarded distributions INAUDIBLE randomly normalize
00:05:05,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,This is the initialization step and then we will repeat until likelihood converges.,00:04:59,14,This initialization step repeat likelihood converges
00:05:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Now how do we know whether likelihood converges?,00:05:05,14,Now know whether likelihood converges
00:05:11,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,We can do compute likelihood at each step and,00:05:08,14,We compute likelihood step
00:05:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,compare the current likelihood with the previous likelihood.,00:05:11,14,compare current likelihood previous likelihood
00:05:17,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"If it doesn't change much and we're going to say it stopped, right.",00:05:14,14,If change much going say stopped right
00:05:23,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So, in each step we're going to do e-step and m-step.",00:05:19,14,So step going e step step
00:05:27,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,In the e-step we're going to do augment the data by predicting,00:05:23,14,In e step going augment data predicting
00:05:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the hidden variables.,00:05:27,14,hidden variables
00:05:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In this case, the hidden variable, z sub d, w,",00:05:30,14,In case hidden variable z sub w
00:05:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,indicates whether the word w in d is from a topic or background.,00:05:34,14,indicates whether word w topic background
00:05:43,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And if it's from a topic, which topic.",00:05:41,14,And topic topic
00:05:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So if you look at the e-step formulas,",00:05:43,14,So look e step formulas
00:05:52,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"essentially we're actually normalizing these counts, sorry,",00:05:46,14,essentially actually normalizing counts sorry
00:05:58,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,these probabilities of observing the word from each distribution.,00:05:52,14,probabilities observing word distribution
00:06:03,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So you can see, basically the prediction of word",00:05:58,14,So see basically prediction word
00:06:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,from topic zero sub j is based on the probability of,00:06:03,14,topic zero sub j based probability
00:06:12,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,selecting that theta sub j as a word distribution to generate the word.,00:06:07,14,selecting theta sub j word distribution generate word
00:06:15,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Multiply by the probability of observing the word from that distribution.,00:06:12,14,Multiply probability observing word distribution
00:06:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And I said it's proportional to this because in the implementation of,00:06:17,14,And I said proportional implementation
00:06:25,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,EM algorithm you can keep counter for,00:06:22,14,EM algorithm keep counter
00:06:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"this quantity, and in the end it just normalizes it.",00:06:25,14,quantity end normalizes
00:06:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,So the normalization here is over all the topics and,00:06:28,14,So normalization topics
00:06:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,then you would get a probability.,00:06:32,14,would get probability
00:06:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Now, in the m-step, we do the same, and we are going to collect these.",00:06:36,14,Now step going collect
00:06:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Allocated account for each topic.,00:06:43,14,Allocated account topic
00:06:49,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And we split words among the topics.,00:06:47,14,And split words among topics
00:06:53,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And then we're going to normalize them in different ways to obtain,00:06:50,14,And going normalize different ways obtain
00:06:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the real estimate.,00:06:53,14,real estimate
00:07:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So for example, we can normalize among all the topics to get the re-estimate of pi,",00:06:54,14,So example normalize among topics get estimate pi
00:07:02,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the coverage.,00:07:00,14,coverage
00:07:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Or we can re-normalize based on all the words.,00:07:02,14,Or normalize based words
00:07:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And that would give us a word distribution.,00:07:08,14,And would give us word distribution
00:07:15,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So it's useful to think algorithm in this way because when implemented, you can just",00:07:10,14,So useful think algorithm way implemented
00:07:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"use variables, but keep track of these quantities in each case.",00:07:15,14,use variables keep track quantities case
00:07:31,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And then you just normalize these variables to make them distribution.,00:07:23,14,And normalize variables make distribution
00:07:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Now I did not put the constraint for this one.,00:07:32,14,Now I put constraint one
00:07:38,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And I intentionally leave this as an exercise for you.,00:07:35,14,And I intentionally leave exercise
00:07:42,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And you can see, what's the normalizer for this one?",00:07:38,14,And see normalizer one
00:07:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,It's of a slightly different form but it's essentially the same as,00:07:42,14,It slightly different form essentially
00:07:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the one that you have seen here in this one.,00:07:47,14,one seen one
00:07:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,So in general in the envisioning of EM algorithms you will see you accumulate,00:07:50,14,So general envisioning EM algorithms see accumulate
00:07:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"the counts, various counts and then you normalize them.",00:07:54,14,counts various counts normalize
00:08:06,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So to summarize, we introduced the PLSA model.",00:08:01,14,So summarize introduced PLSA model
00:08:10,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,Which is a mixture model with k unigram language models representing k topics.,00:08:06,14,Which mixture model k unigram language models representing k topics
00:08:16,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And we also added a pre-determined background language model to,00:08:11,14,And also added pre determined background language model
00:08:19,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"help discover discriminative topics,",00:08:16,14,help discover discriminative topics
00:08:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,because this background language model can help attract the common terms.,00:08:19,14,background language model help attract common terms
00:08:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And we select the maximum estimate that we cant discover topical,00:08:23,14,And select maximum estimate cant discover topical
00:08:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,knowledge from text data.,00:08:28,14,knowledge text data
00:08:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In this case PLSA allows us to discover two things, one is k worded distributions,",00:08:30,14,In case PLSA allows us discover two things one k worded distributions
00:08:37,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,each one representing a topic and,00:08:35,14,one representing topic
00:08:40,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,the other is the proportion of each topic in each document.,00:08:37,14,proportion topic document
00:08:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And such detailed characterization of coverage of topics in documents,00:08:41,14,And detailed characterization coverage topics documents
00:08:48,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,can enable a lot of photo analysis.,00:08:46,14,enable lot photo analysis
00:08:53,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"For example, we can aggregate the documents in the particular",00:08:48,14,For example aggregate documents particular
00:08:58,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,pan period to assess the coverage of a particular topic in a time period.,00:08:53,14,pan period assess coverage particular topic time period
00:09:02,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,That would allow us to generate the temporal chains of topics.,00:08:58,14,That would allow us generate temporal chains topics
00:09:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,We can also aggregate topics covered in documents associated with a particular,00:09:02,14,We also aggregate topics covered documents associated particular
00:09:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"author and then we can categorize the topics written by this author, etc.",00:09:08,14,author categorize topics written author etc
00:09:20,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"And in addition to this, we can also cluster terms and cluster documents.",00:09:14,14,And addition also cluster terms cluster documents
00:09:23,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In fact, each topic can be regarded as a cluster.",00:09:20,14,In fact topic regarded cluster
00:09:25,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,So we already have the term clusters.,00:09:23,14,So already term clusters
00:09:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"In the higher probability, the words can be regarded as",00:09:25,14,In higher probability words regarded
00:09:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,belonging to one cluster represented by the topic.,00:09:29,14,belonging one cluster represented topic
00:09:37,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"Similarly, documents can be clustered in the same way.",00:09:34,14,Similarly documents clustered way
00:09:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,We can assign a document to the topic cluster,00:09:37,14,We assign document topic cluster
00:09:45,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,that's covered most in the document.,00:09:41,14,covered document
00:09:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,"So remember, pi's indicate to what extent each topic is covered in the document,",00:09:45,14,So remember pi indicate extent topic covered document
00:09:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,we can assign the document to the topical cluster that has the highest pi.,00:09:50,14,assign document topical cluster highest pi
00:10:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 2,2.14,And in general there are many useful applications of this technique.,00:09:57,14,And general many useful applications technique
00:00:09,2,Overview Text Mining and Analytics- Part 2,1.2,"[SOUND] So,",00:00:00,2,SOUND So
00:00:14,2,Overview Text Mining and Analytics- Part 2,1.2,"looking at the text mining problem more closely, we see that the problem is",00:00:09,2,looking text mining problem closely see problem
00:00:20,2,Overview Text Mining and Analytics- Part 2,1.2,"similar to general data mining, except that we'll be focusing more on text data.",00:00:14,2,similar general data mining except focusing text data
00:00:26,2,Overview Text Mining and Analytics- Part 2,1.2,And we're going to have text mining algorithms to help us to turn text data,00:00:21,2,And going text mining algorithms help us turn text data
00:00:32,2,Overview Text Mining and Analytics- Part 2,1.2,"into actionable knowledge that we can use in real world,",00:00:26,2,actionable knowledge use real world
00:00:34,2,Overview Text Mining and Analytics- Part 2,1.2,"especially for decision making, or",00:00:32,2,especially decision making
00:00:39,2,Overview Text Mining and Analytics- Part 2,1.2,for completing whatever tasks that require text data to support.,00:00:34,2,completing whatever tasks require text data support
00:00:45,2,Overview Text Mining and Analytics- Part 2,1.2,"Because, in general, in many real world problems of data mining",00:00:39,2,Because general many real world problems data mining
00:00:49,2,Overview Text Mining and Analytics- Part 2,1.2,we also tend to have other kinds of data that are non-textual.,00:00:45,2,also tend kinds data non textual
00:00:54,2,Overview Text Mining and Analytics- Part 2,1.2,So a more general picture would be to include non-text data as well.,00:00:49,2,So general picture would include non text data well
00:01:00,2,Overview Text Mining and Analytics- Part 2,1.2,And for this reason we might be concerned with joint mining of text and,00:00:56,2,And reason might concerned joint mining text
00:01:02,2,Overview Text Mining and Analytics- Part 2,1.2,non-text data.,00:01:00,2,non text data
00:01:05,2,Overview Text Mining and Analytics- Part 2,1.2,"And so in this course we're going to focus more on text mining,",00:01:02,2,And course going focus text mining
00:01:10,2,Overview Text Mining and Analytics- Part 2,1.2,but we're also going to also touch how do to joint analysis of both text data and,00:01:05,2,also going also touch joint analysis text data
00:01:12,2,Overview Text Mining and Analytics- Part 2,1.2,non-text data.,00:01:10,2,non text data
00:01:16,2,Overview Text Mining and Analytics- Part 2,1.2,With this problem definition we can now look at the landscape of,00:01:12,2,With problem definition look landscape
00:01:19,2,Overview Text Mining and Analytics- Part 2,1.2,the topics in text mining and analytics.,00:01:16,2,topics text mining analytics
00:01:25,2,Overview Text Mining and Analytics- Part 2,1.2,Now this slide shows the process of generating text data in more detail.,00:01:21,2,Now slide shows process generating text data detail
00:01:29,2,Overview Text Mining and Analytics- Part 2,1.2,"More specifically, a human sensor or",00:01:27,2,More specifically human sensor
00:01:33,2,Overview Text Mining and Analytics- Part 2,1.2,human observer would look at the word from some perspective.,00:01:29,2,human observer would look word perspective
00:01:38,2,Overview Text Mining and Analytics- Part 2,1.2,Different people would be looking at the world from different angles and,00:01:34,2,Different people would looking world different angles
00:01:41,2,Overview Text Mining and Analytics- Part 2,1.2,they'll pay attention to different things.,00:01:38,2,pay attention different things
00:01:46,2,Overview Text Mining and Analytics- Part 2,1.2,The same person at different times might also pay attention to different aspects,00:01:41,2,The person different times might also pay attention different aspects
00:01:50,2,Overview Text Mining and Analytics- Part 2,1.2,of the observed world.,00:01:46,2,observed world
00:01:55,2,Overview Text Mining and Analytics- Part 2,1.2,And so the humans are able to perceive the world from some perspective.,00:01:50,2,And humans able perceive world perspective
00:02:01,2,Overview Text Mining and Analytics- Part 2,1.2,"And that human, the sensor, would then form a view of the world.",00:01:55,2,And human sensor would form view world
00:02:05,2,Overview Text Mining and Analytics- Part 2,1.2,And that can be called the Observed World.,00:02:01,2,And called Observed World
00:02:10,2,Overview Text Mining and Analytics- Part 2,1.2,"Of course, this would be different from the Real World because of the perspective",00:02:05,2,Of course would different Real World perspective
00:02:14,2,Overview Text Mining and Analytics- Part 2,1.2,that the person has taken can often be biased also.,00:02:10,2,person taken often biased also
00:02:22,2,Overview Text Mining and Analytics- Part 2,1.2,"Now the Observed World can be represented as, for example,",00:02:16,2,Now Observed World represented example
00:02:27,2,Overview Text Mining and Analytics- Part 2,1.2,"entity-relation graphs or in a more general way,",00:02:22,2,entity relation graphs general way
00:02:31,2,Overview Text Mining and Analytics- Part 2,1.2,using knowledge representation language.,00:02:27,2,using knowledge representation language
00:02:39,2,Overview Text Mining and Analytics- Part 2,1.2,"But in general, this is basically what a person has in mind about the world.",00:02:31,2,But general basically person mind world
00:02:43,2,Overview Text Mining and Analytics- Part 2,1.2,"And we don't really know what exactly it looks like, of course.",00:02:39,2,And really know exactly looks like course
00:02:48,2,Overview Text Mining and Analytics- Part 2,1.2,But then the human would express what the person has,00:02:43,2,But human would express person
00:02:52,2,Overview Text Mining and Analytics- Part 2,1.2,"observed using a natural language, such as English.",00:02:48,2,observed using natural language English
00:02:54,2,Overview Text Mining and Analytics- Part 2,1.2,And the result is text data.,00:02:52,2,And result text data
00:03:00,2,Overview Text Mining and Analytics- Part 2,1.2,Of course a person could have used a different language to express what he or,00:02:55,2,Of course person could used different language express
00:03:02,2,Overview Text Mining and Analytics- Part 2,1.2,she has observed.,00:03:00,2,observed
00:03:08,2,Overview Text Mining and Analytics- Part 2,1.2,In that case we might have text data of mixed languages or different languages.,00:03:02,2,In case might text data mixed languages different languages
00:03:15,2,Overview Text Mining and Analytics- Part 2,1.2,The main goal of text mining Is actually to revert this,00:03:10,2,The main goal text mining Is actually revert
00:03:19,2,Overview Text Mining and Analytics- Part 2,1.2,process of generating text data.,00:03:15,2,process generating text data
00:03:24,2,Overview Text Mining and Analytics- Part 2,1.2,We hope to be able to uncover some aspect in this process.,00:03:19,2,We hope able uncover aspect process
00:03:34,2,Overview Text Mining and Analytics- Part 2,1.2,"Specifically, we can think about mining, for example, knowledge about the language.",00:03:28,2,Specifically think mining example knowledge language
00:03:40,2,Overview Text Mining and Analytics- Part 2,1.2,"And that means by looking at text data in English, we may be able to discover",00:03:35,2,And means looking text data English may able discover
00:03:46,2,Overview Text Mining and Analytics- Part 2,1.2,"something about English, some usage of English, some patterns of English.",00:03:40,2,something English usage English patterns English
00:03:52,2,Overview Text Mining and Analytics- Part 2,1.2,"So this is one type of mining problems, where the result is",00:03:47,2,So one type mining problems result
00:03:57,2,Overview Text Mining and Analytics- Part 2,1.2,some knowledge about language which may be useful in various ways.,00:03:52,2,knowledge language may useful various ways
00:04:00,2,Overview Text Mining and Analytics- Part 2,1.2,"If you look at the picture,",00:03:58,2,If look picture
00:04:06,2,Overview Text Mining and Analytics- Part 2,1.2,we can also then mine knowledge about the observed world.,00:04:00,2,also mine knowledge observed world
00:04:10,2,Overview Text Mining and Analytics- Part 2,1.2,And so this has much to do with mining the content of text data.,00:04:06,2,And much mining content text data
00:04:15,2,Overview Text Mining and Analytics- Part 2,1.2,"We're going to look at what the text data are about, and then try to",00:04:11,2,We going look text data try
00:04:20,2,Overview Text Mining and Analytics- Part 2,1.2,get the essence of it or extracting high quality information,00:04:15,2,get essence extracting high quality information
00:04:25,2,Overview Text Mining and Analytics- Part 2,1.2,about a particular aspect of the world that we're interested in.,00:04:20,2,particular aspect world interested
00:04:30,2,Overview Text Mining and Analytics- Part 2,1.2,"For example, everything that has been said about a particular person or",00:04:26,2,For example everything said particular person
00:04:31,2,Overview Text Mining and Analytics- Part 2,1.2,a particular entity.,00:04:30,2,particular entity
00:04:36,2,Overview Text Mining and Analytics- Part 2,1.2,And this can be regarded as mining content,00:04:31,2,And regarded mining content
00:04:43,2,Overview Text Mining and Analytics- Part 2,1.2,to describe the observed world in the user's mind or the person's mind.,00:04:36,2,describe observed world user mind person mind
00:04:50,2,Overview Text Mining and Analytics- Part 2,1.2,"If you look further, then you can also imagine",00:04:45,2,If look also imagine
00:04:54,2,Overview Text Mining and Analytics- Part 2,1.2,"we can mine knowledge about this observer, himself or herself.",00:04:50,2,mine knowledge observer
00:05:00,2,Overview Text Mining and Analytics- Part 2,1.2,So this has also to do with using text data to infer,00:04:54,2,So also using text data infer
00:05:02,2,Overview Text Mining and Analytics- Part 2,1.2,some properties of this person.,00:05:00,2,properties person
00:05:07,2,Overview Text Mining and Analytics- Part 2,1.2,And these properties could include the mood of the person or,00:05:03,2,And properties could include mood person
00:05:09,2,Overview Text Mining and Analytics- Part 2,1.2,sentiment of the person.,00:05:07,2,sentiment person
00:05:15,2,Overview Text Mining and Analytics- Part 2,1.2,And note that we distinguish the observed word from the person,00:05:10,2,And note distinguish observed word person
00:05:21,2,Overview Text Mining and Analytics- Part 2,1.2,because text data can't describe what the person has observed in an objective way.,00:05:15,2,text data describe person observed objective way
00:05:25,2,Overview Text Mining and Analytics- Part 2,1.2,"But the description can be also subjected with sentiment and so,",00:05:21,2,But description also subjected sentiment
00:05:30,2,Overview Text Mining and Analytics- Part 2,1.2,"in general, you can imagine the text data would contain some factual",00:05:25,2,general imagine text data would contain factual
00:05:34,2,Overview Text Mining and Analytics- Part 2,1.2,descriptions of the world plus some subjective comments.,00:05:30,2,descriptions world plus subjective comments
00:05:37,2,Overview Text Mining and Analytics- Part 2,1.2,So that's why it's also possible to,00:05:34,2,So also possible
00:05:41,2,Overview Text Mining and Analytics- Part 2,1.2,do text mining to mine knowledge about the observer.,00:05:37,2,text mining mine knowledge observer
00:05:45,2,Overview Text Mining and Analytics- Part 2,1.2,"Finally, if you look at the picture to the left side of this picture,",00:05:41,2,Finally look picture left side picture
00:05:50,2,Overview Text Mining and Analytics- Part 2,1.2,then you can see we can certainly also say something about the real world.,00:05:45,2,see certainly also say something real world
00:05:50,2,Overview Text Mining and Analytics- Part 2,1.2,Right?,00:05:50,2,Right
00:05:56,2,Overview Text Mining and Analytics- Part 2,1.2,So indeed we can do text mining to infer other real world variables.,00:05:50,2,So indeed text mining infer real world variables
00:05:59,2,Overview Text Mining and Analytics- Part 2,1.2,And this is often called a predictive analytics.,00:05:56,2,And often called predictive analytics
00:06:04,2,Overview Text Mining and Analytics- Part 2,1.2,And we want to predict the value of certain interesting variable.,00:06:00,2,And want predict value certain interesting variable
00:06:08,2,Overview Text Mining and Analytics- Part 2,1.2,"So, this picture basically covered",00:06:04,2,So picture basically covered
00:06:13,2,Overview Text Mining and Analytics- Part 2,1.2,multiple types of knowledge that we can mine from text in general.,00:06:08,2,multiple types knowledge mine text general
00:06:19,2,Overview Text Mining and Analytics- Part 2,1.2,When we infer other real world variables we,00:06:14,2,When infer real world variables
00:06:24,2,Overview Text Mining and Analytics- Part 2,1.2,could also use some of the results from,00:06:19,2,could also use results
00:06:30,2,Overview Text Mining and Analytics- Part 2,1.2,mining text data as intermediate results to help the prediction.,00:06:24,2,mining text data intermediate results help prediction
00:06:31,2,Overview Text Mining and Analytics- Part 2,1.2,"For example,",00:06:30,2,For example
00:06:38,2,Overview Text Mining and Analytics- Part 2,1.2,after we mine the content of text data we might generate some summary of content.,00:06:31,2,mine content text data might generate summary content
00:06:41,2,Overview Text Mining and Analytics- Part 2,1.2,And that summary could be then used,00:06:38,2,And summary could used
00:06:45,2,Overview Text Mining and Analytics- Part 2,1.2,to help us predict the variables of the real world.,00:06:41,2,help us predict variables real world
00:06:51,2,Overview Text Mining and Analytics- Part 2,1.2,"Now of course this is still generated from the original text data,",00:06:45,2,Now course still generated original text data
00:06:58,2,Overview Text Mining and Analytics- Part 2,1.2,but I want to emphasize here that often the processing of text data,00:06:51,2,I want emphasize often processing text data
00:07:03,2,Overview Text Mining and Analytics- Part 2,1.2,to generate some features that can help with the prediction is very important.,00:06:58,2,generate features help prediction important
00:07:10,2,Overview Text Mining and Analytics- Part 2,1.2,And that's why here we show the results of,00:07:04,2,And show results
00:07:15,2,Overview Text Mining and Analytics- Part 2,1.2,"some other mining tasks, including mining the content of text data and",00:07:10,2,mining tasks including mining content text data
00:07:19,2,Overview Text Mining and Analytics- Part 2,1.2,"mining knowledge about the observer, can all be very helpful for prediction.",00:07:15,2,mining knowledge observer helpful prediction
00:07:26,2,Overview Text Mining and Analytics- Part 2,1.2,"In fact, when we have non-text data, we could also use the non-text",00:07:21,2,In fact non text data could also use non text
00:07:31,2,Overview Text Mining and Analytics- Part 2,1.2,"data to help prediction, and of course it depends on the problem.",00:07:26,2,data help prediction course depends problem
00:07:39,2,Overview Text Mining and Analytics- Part 2,1.2,"In general, non-text data can be very important for such prediction tasks.",00:07:31,2,In general non text data important prediction tasks
00:07:44,2,Overview Text Mining and Analytics- Part 2,1.2,"For example, if you want to predict stock prices or",00:07:39,2,For example want predict stock prices
00:07:49,2,Overview Text Mining and Analytics- Part 2,1.2,changes of stock prices based on discussion in the news articles or,00:07:44,2,changes stock prices based discussion news articles
00:07:53,2,Overview Text Mining and Analytics- Part 2,1.2,"in social media, then this is an example",00:07:49,2,social media example
00:07:58,2,Overview Text Mining and Analytics- Part 2,1.2,of using text data to predict some other real world variables.,00:07:53,2,using text data predict real world variables
00:07:59,2,Overview Text Mining and Analytics- Part 2,1.2,"But in this case, obviously,",00:07:58,2,But case obviously
00:08:04,2,Overview Text Mining and Analytics- Part 2,1.2,the historical stock price data would be very important for this prediction.,00:07:59,2,historical stock price data would important prediction
00:08:09,2,Overview Text Mining and Analytics- Part 2,1.2,And so that's an example of non-text data that would be very,00:08:04,2,And example non text data would
00:08:13,2,Overview Text Mining and Analytics- Part 2,1.2,useful for the prediction.,00:08:09,2,useful prediction
00:08:17,2,Overview Text Mining and Analytics- Part 2,1.2,And we're going to combine both kinds of data to make the prediction.,00:08:13,2,And going combine kinds data make prediction
00:08:24,2,Overview Text Mining and Analytics- Part 2,1.2,Now non-text data can be also used for analyzing text by supplying context.,00:08:17,2,Now non text data also used analyzing text supplying context
00:08:27,2,Overview Text Mining and Analytics- Part 2,1.2,"When we look at the text data alone,",00:08:25,2,When look text data alone
00:08:31,2,Overview Text Mining and Analytics- Part 2,1.2,we'll be mostly looking at the content and/or opinions expressed in the text.,00:08:27,2,mostly looking content opinions expressed text
00:08:36,2,Overview Text Mining and Analytics- Part 2,1.2,But text data generally also has context associated.,00:08:32,2,But text data generally also context associated
00:08:44,2,Overview Text Mining and Analytics- Part 2,1.2,"For example, the time and the location that associated are with the text data.",00:08:37,2,For example time location associated text data
00:08:47,2,Overview Text Mining and Analytics- Part 2,1.2,And these are useful context information.,00:08:44,2,And useful context information
00:08:54,2,Overview Text Mining and Analytics- Part 2,1.2,And the context can provide interesting angles for analyzing text data.,00:08:48,2,And context provide interesting angles analyzing text data
00:08:57,2,Overview Text Mining and Analytics- Part 2,1.2,"For example, we might partition text data into different time periods",00:08:54,2,For example might partition text data different time periods
00:09:00,2,Overview Text Mining and Analytics- Part 2,1.2,because of the availability of the time.,00:08:57,2,availability time
00:09:06,2,Overview Text Mining and Analytics- Part 2,1.2,Now we can analyze text data in each time period and then make a comparison.,00:09:00,2,Now analyze text data time period make comparison
00:09:09,2,Overview Text Mining and Analytics- Part 2,1.2,Similarly we can partition text data based on locations or,00:09:06,2,Similarly partition text data based locations
00:09:15,2,Overview Text Mining and Analytics- Part 2,1.2,any meta data that's associated to form interesting comparisons in areas.,00:09:09,2,meta data associated form interesting comparisons areas
00:09:20,2,Overview Text Mining and Analytics- Part 2,1.2,"So, in this sense, non-text data can actually provide",00:09:15,2,So sense non text data actually provide
00:09:24,2,Overview Text Mining and Analytics- Part 2,1.2,interesting angles or perspectives for text data analysis.,00:09:20,2,interesting angles perspectives text data analysis
00:09:29,2,Overview Text Mining and Analytics- Part 2,1.2,And it can help us make context-sensitive,00:09:24,2,And help us make context sensitive
00:09:33,2,Overview Text Mining and Analytics- Part 2,1.2,analysis of content or the language usage or,00:09:29,2,analysis content language usage
00:09:42,2,Overview Text Mining and Analytics- Part 2,1.2,the opinions about the observer or the authors of text data.,00:09:36,2,opinions observer authors text data
00:09:46,2,Overview Text Mining and Analytics- Part 2,1.2,We could analyze the sentiment in different contexts.,00:09:42,2,We could analyze sentiment different contexts
00:09:54,2,Overview Text Mining and Analytics- Part 2,1.2,So this is a fairly general landscape of the topics in text mining and analytics.,00:09:46,2,So fairly general landscape topics text mining analytics
00:09:59,2,Overview Text Mining and Analytics- Part 2,1.2,In this course we're going to selectively cover some of those topics.,00:09:54,2,In course going selectively cover topics
00:10:03,2,Overview Text Mining and Analytics- Part 2,1.2,We actually hope to cover most of these general topics.,00:09:59,2,We actually hope cover general topics
00:10:11,2,Overview Text Mining and Analytics- Part 2,1.2,First we're going to cover natural language processing very,00:10:06,2,First going cover natural language processing
00:10:16,2,Overview Text Mining and Analytics- Part 2,1.2,briefly because this has to do with understanding text data and,00:10:11,2,briefly understanding text data
00:10:21,2,Overview Text Mining and Analytics- Part 2,1.2,this determines how we can represent text data for text mining.,00:10:16,2,determines represent text data text mining
00:10:27,2,Overview Text Mining and Analytics- Part 2,1.2,"Second, we're going to talk about how to mine word associations from text data.",00:10:21,2,Second going talk mine word associations text data
00:10:34,2,Overview Text Mining and Analytics- Part 2,1.2,And word associations is a form of use for lexical knowledge about a language.,00:10:27,2,And word associations form use lexical knowledge language
00:10:38,2,Overview Text Mining and Analytics- Part 2,1.2,"Third, we're going to talk about topic mining and analysis.",00:10:34,2,Third going talk topic mining analysis
00:10:43,2,Overview Text Mining and Analytics- Part 2,1.2,"And this is only one way to analyze content of text, but",00:10:38,2,And one way analyze content text
00:10:46,2,Overview Text Mining and Analytics- Part 2,1.2,it's a very useful ways of analyzing content.,00:10:43,2,useful ways analyzing content
00:10:51,2,Overview Text Mining and Analytics- Part 2,1.2,It's also one of the most useful techniques in text mining.,00:10:46,2,It also one useful techniques text mining
00:10:59,2,Overview Text Mining and Analytics- Part 2,1.2,Then we're going to talk about opinion mining and sentiment analysis.,00:10:53,2,Then going talk opinion mining sentiment analysis
00:11:05,2,Overview Text Mining and Analytics- Part 2,1.2,So this can be regarded as one example of mining knowledge about the observer.,00:10:59,2,So regarded one example mining knowledge observer
00:11:11,2,Overview Text Mining and Analytics- Part 2,1.2,And finally we're going to cover text-based prediction,00:11:07,2,And finally going cover text based prediction
00:11:16,2,Overview Text Mining and Analytics- Part 2,1.2,problems where we try to predict some real world variable based on text data.,00:11:11,2,problems try predict real world variable based text data
00:11:24,2,Overview Text Mining and Analytics- Part 2,1.2,So this slide also serves as a road map for this course.,00:11:17,2,So slide also serves road map course
00:11:27,2,Overview Text Mining and Analytics- Part 2,1.2,And we're going to use this as an outline for,00:11:24,2,And going use outline
00:11:30,2,Overview Text Mining and Analytics- Part 2,1.2,the topics that we'll cover in the rest of this course.,00:11:27,2,topics cover rest course
00:00:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,[SOUND] This lecture is a continued,00:00:00,5,SOUND This lecture continued
00:00:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,discussion of Latent Aspect Rating Analysis.,00:00:05,5,discussion Latent Aspect Rating Analysis
00:00:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Earlier, we talked about how to solve the problem of LARA in two stages.",00:00:12,5,Earlier talked solve problem LARA two stages
00:00:22,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,But we first do segmentation of different aspects.,00:00:18,5,But first segmentation different aspects
00:00:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And then we use a latent regression model to learn the aspect ratings and,00:00:22,5,And use latent regression model learn aspect ratings
00:00:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,then later the weight.,00:00:26,5,later weight
00:00:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now it's also possible to develop a unified generative model for,00:00:28,5,Now also possible develop unified generative model
00:00:35,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"solving this problem, and",00:00:33,5,solving problem
00:00:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,that is we not only model the generational over-rating based on text.,00:00:35,5,model generational rating based text
00:00:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"We also model the generation of text, and so",00:00:41,5,We also model generation text
00:00:47,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,a natural solution would be to use topic model.,00:00:45,5,natural solution would use topic model
00:00:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So given the entity,",00:00:47,5,So given entity
00:00:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,we can assume there are aspects that are described by word distributions.,00:00:49,5,assume aspects described word distributions
00:00:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Topics.,00:00:54,5,Topics
00:01:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And then we an use a topic model to model the generation of the reviewed text.,00:00:55,5,And use topic model model generation reviewed text
00:01:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,I will assume words in the review text are drawn from these distributions.,00:01:01,5,I assume words review text drawn distributions
00:01:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,In the same way as we assumed for generating model like PRSA.,00:01:08,5,In way assumed generating model like PRSA
00:01:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And then we can then plug in the latent regression model to,00:01:13,5,And plug latent regression model
00:01:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,use the text to further predict the overrating.,00:01:18,5,use text predict overrating
00:01:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And that means when we first predict the aspect rating and,00:01:23,5,And means first predict aspect rating
00:01:30,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,then combine them with aspect weights to predict the overall rating.,00:01:26,5,combine aspect weights predict overall rating
00:01:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So this would give us a unified generated model,",00:01:30,5,So would give us unified generated model
00:01:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,where we model both the generation of text and the overall ready condition on text.,00:01:34,5,model generation text overall ready condition text
00:01:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So we don't have time to discuss this model in detail as in,00:01:40,5,So time discuss model detail
00:01:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"many other cases in this part of the cause where we discuss the cutting edge topics,",00:01:46,5,many cases part cause discuss cutting edge topics
00:01:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,but there's a reference site here where you can find more details.,00:01:51,5,reference site find details
00:02:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So now I'm going to show you some simple results that you can get,00:01:57,5,So I going show simple results get
00:02:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,by using these kind of generated models.,00:02:00,5,using kind generated models
00:02:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"First, it's about rating decomposition.",00:02:02,5,First rating decomposition
00:02:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So here, what you see are the decomposed ratings for",00:02:05,5,So see decomposed ratings
00:02:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,three hotels that have the same overall rating.,00:02:09,5,three hotels overall rating
00:02:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So if you just look at the overall rating,",00:02:13,5,So look overall rating
00:02:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,you can't really tell much difference between these hotels.,00:02:15,5,really tell much difference hotels
00:02:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,But by decomposing these ratings into aspect ratings,00:02:18,5,But decomposing ratings aspect ratings
00:02:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"we can see some hotels have higher ratings for some dimensions,",00:02:24,5,see hotels higher ratings dimensions
00:02:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"like value, but others might score better in other dimensions, like location.",00:02:28,5,like value others might score better dimensions like location
00:02:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And so this can give you detailed opinions at the aspect level.,00:02:33,5,And give detailed opinions aspect level
00:02:42,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Now here, the ground-truth is shown in the parenthesis, so",00:02:38,5,Now ground truth shown parenthesis
00:02:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,it also allows you to see whether the prediction is accurate.,00:02:42,5,also allows see whether prediction accurate
00:02:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,It's not always accurate but It's mostly still reflecting some of the trends.,00:02:46,5,It always accurate It mostly still reflecting trends
00:02:58,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,The second result you compare different reviewers on the same hotel.,00:02:53,5,The second result compare different reviewers hotel
00:03:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So the table shows the decomposed ratings for two reviewers about same hotel.,00:02:58,5,So table shows decomposed ratings two reviewers hotel
00:03:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Again their high level overall ratings are the same.,00:03:05,5,Again high level overall ratings
00:03:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So if you just look at the overall ratings, you don't really get that much",00:03:08,5,So look overall ratings really get much
00:03:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,information about the difference between the two reviewers.,00:03:13,5,information difference two reviewers
00:03:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"But after you decompose the ratings,",00:03:15,5,But decompose ratings
00:03:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,you can see clearly that they have high scores on different dimensions.,00:03:17,5,see clearly high scores different dimensions
00:03:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So this shows that model can review differences in opinions of different,00:03:21,5,So shows model review differences opinions different
00:03:30,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,reviewers and such a detailed understanding can help us understand,00:03:26,5,reviewers detailed understanding help us understand
00:03:35,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,better about reviewers and also better about their feedback on the hotel.,00:03:30,5,better reviewers also better feedback hotel
00:03:38,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"This is something very interesting,",00:03:35,5,This something interesting
00:03:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,because this is in some sense some byproduct.,00:03:38,5,sense byproduct
00:03:43,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"In our problem formulation, we did not really have to do this.",00:03:40,5,In problem formulation really
00:03:47,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,But the design of the generating model has this component.,00:03:43,5,But design generating model component
00:03:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And these are sentimental weights for words in different aspects.,00:03:47,5,And sentimental weights words different aspects
00:03:58,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And you can see the highly weighted words versus the negatively loaded weighted,00:03:52,5,And see highly weighted words versus negatively loaded weighted
00:04:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,words here for each of the four dimensions.,00:03:58,5,words four dimensions
00:04:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Value, rooms, location, and cleanliness.",00:04:01,5,Value rooms location cleanliness
00:04:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"The top words clearly make sense, and the bottom words also make sense.",00:04:05,5,The top words clearly make sense bottom words also make sense
00:04:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So this shows that with this approach,",00:04:10,5,So shows approach
00:04:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,we can also learn sentiment information directly from the data.,00:04:12,5,also learn sentiment information directly data
00:04:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Now, this kind of lexicon is very useful because in general, a word like long,",00:04:16,5,Now kind lexicon useful general word like long
00:04:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"let's say, may have different sentiment polarities for different context.",00:04:21,5,let say may different sentiment polarities different context
00:04:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So if I say the battery life of this laptop is long, then that's positive.",00:04:26,5,So I say battery life laptop long positive
00:04:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"But if I say the rebooting time for the laptop is long, that's bad, right?",00:04:31,5,But I say rebooting time laptop long bad right
00:04:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So even for reviews about the same product, laptop,",00:04:36,5,So even reviews product laptop
00:04:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"the word long is ambiguous, it could mean positive or it could mean negative.",00:04:40,5,word long ambiguous could mean positive could mean negative
00:04:50,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"But this kind of lexicon, that we can learn by using this kind of generated",00:04:46,5,But kind lexicon learn using kind generated
00:04:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"models, can show whether a word is positive for a particular aspect.",00:04:50,5,models show whether word positive particular aspect
00:05:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So this is clearly very useful, and in fact such a lexicon can be directly used",00:04:55,5,So clearly useful fact lexicon directly used
00:05:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,to tag other reviews about hotels or,00:05:01,5,tag reviews hotels
00:05:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,tag comments about hotels in social media like Tweets.,00:05:04,5,tag comments hotels social media like Tweets
00:05:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"And what's also interesting is that since this is almost completely unsupervised,",00:05:08,5,And also interesting since almost completely unsupervised
00:05:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,well assuming the reviews whose overall rating are available And,00:05:15,5,well assuming reviews whose overall rating available And
00:05:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,then this can allow us to learn form potentially larger amount of data on,00:05:20,5,allow us learn form potentially larger amount data
00:05:27,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,the internet to reach sentiment lexicon.,00:05:24,5,internet reach sentiment lexicon
00:05:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And here are some results to validate the preference words.,00:05:28,5,And results validate preference words
00:05:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Remember the model can infer wether a reviewer cares more about service or,00:05:31,5,Remember model infer wether reviewer cares service
00:05:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,the price.,00:05:36,5,price
00:05:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now how do we know whether the inferred weights are correct?,00:05:37,5,Now know whether inferred weights correct
00:05:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And this poses a very difficult challenge for evaluation.,00:05:41,5,And poses difficult challenge evaluation
00:05:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now here we show some interesting way of evaluating.,00:05:45,5,Now show interesting way evaluating
00:05:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"What you see here are the prices of hotels in different cities, and",00:05:50,5,What see prices hotels different cities
00:06:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,these are the prices of hotels that are favored by different groups of reviewers.,00:05:55,5,prices hotels favored different groups reviewers
00:06:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,The top ten are the reviewers was the highest,00:06:01,5,The top ten reviewers highest
00:06:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,inferred value to other aspect ratio.,00:06:04,5,inferred value aspect ratio
00:06:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So for example value versus location, value versus room, etcetera.",00:06:09,5,So example value versus location value versus room etcetera
00:06:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now the top ten of the reviewers that have the highest ratios by this measure.,00:06:13,5,Now top ten reviewers highest ratios measure
00:06:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And that means these reviewers tend to put a lot of,00:06:20,5,And means reviewers tend put lot
00:06:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,weight on value as compared with other dimensions.,00:06:23,5,weight value compared dimensions
00:06:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So that means they really emphasize on value.,00:06:26,5,So means really emphasize value
00:06:32,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,The bottom ten on the other hand of the reviewers.,00:06:30,5,The bottom ten hand reviewers
00:06:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"The lowest ratio, what does that mean?",00:06:32,5,The lowest ratio mean
00:06:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Well it means these reviewers have put higher weights on other aspects,00:06:34,5,Well means reviewers put higher weights aspects
00:06:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,than value.,00:06:39,5,value
00:06:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So those are people that cared about another dimension and they didn't care so,00:06:41,5,So people cared another dimension care
00:06:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"much the value in some sense, at least as compared with the top ten group.",00:06:46,5,much value sense least compared top ten group
00:06:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now these ratios are computer based on the inferred weights from the model.,00:06:52,5,Now ratios computer based inferred weights model
00:07:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So now you can see the average prices of hotels favored by top ten reviewers,00:06:57,5,So see average prices hotels favored top ten reviewers
00:07:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,are indeed much cheaper than those that are favored by the bottom ten.,00:07:02,5,indeed much cheaper favored bottom ten
00:07:14,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And this provides some indirect way of validating the inferred weights.,00:07:07,5,And provides indirect way validating inferred weights
00:07:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,It just means the weights are not random.,00:07:14,5,It means weights random
00:07:19,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,They are actually meaningful here.,00:07:16,5,They actually meaningful
00:07:22,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"In comparison, the average price in these three cities,",00:07:19,5,In comparison average price three cities
00:07:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"you can actually see the top ten tend to have below average in price,",00:07:22,5,actually see top ten tend average price
00:07:30,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"whereas the bottom half, where they care a lot about other things like a service or",00:07:26,5,whereas bottom half care lot things like service
00:07:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,room condition tend to have hotels that have higher prices than average.,00:07:30,5,room condition tend hotels higher prices average
00:07:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So with these results we can build a lot of interesting applications.,00:07:36,5,So results build lot interesting applications
00:07:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"For example, a direct application would be to generate the rated aspect, the summary,",00:07:40,5,For example direct application would generate rated aspect summary
00:07:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,and because of the decomposition we have now generated the summaries for,00:07:45,5,decomposition generated summaries
00:07:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,each aspect.,00:07:48,5,aspect
00:07:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,The positive sentences the negative sentences about each aspect.,00:07:49,5,The positive sentences negative sentences aspect
00:07:57,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,It's more informative than original review that just has an overall rating and,00:07:54,5,It informative original review overall rating
00:07:58,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,review text.,00:07:57,5,review text
00:08:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Here are some other results,00:07:58,5,Here results
00:08:06,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,about the aspects that's covered from reviews with no ratings.,00:08:01,5,aspects covered reviews ratings
00:08:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"These are mp3 reviews,",00:08:06,5,These mp3 reviews
00:08:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,and these results show that the model can discover some interesting aspects.,00:08:08,5,results show model discover interesting aspects
00:08:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Commented on low overall ratings versus those higher overall per ratings.,00:08:13,5,Commented low overall ratings versus higher overall per ratings
00:08:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And they care more about the different aspects.,00:08:18,5,And care different aspects
00:08:25,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Or they comment more on the different aspects.,00:08:22,5,Or comment different aspects
00:08:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So that can help us discover for example, consumers'",00:08:25,5,So help us discover example consumers
00:08:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,trend in appreciating different features of products.,00:08:29,5,trend appreciating different features products
00:08:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"For example, one might have discovered the trend that people tend to",00:08:34,5,For example one might discovered trend people tend
00:08:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"like larger screens of cell phones or light weight of laptop, etcetera.",00:08:39,5,like larger screens cell phones light weight laptop etcetera
00:08:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Such knowledge can be useful for,00:08:45,5,Such knowledge useful
00:08:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,manufacturers to design their next generation of products.,00:08:49,5,manufacturers design next generation products
00:09:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Here are some interesting results on analyzing users rating behavior.,00:08:56,5,Here interesting results analyzing users rating behavior
00:09:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So what you see is average weights,00:09:01,5,So see average weights
00:09:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,along different dimensions by different groups of reviewers.,00:09:04,5,along different dimensions different groups reviewers
00:09:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And on the left side you see the weights of viewers that like the expensive hotels.,00:09:09,5,And left side see weights viewers like expensive hotels
00:09:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"They gave the expensive hotels 5 Stars, and",00:09:16,5,They gave expensive hotels 5 Stars
00:09:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,you can see their average rates tend to be more for some service.,00:09:21,5,see average rates tend service
00:09:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"And that suggests that people like expensive hotels because of good service,",00:09:24,5,And suggests people like expensive hotels good service
00:09:30,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,and that's not surprising.,00:09:29,5,surprising
00:09:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,That's also another way to validate it by inferred weights.,00:09:30,5,That also another way validate inferred weights
00:09:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"If you look at the right side where, look at the column of 5 Stars.",00:09:34,5,If look right side look column 5 Stars
00:09:43,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"These are the reviewers that like the cheaper hotels, and",00:09:40,5,These reviewers like cheaper hotels
00:09:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,they gave cheaper hotels five stars.,00:09:43,5,gave cheaper hotels five stars
00:09:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"As we expected and they put more weight on value,",00:09:45,5,As expected put weight value
00:09:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,and that's why they like the cheaper hotels.,00:09:48,5,like cheaper hotels
00:09:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"But if you look at the, when they didn't like expensive hotels, or cheaper hotels,",00:09:52,5,But look like expensive hotels cheaper hotels
00:10:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,then you'll see that they tended to have more weights on the condition of,00:09:56,5,see tended weights condition
00:10:03,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,the room cleanness.,00:10:00,5,room cleanness
00:10:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So this shows that by using this model, we can infer some",00:10:04,5,So shows using model infer
00:10:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,information that's very hard to obtain even if you read all the reviews.,00:10:08,5,information hard obtain even read reviews
00:10:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Even if you read all the reviews it's very hard to infer such preferences or,00:10:13,5,Even read reviews hard infer preferences
00:10:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,such emphasis.,00:10:18,5,emphasis
00:10:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So this is a case where text mining algorithms can go beyond what,00:10:20,5,So case text mining algorithms go beyond
00:10:27,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"humans can do, to review interesting patterns in the data.",00:10:24,5,humans review interesting patterns data
00:10:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And this of course can be very useful.,00:10:27,5,And course useful
00:10:32,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"You can compare different hotels,",00:10:29,5,You compare different hotels
00:10:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"compare the opinions from different consumer groups, in different locations.",00:10:32,5,compare opinions different consumer groups different locations
00:10:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"And of course, the model is general.",00:10:37,5,And course model general
00:10:43,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,It can be applied to any reviews with overall ratings.,00:10:39,5,It applied reviews overall ratings
00:10:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So this is a very useful technique that can,00:10:43,5,So useful technique
00:10:47,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,support a lot of text mining applications.,00:10:45,5,support lot text mining applications
00:10:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Finally the results of applying this model for personalized ranking or,00:10:50,5,Finally results applying model personalized ranking
00:10:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,recommendation of entities.,00:10:54,5,recommendation entities
00:11:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So because we can infer the reviewers weights on different dimensions,",00:10:57,5,So infer reviewers weights different dimensions
00:11:06,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,we can allow a user to actually say what do you care about.,00:11:02,5,allow user actually say care
00:11:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So for example, I have a query here that shows 90% of the weight",00:11:06,5,So example I query shows 90 weight
00:11:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,should be on value and 10% on others.,00:11:09,5,value 10 others
00:11:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So that just means I don't care about other aspect.,00:11:12,5,So means I care aspect
00:11:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,I just care about getting a cheaper hotel.,00:11:15,5,I care getting cheaper hotel
00:11:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,My emphasis is on the value dimension.,00:11:17,5,My emphasis value dimension
00:11:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now what we can do with such query is we can use reviewers that we,00:11:21,5,Now query use reviewers
00:11:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,believe have a similar preference to recommend a hotels for you.,00:11:26,5,believe similar preference recommend hotels
00:11:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,How can we know that?,00:11:31,5,How know
00:11:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Well, we can infer the weights of those reviewers on different aspects.",00:11:31,5,Well infer weights reviewers different aspects
00:11:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"We can find the reviewers whose weights are more precise,",00:11:36,5,We find reviewers whose weights precise
00:11:43,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,of course inferred rates are similar to yours.,00:11:40,5,course inferred rates similar
00:11:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And then use those reviewers to recommend hotels for you and,00:11:43,5,And use reviewers recommend hotels
00:11:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,this is what we call personalized or rather query specific recommendations.,00:11:46,5,call personalized rather query specific recommendations
00:11:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"Now the non-personalized recommendations now shown on the top,",00:11:51,5,Now non personalized recommendations shown top
00:12:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"and you can see the top results generally have much higher price, than the lower",00:11:56,5,see top results generally much higher price lower
00:12:06,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,group and that's because when the reviewer's cared more about the value as,00:12:01,5,group reviewer cared value
00:12:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,dictated by this query they tended to really favor low price hotels.,00:12:06,5,dictated query tended really favor low price hotels
00:12:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So this is yet another application of this technique.,00:12:12,5,So yet another application technique
00:12:22,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,It shows that by doing text mining we can understand the users better.,00:12:18,5,It shows text mining understand users better
00:12:25,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And once we can handle users better we can solve these users better.,00:12:22,5,And handle users better solve users better
00:12:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"So to summarize our discussion of opinion mining in general,",00:12:25,5,So summarize discussion opinion mining general
00:12:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,this is a very important topic and with a lot of applications.,00:12:28,5,important topic lot applications
00:12:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And as a text sentiment analysis can be readily done by,00:12:33,5,And text sentiment analysis readily done
00:12:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,using just text categorization.,00:12:37,5,using text categorization
00:12:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,But standard technique tends to not be enough.,00:12:39,5,But standard technique tends enough
00:12:44,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And so we need to have enriched feature implementation.,00:12:41,5,And need enriched feature implementation
00:12:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And we also need to consider the order of those categories.,00:12:45,5,And also need consider order categories
00:12:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And we'll talk about ordinal regression for some of these problem.,00:12:48,5,And talk ordinal regression problem
00:12:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,We have also assume that the generating models are powerful for,00:12:52,5,We also assume generating models powerful
00:12:57,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,mining latent user preferences.,00:12:55,5,mining latent user preferences
00:13:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,This in particular in the generative model for mining latent regression.,00:12:57,5,This particular generative model mining latent regression
00:13:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And we embed some interesting preference information and,00:13:02,5,And embed interesting preference information
00:13:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,send the weights of words in the model as a result we can learn most,00:13:05,5,send weights words model result learn
00:13:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,useful information when fitting the model to the data.,00:13:09,5,useful information fitting model data
00:13:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Now most approaches have been proposed and evaluated.,00:13:13,5,Now approaches proposed evaluated
00:13:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"For product reviews, and that was because in such a context, the opinion holder and",00:13:16,5,For product reviews context opinion holder
00:13:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,the opinion target are clear.,00:13:21,5,opinion target clear
00:13:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,And they are easy to analyze.,00:13:23,5,And easy analyze
00:13:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"And there, of course, also have a lot of practical applications.",00:13:26,5,And course also lot practical applications
00:13:35,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"But opinion mining from news and social media is also important, but that's",00:13:29,5,But opinion mining news social media also important
00:13:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"more difficult than analyzing review data, mainly because the opinion holders and",00:13:35,5,difficult analyzing review data mainly opinion holders
00:13:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,opinion targets are all interested.,00:13:40,5,opinion targets interested
00:13:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,So that calls for,00:13:45,5,So calls
00:13:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,natural management processing techniques to uncover them accurately.,00:13:46,5,natural management processing techniques uncover accurately
00:13:53,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,Here are some suggested readings.,00:13:50,5,Here suggested readings
00:13:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"The first two are small books that are of some use of this topic,",00:13:53,5,The first two small books use topic
00:14:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,where you can find a lot of discussion about other variations of the problem and,00:13:59,5,find lot discussion variations problem
00:14:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,techniques proposed for solving the problem.,00:14:04,5,techniques proposed solving problem
00:14:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,The next two papers about generating models for,00:14:08,5,The next two papers generating models
00:14:14,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,rating the aspect rating analysis.,00:14:12,5,rating aspect rating analysis
00:14:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,"The first one is about solving the problem using two stages, and",00:14:14,5,The first one solving problem using two stages
00:14:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,the second one is about a unified model where the topic model is integrated,00:14:18,5,second one unified model topic model integrated
00:14:27,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2,4.5,with the regression model to solve the problem using a unified model.,00:14:23,5,regression model solve problem using unified model
00:00:06,2,Syntagmatic Relation Discovery- Entropy,1.10,[SOUND].,00:00:00,10,SOUND
00:00:13,2,Syntagmatic Relation Discovery- Entropy,1.10,"This lecture is about the syntagmatic relation discovery, and entropy.",00:00:06,10,This lecture syntagmatic relation discovery entropy
00:00:17,2,Syntagmatic Relation Discovery- Entropy,1.10,"In this lecture, we're going to continue talking about word association mining.",00:00:13,10,In lecture going continue talking word association mining
00:00:22,2,Syntagmatic Relation Discovery- Entropy,1.10,"In particular, we're going to talk about how to discover syntagmatic relations.",00:00:17,10,In particular going talk discover syntagmatic relations
00:00:25,2,Syntagmatic Relation Discovery- Entropy,1.10,"And we're going to start with the introduction of entropy,",00:00:22,10,And going start introduction entropy
00:00:29,2,Syntagmatic Relation Discovery- Entropy,1.10,which is the basis for designing some measures for discovering such relations.,00:00:25,10,basis designing measures discovering relations
00:00:33,2,Syntagmatic Relation Discovery- Entropy,1.10,"By definition,",00:00:32,10,By definition
00:00:39,2,Syntagmatic Relation Discovery- Entropy,1.10,syntagmatic relations hold between words that have correlated co-occurrences.,00:00:33,10,syntagmatic relations hold words correlated co occurrences
00:00:44,2,Syntagmatic Relation Discovery- Entropy,1.10,"That means, when we see one word occurs in context,",00:00:39,10,That means see one word occurs context
00:00:47,2,Syntagmatic Relation Discovery- Entropy,1.10,we tend to see the occurrence of the other word.,00:00:44,10,tend see occurrence word
00:00:53,2,Syntagmatic Relation Discovery- Entropy,1.10,"So, take a more specific example, here.",00:00:48,10,So take specific example
00:00:55,2,Syntagmatic Relation Discovery- Entropy,1.10,"We can ask the question,",00:00:53,10,We ask question
00:00:59,2,Syntagmatic Relation Discovery- Entropy,1.10,"whenever eats occurs, what other words also tend to occur?",00:00:55,10,whenever eats occurs words also tend occur
00:01:06,2,Syntagmatic Relation Discovery- Entropy,1.10,"Looking at the sentences on the left, we see some words that might occur",00:01:01,10,Looking sentences left see words might occur
00:01:11,2,Syntagmatic Relation Discovery- Entropy,1.10,"together with eats, like cat, dog, or fish is right.",00:01:06,10,together eats like cat dog fish right
00:01:15,2,Syntagmatic Relation Discovery- Entropy,1.10,But if I take them out and if you look at the right side where we,00:01:11,10,But I take look right side
00:01:21,2,Syntagmatic Relation Discovery- Entropy,1.10,"only show eats and some other words, the question then is.",00:01:15,10,show eats words question
00:01:27,2,Syntagmatic Relation Discovery- Entropy,1.10,Can you predict what other words occur to the left or to the right?,00:01:21,10,Can predict words occur left right
00:01:31,2,Syntagmatic Relation Discovery- Entropy,1.10,Right so this would force us to think about what,00:01:28,10,Right would force us think
00:01:33,2,Syntagmatic Relation Discovery- Entropy,1.10,other words are associated with eats.,00:01:31,10,words associated eats
00:01:37,2,Syntagmatic Relation Discovery- Entropy,1.10,"If they are associated with eats, they tend to occur in the context of eats.",00:01:33,10,If associated eats tend occur context eats
00:01:43,2,Syntagmatic Relation Discovery- Entropy,1.10,More specifically our prediction problem is to take,00:01:38,10,More specifically prediction problem take
00:01:47,2,Syntagmatic Relation Discovery- Entropy,1.10,"any text segment which can be a sentence, a paragraph, or a document.",00:01:43,10,text segment sentence paragraph document
00:01:51,2,Syntagmatic Relation Discovery- Entropy,1.10,"And then ask I the question, is a particular word present or",00:01:47,10,And ask I question particular word present
00:01:52,2,Syntagmatic Relation Discovery- Entropy,1.10,absent in this segment?,00:01:51,10,absent segment
00:01:57,2,Syntagmatic Relation Discovery- Entropy,1.10,Right here we ask about the word W.,00:01:54,10,Right ask word W
00:02:00,2,Syntagmatic Relation Discovery- Entropy,1.10,Is W present or absent in this segment?,00:01:57,10,Is W present absent segment
00:02:05,2,Syntagmatic Relation Discovery- Entropy,1.10,Now what's interesting is that,00:02:02,10,Now interesting
00:02:08,2,Syntagmatic Relation Discovery- Entropy,1.10,some words are actually easier to predict than other words.,00:02:05,10,words actually easier predict words
00:02:14,2,Syntagmatic Relation Discovery- Entropy,1.10,"If you take a look at the three words shown here, meat, the, and",00:02:10,10,If take look three words shown meat
00:02:17,2,Syntagmatic Relation Discovery- Entropy,1.10,"unicorn, which one do you think is easier to predict?",00:02:14,10,unicorn one think easier predict
00:02:23,2,Syntagmatic Relation Discovery- Entropy,1.10,Now if you think about it for a moment you might conclude that,00:02:20,10,Now think moment might conclude
00:02:27,2,Syntagmatic Relation Discovery- Entropy,1.10,the is easier to predict because it tends to occur everywhere.,00:02:24,10,easier predict tends occur everywhere
00:02:30,2,Syntagmatic Relation Discovery- Entropy,1.10,"So I can just say, well that would be in the sentence.",00:02:27,10,So I say well would sentence
00:02:37,2,Syntagmatic Relation Discovery- Entropy,1.10,"Unicorn is also relatively easy because unicorn is rare, is very rare.",00:02:31,10,Unicorn also relatively easy unicorn rare rare
00:02:41,2,Syntagmatic Relation Discovery- Entropy,1.10,And I can bet that it doesn't occur in this sentence.,00:02:37,10,And I bet occur sentence
00:02:46,2,Syntagmatic Relation Discovery- Entropy,1.10,But meat is somewhere in between in terms of frequency.,00:02:42,10,But meat somewhere terms frequency
00:02:50,2,Syntagmatic Relation Discovery- Entropy,1.10,And it makes it harder to predict because it's possible that it occurs in a sentence,00:02:46,10,And makes harder predict possible occurs sentence
00:02:52,2,Syntagmatic Relation Discovery- Entropy,1.10,"or the segment, more accurately.",00:02:50,10,segment accurately
00:02:58,2,Syntagmatic Relation Discovery- Entropy,1.10,"But it may also not occur in the sentence, so",00:02:53,10,But may also occur sentence
00:03:01,2,Syntagmatic Relation Discovery- Entropy,1.10,now let's study this problem more formally.,00:02:58,10,let study problem formally
00:03:06,2,Syntagmatic Relation Discovery- Entropy,1.10,So the problem can be formally defined,00:03:02,10,So problem formally defined
00:03:10,2,Syntagmatic Relation Discovery- Entropy,1.10,as predicting the value of a binary random variable.,00:03:06,10,predicting value binary random variable
00:03:14,2,Syntagmatic Relation Discovery- Entropy,1.10,"Here we denote it by X sub w, w denotes a word, so",00:03:10,10,Here denote X sub w w denotes word
00:03:17,2,Syntagmatic Relation Discovery- Entropy,1.10,this random variable is associated with precisely one word.,00:03:14,10,random variable associated precisely one word
00:03:23,2,Syntagmatic Relation Discovery- Entropy,1.10,"When the value of the variable is 1, it means this word is present.",00:03:18,10,When value variable 1 means word present
00:03:26,2,Syntagmatic Relation Discovery- Entropy,1.10,"When it's 0, it means the word is absent.",00:03:23,10,When 0 means word absent
00:03:31,2,Syntagmatic Relation Discovery- Entropy,1.10,"And naturally, the probabilities for 1 and 0 should sum to 1,",00:03:26,10,And naturally probabilities 1 0 sum 1
00:03:34,2,Syntagmatic Relation Discovery- Entropy,1.10,because a word is either present or absent in a segment.,00:03:31,10,word either present absent segment
00:03:36,2,Syntagmatic Relation Discovery- Entropy,1.10,There's no other choice.,00:03:35,10,There choice
00:03:43,2,Syntagmatic Relation Discovery- Entropy,1.10,So the intuition with this concept earlier can be formally stated as follows.,00:03:38,10,So intuition concept earlier formally stated follows
00:03:48,2,Syntagmatic Relation Discovery- Entropy,1.10,"The more random this random variable is, the more difficult the prediction will be.",00:03:43,10,The random random variable difficult prediction
00:03:53,2,Syntagmatic Relation Discovery- Entropy,1.10,Now the question is how does one quantitatively measure the randomness of,00:03:49,10,Now question one quantitatively measure randomness
00:03:55,2,Syntagmatic Relation Discovery- Entropy,1.10,a random variable like X sub w?,00:03:53,10,random variable like X sub w
00:04:01,2,Syntagmatic Relation Discovery- Entropy,1.10,"How in general, can we quantify the randomness of a variable and",00:03:56,10,How general quantify randomness variable
00:04:04,2,Syntagmatic Relation Discovery- Entropy,1.10,that's why we need a measure called entropy and,00:04:01,10,need measure called entropy
00:04:10,2,Syntagmatic Relation Discovery- Entropy,1.10,this measure introduced in information theory to measure the randomness of X.,00:04:04,10,measure introduced information theory measure randomness X
00:04:13,2,Syntagmatic Relation Discovery- Entropy,1.10,There is also some connection with information here but,00:04:10,10,There also connection information
00:04:15,2,Syntagmatic Relation Discovery- Entropy,1.10,that is beyond the scope of this course.,00:04:13,10,beyond scope course
00:04:20,2,Syntagmatic Relation Discovery- Entropy,1.10,So for our purpose we just treat entropy function,00:04:17,10,So purpose treat entropy function
00:04:22,2,Syntagmatic Relation Discovery- Entropy,1.10,as a function defined on a random variable.,00:04:20,10,function defined random variable
00:04:27,2,Syntagmatic Relation Discovery- Entropy,1.10,"In this case, it is a binary random variable, although the definition can",00:04:22,10,In case binary random variable although definition
00:04:30,2,Syntagmatic Relation Discovery- Entropy,1.10,be easily generalized for a random variable with multiple values.,00:04:27,10,easily generalized random variable multiple values
00:04:34,2,Syntagmatic Relation Discovery- Entropy,1.10,"Now the function form looks like this,",00:04:32,10,Now function form looks like
00:04:39,2,Syntagmatic Relation Discovery- Entropy,1.10,there's the sum of all the possible values for this random variable.,00:04:34,10,sum possible values random variable
00:04:44,2,Syntagmatic Relation Discovery- Entropy,1.10,Inside the sum for each value we have a product of the probability,00:04:39,10,Inside sum value product probability
00:04:52,2,Syntagmatic Relation Discovery- Entropy,1.10,that the random variable equals this value and log of this probability.,00:04:45,10,random variable equals value log probability
00:04:55,2,Syntagmatic Relation Discovery- Entropy,1.10,And note that there is also a negative sign there.,00:04:53,10,And note also negative sign
00:04:59,2,Syntagmatic Relation Discovery- Entropy,1.10,Now entropy in general is non-negative.,00:04:56,10,Now entropy general non negative
00:05:01,2,Syntagmatic Relation Discovery- Entropy,1.10,And that can be mathematically proved.,00:04:59,10,And mathematically proved
00:05:10,2,Syntagmatic Relation Discovery- Entropy,1.10,"So if we expand this sum, we'll see that the equation looks like the second one.",00:05:02,10,So expand sum see equation looks like second one
00:05:14,2,Syntagmatic Relation Discovery- Entropy,1.10,"Where I explicitly plugged in the two values, 0 and 1.",00:05:10,10,Where I explicitly plugged two values 0 1
00:05:18,2,Syntagmatic Relation Discovery- Entropy,1.10,"And sometimes when we have 0 log of 0,",00:05:14,10,And sometimes 0 log 0
00:05:25,2,Syntagmatic Relation Discovery- Entropy,1.10,"we would generally define that as 0, because log of 0 is undefined.",00:05:18,10,would generally define 0 log 0 undefined
00:05:30,2,Syntagmatic Relation Discovery- Entropy,1.10,So this is the entropy function.,00:05:28,10,So entropy function
00:05:33,2,Syntagmatic Relation Discovery- Entropy,1.10,And this function will give a different value for,00:05:30,10,And function give different value
00:05:35,2,Syntagmatic Relation Discovery- Entropy,1.10,different distributions of this random variable.,00:05:33,10,different distributions random variable
00:05:40,2,Syntagmatic Relation Discovery- Entropy,1.10,And it clearly depends on the probability,00:05:37,10,And clearly depends probability
00:05:43,2,Syntagmatic Relation Discovery- Entropy,1.10,that the random variable taking value of 1 or 0.,00:05:40,10,random variable taking value 1 0
00:05:49,2,Syntagmatic Relation Discovery- Entropy,1.10,If we plot this function against,00:05:43,10,If plot function
00:05:55,2,Syntagmatic Relation Discovery- Entropy,1.10,the probability that the random variable is equal to 1.,00:05:49,10,probability random variable equal 1
00:05:59,2,Syntagmatic Relation Discovery- Entropy,1.10,And then the function looks like this.,00:05:56,10,And function looks like
00:06:06,2,Syntagmatic Relation Discovery- Entropy,1.10,"At the two ends, that means when the probability of X",00:06:01,10,At two ends means probability X
00:06:13,2,Syntagmatic Relation Discovery- Entropy,1.10,"equals 1 is very small or very large, then the entropy function has a low value.",00:06:07,10,equals 1 small large entropy function low value
00:06:18,2,Syntagmatic Relation Discovery- Entropy,1.10,When it's 0.5 in the middle then it reaches the maximum.,00:06:13,10,When 0 5 middle reaches maximum
00:06:24,2,Syntagmatic Relation Discovery- Entropy,1.10,Now if we plot the function against the probability that X,00:06:20,10,Now plot function probability X
00:06:31,2,Syntagmatic Relation Discovery- Entropy,1.10,is taking a value of 0 and the function,00:06:25,10,taking value 0 function
00:06:37,2,Syntagmatic Relation Discovery- Entropy,1.10,"would show exactly the same curve here, and you can imagine why.",00:06:31,10,would show exactly curve imagine
00:06:40,2,Syntagmatic Relation Discovery- Entropy,1.10,And so that's because,00:06:37,10,And
00:06:46,2,Syntagmatic Relation Discovery- Entropy,1.10,"the two probabilities are symmetric, and completely symmetric.",00:06:42,10,two probabilities symmetric completely symmetric
00:06:52,2,Syntagmatic Relation Discovery- Entropy,1.10,So an interesting question you can think about in general is for,00:06:48,10,So interesting question think general
00:06:59,2,Syntagmatic Relation Discovery- Entropy,1.10,what kind of X does entropy reach maximum or minimum.,00:06:52,10,kind X entropy reach maximum minimum
00:07:02,2,Syntagmatic Relation Discovery- Entropy,1.10,And we can in particular think about some special cases.,00:06:59,10,And particular think special cases
00:07:07,2,Syntagmatic Relation Discovery- Entropy,1.10,"For example, in one case, we might have a random variable that",00:07:02,10,For example one case might random variable
00:07:10,2,Syntagmatic Relation Discovery- Entropy,1.10,always takes a value of 1.,00:07:08,10,always takes value 1
00:07:14,2,Syntagmatic Relation Discovery- Entropy,1.10,The probability is 1.,00:07:10,10,The probability 1
00:07:18,2,Syntagmatic Relation Discovery- Entropy,1.10,Or there's a random variable that,00:07:16,10,Or random variable
00:07:24,2,Syntagmatic Relation Discovery- Entropy,1.10,is equally likely taking a value of one or zero.,00:07:19,10,equally likely taking value one zero
00:07:28,2,Syntagmatic Relation Discovery- Entropy,1.10,So in this case the probability that X equals 1 is 0.5.,00:07:24,10,So case probability X equals 1 0 5
00:07:32,2,Syntagmatic Relation Discovery- Entropy,1.10,Now which one has a higher entropy?,00:07:30,10,Now one higher entropy
00:07:38,2,Syntagmatic Relation Discovery- Entropy,1.10,It's easier to look at the problem by thinking of a simple example,00:07:34,10,It easier look problem thinking simple example
00:07:42,2,Syntagmatic Relation Discovery- Entropy,1.10,using coin tossing.,00:07:40,10,using coin tossing
00:07:47,2,Syntagmatic Relation Discovery- Entropy,1.10,"So when we think about random experiments like tossing a coin,",00:07:43,10,So think random experiments like tossing coin
00:07:55,2,Syntagmatic Relation Discovery- Entropy,1.10,"it gives us a random variable, that can represent the result.",00:07:48,10,gives us random variable represent result
00:07:57,2,Syntagmatic Relation Discovery- Entropy,1.10,It can be head or tail.,00:07:55,10,It head tail
00:08:03,2,Syntagmatic Relation Discovery- Entropy,1.10,"So we can define a random variable X sub coin, so that it's 1",00:07:57,10,So define random variable X sub coin 1
00:08:08,2,Syntagmatic Relation Discovery- Entropy,1.10,"when the coin shows up as head, it's 0 when the coin shows up as tail.",00:08:03,10,coin shows head 0 coin shows tail
00:08:15,2,Syntagmatic Relation Discovery- Entropy,1.10,So now we can compute the entropy of this random variable.,00:08:09,10,So compute entropy random variable
00:08:20,2,Syntagmatic Relation Discovery- Entropy,1.10,And this entropy indicates how difficult it is to predict the outcome,00:08:15,10,And entropy indicates difficult predict outcome
00:08:22,2,Syntagmatic Relation Discovery- Entropy,1.10,of a coin toss.,00:08:22,10,coin toss
00:08:27,2,Syntagmatic Relation Discovery- Entropy,1.10,So we can think about the two cases.,00:08:25,10,So think two cases
00:08:29,2,Syntagmatic Relation Discovery- Entropy,1.10,"One is a fair coin, it's completely fair.",00:08:27,10,One fair coin completely fair
00:08:34,2,Syntagmatic Relation Discovery- Entropy,1.10,The coin shows up as head or tail equally likely.,00:08:29,10,The coin shows head tail equally likely
00:08:39,2,Syntagmatic Relation Discovery- Entropy,1.10,So the two probabilities would be a half.,00:08:34,10,So two probabilities would half
00:08:42,2,Syntagmatic Relation Discovery- Entropy,1.10,Right? So both are equal to one half.,00:08:39,10,Right So equal one half
00:08:47,2,Syntagmatic Relation Discovery- Entropy,1.10,"Another extreme case is completely biased coin,",00:08:44,10,Another extreme case completely biased coin
00:08:50,2,Syntagmatic Relation Discovery- Entropy,1.10,where the coin always shows up as heads.,00:08:47,10,coin always shows heads
00:08:52,2,Syntagmatic Relation Discovery- Entropy,1.10,So it's a completely biased coin.,00:08:50,10,So completely biased coin
00:08:57,2,Syntagmatic Relation Discovery- Entropy,1.10,Now let's think about the entropies in the two cases.,00:08:54,10,Now let think entropies two cases
00:09:04,2,Syntagmatic Relation Discovery- Entropy,1.10,And if you plug in these values you can see the entropies would be as follows.,00:08:57,10,And plug values see entropies would follows
00:09:09,2,Syntagmatic Relation Discovery- Entropy,1.10,"For a fair coin we see the entropy reaches its maximum, that's 1.",00:09:04,10,For fair coin see entropy reaches maximum 1
00:09:14,2,Syntagmatic Relation Discovery- Entropy,1.10,"For the completely biased coin, we see it's 0.",00:09:11,10,For completely biased coin see 0
00:09:17,2,Syntagmatic Relation Discovery- Entropy,1.10,And that intuitively makes a lot of sense.,00:09:14,10,And intuitively makes lot sense
00:09:20,2,Syntagmatic Relation Discovery- Entropy,1.10,Because a fair coin is most difficult to predict.,00:09:17,10,Because fair coin difficult predict
00:09:24,2,Syntagmatic Relation Discovery- Entropy,1.10,Whereas a completely biased coin is very easy to predict.,00:09:22,10,Whereas completely biased coin easy predict
00:09:26,2,Syntagmatic Relation Discovery- Entropy,1.10,"We can always say, well, it's a head.",00:09:24,10,We always say well head
00:09:29,2,Syntagmatic Relation Discovery- Entropy,1.10,Because it is a head all the time.,00:09:26,10,Because head time
00:09:34,2,Syntagmatic Relation Discovery- Entropy,1.10,So they can be shown on the curve as follows.,00:09:29,10,So shown curve follows
00:09:40,2,Syntagmatic Relation Discovery- Entropy,1.10,So the fair coin corresponds to the middle point where it's very uncertain.,00:09:34,10,So fair coin corresponds middle point uncertain
00:09:45,2,Syntagmatic Relation Discovery- Entropy,1.10,The completely biased coin corresponds to the end,00:09:40,10,The completely biased coin corresponds end
00:09:48,2,Syntagmatic Relation Discovery- Entropy,1.10,point where we have a probability of 1.0 and the entropy is 0.,00:09:45,10,point probability 1 0 entropy 0
00:09:54,2,Syntagmatic Relation Discovery- Entropy,1.10,"So, now let's see how we can use entropy for word prediction.",00:09:48,10,So let see use entropy word prediction
00:09:59,2,Syntagmatic Relation Discovery- Entropy,1.10,Let's think about our problem is to predict whether W is present or,00:09:54,10,Let think problem predict whether W present
00:10:01,2,Syntagmatic Relation Discovery- Entropy,1.10,absent in this segment.,00:09:59,10,absent segment
00:10:05,2,Syntagmatic Relation Discovery- Entropy,1.10,"Again, think about the three words, particularly think about their entropies.",00:10:01,10,Again think three words particularly think entropies
00:10:10,2,Syntagmatic Relation Discovery- Entropy,1.10,Now we can assume high entropy words are harder to predict.,00:10:06,10,Now assume high entropy words harder predict
00:10:18,2,Syntagmatic Relation Discovery- Entropy,1.10,And so we now have a quantitative way to tell us which word is harder to predict.,00:10:11,10,And quantitative way tell us word harder predict
00:10:25,2,Syntagmatic Relation Discovery- Entropy,1.10,"Now if you look at the three words meat, the, unicorn, again, and",00:10:20,10,Now look three words meat unicorn
00:10:33,2,Syntagmatic Relation Discovery- Entropy,1.10,we clearly would expect meat to have a higher entropy than the unicorn.,00:10:25,10,clearly would expect meat higher entropy unicorn
00:10:39,2,Syntagmatic Relation Discovery- Entropy,1.10,"In fact if you look at the entropy of the, it's close to zero.",00:10:33,10,In fact look entropy close zero
00:10:41,2,Syntagmatic Relation Discovery- Entropy,1.10,Because it occurs everywhere.,00:10:39,10,Because occurs everywhere
00:10:43,2,Syntagmatic Relation Discovery- Entropy,1.10,So it's like a completely biased coin.,00:10:41,10,So like completely biased coin
00:10:46,2,Syntagmatic Relation Discovery- Entropy,1.10,Therefore the entropy is zero.,00:10:44,10,Therefore entropy zero
00:00:04,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"[SOUND] This lecture is about,",00:00:00,1,SOUND This lecture
00:00:11,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Opinion Mining and Sentiment Analysis,",00:00:04,1,Opinion Mining Sentiment Analysis
00:00:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"covering, Motivation.",00:00:11,1,covering Motivation
00:00:18,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In this lecture, we're going to start, talking about,",00:00:15,1,In lecture going start talking
00:00:21,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,mining a different kind of knowledge.,00:00:18,1,mining different kind knowledge
00:00:27,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Namely, knowledge about the observer or humans that have generated the text data.",00:00:21,1,Namely knowledge observer humans generated text data
00:00:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In particular, we're going to talk about the opinion mining and sentiment analysis.",00:00:27,1,In particular going talk opinion mining sentiment analysis
00:00:37,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"As we discussed earlier, text data can be regarded as data generated",00:00:32,1,As discussed earlier text data regarded data generated
00:00:41,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,from humans as subjective sensors.,00:00:37,1,humans subjective sensors
00:00:50,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In contrast, we have other devices such as video recorder that can report what's",00:00:43,1,In contrast devices video recorder report
00:00:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,happening in the real world objective to generate the viewer data for example.,00:00:50,1,happening real world objective generate viewer data example
00:01:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now the main difference between test data and other data, like video data,",00:00:58,1,Now main difference test data data like video data
00:01:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"is that it has rich opinions,",00:01:03,1,rich opinions
00:01:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,and the content tends to be subjective because it's generated from humans.,00:01:10,1,content tends subjective generated humans
00:01:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, this is actually a unique advantaged of text data, as compared with other data,",00:01:16,1,Now actually unique advantaged text data compared data
00:01:28,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,because the office is a great opportunity to understand the observers.,00:01:22,1,office great opportunity understand observers
00:01:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,We can mine text data to understand their opinions.,00:01:28,1,We mine text data understand opinions
00:01:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Understand people's preferences, how people think about something.",00:01:31,1,Understand people preferences people think something
00:01:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So this lecture and the following lectures will be mainly about how we can mine and,00:01:37,1,So lecture following lectures mainly mine
00:01:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,analyze opinions buried in a lot of text data.,00:01:43,1,analyze opinions buried lot text data
00:01:53,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So let's start with the concept of opinion.,00:01:49,1,So let start concept opinion
00:01:57,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"It's not that easy to formally define opinion, but",00:01:53,1,It easy formally define opinion
00:02:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,mostly we would define opinion as a subjective,00:01:57,1,mostly would define opinion subjective
00:02:07,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,statement describing what a person believes or thinks about something.,00:02:02,1,statement describing person believes thinks something
00:02:11,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, I highlighted quite a few words here.",00:02:08,1,Now I highlighted quite words
00:02:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that's because it's worth thinking a little bit more about these words.,00:02:11,1,And worth thinking little bit words
00:02:20,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that will help us better understand what's in an opinion.,00:02:15,1,And help us better understand opinion
00:02:25,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And this further helps us to define opinion more formally.,00:02:20,1,And helps us define opinion formally
00:02:30,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Which is always needed to computation to resolve the problem of opinion mining.,00:02:25,1,Which always needed computation resolve problem opinion mining
00:02:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So let's first look at the key word of subjective here.,00:02:30,1,So let first look key word subjective
00:02:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,This is in contrast with objective statement or factual statement.,00:02:35,1,This contrast objective statement factual statement
00:02:44,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Those statements can be proved right or wrong.,00:02:40,1,Those statements proved right wrong
00:02:49,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And this is a key differentiating factor from opinions,00:02:45,1,And key differentiating factor opinions
00:02:53,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,which tends to be not easy to prove wrong or,00:02:49,1,tends easy prove wrong
00:02:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"right, because it reflects what the person thinks about something.",00:02:53,1,right reflects person thinks something
00:03:06,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So in contrast, objective statement can usually be proved wrong or correct.",00:02:59,1,So contrast objective statement usually proved wrong correct
00:03:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, you might say this computer has a screen and a battery.",00:03:07,1,For example might say computer screen battery
00:03:18,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now that's something you can check.,00:03:16,1,Now something check
00:03:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,It's either having a battery or not.,00:03:18,1,It either battery
00:03:28,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"But in contrast with this, think about the sentence such as, this laptop has",00:03:23,1,But contrast think sentence laptop
00:03:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,the best battery or this laptop has a nice screen.,00:03:28,1,best battery laptop nice screen
00:03:38,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now these statements are more subjective and,00:03:34,1,Now statements subjective
00:03:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,it's very hard to prove whether it's wrong or correct.,00:03:38,1,hard prove whether wrong correct
00:03:49,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So opinion, is a subjective statement.",00:03:45,1,So opinion subjective statement
00:03:54,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And next lets look at the keyword person here.,00:03:50,1,And next lets look keyword person
00:03:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that indicates that is an opinion holder.,00:03:54,1,And indicates opinion holder
00:04:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Because when we talk about opinion, it's about an opinion held by someone.",00:03:56,1,Because talk opinion opinion held someone
00:04:04,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And then we notice that there is something here.,00:04:02,1,And notice something
00:04:06,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So that is the target of the opinion.,00:04:04,1,So target opinion
00:04:09,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The opinion is expressed on this something.,00:04:06,1,The opinion expressed something
00:04:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And now, of course, believes or thinks implies that",00:04:11,1,And course believes thinks implies
00:04:21,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,an opinion will depend on the culture or background and the context in general.,00:04:15,1,opinion depend culture background context general
00:04:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Because a person might think different in a different context.,00:04:21,1,Because person might think different different context
00:04:29,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,People from different background may also think in different ways.,00:04:24,1,People different background may also think different ways
00:04:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So this analysis shows that there are multiple elements that we need to include,00:04:29,1,So analysis shows multiple elements need include
00:04:36,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,in order to characterize opinion.,00:04:34,1,order characterize opinion
00:04:42,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, what's a basic opinion representation like?",00:04:38,1,So basic opinion representation like
00:04:46,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Well, it should include at least three elements, right?",00:04:42,1,Well include least three elements right
00:04:49,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Firstly, it has to specify what's the opinion holder.",00:04:46,1,Firstly specify opinion holder
00:04:51,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So whose opinion is this?,00:04:49,1,So whose opinion
00:04:55,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Second, it must also specify the target, what's this opinion about?",00:04:51,1,Second must also specify target opinion
00:05:00,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And third, of course, we want opinion content.",00:04:57,1,And third course want opinion content
00:05:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And so what exactly is opinion?,00:05:00,1,And exactly opinion
00:05:05,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"If you can identify these,",00:05:03,1,If identify
00:05:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,we get a basic understanding of opinion and can already be useful sometimes.,00:05:05,1,get basic understanding opinion already useful sometimes
00:05:14,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"You want to understand further, we want enriched opinion representation.",00:05:10,1,You want understand want enriched opinion representation
00:05:17,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And that means we also want to understand that, for example,",00:05:15,1,And means also want understand example
00:05:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,the context of the opinion and what situation was the opinion expressed.,00:05:17,1,context opinion situation opinion expressed
00:05:26,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, what time was it expressed?",00:05:22,1,For example time expressed
00:05:32,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"We, also, would like to, people understand the opinion sentiment, and this is",00:05:26,1,We also would like people understand opinion sentiment
00:05:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,to understand that what the opinion tells us about the opinion holder's feeling.,00:05:32,1,understand opinion tells us opinion holder feeling
00:05:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, is this opinion positive, or negative?",00:05:39,1,For example opinion positive negative
00:05:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Or perhaps the opinion holder was happy or was sad, and",00:05:43,1,Or perhaps opinion holder happy sad
00:05:52,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,so such understanding obvious to those beyond just Extracting,00:05:47,1,understanding obvious beyond Extracting
00:05:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"the opinion content, it needs some analysis.",00:05:52,1,opinion content needs analysis
00:06:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So let's take a simple example of a product review.,00:06:00,1,So let take simple example product review
00:06:09,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In this case, this actually expressed the opinion holder, and expressed the target.",00:06:03,1,In case actually expressed opinion holder expressed target
00:06:12,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So its obviously whats opinion holder and,00:06:09,1,So obviously whats opinion holder
00:06:16,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,that's just reviewer and its also often very clear whats the opinion target and,00:06:12,1,reviewer also often clear whats opinion target
00:06:21,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,that's the product review for example iPhone 6.,00:06:16,1,product review example iPhone 6
00:06:26,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,When the review is posted usually you can't such information easier.,00:06:21,1,When review posted usually information easier
00:06:30,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now the content, of course, is a review text that's, in general,",00:06:27,1,Now content course review text general
00:06:32,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,also easy to obtain.,00:06:30,1,also easy obtain
00:06:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So you can see product reviews are fairly,00:06:32,1,So see product reviews fairly
00:06:40,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,easy to analyze in terms of obtaining a basic opinion of representation.,00:06:35,1,easy analyze terms obtaining basic opinion representation
00:06:45,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"But of course, if you want to get more information, you might know the Context,",00:06:40,1,But course want get information might know Context
00:06:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,for example.,00:06:45,1,example
00:06:51,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The review was written in 2015.,00:06:47,1,The review written 2015
00:06:57,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Or, we want to know that the sentiment of this review is positive.",00:06:51,1,Or want know sentiment review positive
00:07:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, this additional understanding of course adds value to mining the opinions.",00:06:57,1,So additional understanding course adds value mining opinions
00:07:09,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, you can see in this case the task is relatively easy and that's",00:07:04,1,Now see case task relatively easy
00:07:13,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,because the opinion holder and the opinion target have already been identified.,00:07:09,1,opinion holder opinion target already identified
00:07:17,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now let's take a look at the sentence in the news.,00:07:14,1,Now let take look sentence news
00:07:21,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In this case, we have a implicit holder and a implicit target.",00:07:17,1,In case implicit holder implicit target
00:07:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And the tasker is in general harder.,00:07:21,1,And tasker general harder
00:07:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, we can identify opinion holder here, and that's the governor of Connecticut.",00:07:24,1,So identify opinion holder governor Connecticut
00:07:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,We can also identify the target.,00:07:32,1,We also identify target
00:07:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So one target is Hurricane Sandy, but",00:07:35,1,So one target Hurricane Sandy
00:07:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,there is also another target mentioned which is hurricane of 1938.,00:07:39,1,also another target mentioned hurricane 1938
00:07:45,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So what's the opinion?,00:07:43,1,So opinion
00:07:49,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Well, there's a negative sentiment here",00:07:45,1,Well negative sentiment
00:07:52,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,that's indicated by words like bad and worst.,00:07:49,1,indicated words like bad worst
00:07:59,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And we can also, then, identify context, New England in this case.",00:07:53,1,And also identify context New England case
00:08:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, unlike in the playoff review,",00:08:00,1,Now unlike playoff review
00:08:08,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,all these elements must be extracted by using natural RAM processing techniques.,00:08:03,1,elements must extracted using natural RAM processing techniques
00:08:11,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, the task Is much harder.",00:08:08,1,So task Is much harder
00:08:13,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And we need a deeper natural language processing.,00:08:11,1,And need deeper natural language processing
00:08:16,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And these examples also,00:08:14,1,And examples also
00:08:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,suggest that a lot of work can be easy to done for product reviews.,00:08:17,1,suggest lot work easy done product reviews
00:08:23,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,That's indeed what has happened.,00:08:22,1,That indeed happened
00:08:29,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Analyzing and assembling news is still quite difficult,",00:08:23,1,Analyzing assembling news still quite difficult
00:08:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,it's more difficult than the analysis of opinions in product reviews.,00:08:29,1,difficult analysis opinions product reviews
00:08:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now there are also some other interesting variations.,00:08:36,1,Now also interesting variations
00:08:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In fact, here we're going to examine the variations of opinions,",00:08:39,1,In fact going examine variations opinions
00:08:44,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,more systematically.,00:08:43,1,systematically
00:08:46,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"First, let's think about the opinion holder.",00:08:44,1,First let think opinion holder
00:08:50,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The holder could be an individual or it could be group of people.,00:08:47,1,The holder could individual could group people
00:08:53,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Sometimes, the opinion was from a committee.",00:08:50,1,Sometimes opinion committee
00:08:55,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Or from a whole country of people.,00:08:53,1,Or whole country people
00:08:58,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Opinion target accounts will vary a lot.,00:08:56,1,Opinion target accounts vary lot
00:09:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"It can be about one entity, a particular person, a particular product,",00:08:58,1,It one entity particular person particular product
00:09:04,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"a particular policy, ect.",00:09:02,1,particular policy ect
00:09:08,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,But it could be about a group of products.,00:09:04,1,But could group products
00:09:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Could be about the products from a company in general.,00:09:08,1,Could products company general
00:09:14,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Could also be very specific about one attribute, though.",00:09:11,1,Could also specific one attribute though
00:09:16,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,An attribute of the entity.,00:09:14,1,An attribute entity
00:09:21,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, it's just about the battery of iPhone.",00:09:16,1,For example battery iPhone
00:09:23,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,It could be someone else's opinion.,00:09:21,1,It could someone else opinion
00:09:27,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And one person might comment on another person's Opinion, etc.",00:09:23,1,And one person might comment another person Opinion etc
00:09:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, you can see there is a lot of variation here that will cause",00:09:27,1,So see lot variation cause
00:09:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,the problem to vary a lot.,00:09:31,1,problem vary lot
00:09:38,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, opinion content, of course, can also vary a lot on the surface,",00:09:34,1,Now opinion content course also vary lot surface
00:09:42,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,you can identify one-sentence opinion or one-phrase opinion.,00:09:38,1,identify one sentence opinion one phrase opinion
00:09:45,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"But you can also have longer text to express an opinion,",00:09:42,1,But also longer text express opinion
00:09:46,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,like the whole article.,00:09:45,1,like whole article
00:09:51,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And furthermore we identify the variation in the sentiment or,00:09:48,1,And furthermore identify variation sentiment
00:09:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,emotion damage that's above the feeding of the opinion holder.,00:09:51,1,emotion damage feeding opinion holder
00:10:00,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, we can distinguish a positive versus negative or mutual or",00:09:56,1,So distinguish positive versus negative mutual
00:10:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"happy versus sad, separate.",00:10:00,1,happy versus sad separate
00:10:05,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Finally, the opinion context can also vary.",00:10:03,1,Finally opinion context also vary
00:10:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"We can have a simple context, like different time or different locations.",00:10:05,1,We simple context like different time different locations
00:10:13,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"But there could be also complex contexts,",00:10:10,1,But could also complex contexts
00:10:17,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,such as some background of topic being discussed.,00:10:13,1,background topic discussed
00:10:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So when opinion is expressed in particular discourse context, it has to",00:10:17,1,So opinion expressed particular discourse context
00:10:26,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,be interpreted in different ways than when it's expressed in another context.,00:10:22,1,interpreted different ways expressed another context
00:10:32,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So the context can be very [INAUDIBLE] to entire discourse context of the opinion.,00:10:26,1,So context INAUDIBLE entire discourse context opinion
00:10:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"From computational perspective,",00:10:32,1,From computational perspective
00:10:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,we're mostly interested in what opinions can be extracted from text data.,00:10:34,1,mostly interested opinions extracted text data
00:10:42,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, it turns out that we can also differentiate, distinguish,",00:10:39,1,So turns also differentiate distinguish
00:10:46,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,different kinds of opinions in text data from computation perspective.,00:10:42,1,different kinds opinions text data computation perspective
00:10:50,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"First, the observer might make a comment about opinion targeting,",00:10:46,1,First observer might make comment opinion targeting
00:10:54,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,observe the word So in case we have the author's opinion.,00:10:50,1,observe word So case author opinion
00:10:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, I don't like this phone at all.",00:10:54,1,For example I like phone
00:10:58,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that's an opinion of this author.,00:10:56,1,And opinion author
00:11:07,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In contrast, the text might also report opinions about others.",00:10:59,1,In contrast text might also report opinions others
00:11:13,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So the person could also Make observation about another person's opinion and,00:11:07,1,So person could also Make observation another person opinion
00:11:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,reported this opinion.,00:11:13,1,reported opinion
00:11:19,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So for example, I believe he loves the painting.",00:11:15,1,So example I believe loves painting
00:11:28,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that opinion is really about the It is really expressed by another person here.,00:11:19,1,And opinion really It really expressed another person
00:11:32,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, it doesn't mean this author loves that painting.",00:11:28,1,So mean author loves painting
00:11:38,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So clearly, the two kinds of opinions need to be analyzed in different ways,",00:11:33,1,So clearly two kinds opinions need analyzed different ways
00:11:40,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"and sometimes in product reviews,",00:11:38,1,sometimes product reviews
00:11:45,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"you can see, although mostly the opinions are false from this reviewer.",00:11:40,1,see although mostly opinions false reviewer
00:11:49,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Sometimes, a reviewer might mention opinions of his friend or her friend.",00:11:45,1,Sometimes reviewer might mention opinions friend friend
00:11:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Another complication is that there may be indirect opinions or,00:11:51,1,Another complication may indirect opinions
00:11:59,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,inferred opinions that can be obtained.,00:11:56,1,inferred opinions obtained
00:12:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,By making inferences on,00:11:59,1,By making inferences
00:12:06,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,what's expressed in the text that might not necessarily look like opinion.,00:12:02,1,expressed text might necessarily look like opinion
00:12:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, one statement that might be,",00:12:06,1,For example one statement might
00:12:14,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,this phone ran out of battery in just one hour.,00:12:10,1,phone ran battery one hour
00:12:23,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, this is in a way a factual statement because It's either true or false, right?",00:12:14,1,Now way factual statement It either true false right
00:12:27,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"You can even verify that, but from this statement,",00:12:23,1,You even verify statement
00:12:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,one can also infer some negative opinions about the quality of the battery of,00:12:27,1,one also infer negative opinions quality battery
00:12:37,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"this phone, or the feeling of the opinion holder about the battery.",00:12:31,1,phone feeling opinion holder battery
00:12:40,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The opinion holder clearly wished that the battery do last longer.,00:12:37,1,The opinion holder clearly wished battery last longer
00:12:46,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So these are interesting variations that we need to pay attention to when we,00:12:42,1,So interesting variations need pay attention
00:12:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,extract opinions.,00:12:46,1,extract opinions
00:12:52,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Also, for this reason about indirect opinions,",00:12:47,1,Also reason indirect opinions
00:12:58,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,it's often also very useful to extract whatever the person has said about,00:12:53,1,often also useful extract whatever person said
00:13:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"the product, and sometimes factual sentences like these are also very useful.",00:12:58,1,product sometimes factual sentences like also useful
00:13:05,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So, from a practical viewpoint,",00:13:03,1,So practical viewpoint
00:13:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,sometimes we don't necessarily extract the subject of sentences.,00:13:05,1,sometimes necessarily extract subject sentences
00:13:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Instead, again, all the sentences that are about the opinions are useful for",00:13:10,1,Instead sentences opinions useful
00:13:18,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,understanding the person or understanding the product that we commend.,00:13:15,1,understanding person understanding product commend
00:13:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,So the task of opinion mining can be defined as taking textualized input,00:13:19,1,So task opinion mining defined taking textualized input
00:13:27,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,to generate a set of opinion representations.,00:13:24,1,generate set opinion representations
00:13:32,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Each representation we should identify opinion holder,",00:13:27,1,Each representation identify opinion holder
00:13:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"target, content, and the context.",00:13:32,1,target content context
00:13:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Ideally we can also infer opinion sentiment from the comment and,00:13:34,1,Ideally also infer opinion sentiment comment
00:13:41,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,the context to better understand.,00:13:39,1,context better understand
00:13:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The opinion.,00:13:43,1,The opinion
00:13:48,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now often, some elements of the representation are already known.",00:13:44,1,Now often elements representation already known
00:13:52,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,I just gave a good example in the case of product we'd use,00:13:48,1,I gave good example case product use
00:13:57,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,where the opinion holder and the opinion target are often expressly identified.,00:13:52,1,opinion holder opinion target often expressly identified
00:14:05,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And that's not why this turns out to be one of the simplest opinion mining tasks.,00:13:57,1,And turns one simplest opinion mining tasks
00:14:09,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now, it's interesting to think about the other tasks that might be also simple.",00:14:05,1,Now interesting think tasks might also simple
00:14:12,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Because those are the cases where you can easily build,00:14:09,1,Because cases easily build
00:14:15,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,applications by using opinion mining techniques.,00:14:12,1,applications using opinion mining techniques
00:14:23,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So now that we have talked about what is opinion mining, we have defined the task.",00:14:17,1,So talked opinion mining defined task
00:14:29,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Let's also just talk a little bit about why opinion mining is very important and,00:14:23,1,Let also talk little bit opinion mining important
00:14:31,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,why it's very useful.,00:14:29,1,useful
00:14:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So here, I identify three major reasons, three broad reasons.",00:14:31,1,So I identify three major reasons three broad reasons
00:14:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,The first is it can help decision support.,00:14:35,1,The first help decision support
00:14:41,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,It can help us optimize our decisions.,00:14:39,1,It help us optimize decisions
00:14:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"We often look at other people's opinions, look at read the reviews",00:14:41,1,We often look people opinions look read reviews
00:14:51,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,in order to make a decisions like buying a product or using a service.,00:14:47,1,order make decisions like buying product using service
00:14:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,We also would be interested in others opinions,00:14:52,1,We also would interested others opinions
00:14:59,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,when we decide whom to vote for example.,00:14:56,1,decide vote example
00:15:02,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And policy makers,",00:15:00,1,And policy makers
00:15:07,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,may also want to know people's opinions when designing a new policy.,00:15:02,1,may also want know people opinions designing new policy
00:15:10,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"So that's one general, kind of, applications.",00:15:07,1,So one general kind applications
00:15:12,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And it's very broad, of course.",00:15:10,1,And broad course
00:15:17,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"The second application is to understand people, and this is also very important.",00:15:12,1,The second application understand people also important
00:15:22,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, it could help understand people's preferences.",00:15:17,1,For example could help understand people preferences
00:15:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And this could help us better serve people.,00:15:22,1,And could help us better serve people
00:15:30,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, we optimize a product search engine or optimize a recommender system",00:15:24,1,For example optimize product search engine optimize recommender system
00:15:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"if we know what people are interested in, what people think about product.",00:15:30,1,know people interested people think product
00:15:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"It can also help with advertising, of course, and we can have targeted",00:15:35,1,It also help advertising course targeted
00:15:47,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,advertising if we know what kind of people tend to like what kind of plot.,00:15:39,1,advertising know kind people tend like kind plot
00:15:53,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now the third kind of application can be called voluntary survey.,00:15:48,1,Now third kind application called voluntary survey
00:15:59,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now this is most important research that used to be done by doing surveys,",00:15:53,1,Now important research used done surveys
00:16:00,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,doing manual surveys.,00:15:59,1,manual surveys
00:16:03,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Question, answer it.",00:16:00,1,Question answer
00:16:07,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,People need to feel informs to answer their questions.,00:16:03,1,People need feel informs answer questions
00:16:13,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"Now this is directly related to humans as sensors, and we can usually aggregate",00:16:07,1,Now directly related humans sensors usually aggregate
00:16:18,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,opinions from a lot of humans through kind of assess the general opinion.,00:16:13,1,opinions lot humans kind assess general opinion
00:16:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Now this would be very useful for business intelligence where manufacturers,00:16:18,1,Now would useful business intelligence manufacturers
00:16:29,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,want to know where their products have advantages over others.,00:16:24,1,want know products advantages others
00:16:33,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"What are the winning features of their products,",00:16:31,1,What winning features products
00:16:35,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,winning features of competitive products.,00:16:33,1,winning features competitive products
00:16:40,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Market research has to do with understanding consumers oppinions.,00:16:37,1,Market research understanding consumers oppinions
00:16:43,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And this create very useful directive for that.,00:16:40,1,And create useful directive
00:16:48,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,Data-driven social science research can benefit from this because they can,00:16:43,1,Data driven social science research benefit
00:16:52,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,do text mining to understand the people's opinions.,00:16:48,1,text mining understand people opinions
00:16:56,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"And if you can aggregate a lot of opinions from social media, from a lot of, popular",00:16:52,1,And aggregate lot opinions social media lot popular
00:17:04,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,information then you can actually do some study of some questions.,00:16:58,1,information actually study questions
00:17:12,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"For example, we can study the behavior of people on social media on social networks.",00:17:04,1,For example study behavior people social media social networks
00:17:17,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And these can be regarded as voluntary survey done by those people.,00:17:12,1,And regarded voluntary survey done people
00:17:24,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,"In general, we can gain a lot of advantage in any prediction task because we can",00:17:19,1,In general gain lot advantage prediction task
00:17:29,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,leverage the text data as extra data above any problem.,00:17:24,1,leverage text data extra data problem
00:17:34,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,And so we can use text based prediction techniques to help you,00:17:29,1,And use text based prediction techniques help
00:17:39,5,Opinion Mining and Sentiment Analysis- Motivation,4.1,make predictions or improve the accuracy of prediction.,00:17:34,1,make predictions improve accuracy prediction
00:00:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,[NOISE] This lecture is about the ordinal,00:00:00,3,NOISE This lecture ordinal
00:00:12,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,logistic regression for sentiment analysis.,00:00:05,3,logistic regression sentiment analysis
00:00:18,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So, this is our problem set up for a typical sentiment classification problem.",00:00:12,3,So problem set typical sentiment classification problem
00:00:21,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Or more specifically a rating prediction.,00:00:18,3,Or specifically rating prediction
00:00:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"We have an opinionated text document d as input, and we want to generate as output,",00:00:21,3,We opinionated text document input want generate output
00:00:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,a rating in the range of 1 through k so,00:00:27,3,rating range 1 k
00:00:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"it's a discrete rating, and this is a categorization problem.",00:00:30,3,discrete rating categorization problem
00:00:38,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,We have k categories here.,00:00:37,3,We k categories
00:00:40,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now we could use a regular text for,00:00:38,3,Now could use regular text
00:00:42,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,categorization technique to solve this problem.,00:00:40,3,categorization technique solve problem
00:00:48,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,But such a solution would not consider the order and dependency of the categories.,00:00:42,3,But solution would consider order dependency categories
00:00:53,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Intuitively, the features that can distinguish category 2 from 1,",00:00:48,3,Intuitively features distinguish category 2 1
00:00:58,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"or rather rating 2 from 1, may be similar to",00:00:53,3,rather rating 2 1 may similar
00:01:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,those that can distinguish k from k-1.,00:00:58,3,distinguish k k 1
00:01:08,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"For example, positive words generally suggest a higher rating.",00:01:02,3,For example positive words generally suggest higher rating
00:01:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,When we train categorization,00:01:08,3,When train categorization
00:01:16,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,problem by treating these categories as independent we would not capture this.,00:01:11,3,problem treating categories independent would capture
00:01:18,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So what's the solution?,00:01:17,3,So solution
00:01:23,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Well in general we can order to classify and there are many different approaches.,00:01:18,3,Well general order classify many different approaches
00:01:26,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And here we're going to talk about one of them that,00:01:23,3,And going talk one
00:01:29,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,called ordinal logistic regression.,00:01:26,3,called ordinal logistic regression
00:01:33,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now, let's first think about how we use logistical regression for",00:01:29,3,Now let first think use logistical regression
00:01:34,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,a binary sentiment.,00:01:33,3,binary sentiment
00:01:36,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,A categorization problem.,00:01:34,3,A categorization problem
00:01:40,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So suppose we just wanted to distinguish a positive from a negative and,00:01:36,3,So suppose wanted distinguish positive negative
00:01:44,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,that is just a two category categorization problem.,00:01:40,3,two category categorization problem
00:01:47,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So the predictors are represented as X and these are the features.,00:01:44,3,So predictors represented X features
00:01:50,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And there are M features all together.,00:01:47,3,And M features together
00:01:52,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,The feature value is a real number.,00:01:50,3,The feature value real number
00:01:55,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And this can be representation of a text document.,00:01:52,3,And representation text document
00:02:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And why it has two values, binary response variable 0 or 1.",00:01:56,3,And two values binary response variable 0 1
00:02:04,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"1 means X is positive, 0 means X is negative.",00:02:02,3,1 means X positive 0 means X negative
00:02:09,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And then of course this is a standard two category categorization problem.,00:02:04,3,And course standard two category categorization problem
00:02:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,We can apply logistical regression.,00:02:09,3,We apply logistical regression
00:02:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"You may recall that in logistical regression, we assume the log",00:02:11,3,You may recall logistical regression assume log
00:02:22,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"of probability that the Y is equal to one, is",00:02:17,3,probability Y equal one
00:02:28,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"assumed to be a linear function of these features, as shown here.",00:02:22,3,assumed linear function features shown
00:02:35,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So this would allow us to also write the probability of Y equals one, given X",00:02:28,3,So would allow us also write probability Y equals one given X
00:02:41,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,in this equation that you are seeing on the bottom.,00:02:36,3,equation seeing bottom
00:02:47,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So that's a logistical function and,00:02:43,3,So logistical function
00:02:52,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"you can see it relates this probability to,",00:02:47,3,see relates probability
00:02:57,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,probability that y=1 to the feature values.,00:02:52,3,probability 1 feature values
00:03:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And of course beta i's are parameters here, so this is",00:02:57,3,And course beta parameters
00:03:07,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,just a direct application of logistical regression for binary categorization.,00:03:02,3,direct application logistical regression binary categorization
00:03:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"What if we have multiple categories, multiple levels?",00:03:08,3,What multiple categories multiple levels
00:03:16,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Well we have to use such a binary logistical regression problem,00:03:11,3,Well use binary logistical regression problem
00:03:20,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,to solve this multi level rating prediction.,00:03:16,3,solve multi level rating prediction
00:03:26,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And the idea is we can introduce multiple binary class files.,00:03:21,3,And idea introduce multiple binary class files
00:03:29,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"In each case we asked the class file to predict the,",00:03:26,3,In case asked class file predict
00:03:35,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"whether the rating is j or above, or the rating's lower than j.",00:03:29,3,whether rating j rating lower j
00:03:41,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So when Yj is equal to 1, it means rating is j or above.",00:03:35,3,So Yj equal 1 means rating j
00:03:44,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"When it's 0, that means the rating is Lower than j.",00:03:41,3,When 0 means rating Lower j
00:03:51,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So basically if we want to predict a rating in the range of 1-k,",00:03:45,3,So basically want predict rating range 1 k
00:03:57,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,we first have one classifier to distinguish a k versus others.,00:03:51,3,first one classifier distinguish k versus others
00:03:59,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And that's our classifier one.,00:03:57,3,And classifier one
00:04:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And then we're going to have another classifier to distinguish it.,00:03:59,3,And going another classifier distinguish
00:04:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,At k-1 from the rest.,00:04:02,3,At k 1 rest
00:04:06,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,That's Classifier 2.,00:04:05,3,That Classifier 2
00:04:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And in the end, we need a Classifier to distinguish between 2 and 1.",00:04:06,3,And end need Classifier distinguish 2 1
00:04:16,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So altogether we'll have k-1 classifiers.,00:04:11,3,So altogether k 1 classifiers
00:04:23,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now if we do that of course then we can also solve this problem,00:04:17,3,Now course also solve problem
00:04:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,and the logistical regression program will be also very straight forward,00:04:23,3,logistical regression program also straight forward
00:04:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,as you have just seen on the previous slide.,00:04:27,3,seen previous slide
00:04:33,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Only that here we have more parameters.,00:04:30,3,Only parameters
00:04:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Because for each classifier, we need a different set of parameters.",00:04:33,3,Because classifier need different set parameters
00:04:41,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So now the logistical regression classifies index by J,",00:04:37,3,So logistical regression classifies index J
00:04:44,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,which corresponds to a rating level.,00:04:41,3,corresponds rating level
00:04:51,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And I have also used of J to replace beta 0.,00:04:46,3,And I also used J replace beta 0
00:04:54,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And this is to.,00:04:51,3,And
00:04:57,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Make the notation more consistent,",00:04:54,3,Make notation consistent
00:05:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,than was what we can show in the ordinal logistical regression.,00:04:57,3,show ordinal logistical regression
00:05:09,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So here we now have basically k minus one regular logistic regression classifiers.,00:05:02,3,So basically k minus one regular logistic regression classifiers
00:05:12,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Each has it's own set of parameters.,00:05:09,3,Each set parameters
00:05:18,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So now with this approach, we can now do ratings as follows.",00:05:12,3,So approach ratings follows
00:05:23,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"After we have trained these k-1 logistic regression classifiers,",00:05:19,3,After trained k 1 logistic regression classifiers
00:05:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"separately of course, then we can take a new instance and",00:05:23,3,separately course take new instance
00:05:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,then invoke a classifier sequentially to make the decision.,00:05:30,3,invoke classifier sequentially make decision
00:05:43,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So first let look at the classifier that corresponds to level of rating K.,00:05:37,3,So first let look classifier corresponds level rating K
00:05:49,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So this classifier will tell us whether this object should,00:05:43,3,So classifier tell us whether object
00:05:54,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,have a rating of K or about.,00:05:49,3,rating K
00:05:58,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,If probability according to this logistical regression classifier is,00:05:54,3,If probability according logistical regression classifier
00:06:00,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"larger than point five, we're going to say yes.",00:05:58,3,larger point five going say yes
00:06:01,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,The rating is K.,00:06:00,3,The rating K
00:06:06,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now, what if it's not as large as twenty-five?",00:06:02,3,Now large twenty five
00:06:10,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Well, that means the rating's below K, right?",00:06:06,3,Well means rating K right
00:06:13,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So now, we need to invoke the next classifier,",00:06:11,3,So need invoke next classifier
00:06:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,which tells us whether it's above K minus one.,00:06:13,3,tells us whether K minus one
00:06:20,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,It's at least K minus one.,00:06:18,3,It least K minus one
00:06:23,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And if the probability is larger than twenty-five,",00:06:20,3,And probability larger twenty five
00:06:26,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"then we'll say, well, then it's k-1.",00:06:23,3,say well k 1
00:06:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,What if it says no?,00:06:26,3,What says
00:06:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Well, that means the rating would be even below k-1.",00:06:27,3,Well means rating would even k 1
00:06:34,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And so we're going to just keep invoking these classifiers.,00:06:30,3,And going keep invoking classifiers
00:06:41,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And here we hit the end when we need to decide whether it's two or one.,00:06:34,3,And hit end need decide whether two one
00:06:43,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So this would help us solve the problem.,00:06:41,3,So would help us solve problem
00:06:44,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Right?,00:06:43,3,Right
00:06:49,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So we can have a classifier that would actually give us a prediction of a rating,00:06:44,3,So classifier would actually give us prediction rating
00:06:51,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,in the range of 1 through k.,00:06:49,3,range 1 k
00:06:55,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now unfortunately such a strategy is not an optimal way of solving this problem.,00:06:51,3,Now unfortunately strategy optimal way solving problem
00:07:01,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And specifically there are two problems with this approach.,00:06:55,3,And specifically two problems approach
00:07:03,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So these equations are the same as.,00:07:01,3,So equations
00:07:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,You have seen before.,00:07:03,3,You seen
00:07:10,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now the first problem is that there are just too many parameters.,00:07:06,3,Now first problem many parameters
00:07:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,There are many parameters.,00:07:10,3,There many parameters
00:07:15,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now, can you count how many parameters do we have exactly here?",00:07:11,3,Now count many parameters exactly
00:07:18,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now this may be a interesting exercise.,00:07:15,3,Now may interesting exercise
00:07:19,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,To do. So,00:07:18,3,To So
00:07:24,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,you might want to just pause the video and try to figure out the solution.,00:07:19,3,might want pause video try figure solution
00:07:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,How many parameters do I have for each classifier?,00:07:24,3,How many parameters I classifier
00:07:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And how many classifiers do we have?,00:07:28,3,And many classifiers
00:07:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Well you can see the, and so it is that for each classifier we have",00:07:31,3,Well see classifier
00:07:42,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"n plus one parameters, and we have k minus one classifiers all together,",00:07:37,3,n plus one parameters k minus one classifiers together
00:07:49,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,so the total number of parameters is k minus one multiplied by n plus one.,00:07:42,3,total number parameters k minus one multiplied n plus one
00:07:49,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,That's a lot.,00:07:49,3,That lot
00:07:54,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"A lot of parameters, so when the classifier has a lot of parameters,",00:07:49,3,A lot parameters classifier lot parameters
00:07:58,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"we would in general need a lot of data out to actually help us, training data,",00:07:54,3,would general need lot data actually help us training data
00:08:03,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,to help us decide the optimal parameters of such a complex model.,00:07:58,3,help us decide optimal parameters complex model
00:08:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So that's not ideal.,00:08:04,3,So ideal
00:08:10,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now the second problems is that these problems,",00:08:07,3,Now second problems problems
00:08:15,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"these k minus 1 plus fives, are not really independent.",00:08:10,3,k minus 1 plus fives really independent
00:08:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,These problems are actually dependent.,00:08:15,3,These problems actually dependent
00:08:23,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"In general, words that are positive would make the rating higher",00:08:18,3,In general words positive would make rating higher
00:08:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,for any of these classifiers.,00:08:25,3,classifiers
00:08:28,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,For all these classifiers.,00:08:27,3,For classifiers
00:08:31,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So we should be able to take advantage of this fact.,00:08:28,3,So able take advantage fact
00:08:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Now the idea of ordinal logistical regression is precisely that.,00:08:33,3,Now idea ordinal logistical regression precisely
00:08:42,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,The key idea is just the improvement over the k-1,00:08:37,3,The key idea improvement k 1
00:08:46,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,independent logistical regression classifiers.,00:08:42,3,independent logistical regression classifiers
00:08:51,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And that idea is to tie these beta parameters.,00:08:46,3,And idea tie beta parameters
00:08:59,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And that means we are going to assume the beta parameters.,00:08:51,3,And means going assume beta parameters
00:09:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,These are the parameters that indicated the inference of those weights.,00:08:59,3,These parameters indicated inference weights
00:09:09,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And we're going to assume these beta values are the same for,00:09:05,3,And going assume beta values
00:09:10,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,all the K- 1 parameters.,00:09:09,3,K 1 parameters
00:09:13,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And this just encodes our intuition that,",00:09:10,3,And encodes intuition
00:09:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,positive words in general would make a higher rating more likely.,00:09:13,3,positive words general would make higher rating likely
00:09:25,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So this is intuitively assumptions, so reasonable for our problem setup.",00:09:19,3,So intuitively assumptions reasonable problem setup
00:09:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And we have this order in these categories.,00:09:25,3,And order categories
00:09:34,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now in fact, this would allow us to have two positive benefits.",00:09:28,3,Now fact would allow us two positive benefits
00:09:37,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,One is it's going to reduce the number of families significantly.,00:09:34,3,One going reduce number families significantly
00:09:42,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And the other is to allow us to share the training data.,00:09:38,3,And allow us share training data
00:09:45,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Because all these parameters are similar to be equal.,00:09:42,3,Because parameters similar equal
00:09:51,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So these training data, for different classifiers can then be",00:09:45,3,So training data different classifiers
00:09:55,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,shared to help us set the optimal value for beta.,00:09:51,3,shared help us set optimal value beta
00:10:00,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So we have more data to help us choose a good beta value.,00:09:56,3,So data help us choose good beta value
00:10:02,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So what's the consequence,",00:10:01,3,So consequence
00:10:08,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"well the formula would look very similar to what you have seen before only that,",00:10:02,3,well formula would look similar seen
00:10:13,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,now the beta parameter has just one index that corresponds to the feature.,00:10:08,3,beta parameter one index corresponds feature
00:10:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,It no longer has the other index that corresponds to the level of rating.,00:10:13,3,It longer index corresponds level rating
00:10:21,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,So that means we tie them together.,00:10:19,3,So means tie together
00:10:26,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And there's only one set of better values for all the classifiers.,00:10:21,3,And one set better values classifiers
00:10:31,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"However, each classifier still has the distinct R for value.",00:10:26,3,However classifier still distinct R value
00:10:33,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,The R for parameter.,00:10:31,3,The R parameter
00:10:35,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Except it's different.,00:10:33,3,Except different
00:10:39,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,And this is of course needed to predict the different levels of ratings.,00:10:35,3,And course needed predict different levels ratings
00:10:43,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So R for sub j is different it depends on j, different than j,",00:10:39,3,So R sub j different depends j different j
00:10:46,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,has a different R value.,00:10:43,3,different R value
00:10:48,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"But the rest of the parameters, the beta i's are the same.",00:10:46,3,But rest parameters beta
00:10:53,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So now you can also ask the question, how many parameters do we have now?",00:10:48,3,So also ask question many parameters
00:10:57,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Again, that's an interesting question to think about.",00:10:53,3,Again interesting question think
00:11:00,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So if you think about it for a moment, and",00:10:57,3,So think moment
00:11:05,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"you will see now, the param, we have far fewer parameters.",00:11:00,3,see param far fewer parameters
00:11:08,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Specifically we have M plus K minus one.,00:11:05,3,Specifically M plus K minus one
00:11:13,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Because we have M, beta values, and plus K minus one of our values.",00:11:08,3,Because M beta values plus K minus one values
00:11:17,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So let's just look basically,",00:11:15,3,So let look basically
00:11:21,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,that's basically the main idea of ordinal logistical regression.,00:11:17,3,basically main idea ordinal logistical regression
00:11:31,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So, now, let's see how we can use such a method to actually assign ratings.",00:11:24,3,So let see use method actually assign ratings
00:11:39,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"It turns out that with this, this idea of tying all the parameters, the beta values.",00:11:31,3,It turns idea tying parameters beta values
00:11:44,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,We also end up by having a similar way to make decisions.,00:11:39,3,We also end similar way make decisions
00:11:50,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And more specifically now, the criteria whether the predictor probabilities",00:11:44,3,And specifically criteria whether predictor probabilities
00:11:55,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"are at least 0.5 above, and now is equivalent to",00:11:50,3,least 0 5 equivalent
00:12:00,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,whether the score of the object is larger than or,00:11:55,3,whether score object larger
00:12:06,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"equal to negative authors of j, as shown here.",00:12:00,3,equal negative authors j shown
00:12:11,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now, the scoring function is just taking the linear combination of",00:12:06,3,Now scoring function taking linear combination
00:12:14,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,all the features with the divided beta values.,00:12:11,3,features divided beta values
00:12:21,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So, this means now we can simply make a decision of rating, by looking at",00:12:15,3,So means simply make decision rating looking
00:12:27,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"the value of this scoring function, and see which bracket it falls into.",00:12:21,3,value scoring function see bracket falls
00:12:33,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Now you can see the general decision rule is thus,",00:12:27,3,Now see general decision rule thus
00:12:39,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"when the score is in the particular range of all of our values,",00:12:33,3,score particular range values
00:12:46,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,then we will assign the corresponding rating to that text object.,00:12:39,3,assign corresponding rating text object
00:12:53,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"So in this approach, we're going to score the object",00:12:49,3,So approach going score object
00:12:59,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,by using the features and trained parameter values.,00:12:55,3,using features trained parameter values
00:13:04,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,This score will then be compared with a set of trained,00:13:00,3,This score compared set trained
00:13:09,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,alpha values to see which range the score is in.,00:13:04,3,alpha values see range score
00:13:09,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"And then,",00:13:09,3,And
00:13:14,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"using the range, we can then decide which rating the object should be getting.",00:13:09,3,using range decide rating object getting
00:13:19,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"Because, these ranges of alpha values correspond to the different",00:13:14,3,Because ranges alpha values correspond different
00:13:24,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,"levels of ratings, and that's from the way we train these alpha values.",00:13:19,3,levels ratings way train alpha values
00:13:30,5,Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression,4.3,Each is tied to some level of rating.,00:13:24,3,Each tied level rating
00:00:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,[SOUND] This,00:00:00,13,SOUND This
00:00:11,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,lecture is about probabilistic and latent Semantic Analysis or PLSA.,00:00:07,13,lecture probabilistic latent Semantic Analysis PLSA
00:00:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"In this lecture we're going to introduce probabilistic latent semantic analysis,",00:00:12,13,In lecture going introduce probabilistic latent semantic analysis
00:00:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,often called PLSA.,00:00:18,13,often called PLSA
00:00:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"This is the most basic topic model, also one of the most useful topic models.",00:00:18,13,This basic topic model also one useful topic models
00:00:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now this kind of models can in general be used to,00:00:26,13,Now kind models general used
00:00:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,mine multiple topics from text documents.,00:00:30,13,mine multiple topics text documents
00:00:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And PRSA is one of the most basic topic models for doing this.,00:00:34,13,And PRSA one basic topic models
00:00:43,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So let's first examine this power in the e-mail for more detail.,00:00:39,13,So let first examine power e mail detail
00:00:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Here I show a sample article which is a blog article about Hurricane Katrina.,00:00:43,13,Here I show sample article blog article Hurricane Katrina
00:00:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And I show some simple topics.,00:00:48,13,And I show simple topics
00:00:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"For example government response, flood of the city of New Orleans.",00:00:51,13,For example government response flood city New Orleans
00:00:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Donation and the background.,00:00:55,13,Donation background
00:01:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,You can see in the article we use words from all these distributions.,00:00:59,13,You see article use words distributions
00:01:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So we first for example see there's a criticism of government response and,00:01:05,13,So first example see criticism government response
00:01:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,this is followed by discussion of flooding of the city and donation et cetera.,00:01:09,13,followed discussion flooding city donation et cetera
00:01:17,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,We also see background words mixed with them.,00:01:14,13,We also see background words mixed
00:01:23,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So the overall of topic analysis here is to try to decode these topics behind,00:01:18,13,So overall topic analysis try decode topics behind
00:01:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"the text, to segment the topics, to figure out which words are from which",00:01:23,13,text segment topics figure words
00:01:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"distribution and to figure out first, what are these topics?",00:01:28,13,distribution figure first topics
00:01:36,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,How do we know there's a topic about government response.,00:01:33,13,How know topic government response
00:01:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,There's a topic about a flood in the city.,00:01:36,13,There topic flood city
00:01:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So these are the tasks at the top of the model.,00:01:39,13,So tasks top model
00:01:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"If we had discovered these topics can color these words,",00:01:42,13,If discovered topics color words
00:01:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"as you see here, to separate the different topics.",00:01:46,13,see separate different topics
00:01:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Then you can do a lot of things, such as summarization, or segmentation,",00:01:50,13,Then lot things summarization segmentation
00:01:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"of the topics, clustering of the sentences etc.",00:01:54,13,topics clustering sentences etc
00:02:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So the formal definition of problem of mining multiple topics from text is,00:01:59,13,So formal definition problem mining multiple topics text
00:02:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,shown here.,00:02:04,13,shown
00:02:09,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And this is after a slide that you have seen in an earlier lecture.,00:02:04,13,And slide seen earlier lecture
00:02:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So the input is a collection, the number of topics, and a vocabulary set, and",00:02:09,13,So input collection number topics vocabulary set
00:02:15,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,of course the text data.,00:02:14,13,course text data
00:02:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And then the output is of two kinds.,00:02:16,13,And output two kinds
00:02:21,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"One is the topic category, characterization.",00:02:18,13,One topic category characterization
00:02:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Theta i's.,00:02:21,13,Theta
00:02:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Each theta i is a word distribution.,00:02:22,13,Each theta word distribution
00:02:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And second, it's the topic coverage for each document.",00:02:24,13,And second topic coverage document
00:02:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,These are pi sub i j's.,00:02:28,13,These pi sub j
00:02:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And they tell us which document it covers.,00:02:30,13,And tell us document covers
00:02:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Which topic to what extent.,00:02:33,13,Which topic extent
00:02:37,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So we hope to generate these as output.,00:02:35,13,So hope generate output
00:02:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Because there are many useful applications if we can do that.,00:02:37,13,Because many useful applications
00:02:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So the idea of PLSA is actually very similar to,00:02:42,13,So idea PLSA actually similar
00:02:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,the two component mixture model that we have already introduced.,00:02:47,13,two component mixture model already introduced
00:02:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,The only difference is that we are going to have more than two topics.,00:02:50,13,The difference going two topics
00:02:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Otherwise, it is essentially the same.",00:02:54,13,Otherwise essentially
00:03:03,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So here I illustrate how we can generate the text that has multiple topics and,00:02:57,13,So I illustrate generate text multiple topics
00:03:06,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,naturally in all cases,00:03:03,13,naturally cases
00:03:11,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,of Probabilistic modelling would want to figure out the likelihood function.,00:03:06,13,Probabilistic modelling would want figure likelihood function
00:03:13,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So we would also ask the question,",00:03:11,13,So would also ask question
00:03:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,what's the probability of observing a word from such a mixture model?,00:03:13,13,probability observing word mixture model
00:03:19,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now if you look at this picture and,00:03:18,13,Now look picture
00:03:21,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"compare this with the picture that we have seen earlier,",00:03:19,13,compare picture seen earlier
00:03:25,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,you will see the only difference is that we have added more topics here.,00:03:21,13,see difference added topics
00:03:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So, before we have just one topic, besides the background topic.",00:03:26,13,So one topic besides background topic
00:03:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,But now we have more topics.,00:03:32,13,But topics
00:03:38,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Specifically, we have k topics now.",00:03:35,13,Specifically k topics
00:03:43,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,All these are topics that we assume that exist in the text data.,00:03:38,13,All topics assume exist text data
00:03:49,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So the consequence is that our switch for choosing a topic is now a multiway switch.,00:03:43,13,So consequence switch choosing topic multiway switch
00:03:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Before it's just a two way switch.,00:03:49,13,Before two way switch
00:03:53,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,We can think of it as flipping a coin.,00:03:51,13,We think flipping coin
00:03:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,But now we have multiple ways.,00:03:53,13,But multiple ways
00:03:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,First we can flip a coin to decide whether we're talk about the background.,00:03:55,13,First flip coin decide whether talk background
00:04:06,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So it's the background lambda sub B versus non-background.,00:03:59,13,So background lambda sub B versus non background
00:04:11,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,1 minus lambda sub B gives us the probability of,00:04:06,13,1 minus lambda sub B gives us probability
00:04:16,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,actually choosing a non-background topic.,00:04:11,13,actually choosing non background topic
00:04:17,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"After we have made this decision,",00:04:16,13,After made decision
00:04:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,we have to make another decision to choose one of these K distributions.,00:04:17,13,make another decision choose one K distributions
00:04:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So there are K way switch here.,00:04:24,13,So K way switch
00:04:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And this is characterized by pi, and this sum to one.",00:04:26,13,And characterized pi sum one
00:04:33,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,This is just the difference of designs.,00:04:31,13,This difference designs
00:04:36,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Which is a little bit more complicated.,00:04:33,13,Which little bit complicated
00:04:40,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,But once we decide which distribution to use the rest is the same we are going to,00:04:36,13,But decide distribution use rest going
00:04:45,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,just generate a word by using one of these distributions as shown here.,00:04:40,13,generate word using one distributions shown
00:04:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So now lets look at the question about the likelihood.,00:04:46,13,So lets look question likelihood
00:04:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So what's the probability of observing a word from such a distribution?,00:04:50,13,So probability observing word distribution
00:04:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,What do you think?,00:04:55,13,What think
00:05:01,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now we've seen this problem many times now and,00:04:57,13,Now seen problem many times
00:05:05,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"if you can recall, it's generally a sum.",00:05:01,13,recall generally sum
00:05:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Of all the different possibilities of generating a word.,00:05:05,13,Of different possibilities generating word
00:05:14,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So let's first look at how the word can be generated from the background mode.,00:05:08,13,So let first look word generated background mode
00:05:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Well, the probability that the word is generated from the background model",00:05:14,13,Well probability word generated background model
00:05:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,is lambda multiplied by the probability of the word from the background mode.,00:05:18,13,lambda multiplied probability word background mode
00:05:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Model, right.",00:05:22,13,Model right
00:05:25,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Two things must happen.,00:05:24,13,Two things must happen
00:05:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"First, we have to have chosen the background model,",00:05:25,13,First chosen background model
00:05:31,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"and that's the probability of lambda, of sub b.",00:05:28,13,probability lambda sub b
00:05:36,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Then second, we must have actually obtained the word w from the background,",00:05:31,13,Then second must actually obtained word w background
00:05:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,and that's probability of w given theta sub b.,00:05:36,13,probability w given theta sub b
00:05:41,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Okay, so similarly,",00:05:40,13,Okay similarly
00:05:46,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,we can figure out the probability of observing the word from another topic.,00:05:41,13,figure probability observing word another topic
00:05:48,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Like the topic theta sub k.,00:05:46,13,Like topic theta sub k
00:05:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now notice that here's the product of three terms.,00:05:48,13,Now notice product three terms
00:05:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And that's because of the choice of topic theta sub k,",00:05:51,13,And choice topic theta sub k
00:06:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,only happens if two things happen.,00:05:57,13,happens two things happen
00:06:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,One is we decide not to talk about background.,00:06:00,13,One decide talk background
00:06:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So, that's a probability of 1 minus lambda sub B.",00:06:04,13,So probability 1 minus lambda sub B
00:06:13,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Second, we also have to actually choose theta sub K among these K topics.",00:06:07,13,Second also actually choose theta sub K among K topics
00:06:16,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So that's probability of theta sub K, or pi.",00:06:13,13,So probability theta sub K pi
00:06:21,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And similarly, the probability of generating a word from the second.",00:06:17,13,And similarly probability generating word second
00:06:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,The topic and the first topic are like what you are seeing here.,00:06:21,13,The topic first topic like seeing
00:06:27,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And so,00:06:26,13,And
00:06:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,in the end the probability of observing the word is just a sum of all these cases.,00:06:27,13,end probability observing word sum cases
00:06:38,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And I have to stress again this is a very important formula to know because this is,00:06:32,13,And I stress important formula know
00:06:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,really key to understanding all the topic models and indeed a lot of mixture models.,00:06:38,13,really key understanding topic models indeed lot mixture models
00:06:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So make sure that you really understand the probability,00:06:44,13,So make sure really understand probability
00:06:53,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,of w is indeed the sum of these terms.,00:06:49,13,w indeed sum terms
00:07:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So, next, once we have the likelihood function,",00:06:56,13,So next likelihood function
00:07:05,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,we would be interested in knowing the parameters.,00:07:00,13,would interested knowing parameters
00:07:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"All right, so to estimate the parameters.",00:07:05,13,All right estimate parameters
00:07:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"But firstly,",00:07:07,13,But firstly
00:07:13,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,let's put all these together to have the complete likelihood of function for PLSA.,00:07:07,13,let put together complete likelihood function PLSA
00:07:19,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,The first line shows the probability of a word as illustrated on the previous slide.,00:07:13,13,The first line shows probability word illustrated previous slide
00:07:20,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And this is an important formula as I said.,00:07:19,13,And important formula I said
00:07:24,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So let's take a closer look at this.,00:07:22,13,So let take closer look
00:07:27,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,This actually commands all the important parameters.,00:07:24,13,This actually commands important parameters
00:07:29,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So first of all we see lambda sub b here.,00:07:27,13,So first see lambda sub b
00:07:31,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,This represents a percentage of background words,00:07:29,13,This represents percentage background words
00:07:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,that we believe exist in the text data.,00:07:32,13,believe exist text data
00:07:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And this can be a known value that we set empirically.,00:07:35,13,And known value set empirically
00:07:43,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Second, we see the background language model, and",00:07:41,13,Second see background language model
00:07:45,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,typically we also assume this is known.,00:07:43,13,typically also assume known
00:07:48,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"We can use a large collection of text, or",00:07:45,13,We use large collection text
00:07:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,use all the text that we have available to estimate the world of distribution.,00:07:48,13,use text available estimate world distribution
00:07:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now next in the next stop this formula.,00:07:52,13,Now next next stop formula
00:07:57,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,[COUGH] Excuse me.,00:07:55,13,COUGH Excuse
00:08:00,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"You see two interesting kind of parameters,",00:07:57,13,You see two interesting kind parameters
00:08:01,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,those are the most important parameters.,00:08:00,13,important parameters
00:08:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,That we are.,00:08:01,13,That
00:08:06,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So one is pi's.,00:08:04,13,So one pi
00:08:10,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And these are the coverage of a topic in the document.,00:08:06,13,And coverage topic document
00:08:15,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And the other is word distributions that characterize all the topics.,00:08:11,13,And word distributions characterize topics
00:08:23,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So the next line, then is simply to plug this",00:08:18,13,So next line simply plug
00:08:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,in to calculate the probability of document.,00:08:23,13,calculate probability document
00:08:29,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"This is, again, of the familiar form where you have a sum and",00:08:26,13,This familiar form sum
00:08:32,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,you have a count of a word in the document.,00:08:29,13,count word document
00:08:35,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And then log of a probability.,00:08:32,13,And log probability
00:08:39,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Now it's a little bit more complicated than the two component.,00:08:35,13,Now little bit complicated two component
00:08:43,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Because now we have more components, so the sum involves more terms.",00:08:39,13,Because components sum involves terms
00:08:47,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And then this line is just the likelihood for the whole collection.,00:08:43,13,And line likelihood whole collection
00:08:51,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And it's very similar, just accounting for more documents in the collection.",00:08:47,13,And similar accounting documents collection
00:08:54,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,So what are the unknown parameters?,00:08:52,13,So unknown parameters
00:08:55,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,I already said that there are two kinds.,00:08:54,13,I already said two kinds
00:08:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"One is coverage, one is word distributions.",00:08:55,13,One coverage one word distributions
00:09:02,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Again, it's a useful exercise for you to think about.",00:08:59,13,Again useful exercise think
00:09:04,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Exactly how many parameters there are here.,00:09:02,13,Exactly many parameters
00:09:07,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,How many unknown parameters are there?,00:09:05,13,How many unknown parameters
00:09:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"Now, try and",00:09:07,13,Now try
00:09:13,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,think out that question will help you understand the model in more detail.,00:09:08,13,think question help understand model detail
00:09:17,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And will also allow you to understand what would be the output that we generate,00:09:13,13,And also allow understand would output generate
00:09:20,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,when use PLSA to analyze text data?,00:09:17,13,use PLSA analyze text data
00:09:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,And these are precisely the unknown parameters.,00:09:20,13,And precisely unknown parameters
00:09:28,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So after we have obtained the likelihood function shown here,",00:09:24,13,So obtained likelihood function shown
00:09:30,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,the next is to worry about the parameter estimation.,00:09:28,13,next worry parameter estimation
00:09:34,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And we can do the usual think, maximum likelihood estimator.",00:09:32,13,And usual think maximum likelihood estimator
00:09:40,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So again, it's a constrained optimization problem, like what we have seen before.",00:09:34,13,So constrained optimization problem like seen
00:09:44,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,Only that we have a collection of text and we have more parameters to estimate.,00:09:40,13,Only collection text parameters estimate
00:09:48,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"And we still have two constraints, two kinds of constraints.",00:09:44,13,And still two constraints two kinds constraints
00:09:50,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,One is the word distributions.,00:09:48,13,One word distributions
00:09:56,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,All the words must have probabilities that's sum to one for one distribution.,00:09:51,13,All words must probabilities sum one one distribution
00:09:59,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,The other is the topic coverage distribution and,00:09:56,13,The topic coverage distribution
00:10:05,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,a document will have to cover precisely these k topics so,00:09:59,13,document cover precisely k topics
00:10:08,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,the probability of covering each topic that would have to sum to 1.,00:10:05,13,probability covering topic would sum 1
00:10:13,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,"So at this point though it's basically a well defined applied math problem,",00:10:08,13,So point though basically well defined applied math problem
00:10:16,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,you just need to figure out the solutions to optimization problem.,00:10:13,13,need figure solutions optimization problem
00:10:18,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,There's a function with many variables.,00:10:16,13,There function many variables
00:10:22,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,and we need to just figure out the patterns of these,00:10:18,13,need figure patterns
00:10:26,3,Probabilistic Latent Semantic Analysis (PLSA)- Part 1,2.13,variables to make the function reach its maximum.,00:10:22,13,variables make function reach maximum
00:00:04,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,[MUSIC],00:00:00,7,MUSIC
00:00:10,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,This lecture is about the mixture of unigram language models.,00:00:06,7,This lecture mixture unigram language models
00:00:16,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,In this lecture we will continue discussing probabilistic topic models.,00:00:11,7,In lecture continue discussing probabilistic topic models
00:00:20,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"In particular, what we introduce a mixture of unigram language models.",00:00:16,7,In particular introduce mixture unigram language models
00:00:24,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,This is a slide that you have seen earlier.,00:00:20,7,This slide seen earlier
00:00:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Where we talked about how to get rid of the background,00:00:24,7,Where talked get rid background
00:00:34,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,words that we have on top of for one document.,00:00:29,7,words top one document
00:00:38,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So if you want to solve the problem,",00:00:36,7,So want solve problem
00:00:44,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,it would be useful to think about why we end up having this problem.,00:00:38,7,would useful think end problem
00:00:49,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well, this obviously because these words are very frequent in our data and",00:00:44,7,Well obviously words frequent data
00:00:52,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we are using a maximum likelihood to estimate.,00:00:49,7,using maximum likelihood estimate
00:00:56,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Then the estimate obviously would have to assign high probability for,00:00:52,7,Then estimate obviously would assign high probability
00:00:59,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,these words in order to maximize the likelihood.,00:00:56,7,words order maximize likelihood
00:01:03,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So, in order to get rid of them that would mean we'd have to do something",00:00:59,7,So order get rid would mean something
00:01:04,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,differently here.,00:01:03,7,differently
00:01:09,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,In particular we'll have to say this distribution,00:01:05,7,In particular say distribution
00:01:12,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,doesn't have to explain all the words in the tax data.,00:01:09,7,explain words tax data
00:01:13,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"What were going to say is that,",00:01:12,7,What going say
00:01:19,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,these common words should not be explained by this distribution.,00:01:13,7,common words explained distribution
00:01:25,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So one natural way to solve the problem is to think about using another distribution,00:01:19,7,So one natural way solve problem think using another distribution
00:01:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,to account for just these common words.,00:01:25,7,account common words
00:01:33,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"This way, the two distributions can be mixed together to generate the text data.",00:01:29,7,This way two distributions mixed together generate text data
00:01:38,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And we'll let the other model which we'll call background topic model,00:01:33,7,And let model call background topic model
00:01:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,to generate the common words.,00:01:38,7,generate common words
00:01:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,This way our target topic theta here will be only generating,00:01:40,7,This way target topic theta generating
00:01:51,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the common handle words that are characterised the content of the document.,00:01:47,7,common handle words characterised content document
00:01:54,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So, how does this work?",00:01:52,7,So work
00:01:58,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well, it is just a small modification of the previous setup",00:01:54,7,Well small modification previous setup
00:02:01,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,where we have just one distribution.,00:01:58,7,one distribution
00:02:02,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Since we now have two distributions,",00:02:01,7,Since two distributions
00:02:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we have to decide which distribution to use when we generate the word.,00:02:02,7,decide distribution use generate word
00:02:12,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Each word will still be a sample from one of the two distributions.,00:02:07,7,Each word still sample one two distributions
00:02:16,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Text data is still generating the same way.,00:02:13,7,Text data still generating way
00:02:20,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Namely, look at the generating of the one word at each time and",00:02:16,7,Namely look generating one word time
00:02:23,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,eventually we generate a lot of words.,00:02:20,7,eventually generate lot words
00:02:24,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"When we generate the word,",00:02:23,7,When generate word
00:02:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"however, we're going to first decide which of the two distributions to use.",00:02:24,7,however going first decide two distributions use
00:02:34,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And this is controlled by another probability, the probability of",00:02:29,7,And controlled another probability probability
00:02:39,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,theta sub d and the probability of theta sub B here.,00:02:34,7,theta sub probability theta sub B
00:02:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So this is a probability of enacting the topic word of distribution.,00:02:41,7,So probability enacting topic word distribution
00:02:51,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,This is the probability of enacting the background word,00:02:47,7,This probability enacting background word
00:02:54,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,of distribution denoted by theta sub B.,00:02:52,7,distribution denoted theta sub B
00:02:59,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,On this case I just give example where we can set both to 0.5.,00:02:55,7,On case I give example set 0 5
00:03:03,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So you're going to basically flip a coin, a fair coin,",00:02:59,7,So going basically flip coin fair coin
00:03:05,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,to decide what you want to use.,00:03:03,7,decide want use
00:03:09,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,But in general these probabilities don't have to be equal.,00:03:05,7,But general probabilities equal
00:03:15,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So you might bias toward using one topic more than the other.,00:03:09,7,So might bias toward using one topic
00:03:19,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So now the process of generating a word would be to first we flip a coin.,00:03:15,7,So process generating word would first flip coin
00:03:26,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Based on these probabilities choosing each model and if let's say the coin,00:03:19,7,Based probabilities choosing model let say coin
00:03:31,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"shows up as head, which means we're going to use the topic two word distribution.",00:03:26,7,shows head means going use topic two word distribution
00:03:37,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Then we're going to use this word distribution to generate a word.,00:03:31,7,Then going use word distribution generate word
00:03:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Otherwise we might be going slow this path.,00:03:37,7,Otherwise might going slow path
00:03:45,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And we're going to use the background word distribution to generate a word.,00:03:41,7,And going use background word distribution generate word
00:03:51,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So in such a case, we have a model that has some uncertainty",00:03:46,7,So case model uncertainty
00:03:54,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,associated with the use of a word distribution.,00:03:51,7,associated use word distribution
00:03:59,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,But we can still think of this as a model for generating text data.,00:03:54,7,But still think model generating text data
00:04:01,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And such a model is called a mixture model.,00:03:59,7,And model called mixture model
00:04:03,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So now let's see.,00:04:02,7,So let see
00:04:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"In this case, what's the probability of observing a word w?",00:04:03,7,In case probability observing word w
00:04:10,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Now here I showed some words.,00:04:07,7,Now I showed words
00:04:12,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"like ""the"" and ""text"".",00:04:10,7,like text
00:04:13,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So as in all cases,",00:04:12,7,So cases
00:04:17,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,once we setup a model we are interested in computing the likelihood function.,00:04:13,7,setup model interested computing likelihood function
00:04:19,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"The basic question is, so",00:04:17,7,The basic question
00:04:23,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,what's the probability of observing a specific word here?,00:04:19,7,probability observing specific word
00:04:27,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Now we know that the word can be observed from each of the two distributions, so",00:04:23,7,Now know word observed two distributions
00:04:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we have to consider two cases.,00:04:27,7,consider two cases
00:04:32,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Therefore it's a sum over these two cases.,00:04:29,7,Therefore sum two cases
00:04:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,The first case is to use the topic for the distribution to generate the word.,00:04:34,7,The first case use topic distribution generate word
00:04:46,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And in such a case then the probably would be theta sub d,",00:04:40,7,And case probably would theta sub
00:04:48,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,which is the probability of choosing the model,00:04:46,7,probability choosing model
00:04:53,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,multiplied by the probability of actually observing the word from that model.,00:04:48,7,multiplied probability actually observing word model
00:04:56,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Both events must happen in order to observe.,00:04:53,7,Both events must happen order observe
00:05:02,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"We first must have choosing the topic theta sub d and then,",00:04:56,7,We first must choosing topic theta sub
00:05:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we also have to actually have sampled the word the from the distribution.,00:05:02,7,also actually sampled word distribution
00:05:11,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And similarly, the second part accounts for",00:05:07,7,And similarly second part accounts
00:05:13,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,a different way of generally the word from the background.,00:05:11,7,different way generally word background
00:05:20,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Now obviously the probability of text the same is all similar, right?",00:05:15,7,Now obviously probability text similar right
00:05:25,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So we also can see the two ways of generating the text.,00:05:20,7,So also see two ways generating text
00:05:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And in each case, it's a product of the probability of choosing a particular word",00:05:25,7,And case product probability choosing particular word
00:05:34,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,is multiplied by the probability of observing the word from that distribution.,00:05:29,7,multiplied probability observing word distribution
00:05:38,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Now whether you will see, this is actually a general form.",00:05:35,7,Now whether see actually general form
00:05:43,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So might want to make sure that you have really understood this expression here.,00:05:38,7,So might want make sure really understood expression
00:05:48,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And you should convince yourself that this is indeed the probability of,00:05:43,7,And convince indeed probability
00:05:49,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,obsolete text.,00:05:48,7,obsolete text
00:05:52,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So to summarize what we observed here.,00:05:49,7,So summarize observed
00:05:57,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,The probability of a word from a mixture model is a general sum,00:05:52,7,The probability word mixture model general sum
00:05:59,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,of different ways of generating the word.,00:05:57,7,different ways generating word
00:06:01,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"In each case,",00:06:00,7,In case
00:06:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,it's a product of the probability of selecting that component model.,00:06:01,7,product probability selecting component model
00:06:12,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Multiplied by the probability of actually observing the data point,00:06:07,7,Multiplied probability actually observing data point
00:06:14,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,from that component of the model.,00:06:12,7,component model
00:06:20,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And this is something quite general and you will see this occurring often later.,00:06:14,7,And something quite general see occurring often later
00:06:23,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So the basic idea of a mixture model is just to retrieve,00:06:20,7,So basic idea mixture model retrieve
00:06:28,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,thesetwo distributions together as one model.,00:06:23,7,thesetwo distributions together one model
00:06:32,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So I used a box to bring all these components together.,00:06:28,7,So I used box bring components together
00:06:36,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So if you view this whole box as one model,",00:06:32,7,So view whole box one model
00:06:38,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,it's just like any other generative model.,00:06:36,7,like generative model
00:06:41,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,It would just give us the probability of a word.,00:06:38,7,It would give us probability word
00:06:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,But the way that determines this probability is quite the different from,00:06:42,7,But way determines probability quite different
00:06:48,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,when we have just one distribution.,00:06:47,7,one distribution
00:06:54,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And this is basically a more complicated mixture model.,00:06:50,7,And basically complicated mixture model
00:06:57,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So the more complicated is more than just one distribution.,00:06:54,7,So complicated one distribution
00:06:58,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And it's called a mixture model.,00:06:57,7,And called mixture model
00:07:04,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So as I just said we can treat this as a generative model.,00:07:00,7,So I said treat generative model
00:07:08,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And it's often useful to think of just as a likelihood function.,00:07:04,7,And often useful think likelihood function
00:07:10,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"The illustration that you have seen before,",00:07:08,7,The illustration seen
00:07:14,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"which is dimmer now, is just the illustration of this generated model.",00:07:10,7,dimmer illustration generated model
00:07:18,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So mathematically, this model is nothing but",00:07:14,7,So mathematically model nothing
00:07:21,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,to just define the following generative model.,00:07:18,7,define following generative model
00:07:25,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Where the probability of a word is assumed to be a sum over two cases,00:07:21,7,Where probability word assumed sum two cases
00:07:28,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,of generating the word.,00:07:26,7,generating word
00:07:32,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And the form you are seeing now is a more general form that,00:07:28,7,And form seeing general form
00:07:36,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,what you have seen in the calculation earlier.,00:07:32,7,seen calculation earlier
00:07:41,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Well I just use the symbol w to denote any water but,00:07:36,7,Well I use symbol w denote water
00:07:46,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,you can still see this is basically first a sum.,00:07:41,7,still see basically first sum
00:07:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Right?,00:07:46,7,Right
00:07:53,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And this sum is due to the fact that the water can be generated in much more ways,",00:07:47,7,And sum due fact water generated much ways
00:07:55,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,two ways in this case.,00:07:53,7,two ways case
00:08:00,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And inside a sum, each term is a product of two terms.",00:07:55,7,And inside sum term product two terms
00:08:05,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And the two terms are first the probability of selecting a component,00:08:00,7,And two terms first probability selecting component
00:08:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"like of D Second,",00:08:05,7,like D Second
00:08:12,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the probability of actually observing the word from this component of the model.,00:08:07,7,probability actually observing word component model
00:08:18,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So this is a very general description of all the mixture models.,00:08:12,7,So general description mixture models
00:08:23,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,I just want to make sure that you understand,00:08:18,7,I want make sure understand
00:08:27,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,this because this is really the basis for understanding all kinds of on top models.,00:08:23,7,really basis understanding kinds top models
00:08:31,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So now once we setup model.,00:08:28,7,So setup model
00:08:34,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,We can write down that like functioning as we see here.,00:08:31,7,We write like functioning see
00:08:37,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"The next question is, how can we estimate the parameter,",00:08:34,7,The next question estimate parameter
00:08:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,or what to do with the parameters.,00:08:37,7,parameters
00:08:41,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Given the data.,00:08:40,7,Given data
00:08:42,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well, in general,",00:08:41,7,Well general
00:08:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we can use some of the text data to estimate the model parameters.,00:08:42,7,use text data estimate model parameters
00:08:50,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And this estimation would allow us to,00:08:47,7,And estimation would allow us
00:08:55,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,discover the interesting knowledge about the text.,00:08:50,7,discover interesting knowledge text
00:08:58,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So you, in this case, what do we discover?",00:08:55,7,So case discover
00:09:01,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well, these are presented by our parameters and",00:08:58,7,Well presented parameters
00:09:03,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,we will have two kinds of parameters.,00:09:01,7,two kinds parameters
00:09:07,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"One is the two worded distributions, that result in topics, and",00:09:03,7,One two worded distributions result topics
00:09:10,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the other is the coverage of each topic in each.,00:09:07,7,coverage topic
00:09:14,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,The coverage of each topic.,00:09:12,7,The coverage topic
00:09:17,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And this is determined by probability of C less of D and,00:09:14,7,And determined probability C less D
00:09:22,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"probability of theta, so this is to one.",00:09:17,7,probability theta one
00:09:25,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Now, what's interesting is also to think about special",00:09:22,7,Now interesting also think special
00:09:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,cases like when we send one of them to want what would happen?,00:09:25,7,cases like send one want would happen
00:09:32,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well with the other, with the zero right?",00:09:29,7,Well zero right
00:09:35,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And if you look at the likelihood function,",00:09:32,7,And look likelihood function
00:09:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,it will then degenerate to the special case of just one distribution.,00:09:36,7,degenerate special case one distribution
00:09:46,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Okay so you can easily verify that by assuming one of these two is 1.0 and,00:09:40,7,Okay easily verify assuming one two 1 0
00:09:47,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the other is Zero.,00:09:46,7,Zero
00:09:53,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So in this sense, the mixture model is more general than",00:09:49,7,So sense mixture model general
00:09:56,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the previous model where we have just one distribution.,00:09:53,7,previous model one distribution
00:09:58,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,It can cover that as a special case.,00:09:56,7,It cover special case
00:10:05,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So to summarize, we talked about the mixture of two Unigram Language Models and",00:09:59,7,So summarize talked mixture two Unigram Language Models
00:10:09,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the data we're considering here is just One document.,00:10:05,7,data considering One document
00:10:13,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And the model is a mixture model with two components,",00:10:09,7,And model mixture model two components
00:10:16,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"two unigram LM models, specifically theta sub d,",00:10:13,7,two unigram LM models specifically theta sub
00:10:22,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"which is intended to denote the topic of document d, and theta sub B, which is",00:10:16,7,intended denote topic document theta sub B
00:10:28,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,representing a background topic that we can set to attract the common,00:10:22,7,representing background topic set attract common
00:10:32,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,words because common words would be assigned a high probability in this model.,00:10:28,7,words common words would assigned high probability model
00:10:36,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So the parameters can be collectively called,00:10:33,7,So parameters collectively called
00:10:40,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,Lambda which I show here you can again,00:10:36,7,Lambda I show
00:10:45,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,think about the question about how many parameters are we talking about exactly.,00:10:41,7,think question many parameters talking exactly
00:10:50,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,This is usually a good exercise to do because it allows you to see the model in,00:10:45,7,This usually good exercise allows see model
00:10:56,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,depth and to have a complete understanding of what's going on this model.,00:10:50,7,depth complete understanding going model
00:10:58,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And we have mixing weights, of course, also.",00:10:56,7,And mixing weights course also
00:11:02,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So what does a likelihood function look like?,00:10:59,7,So likelihood function look like
00:11:06,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"Well, it looks very similar to what we had before.",00:11:02,7,Well looks similar
00:11:09,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"So for the document,",00:11:06,7,So document
00:11:14,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,first it's a product over all the words in the document exactly the same as before.,00:11:09,7,first product words document exactly
00:11:20,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,The only difference is that inside here now it's a sum instead of just one.,00:11:14,7,The difference inside sum instead one
00:11:24,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,So you might have recalled before we just had this one there.,00:11:20,7,So might recalled one
00:11:30,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,But now we have this sum because of the mixture model.,00:11:25,7,But sum mixture model
00:11:34,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And because of the mixture model we also have to introduce a probability of,00:11:30,7,And mixture model also introduce probability
00:11:37,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,choosing that particular component of distribution.,00:11:34,7,choosing particular component distribution
00:11:44,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And so this is just another way of writing, and",00:11:39,7,And another way writing
00:11:49,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,by using a product over all the unique words in our vocabulary instead of,00:11:44,7,using product unique words vocabulary instead
00:11:52,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,having that product over all the positions in the document.,00:11:49,7,product positions document
00:11:57,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And this form where we look at the different and unique words is,00:11:52,7,And form look different unique words
00:12:04,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,a commutative that formed for computing the maximum likelihood estimate later.,00:11:57,7,commutative formed computing maximum likelihood estimate later
00:12:09,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,"And the maximum likelihood estimator is, as usual,",00:12:04,7,And maximum likelihood estimator usual
00:12:15,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,just to find the parameters that would maximize the likelihood function.,00:12:09,7,find parameters would maximize likelihood function
00:12:18,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,And the constraints here are of course two kinds.,00:12:15,7,And constraints course two kinds
00:12:24,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,One is what are probabilities in each,00:12:18,7,One probabilities
00:12:29,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,[INAUDIBLE] must sum to 1 the other is,00:12:24,7,INAUDIBLE must sum 1
00:12:35,3,Probabilistic Topic Models- Mixture of Unigram Language Models,2.7,the choice of each [INAUDIBLE] must sum to 1.,00:12:29,7,choice INAUDIBLE must sum 1
00:00:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,[MUSIC],00:00:00,5,MUSIC
00:00:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So now let's talk about the problem a little bit more, and specifically let's",00:00:06,5,So let talk problem little bit specifically let
00:00:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,talk about the two different ways of estimating the parameters.,00:00:11,5,talk two different ways estimating parameters
00:00:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,One is called the Maximum Likelihood estimate that I already just mentioned.,00:00:15,5,One called Maximum Likelihood estimate I already mentioned
00:00:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,The other is Bayesian estimation.,00:00:19,5,The Bayesian estimation
00:00:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So in maximum likelihood estimation, we define best as",00:00:22,5,So maximum likelihood estimation define best
00:00:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,meaning the data likelihood has reached the maximum.,00:00:27,5,meaning data likelihood reached maximum
00:00:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So formally it's given by this expression here,",00:00:31,5,So formally given expression
00:00:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,where we define the estimate as a arg max of the probability of x given theta.,00:00:36,5,define estimate arg max probability x given theta
00:00:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, arg max here just means its actually a function that will turn.",00:00:46,5,So arg max means actually function turn
00:00:58,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"The argument that gives the function maximum value, adds the value.",00:00:53,5,The argument gives function maximum value adds value
00:01:01,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,So the value of arg max is not the value of this function.,00:00:58,5,So value arg max value function
00:01:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"But rather, the argument that has made it the function reaches maximum.",00:01:01,5,But rather argument made function reaches maximum
00:01:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,So in this case the value of arg max is theta.,00:01:06,5,So case value arg max theta
00:01:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"It's the theta that makes the probability of X, given theta, reach it's maximum.",00:01:09,5,It theta makes probability X given theta reach maximum
00:01:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So this estimate that in due it also makes sense and it's often very useful,",00:01:16,5,So estimate due also makes sense often useful
00:01:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,and it seeks the premise that best explains the data.,00:01:22,5,seeks premise best explains data
00:01:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"But it has a problem, when the data is too small because when the data",00:01:27,5,But problem data small data
00:01:35,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"points are too small, there are very few data points.",00:01:31,5,points small data points
00:01:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"The sample is small, then if we trust data in entirely and",00:01:35,5,The sample small trust data entirely
00:01:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,try to fit the data and then we'll be biased.,00:01:39,5,try fit data biased
00:01:47,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So in the case of text data, let's say, all observed 100",00:01:42,5,So case text data let say observed 100
00:01:52,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,words did not contain another word related to text mining.,00:01:47,5,words contain another word related text mining
00:01:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now, our maximum likelihood estimator will give that word a zero probability.",00:01:52,5,Now maximum likelihood estimator give word zero probability
00:02:00,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,Because giving the non-zero probability,00:01:57,5,Because giving non zero probability
00:02:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,would take away probability mass from some observer word.,00:02:00,5,would take away probability mass observer word
00:02:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,Which obviously is not optimal in terms of maximizing the likelihood of,00:02:04,5,Which obviously optimal terms maximizing likelihood
00:02:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,the observer data.,00:02:08,5,observer data
00:02:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,But this zero probability for,00:02:11,5,But zero probability
00:02:20,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,all the unseen words may not be reasonable sometimes.,00:02:15,5,unseen words may reasonable sometimes
00:02:25,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Especially, if we want the distribution to characterize the topic of text mining.",00:02:20,5,Especially want distribution characterize topic text mining
00:02:29,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So one way to address this problem is actually to use Bayesian estimation,",00:02:25,5,So one way address problem actually use Bayesian estimation
00:02:33,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"where we actually would look at the both the data, and",00:02:29,5,actually would look data
00:02:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,our prior knowledge about the parameters.,00:02:33,5,prior knowledge parameters
00:02:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,We assume that we have some prior belief about the parameters.,00:02:36,5,We assume prior belief parameters
00:02:46,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now in this case of course, so we are not",00:02:42,5,Now case course
00:02:52,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"going to look at just the data, but also look at the prior.",00:02:47,5,going look data also look prior
00:02:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So the prior here is defined by P of theta, and",00:02:54,5,So prior defined P theta
00:03:05,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"this means, we will impose some preference on certain theta's of others.",00:02:59,5,means impose preference certain theta others
00:03:10,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And by using Bayes Rule, that I have shown here,",00:03:06,5,And using Bayes Rule I shown
00:03:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,we can then combine the likelihood function.,00:03:12,5,combine likelihood function
00:03:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,With the prior to give us this,00:03:18,5,With prior give us
00:03:29,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,posterior probability of the parameter.,00:03:23,5,posterior probability parameter
00:03:34,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now, a full explanation of Bayes rule, and some of these things",00:03:29,5,Now full explanation Bayes rule things
00:03:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"related to Bayesian reasoning, would be outside the scope of this course.",00:03:34,5,related Bayesian reasoning would outside scope course
00:03:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,But I just gave a brief introduction because this is,00:03:39,5,But I gave brief introduction
00:03:44,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,general knowledge that might be useful to you.,00:03:42,5,general knowledge might useful
00:03:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"The Bayes Rule is basically defined here, and",00:03:44,5,The Bayes Rule basically defined
00:03:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,allows us to write down one conditional probability of X,00:03:49,5,allows us write one conditional probability X
00:04:00,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,given Y in terms of the conditional probability of Y given X.,00:03:54,5,given Y terms conditional probability Y given X
00:04:03,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And you can see the two probabilities,00:04:00,5,And see two probabilities
00:04:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,are different in the order of the two variables.,00:04:03,5,different order two variables
00:04:14,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,But often the rule is used for making inferences,00:04:09,5,But often rule used making inferences
00:04:23,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"of the variable, so let's take a look at it again.",00:04:14,5,variable let take look
00:04:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,We can assume that p(X) Encodes our prior belief about X.,00:04:23,5,We assume p X Encodes prior belief X
00:04:35,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"That means before we observe any other data, that's our belief about X,",00:04:30,5,That means observe data belief X
00:04:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,what we believe some X values have higher probability than others.,00:04:35,5,believe X values higher probability others
00:04:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And this probability of X given Y,00:04:40,5,And probability X given Y
00:04:50,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"is a conditional probability, and this is our posterior belief about X.",00:04:45,5,conditional probability posterior belief X
00:04:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,Because this is our belief about X values after we have observed the Y.,00:04:50,5,Because belief X values observed Y
00:05:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Given that we have observed the Y, now what do we believe about X?",00:04:57,5,Given observed Y believe X
00:05:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now, do we believe some values have higher probabilities than others?",00:05:02,5,Now believe values higher probabilities others
00:05:14,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now the two probabilities are related through this one,",00:05:09,5,Now two probabilities related one
00:05:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,this can be regarded as the probability of,00:05:14,5,regarded probability
00:05:26,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"the observed evidence Y, given a particular X.",00:05:19,5,observed evidence Y given particular X
00:05:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So you can think about X as our hypothesis, and",00:05:26,5,So think X hypothesis
00:05:35,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,we have some prior belief about which hypothesis to choose.,00:05:30,5,prior belief hypothesis choose
00:05:40,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And after we have observed Y, we will update our belief and",00:05:35,5,And observed Y update belief
00:05:46,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,this updating formula is based on the combination of our prior.,00:05:40,5,updating formula based combination prior
00:05:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And the likelihood of observing this Y if X is indeed true,",00:05:48,5,And likelihood observing Y X indeed true
00:06:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,so much for detour about Bayes Rule.,00:05:57,5,much detour Bayes Rule
00:06:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"In our case, what we are interested in is inferring the theta values.",00:06:02,5,In case interested inferring theta values
00:06:14,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, we have a prior here that includes our prior knowledge about the parameters.",00:06:07,5,So prior includes prior knowledge parameters
00:06:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And then we have the data likelihood here,",00:06:15,5,And data likelihood
00:06:23,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,that would tell us which parameter value can explain the data well.,00:06:18,5,would tell us parameter value explain data well
00:06:28,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"The posterior probability combines both of them,",00:06:23,5,The posterior probability combines
00:06:34,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,so it represents a compromise of the the two preferences.,00:06:30,5,represents compromise two preferences
00:06:41,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And in such a case, we can maximize this posterior probability.",00:06:34,5,And case maximize posterior probability
00:06:47,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"To find this theta that would maximize this posterior probability,",00:06:41,5,To find theta would maximize posterior probability
00:06:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"and this estimator is called a Maximum a Posteriori, or MAP estimate.",00:06:47,5,estimator called Maximum Posteriori MAP estimate
00:06:58,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And this estimator is,00:06:55,5,And estimator
00:07:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,a more general estimator than the maximum likelihood estimator.,00:06:58,5,general estimator maximum likelihood estimator
00:07:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Because if we define our prior as a noninformative prior,",00:07:02,5,Because define prior noninformative prior
00:07:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,meaning that it's uniform over all the theta values.,00:07:08,5,meaning uniform theta values
00:07:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"No preference, then we basically would go back to the maximum likelihood estimated.",00:07:11,5,No preference basically would go back maximum likelihood estimated
00:07:21,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Because in such a case, it's mainly going to be determined by",00:07:16,5,Because case mainly going determined
00:07:25,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"this likelihood value, the same as here.",00:07:21,5,likelihood value
00:07:33,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"But if we have some not informative prior, some bias towards",00:07:28,5,But informative prior bias towards
00:07:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,the different values then map estimator can allow us to incorporate that.,00:07:33,5,different values map estimator allow us incorporate
00:07:43,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"But the problem here of course, is how to define the prior.",00:07:39,5,But problem course define prior
00:07:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"There is no free lunch and if you want to solve the problem with more knowledge,",00:07:44,5,There free lunch want solve problem knowledge
00:07:51,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,we have to have that knowledge.,00:07:49,5,knowledge
00:07:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And that knowledge, ideally, should be reliable.",00:07:51,5,And knowledge ideally reliable
00:07:58,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Otherwise, your estimate may not necessarily be more accurate than that",00:07:54,5,Otherwise estimate may necessarily accurate
00:07:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,maximum likelihood estimate.,00:07:58,5,maximum likelihood estimate
00:08:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, now let's look at the Bayesian estimation in more detail.",00:08:01,5,So let look Bayesian estimation detail
00:08:12,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, I show the theta values as just a one",00:08:08,5,So I show theta values one
00:08:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,dimension value and that's a simplification of course.,00:08:12,5,dimension value simplification course
00:08:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And so, we're interested in which variable of theta is optimal.",00:08:18,5,And interested variable theta optimal
00:08:26,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So now, first we have the Prior.",00:08:24,5,So first Prior
00:08:29,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,The Prior tells us that some of the variables,00:08:26,5,The Prior tells us variables
00:08:33,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,are more likely the others would believe.,00:08:29,5,likely others would believe
00:08:38,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"For example, these values are more likely than the values over here,",00:08:33,5,For example values likely values
00:08:40,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"or here, or other places.",00:08:38,5,places
00:08:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So this is our Prior, and",00:08:42,5,So Prior
00:08:51,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,then we have our theta likelihood.,00:08:45,5,theta likelihood
00:08:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And in this case, the theta also tells us which values of theta are more likely.",00:08:51,5,And case theta also tells us values theta likely
00:08:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And that just means loose syllables can best expand our theta.,00:08:56,5,And means loose syllables best expand theta
00:09:05,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And then when we combine the two we get the posterior distribution,",00:09:01,5,And combine two get posterior distribution
00:09:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,and that's just a compromise of the two.,00:09:05,5,compromise two
00:09:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,It would say that it's somewhere in-between.,00:09:07,5,It would say somewhere
00:09:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, we can now look at some interesting point that is made of.",00:09:11,5,So look interesting point made
00:09:21,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"This point represents the mode of prior, that means the most likely parameter",00:09:16,5,This point represents mode prior means likely parameter
00:09:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"value according to our prior, before we observe any data.",00:09:21,5,value according prior observe data
00:09:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"This point is the maximum likelihood estimator,",00:09:25,5,This point maximum likelihood estimator
00:09:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,it represents the theta that gives the theta of maximum probability.,00:09:27,5,represents theta gives theta maximum probability
00:09:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now this point is interesting, it's the posterior mode.",00:09:32,5,Now point interesting posterior mode
00:09:43,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,It's the most likely value of the theta given by the posterior of this.,00:09:38,5,It likely value theta given posterior
00:09:48,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And it represents a good compromise of the prior mode and,00:09:43,5,And represents good compromise prior mode
00:09:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,the maximum likelihood estimate.,00:09:48,5,maximum likelihood estimate
00:09:55,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Now in general in Bayesian inference, we are interested in",00:09:51,5,Now general Bayesian inference interested
00:09:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,the distribution of all these parameter additives as you see here.,00:09:55,5,distribution parameter additives see
00:10:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,If there's a distribution over see how values that you can see.,00:09:59,5,If distribution see values see
00:10:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Here, P of theta given X.",00:10:04,5,Here P theta given X
00:10:13,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,So the problem of Bayesian inference is,00:10:09,5,So problem Bayesian inference
00:10:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"to infer this posterior, this regime, and",00:10:14,5,infer posterior regime
00:10:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,also to infer other interesting quantities that might depend on theta.,00:10:18,5,also infer interesting quantities might depend theta
00:10:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, I show f of theta here",00:10:24,5,So I show f theta
00:10:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,as an interesting variable that we want to compute.,00:10:27,5,interesting variable want compute
00:10:34,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"But in order to compute this value, we need to know the value of theta.",00:10:30,5,But order compute value need know value theta
00:10:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"In Bayesian inference, we treat theta as an uncertain variable.",00:10:34,5,In Bayesian inference treat theta uncertain variable
00:10:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,So we think about all the possible variables of theta.,00:10:39,5,So think possible variables theta
00:10:50,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Therefore, we can estimate the value of this function f as extracted value of f,",00:10:42,5,Therefore estimate value function f extracted value f
00:10:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"according to the posterior distribution of theta, given the observed evidence X.",00:10:50,5,according posterior distribution theta given observed evidence X
00:11:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"As a special case, we can assume f of theta is just equal to theta.",00:10:58,5,As special case assume f theta equal theta
00:11:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"In this case, we get the expected value of the theta,",00:11:04,5,In case get expected value theta
00:11:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,that's basically the posterior mean.,00:11:08,5,basically posterior mean
00:11:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"That gives us also one point of theta, and",00:11:11,5,That gives us also one point theta
00:11:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"it's sometimes the same as posterior mode, but it's not always the same.",00:11:15,5,sometimes posterior mode always
00:11:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, it gives us another way to estimate the parameter.",00:11:19,5,So gives us another way estimate parameter
00:11:29,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So, this is a general illustration of Bayesian estimation and its an influence.",00:11:24,5,So general illustration Bayesian estimation influence
00:11:33,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And later, you will see this can be useful for",00:11:29,5,And later see useful
00:11:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,topic mining where we want to inject the sum prior knowledge about the topics.,00:11:33,5,topic mining want inject sum prior knowledge topics
00:11:43,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"So to summarize, we've used the language model",00:11:39,5,So summarize used language model
00:11:46,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,which is basically probability distribution over text.,00:11:43,5,basically probability distribution text
00:11:48,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,It's also called a generative model for text data.,00:11:46,5,It also called generative model text data
00:11:51,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"The simplest language model is Unigram Language Model,",00:11:48,5,The simplest language model Unigram Language Model
00:11:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,it's basically a word distribution.,00:11:51,5,basically word distribution
00:11:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"We introduced the concept of likelihood function,",00:11:54,5,We introduced concept likelihood function
00:12:00,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,which is the probability of the a data given some model.,00:11:57,5,probability data given model
00:12:03,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And this function is very important,",00:12:02,5,And function important
00:12:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"given a particular set of parameter values this function can tell us which X,",00:12:05,5,given particular set parameter values function tell us X
00:12:13,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"which data point has a higher likelihood, higher probability.",00:12:09,5,data point higher likelihood higher probability
00:12:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"Given a data sample X, we can use this function to determine",00:12:16,5,Given data sample X use function determine
00:12:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"which parameter values would maximize the probability of the observed data,",00:12:22,5,parameter values would maximize probability observed data
00:12:29,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,and this is the maximum livelihood estimate.,00:12:27,5,maximum livelihood estimate
00:12:34,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,We also talk about the Bayesian estimation or inference.,00:12:31,5,We also talk Bayesian estimation inference
00:12:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"In this case we, must define a prior on the parameters p of theta.",00:12:34,5,In case must define prior parameters p theta
00:12:43,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,"And then we're interested in computing the posterior distribution of the parameters,",00:12:39,5,And interested computing posterior distribution parameters
00:12:47,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,which is proportional to the prior and the likelihood.,00:12:43,5,proportional prior likelihood
00:12:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 2,2.5,And this distribution would allow us then to infer any derive that is from theta.,00:12:48,5,And distribution would allow us infer derive theta
00:00:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,[SOUND] So,00:00:00,11,SOUND So
00:00:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"this is indeed a general idea of the Expectation-Maximization, or EM,",00:00:08,11,indeed general idea Expectation Maximization EM
00:00:13,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Algorithm.,00:00:12,11,Algorithm
00:00:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So in all the EM algorithms we introduce a hidden variable,00:00:14,11,So EM algorithms introduce hidden variable
00:00:21,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,to help us solve the problem more easily.,00:00:19,11,help us solve problem easily
00:00:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,In our case the hidden variable is a binary variable for,00:00:21,11,In case hidden variable binary variable
00:00:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,each occurrence of a word.,00:00:25,11,occurrence word
00:00:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And this binary variable would indicate whether the word has,00:00:27,11,And binary variable would indicate whether word
00:00:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,been generated from 0 sub d or 0 sub p.,00:00:32,11,generated 0 sub 0 sub p
00:00:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And here we show some possible values of these variables.,00:00:35,11,And show possible values variables
00:00:43,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"For example, for the it's from background, the z value is one.",00:00:38,11,For example background z value one
00:00:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And text on the other hand.,00:00:43,11,And text hand
00:00:52,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Is from the topic then it's zero for z, etc.",00:00:45,11,Is topic zero z etc
00:00:58,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Now, of course, we don't observe these z values, we just imagine they're all such.",00:00:53,11,Now course observe z values imagine
00:01:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Values of z attaching to other words.,00:00:58,11,Values z attaching words
00:01:04,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And that's why we call these hidden variables.,00:01:02,11,And call hidden variables
00:01:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Now, the idea that we talked about before for",00:01:06,11,Now idea talked
00:01:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,predicting the word distribution that has been used when we generate the word,00:01:08,11,predicting word distribution used generate word
00:01:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"is it a predictor, the value of this hidden variable?",00:01:12,11,predictor value hidden variable
00:01:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And, so, the EM algorithm then, would work as follows.",00:01:18,11,And EM algorithm would work follows
00:01:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"First, we'll initialize all the parameters with random values.",00:01:25,11,First initialize parameters random values
00:01:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"In our case, the parameters are mainly the probability.",00:01:30,11,In case parameters mainly probability
00:01:37,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"of a word, given by theta sub d.",00:01:34,11,word given theta sub
00:01:39,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So this is an initial addition stage.,00:01:37,11,So initial addition stage
00:01:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,These initialized values would allow us to use base roll to take a guess,00:01:39,11,These initialized values would allow us use base roll take guess
00:01:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"of these z values, so we'd guess these values.",00:01:44,11,z values guess values
00:01:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,We can't say for sure whether textt is from background or not.,00:01:48,11,We say sure whether textt background
00:01:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,But we can have our guess.,00:01:53,11,But guess
00:01:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,This is given by this formula.,00:01:55,11,This given formula
00:01:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,It's called an E-step.,00:01:57,11,It called E step
00:02:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And so the algorithm would then try to use the E-step to guess these z values.,00:01:59,11,And algorithm would try use E step guess z values
00:02:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"After that, it would then invoke another that's called M-step.",00:02:06,11,After would invoke another called M step
00:02:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,In this step we simply take advantage of the inferred z values and,00:02:12,11,In step simply take advantage inferred z values
00:02:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,then just group words that are in the same distribution like these,00:02:17,11,group words distribution like
00:02:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,from that ground including this as well.,00:02:22,11,ground including well
00:02:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,We can then normalize the count to estimate the probabilities or,00:02:27,11,We normalize count estimate probabilities
00:02:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,to revise our estimate of the parameters.,00:02:32,11,revise estimate parameters
00:02:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So let me also illustrate that we can group the words,00:02:36,11,So let also illustrate group words
00:02:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"that are believed to have come from zero sub d, and",00:02:42,11,believed come zero sub
00:02:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"that's text, mining algorithm, for example, and clustering.",00:02:46,11,text mining algorithm example clustering
00:02:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And we group them together to help us,00:02:51,11,And group together help us
00:03:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,re-estimate the parameters that we're interested in.,00:02:55,11,estimate parameters interested
00:03:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So these will help us estimate these parameters.,00:03:01,11,So help us estimate parameters
00:03:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Note that before we just set these parameter values randomly.,00:03:06,11,Note set parameter values randomly
00:03:15,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"But with this guess, we will have somewhat improved estimate of this.",00:03:09,11,But guess somewhat improved estimate
00:03:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Of course, we don't know exactly whether it's zero or one.",00:03:15,11,Of course know exactly whether zero one
00:03:24,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So we're not going to really do the split in a hard way.,00:03:18,11,So going really split hard way
00:03:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,But rather we're going to do a softer split.,00:03:24,11,But rather going softer split
00:03:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And this is what happened here.,00:03:26,11,And happened
00:03:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So we're going to adjust the count by the probability that would believe,00:03:29,11,So going adjust count probability would believe
00:03:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,this word has been generated by using the theta sub d.,00:03:34,11,word generated using theta sub
00:03:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And you can see this, where does this come from?",00:03:39,11,And see come
00:03:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Well, this has come from here, right?",00:03:42,11,Well come right
00:03:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,From the E-step.,00:03:46,11,From E step
00:03:52,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So the EM Algorithm would iteratively improve uur initial,00:03:48,11,So EM Algorithm would iteratively improve uur initial
00:03:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,estimate of parameters by using E-step first and then M-step.,00:03:52,11,estimate parameters using E step first M step
00:04:02,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"The E-step is to augment the data with additional information, like z.",00:03:57,11,The E step augment data additional information like z
00:04:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And the M-step is to take advantage,00:04:02,11,And M step take advantage
00:04:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,of the additional information to separate the data.,00:04:05,11,additional information separate data
00:04:13,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,To split the data accounts and then collect the right data accounts to,00:04:08,11,To split data accounts collect right data accounts
00:04:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,re-estimate our parameter.,00:04:13,11,estimate parameter
00:04:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And then once we have a new generation of parameter, we're going to repeat this.",00:04:17,11,And new generation parameter going repeat
00:04:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,We are going the E-step again.,00:04:22,11,We going E step
00:04:28,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,To improve our estimate of the hidden variables.,00:04:25,11,To improve estimate hidden variables
00:04:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And then that would lead to another generation of re-estimated parameters.,00:04:28,11,And would lead another generation estimated parameters
00:04:37,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,For the word distribution that we are interested in.,00:04:34,11,For word distribution interested
00:04:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Okay, so, as I said, the bridge between the two",00:04:39,11,Okay I said bridge two
00:04:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"is really the variable z, hidden variable, which indicates how likely",00:04:44,11,really variable z hidden variable indicates likely
00:04:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"this water is from the top water distribution, theta sub p.",00:04:50,11,water top water distribution theta sub p
00:05:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So, this slide has a lot of content and you may need to.",00:04:56,11,So slide lot content may need
00:05:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Pause the reader to digest it.,00:05:00,11,Pause reader digest
00:05:07,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,But this basically captures the essence of EM Algorithm.,00:05:03,11,But basically captures essence EM Algorithm
00:05:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Start with initial values that are often random themself.,00:05:07,11,Start initial values often random themself
00:05:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And then we invoke E-step followed by M-step to get an improved,00:05:12,11,And invoke E step followed M step get improved
00:05:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,setting of parameters.,00:05:18,11,setting parameters
00:05:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And then we repeated this, so this a Hill-Climbing algorithm",00:05:19,11,And repeated Hill Climbing algorithm
00:05:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,that would gradually improve the estimate of parameters.,00:05:23,11,would gradually improve estimate parameters
00:05:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,As I will explain later there is some guarantee for,00:05:27,11,As I explain later guarantee
00:05:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,reaching a local maximum of the log-likelihood function.,00:05:30,11,reaching local maximum log likelihood function
00:05:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So lets take a look at the computation for a specific case, so",00:05:35,11,So lets take look computation specific case
00:05:41,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,these formulas are the EM.,00:05:40,11,formulas EM
00:05:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Formulas that you see before, and you can also see there are superscripts,",00:05:41,11,Formulas see also see superscripts
00:05:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"here, like here, n, to indicate the generation of parameters.",00:05:48,11,like n indicate generation parameters
00:05:56,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Like here for example we have n plus one.,00:05:53,11,Like example n plus one
00:05:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,That means we have improved.,00:05:56,11,That means improved
00:06:04,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,From here to here we have an improvement.,00:05:59,11,From improvement
00:06:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So in this setting we have assumed the two numerals have equal probabilities and,00:06:04,11,So setting assumed two numerals equal probabilities
00:06:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,the background model is null.,00:06:08,11,background model null
00:06:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So what are the relevance of the statistics?,00:06:09,11,So relevance statistics
00:06:13,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Well these are the word counts.,00:06:11,11,Well word counts
00:06:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So assume we have just four words, and their counts are like this.",00:06:13,11,So assume four words counts like
00:06:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And this is our background model that assigns high probabilities to common,00:06:18,11,And background model assigns high probabilities common
00:06:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,words like the.,00:06:22,11,words like
00:06:29,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And in the first iteration, you can picture what will happen.",00:06:25,11,And first iteration picture happen
00:06:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Well first we initialize all the values.,00:06:29,11,Well first initialize values
00:06:37,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So here, this probability that we're interested in is normalized into a uniform",00:06:32,11,So probability interested normalized uniform
00:06:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,distribution of all the words.,00:06:37,11,distribution words
00:06:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And then the E-step would give us a guess of the distribution that has been used.,00:06:40,11,And E step would give us guess distribution used
00:06:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,That will generate each word.,00:06:45,11,That generate word
00:06:51,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,We can see we have different probabilities for different words.,00:06:48,11,We see different probabilities different words
00:06:52,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Why?,00:06:51,11,Why
00:06:56,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Well, that's because these words have different probabilities in the background.",00:06:52,11,Well words different probabilities background
00:07:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So even though the two distributions are equally likely.,00:06:56,11,So even though two distributions equally likely
00:07:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And then our initial audition say uniform distribution because of the difference,00:07:00,11,And initial audition say uniform distribution difference
00:07:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"in the background of the distribution, we have different guess the probability.",00:07:05,11,background distribution different guess probability
00:07:14,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So these words are believed to be more likely from the topic.,00:07:09,11,So words believed likely topic
00:07:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,These on the other hand are less likely.,00:07:15,11,These hand less likely
00:07:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Probably from background.,00:07:17,11,Probably background
00:07:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So once we have these z values,",00:07:20,11,So z values
00:07:28,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,we know in the M-step these probabilities will be used to adjust the counts.,00:07:23,11,know M step probabilities used adjust counts
00:07:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So four must be multiplied by this 0.33,00:07:28,11,So four must multiplied 0 33
00:07:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,in order to get the allocated accounts toward the topic.,00:07:33,11,order get allocated accounts toward topic
00:07:43,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And this is done by this multiplication.,00:07:39,11,And done multiplication
00:07:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Note that if our guess says this is 100% If this is one point zero,",00:07:43,11,Note guess says 100 If one point zero
00:07:58,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,then we just get the full count of this word for this topic.,00:07:52,11,get full count word topic
00:08:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,In general it's not going to be one point zero.,00:07:58,11,In general going one point zero
00:08:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So we're just going to get some percentage of this counts toward this topic.,00:08:01,11,So going get percentage counts toward topic
00:08:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Then we simply normalize these counts,00:08:06,11,Then simply normalize counts
00:08:13,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,to have a new generation of parameters estimate.,00:08:09,11,new generation parameters estimate
00:08:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So you can see, compare this with the older one, which is here.",00:08:13,11,So see compare older one
00:08:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So compare this with this one and we'll see the probability is different.,00:08:18,11,So compare one see probability different
00:08:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Not only that, we also see some",00:08:23,11,Not also see
00:08:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,words that are believed to have come from the topic will have a higher probability.,00:08:25,11,words believed come topic higher probability
00:08:31,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Like this one, text.",00:08:30,11,Like one text
00:08:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And of course, this new generation of parameters would allow us to further",00:08:32,11,And course new generation parameters would allow us
00:08:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,adjust the inferred latent variable or hidden variable values.,00:08:35,11,adjust inferred latent variable hidden variable values
00:08:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"So we have a new generation of values,",00:08:42,11,So new generation values
00:08:51,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,because of the E-step based on the new generation of parameters.,00:08:45,11,E step based new generation parameters
00:08:56,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And these new inferred values of Zs will give us then,00:08:51,11,And new inferred values Zs give us
00:09:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,another generation of the estimate of probabilities of the word.,00:08:56,11,another generation estimate probabilities word
00:09:07,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And so on and so forth so this is what would actually happen when we compute,00:09:03,11,And forth would actually happen compute
00:09:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,these probabilities using the EM Algorithm.,00:09:07,11,probabilities using EM Algorithm
00:09:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"As you can see in the last row where we show the log-likelihood,",00:09:11,11,As see last row show log likelihood
00:09:20,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,and the likelihood is increasing as we do the iteration.,00:09:16,11,likelihood increasing iteration
00:09:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And note that these log-likelihood is negative because the probability is,00:09:20,11,And note log likelihood negative probability
00:09:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"between 0 and 1 when you take a logarithm, it becomes a negative value.",00:09:25,11,0 1 take logarithm becomes negative value
00:09:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"Now what's also interesting is, you'll note the last column.",00:09:30,11,Now also interesting note last column
00:09:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And these are the inverted word split.,00:09:33,11,And inverted word split
00:09:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And these are the probabilities that a word is believed to,00:09:36,11,And probabilities word believed
00:09:47,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"have come from one distribution, in this case the topical distribution, all right.",00:09:42,11,come one distribution case topical distribution right
00:09:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,And you might wonder whether this would be also useful.,00:09:47,11,And might wonder whether would also useful
00:09:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,Because our main goal is to estimate these word distributions.,00:09:50,11,Because main goal estimate word distributions
00:09:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,So this is our primary goal.,00:09:55,11,So primary goal
00:10:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,We hope to have a more discriminative order of distribution.,00:09:57,11,We hope discriminative order distribution
00:10:04,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,But the last column is also bi-product.,00:10:00,11,But last column also bi product
00:10:07,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,This also can actually be very useful.,00:10:04,11,This also actually useful
00:10:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,You can think about that.,00:10:07,11,You think
00:10:10,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"We want to use, is to for",00:10:08,11,We want use
00:10:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,example is to estimate to what extent this document has covered background words.,00:10:10,11,example estimate extent document covered background words
00:10:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,"And this, when we add this up or",00:10:16,11,And add
00:10:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,take the average we will kind of know to what extent it has covered background,00:10:18,11,take average kind know extent covered background
00:10:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2,2.11,versus content was that are not explained well by the background.,00:10:23,11,versus content explained well background
00:00:07,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,[MUSIC],00:00:00,8,MUSIC
00:00:10,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,This lecture is about a specific technique for,00:00:07,8,This lecture specific technique
00:00:16,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Contextual Text Mining called Contextual Probabilistic Latent Semantic Analysis.,00:00:10,8,Contextual Text Mining called Contextual Probabilistic Latent Semantic Analysis
00:00:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"In this lecture, we're going to continue discussing Contextual Text Mining.",00:00:19,8,In lecture going continue discussing Contextual Text Mining
00:00:28,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And we're going to introduce Contextual Probablitistic Latent Semantic Analysis,00:00:23,8,And going introduce Contextual Probablitistic Latent Semantic Analysis
00:00:32,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,as exchanging of POS for doing contextual text mining.,00:00:28,8,exchanging POS contextual text mining
00:00:40,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Recall that in contextual text mining we hope to analyze topics in text,",00:00:34,8,Recall contextual text mining hope analyze topics text
00:00:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,in consideration of the context so,00:00:40,8,consideration context
00:00:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,that we can associate the topics with a property of the context were interesting.,00:00:42,8,associate topics property context interesting
00:00:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So in this approach, contextual probabilistic latent semantic analysis,",00:00:48,8,So approach contextual probabilistic latent semantic analysis
00:00:58,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"or CPLSA, the main idea is to express to the add interesting",00:00:54,8,CPLSA main idea express add interesting
00:01:01,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,context variables into a generating model.,00:00:58,8,context variables generating model
00:01:06,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Recall that before when we generate the text we generally assume we'll start,00:01:03,8,Recall generate text generally assume start
00:01:10,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"wIth some topics, and then assemble words from some topics.",00:01:06,8,wIth topics assemble words topics
00:01:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But here, we're going to add context variables, so that the coverage of topics,",00:01:10,8,But going add context variables coverage topics
00:01:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,and also the content of topics would be tied in context.,00:01:18,8,also content topics would tied context
00:01:27,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Or in other words, we're going to let the context Influence both coverage and",00:01:23,8,Or words going let context Influence coverage
00:01:28,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the content of a topic.,00:01:27,8,content topic
00:01:37,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The consequences that this will enable us to discover contextualized topics.,00:01:31,8,The consequences enable us discover contextualized topics
00:01:41,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Make the topics more interesting, more meaningful.",00:01:37,8,Make topics interesting meaningful
00:01:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Because we can then have topics that can be interpreted as,00:01:41,8,Because topics interpreted
00:01:49,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,specifically to a particular context that we are interested in.,00:01:46,8,specifically particular context interested
00:01:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"For example, a particular time period.",00:01:49,8,For example particular time period
00:01:55,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"As an extension of PLSA model,",00:01:52,8,As extension PLSA model
00:02:01,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,CPLSA does the following changes.,00:01:55,8,CPLSA following changes
00:02:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Firstly it would model the conditional likelihood of text given context.,00:02:01,8,Firstly would model conditional likelihood text given context
00:02:12,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"That clearly suggests that the generation of text would then depend on context,",00:02:07,8,That clearly suggests generation text would depend context
00:02:16,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,and that allows us to bring context into the generative model.,00:02:12,8,allows us bring context generative model
00:02:22,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Secondly, it makes two specific assumptions about the dependency",00:02:18,8,Secondly makes two specific assumptions dependency
00:02:24,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,of topics on context.,00:02:22,8,topics context
00:02:28,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"One is to assume that depending on the context, depending on different time",00:02:24,8,One assume depending context depending different time
00:02:33,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"periods or different locations, we assume that there are different views of a topic",00:02:28,8,periods different locations assume different views topic
00:02:37,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,or different versions of word descriptions that characterize a topic.,00:02:33,8,different versions word descriptions characterize topic
00:02:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this assumption allows us to discover different,00:02:38,8,And assumption allows us discover different
00:02:45,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,variations of the same topic in different contexts.,00:02:42,8,variations topic different contexts
00:02:53,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The other is that we assume the topic coverage also depends on the context.,00:02:46,8,The assume topic coverage also depends context
00:02:56,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,That means depending on the time or,00:02:55,8,That means depending time
00:02:59,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"location, we might cover topics differently.",00:02:56,8,location might cover topics differently
00:03:03,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Again, this dependency would then allow us to",00:03:00,8,Again dependency would allow us
00:03:08,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,capture the association of topics with specific contexts.,00:03:03,8,capture association topics specific contexts
00:03:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,We can still use the EM algorithm to solve the problem of parameter estimation.,00:03:08,8,We still use EM algorithm solve problem parameter estimation
00:03:22,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And in this case, the estimated parameters would naturally contain context variables.",00:03:16,8,And case estimated parameters would naturally contain context variables
00:03:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And in particular,",00:03:22,8,And particular
00:03:29,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,a lot of conditional probabilities of topics given certain context.,00:03:23,8,lot conditional probabilities topics given certain context
00:03:33,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this is what allows you to do contextual text mining.,00:03:29,8,And allows contextual text mining
00:03:34,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So this is the basic idea.,00:03:33,8,So basic idea
00:03:41,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now, we don't have time to introduce this model in detail,",00:03:35,8,Now time introduce model detail
00:03:45,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,but there are references here that you can look into to know more detail.,00:03:41,8,references look know detail
00:03:52,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Here I just want to explain the high level ideas in more detail.,00:03:45,8,Here I want explain high level ideas detail
00:03:55,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Particularly I want to explain the generation process.,00:03:52,8,Particularly I want explain generation process
00:04:00,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Of text data that has context associated in such a model.,00:03:55,8,Of text data context associated model
00:04:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So as you see here, we can assume there are still multiple topics.",00:04:01,8,So see assume still multiple topics
00:04:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"For example, some topics might represent a themes like a government response,",00:04:05,8,For example topics might represent themes like government response
00:04:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,donation Or the city of New Orleans.,00:04:11,8,donation Or city New Orleans
00:04:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Now this example is in the context of Hurricane Katrina and,00:04:14,8,Now example context Hurricane Katrina
00:04:20,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,that hit New Orleans.,00:04:18,8,hit New Orleans
00:04:27,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Now as you can see we assume there are different,00:04:22,8,Now see assume different
00:04:31,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,views associated with each of the topics.,00:04:27,8,views associated topics
00:04:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And these are shown as View 1, View 2, View 3.",00:04:31,8,And shown View 1 View 2 View 3
00:04:41,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Each view is a different version of word distributions.,00:04:36,8,Each view different version word distributions
00:04:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And these views are tied to some context variables.,00:04:41,8,And views tied context variables
00:04:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"For example, tied to the location Texas, or the time July 2005,",00:04:44,8,For example tied location Texas time July 2005
00:04:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,or the occupation of the author being a sociologist.,00:04:50,8,occupation author sociologist
00:05:01,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now, on the right side, now we assume the document has context information.",00:04:56,8,Now right side assume document context information
00:05:04,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So the time is known to be July 2005.,00:05:01,8,So time known July 2005
00:05:06,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"The location is Texas, etc.",00:05:04,8,The location Texas etc
00:05:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And such context information is what we hope to model as well.,00:05:06,8,And context information hope model well
00:05:13,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So we're not going to just model the text.,00:05:11,8,So going model text
00:05:20,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And so one idea here is to model the variations of top content and,00:05:15,8,And one idea model variations top content
00:05:21,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,various content.,00:05:20,8,various content
00:05:25,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this gives us different views of the water distributions.,00:05:21,8,And gives us different views water distributions
00:05:32,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Now on the bottom you will see the theme coverage of top Coverage might also vary,00:05:27,8,Now bottom see theme coverage top Coverage might also vary
00:05:39,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,according to these context because in the case,00:05:32,8,according context case
00:05:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"of a location like Texas, people might want to cover the red topics more.",00:05:39,8,location like Texas people might want cover red topics
00:05:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,That's New Orleans.,00:05:44,8,That New Orleans
00:05:47,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,That's visualized here.,00:05:46,8,That visualized
00:05:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But in a certain time period,",00:05:47,8,But certain time period
00:05:56,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,maybe Particular topic and will be covered more.,00:05:50,8,maybe Particular topic covered
00:06:00,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So this variation is also considered in CPLSA.,00:05:56,8,So variation also considered CPLSA
00:06:07,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So to generate the searcher document With context, with first also choose a view.",00:06:00,8,So generate searcher document With context first also choose view
00:06:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this view of course now could be from any of these contexts.,00:06:08,8,And view course could contexts
00:06:17,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Let's say, we have taken this view that depends on the time.",00:06:14,8,Let say taken view depends time
00:06:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,In the middle.,00:06:17,8,In middle
00:06:21,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So now, we will have a specific version of word distributions.",00:06:18,8,So specific version word distributions
00:06:25,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now, you can see some probabilities of words for each topic.",00:06:21,8,Now see probabilities words topic
00:06:28,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now, once we have chosen a view,",00:06:26,8,Now chosen view
00:06:34,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,now the situation will be very similar to what happened in standard ((PRSA)),00:06:28,8,situation similar happened standard PRSA
00:06:38,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"We assume we have got word distribution associated with each topic, right?",00:06:34,8,We assume got word distribution associated topic right
00:06:43,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And then next, we will also choose a coverage from the bottom, so",00:06:39,8,And next also choose coverage bottom
00:06:47,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"we're going to choose a particular coverage, and that coverage,",00:06:43,8,going choose particular coverage coverage
00:06:55,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"before is fixed in PLSA, and assigned to a particular document.",00:06:47,8,fixed PLSA assigned particular document
00:06:57,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Each document has just one coverage distribution.,00:06:55,8,Each document one coverage distribution
00:07:03,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now here, because we consider context, so the distribution of topics or the coverage",00:06:58,8,Now consider context distribution topics coverage
00:07:08,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,of Topics can vary depending on the context that has influenced the coverage.,00:07:03,8,Topics vary depending context influenced coverage
00:07:13,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So, for example, we might pick a particular coverage.",00:07:10,8,So example might pick particular coverage
00:07:19,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Let's say in this case we picked a document specific coverage.,00:07:13,8,Let say case picked document specific coverage
00:07:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Now with the coverage and these word distributions,00:07:20,8,Now coverage word distributions
00:07:26,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,we can generate a document in exactly the same way as in PLSA.,00:07:23,8,generate document exactly way PLSA
00:07:32,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So what it means, we're going to use the coverage to choose a topic,",00:07:26,8,So means going use coverage choose topic
00:07:34,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,to choose one of these three topics.,00:07:32,8,choose one three topics
00:07:38,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Let's say we have picked the yellow topic.,00:07:34,8,Let say picked yellow topic
00:07:43,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Then we'll draw a word from this particular topic on the top.,00:07:38,8,Then draw word particular topic top
00:07:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Okay, so we might get a word like government.",00:07:44,8,Okay might get word like government
00:07:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And then next time we might choose a different topic, and",00:07:46,8,And next time might choose different topic
00:07:53,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"we'll get donate, etc.",00:07:50,8,get donate etc
00:07:55,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Until we generate all the words.,00:07:53,8,Until generate words
00:07:58,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this is basically the same process as in PLSA.,00:07:55,8,And basically process PLSA
00:08:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So the main difference is when we obtain the coverage.,00:08:00,8,So main difference obtain coverage
00:08:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And the word distribution, we let the context influence our choice So",00:08:05,8,And word distribution let context influence choice So
00:08:16,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,in other words we have extra switches that are tied to these contacts that will,00:08:11,8,words extra switches tied contacts
00:08:20,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,control the choices of different views of topics and the choices of coverage.,00:08:16,8,control choices different views topics choices coverage
00:08:25,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And naturally the model we have more parameters to estimate.,00:08:22,8,And naturally model parameters estimate
00:08:29,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But once we can estimate those parameters that involve the context,",00:08:25,8,But estimate parameters involve context
00:08:33,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"then we will be able to understand the context specific views of topics,",00:08:29,8,able understand context specific views topics
00:08:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,or context specific coverages of topics.,00:08:33,8,context specific coverages topics
00:08:38,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this is precisely what we want in contextual text mining.,00:08:36,8,And precisely want contextual text mining
00:08:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So here are some simple results.,00:08:40,8,So simple results
00:08:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,From using such a model.,00:08:42,8,From using model
00:08:48,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Not necessary exactly the same model, but similar models.",00:08:44,8,Not necessary exactly model similar models
00:08:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So on this slide you see some sample results of,00:08:48,8,So slide see sample results
00:08:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,comparing news articles about Iraq War and Afghanistan War.,00:08:50,8,comparing news articles Iraq War Afghanistan War
00:09:02,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now we have about 30 articles on Iraq wa,r and 26 articles on Afghanistan war.",00:08:56,8,Now 30 articles Iraq wa r 26 articles Afghanistan war
00:09:08,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And in this case, the goal is to review the common topic.",00:09:02,8,And case goal review common topic
00:09:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It's covered in both sets of articles and,00:09:08,8,It covered sets articles
00:09:17,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the differences of variations of the topic in each of the two collections.,00:09:11,8,differences variations topic two collections
00:09:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So in this case the context is explicitly specified by the topic or collection.,00:09:18,8,So case context explicitly specified topic collection
00:09:30,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And we see the results here show that there is a common,00:09:25,8,And see results show common
00:09:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,theme that's corresponding to Cluster 1 here in this column.,00:09:30,8,theme corresponding Cluster 1 column
00:09:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And there is a common theme indicting that United Nations is involved in both Wars.,00:09:36,8,And common theme indicting United Nations involved Wars
00:09:45,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It's a common topic covered in both sets of articles.,00:09:42,8,It common topic covered sets articles
00:09:48,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And that's indicated by the high probability words shown here, united and",00:09:45,8,And indicated high probability words shown united
00:09:49,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,nations.,00:09:48,8,nations
00:09:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now if you know the background, of course this is not surprising and",00:09:51,8,Now know background course surprising
00:10:00,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,this topic is indeed very relevant to both wars.,00:09:54,8,topic indeed relevant wars
00:10:04,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,If you look at the column further and then what's interesting's that the next,00:10:00,8,If look column interesting next
00:10:09,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,two cells of word distributions actually tell us,00:10:04,8,two cells word distributions actually tell us
00:10:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,collection specific variations of the topic of United Nations.,00:10:09,8,collection specific variations topic United Nations
00:10:16,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So it indicates that the Iraq War,",00:10:14,8,So indicates Iraq War
00:10:21,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"United Nations was more involved in weapons factions, whereas in",00:10:16,8,United Nations involved weapons factions whereas
00:10:25,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the Afghanistan War it was more involved in maybe aid to Northern Alliance.,00:10:21,8,Afghanistan War involved maybe aid Northern Alliance
00:10:29,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It's a different variation of the topic of United Nations.,00:10:25,8,It different variation topic United Nations
00:10:33,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So this shows that by bringing the context.,00:10:30,8,So shows bringing context
00:10:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,In this case different the walls or different the collection of texts.,00:10:33,8,In case different walls different collection texts
00:10:40,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"We can have topical variations tied to these contexts,",00:10:36,8,We topical variations tied contexts
00:10:45,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,to review the differences of coverage of the United Nations in the two wars.,00:10:40,8,review differences coverage United Nations two wars
00:10:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Now similarly if you look at the second cluster Class two,",00:10:46,8,Now similarly look second cluster Class two
00:10:52,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"it has to do with the killing of people, and, again,",00:10:50,8,killing people
00:10:56,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,it's not surprising if you know the background about wars.,00:10:52,8,surprising know background wars
00:10:59,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"All the wars involve killing of people, but",00:10:56,8,All wars involve killing people
00:11:03,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,imagine if you are not familiar with the text collections.,00:10:59,8,imagine familiar text collections
00:11:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"We have a lot of text articles, and",00:11:03,8,We lot text articles
00:11:10,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,such a technique can reveal the common topics covered in both sets of articles.,00:11:05,8,technique reveal common topics covered sets articles
00:11:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It can be used to review common topics in multiple sets of articles as well.,00:11:10,8,It used review common topics multiple sets articles well
00:11:19,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"If you look at of course in that column of cluster two,",00:11:14,8,If look course column cluster two
00:11:26,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,you see variations of killing of people and that corresponds to different contexts,00:11:19,8,see variations killing people corresponds different contexts
00:11:31,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And here is another example of results,00:11:28,8,And another example results
00:11:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,obtained from blog articles about Hurricane Katrina.,00:11:31,8,obtained blog articles Hurricane Katrina
00:11:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"In this case, what you see here is visualization of",00:11:37,8,In case see visualization
00:11:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the trends of topics over time.,00:11:42,8,trends topics time
00:11:52,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And the top one shows just the temporal trends of two topics.,00:11:47,8,And top one shows temporal trends two topics
00:11:58,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"One is oil price, and one is about the flooding of the city of New Orleans.",00:11:52,8,One oil price one flooding city New Orleans
00:12:06,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Now these topics are obtained from blog articles about Hurricane Katrina.,00:12:00,8,Now topics obtained blog articles Hurricane Katrina
00:12:09,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And people talk about these topics.,00:12:07,8,And people talk topics
00:12:12,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And end up teaching to some other topics.,00:12:09,8,And end teaching topics
00:12:15,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But the visualisation shows that with this technique,",00:12:12,8,But visualisation shows technique
00:12:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,we can have conditional distribution of time.,00:12:15,8,conditional distribution time
00:12:19,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Given a topic.,00:12:18,8,Given topic
00:12:23,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So this allows us to plot this conditional probability,00:12:19,8,So allows us plot conditional probability
00:12:26,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the curve is like what you're seeing here.,00:12:23,8,curve like seeing
00:12:31,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"We see that, initially, the two curves tracked each other very well.",00:12:26,8,We see initially two curves tracked well
00:12:40,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,But later we see the topic of New Orleans was mentioned again but oil price was not.,00:12:31,8,But later see topic New Orleans mentioned oil price
00:12:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And this turns out to be,00:12:40,8,And turns
00:12:49,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"the time period when another hurricane, hurricane Rita hit the region.",00:12:44,8,time period another hurricane hurricane Rita hit region
00:12:52,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And that apparently triggered more discussion about the flooding of the city.,00:12:49,8,And apparently triggered discussion flooding city
00:13:00,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The bottom curve shows the coverage of this topic,00:12:54,8,The bottom curve shows coverage topic
00:13:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,about flooding of the city by block articles in different locations.,00:13:00,8,flooding city block articles different locations
00:13:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And it also shows some shift of coverage that might be related to,00:13:05,8,And also shows shift coverage might related
00:13:19,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,people's migrating from the state of Louisiana to Texas for example.,00:13:11,8,people migrating state Louisiana Texas example
00:13:25,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So in this case we can see the time can be used as context to review trends of,00:13:20,8,So case see time used context review trends
00:13:26,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,topics.,00:13:25,8,topics
00:13:33,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,These are some additional results on spacial patterns.,00:13:27,8,These additional results spacial patterns
00:13:37,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,In this case it was about the topic of government response.,00:13:33,8,In case topic government response
00:13:41,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And there was some criticism about the slow response of government,00:13:37,8,And criticism slow response government
00:13:42,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,in the case of Hurricane Katrina.,00:13:41,8,case Hurricane Katrina
00:13:48,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And the discussion now is covered in different locations.,00:13:44,8,And discussion covered different locations
00:13:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And these visualizations show the coverage in different weeks of the event.,00:13:48,8,And visualizations show coverage different weeks event
00:13:59,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And initially it's covered mostly in the victim states,",00:13:54,8,And initially covered mostly victim states
00:14:05,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"in the South, but then gradually spread into other locations.",00:13:59,8,South gradually spread locations
00:14:09,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But in week four, which is shown on the bottom left,",00:14:05,8,But week four shown bottom left
00:14:14,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,we see a pattern that's very similar to the first week on the top left.,00:14:09,8,see pattern similar first week top left
00:14:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And that's when again Hurricane Rita hit in the region.,00:14:14,8,And Hurricane Rita hit region
00:14:22,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So such a technique would allow us to use location as context,00:14:18,8,So technique would allow us use location context
00:14:24,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,to examine their issues of topics.,00:14:22,8,examine issues topics
00:14:27,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And of course the moral is completely general so,00:14:24,8,And course moral completely general
00:14:30,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,you can apply this to any other connections of text.,00:14:27,8,apply connections text
00:14:32,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,To review spatial temporal patterns.,00:14:30,8,To review spatial temporal patterns
00:14:37,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"His view found another application of this kind of model,",00:14:34,8,His view found another application kind model
00:14:41,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,where we look at the use of the model for event impact analysis.,00:14:37,8,look use model event impact analysis
00:14:46,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So here we're looking at the research articles information retrieval.,00:14:43,8,So looking research articles information retrieval
00:14:49,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"IR, particularly SIGIR papers.",00:14:46,8,IR particularly SIGIR papers
00:14:53,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And the topic we are focusing on is about the retrieval models.,00:14:49,8,And topic focusing retrieval models
00:14:58,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And you can see the top words with high probability about this model on the left.,00:14:53,8,And see top words high probability model left
00:15:04,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And then we hope to examine the impact of two events.,00:14:59,8,And hope examine impact two events
00:15:08,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"One is a start of TREC, for Text and Retrieval Conference.",00:15:04,8,One start TREC Text Retrieval Conference
00:15:11,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,This is a major evaluation sponsored by U.S.,00:15:08,8,This major evaluation sponsored U S
00:15:16,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"government, and was launched in 1992 or around that time.",00:15:11,8,government launched 1992 around time
00:15:20,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And that is known to have made a impact on,00:15:16,8,And known made impact
00:15:22,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the topics of research information retrieval.,00:15:20,8,topics research information retrieval
00:15:28,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"The other is the publication of a seminal paper, by Croft and Porte.",00:15:23,8,The publication seminal paper Croft Porte
00:15:31,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,This is about a language model approach to information retrieval.,00:15:28,8,This language model approach information retrieval
00:15:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It's also known to have made a high impact on information retrieval research.,00:15:31,8,It also known made high impact information retrieval research
00:15:39,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So we hope to use this kind of model to understand impact.,00:15:36,8,So hope use kind model understand impact
00:15:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The idea here is simply to use the time as context.,00:15:39,8,The idea simply use time context
00:15:48,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And use these events to divide the time periods into a period before.,00:15:44,8,And use events divide time periods period
00:15:51,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,For the event and another after this event.,00:15:48,8,For event another event
00:15:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And then we can compare the differences of the topics.,00:15:51,8,And compare differences topics
00:15:57,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"The and the variations, etc.",00:15:54,8,The variations etc
00:16:02,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"So in this case, the results show before track the study of",00:15:57,8,So case results show track study
00:16:07,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"retrieval models was mostly a vector space model, Boolean model etc.",00:16:02,8,retrieval models mostly vector space model Boolean model etc
00:16:08,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"But the after Trec,",00:16:07,8,But Trec
00:16:13,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,apparently the study of retrieval models have involved a lot of other words.,00:16:08,8,apparently study retrieval models involved lot words
00:16:18,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"That seems to suggest some different retrieval tasks, so for",00:16:13,8,That seems suggest different retrieval tasks
00:16:22,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"example, email was used in the enterprise search tasks and",00:16:18,8,example email used enterprise search tasks
00:16:26,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,subtopical retrieval was another task later introduced by Trec.,00:16:22,8,subtopical retrieval another task later introduced Trec
00:16:32,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"On the bottom, we see the variations that are correlated",00:16:28,8,On bottom see variations correlated
00:16:36,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,with the propagation of the language model paper.,00:16:32,8,propagation language model paper
00:16:40,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"Before, we have those classic probability risk model,",00:16:36,8,Before classic probability risk model
00:16:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"logic model, Boolean etc., but after 1998,",00:16:40,8,logic model Boolean etc 1998
00:16:50,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,we see clear dominance of language model as probabilistic models.,00:16:44,8,see clear dominance language model probabilistic models
00:16:54,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,"And we see words like language model, estimation of parameters, etc.",00:16:50,8,And see words like language model estimation parameters etc
00:17:00,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,So this technique here can use events as context to understand the impact of event.,00:16:54,8,So technique use events context understand impact event
00:17:03,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Again the technique is generals so,00:17:00,8,Again technique generals
00:17:07,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,you can use this to analyze the impact of any event.,00:17:03,8,use analyze impact event
00:17:10,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Here are some suggested readings.,00:17:07,8,Here suggested readings
00:17:20,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The first is paper about simple staging of psi to label cross-collection comparison.,00:17:11,8,The first paper simple staging psi label cross collection comparison
00:17:24,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,It's to perform comparative text mining to allow us to,00:17:21,8,It perform comparative text mining allow us
00:17:27,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,extract common topics shared by multiple collections.,00:17:24,8,extract common topics shared multiple collections
00:17:29,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,And there are variations in each collection.,00:17:27,8,And variations collection
00:17:35,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The second one is the main paper about the CPLSA model.,00:17:31,8,The second one main paper CPLSA model
00:17:38,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,Was a discussion of a lot of applications.,00:17:35,8,Was discussion lot applications
00:17:44,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,The third one has a lot of details about the special temporal patterns for,00:17:38,8,The third one lot details special temporal patterns
00:17:47,5,Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis,4.8,the Hurricane Katrina example.,00:17:44,8,Hurricane Katrina example
00:00:08,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,[SOUND] This,00:00:00,8,SOUND This
00:00:10,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,lecture is about mixture model estimation.,00:00:08,8,lecture mixture model estimation
00:00:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,In this lecture we're going to continue discussing probabilistic topic models.,00:00:12,8,In lecture going continue discussing probabilistic topic models
00:00:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"In particular,",00:00:16,8,In particular
00:00:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we're going to talk about how to estimate the parameters of a mixture model.,00:00:16,8,going talk estimate parameters mixture model
00:00:26,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So let's first look at our motivation for using a mixture model.,00:00:23,8,So let first look motivation using mixture model
00:00:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And we hope to factor out the background words.,00:00:26,8,And hope factor background words
00:00:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,From the top-words equation.,00:00:30,8,From top words equation
00:00:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,The idea is to assume that the text data actually contained two kinds of words.,00:00:33,8,The idea assume text data actually contained two kinds words
00:00:44,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,One kind is from the background here.,00:00:39,8,One kind background
00:00:48,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So, the is, we, etc.",00:00:44,8,So etc
00:00:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And the other kind is from our pop board distribution that we are interested in.,00:00:48,8,And kind pop board distribution interested
00:01:01,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So in order to solve this problem of factoring out background words,",00:00:56,8,So order solve problem factoring background words
00:01:05,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we can set up our mixture model as false.,00:01:01,8,set mixture model false
00:01:09,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,We're going to assume that we already know the parameters of,00:01:05,8,We going assume already know parameters
00:01:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"all the values for all the parameters in the mixture model,",00:01:09,8,values parameters mixture model
00:01:19,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,except for the water distribution of which is our target.,00:01:14,8,except water distribution target
00:01:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So this is a case of customizing a probabilist model so,00:01:20,8,So case customizing probabilist model
00:01:29,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,that we embedded a known variable that we are interested in.,00:01:25,8,embedded known variable interested
00:01:31,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,But we're going to simplify other things.,00:01:29,8,But going simplify things
00:01:34,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,We're going to assume we have knowledge above others.,00:01:31,8,We going assume knowledge others
00:01:37,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And this is a powerful way of customizing a model.,00:01:34,8,And powerful way customizing model
00:01:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,For a particular need.,00:01:37,8,For particular need
00:01:40,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Now you can imagine,",00:01:39,8,Now imagine
00:01:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we could have assumed that we also don't know the background words.,00:01:40,8,could assumed also know background words
00:01:46,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"But in this case,",00:01:45,8,But case
00:01:51,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,our goal is to factor out precisely those high probability background words.,00:01:46,8,goal factor precisely high probability background words
00:01:55,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So we assume the background model is already fixed.,00:01:51,8,So assume background model already fixed
00:02:01,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And one problem here is how can we adjust theta sub d,00:01:56,8,And one problem adjust theta sub
00:02:06,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,in order to maximize the probability of the observed document here and,00:02:01,8,order maximize probability observed document
00:02:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we assume all the other perimeters are now.,00:02:06,8,assume perimeters
00:02:12,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Now although we designed the model holistically.,00:02:09,8,Now although designed model holistically
00:02:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,To try to factor out these background words.,00:02:12,8,To try factor background words
00:02:21,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"It's unclear whether, if we use maximum write or estimator.",00:02:16,8,It unclear whether use maximum write estimator
00:02:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,We will actually end up having a whole distribution where the Common,00:02:21,8,We actually end whole distribution Common
00:02:29,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,words like the would indeed have smaller probabilities than before.,00:02:25,8,words like would indeed smaller probabilities
00:02:36,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Now in this case it turns out the answer is yes.,00:02:31,8,Now case turns answer yes
00:02:41,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"And when we set up the probability in this way,",00:02:36,8,And set probability way
00:02:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,when we use maximum likelihood or we will end up having a word distribution,00:02:41,8,use maximum likelihood end word distribution
00:02:49,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,where the use common words would be factored out.,00:02:46,8,use common words would factored
00:02:52,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,By the use of the background rule of distribution.,00:02:49,8,By use background rule distribution
00:02:56,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So to understand why this is so,",00:02:53,8,So understand
00:03:00,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,it's useful to examine the behavior of a mixture model.,00:02:56,8,useful examine behavior mixture model
00:03:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So we're going to look at a very very simple case.,00:03:00,8,So going look simple case
00:03:08,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,In order to understand some interesting behaviors of a mixture model.,00:03:03,8,In order understand interesting behaviors mixture model
00:03:15,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,The observed pattern here actually are generalizable to mixture model in general.,00:03:08,8,The observed pattern actually generalizable mixture model general
00:03:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,But it's much easier to understand this behavior,00:03:15,8,But much easier understand behavior
00:03:21,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,when we use A very simple case like what we are seeing here.,00:03:17,8,use A simple case like seeing
00:03:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So specifically in this case, let's assume that",00:03:21,8,So specifically case let assume
00:03:29,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,the probability choosing each of the two models is exactly the same.,00:03:25,8,probability choosing two models exactly
00:03:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So we're going to flip a fair coin to decide which model to use.,00:03:29,8,So going flip fair coin decide model use
00:03:36,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Furthermore, we're going to assume there are.",00:03:34,8,Furthermore going assume
00:03:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Precisely two words, the and text.",00:03:36,8,Precisely two words text
00:03:46,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Obviously this is a very naive oversimplification of the actual text,",00:03:39,8,Obviously naive oversimplification actual text
00:03:52,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"but again, it's useful to examine the behavior in such a special case.",00:03:46,8,useful examine behavior special case
00:03:58,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So we further assume that the background model gives probability of,00:03:53,8,So assume background model gives probability
00:04:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,0.9 towards the end text 0.1.,00:03:58,8,0 9 towards end text 0 1
00:04:08,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Now, lets also assume that our data is extremely simple the document has just",00:04:03,8,Now lets also assume data extremely simple document
00:04:13,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,two words text and the so now lets right down the likeable function in such a case.,00:04:08,8,two words text lets right likeable function case
00:04:18,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"First, what's the probability of text, and what's the probably of the.",00:04:13,8,First probability text probably
00:04:22,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,I hope by this point you'll be able to write it down.,00:04:19,8,I hope point able write
00:04:28,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So the probability of text is basically the sum over two cases,",00:04:23,8,So probability text basically sum two cases
00:04:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,where each case corresponds with to each of the order distribution,00:04:28,8,case corresponds order distribution
00:04:38,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,and it accounts for the two ways of generating text.,00:04:34,8,accounts two ways generating text
00:04:43,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"And inside each case, we have the probability of choosing the model,",00:04:39,8,And inside case probability choosing model
00:04:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,which is 0.5 multiplied by the probability of observing text from that model.,00:04:43,8,0 5 multiplied probability observing text model
00:04:54,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Similarly, the, would have a probability of the same form,",00:04:50,8,Similarly would probability form
00:04:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,just what is different is the exact probabilities.,00:04:54,8,different exact probabilities
00:05:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So naturally our lateral function is just a product of the two.,00:04:58,8,So naturally lateral function product two
00:05:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So It's very easy to see that,",00:05:03,8,So It easy see
00:05:11,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,once you understand what's the probability of each word.,00:05:08,8,understand probability word
00:05:15,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Which is also why it's so important to understand what's exactly,00:05:11,8,Which also important understand exactly
00:05:19,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,the probability of observing each word from such a mixture model.,00:05:15,8,probability observing word mixture model
00:05:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Now, the interesting question now is, how can we then optimize this likelihood?",00:05:19,8,Now interesting question optimize likelihood
00:05:29,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Well, you will note that there are only two variables.",00:05:25,8,Well note two variables
00:05:32,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,They are precisely the two probabilities of the two words.,00:05:29,8,They precisely two probabilities two words
00:05:35,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Text [INAUDIBLE] given by theta sub d.,00:05:32,8,Text INAUDIBLE given theta sub
00:05:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And this is because we have assumed that all the other parameters are known.,00:05:35,8,And assumed parameters known
00:05:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So, now the question is a very simple algebra question.",00:05:41,8,So question simple algebra question
00:05:48,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So, we have a simple expression with two variables and",00:05:45,8,So simple expression two variables
00:05:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we hope to choose the values of these two variables to maximize this function.,00:05:48,8,hope choose values two variables maximize function
00:05:58,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And the exercises that we have seen some simple algebra problems.,00:05:54,8,And exercises seen simple algebra problems
00:06:04,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Note that the two probabilities must sum to one, so there's some constraint.",00:06:00,8,Note two probabilities must sum one constraint
00:06:08,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"If there were no constraint of course,",00:06:06,8,If constraint course
00:06:12,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"we would set both probabilities to their maximum value which would be one,",00:06:08,8,would set probabilities maximum value would one
00:06:18,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"to maximize, But we can't do that because text then the must sum to one.",00:06:12,8,maximize But text must sum one
00:06:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,We can't give both a probability of one.,00:06:18,8,We give probability one
00:06:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So, now the question is how should we allocate the probability and",00:06:21,8,So question allocate probability
00:06:27,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,the math between the two words.,00:06:25,8,math two words
00:06:28,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,What do you think?,00:06:27,8,What think
00:06:32,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Now, it would be useful to look at this formula For a moment, and",00:06:28,8,Now would useful look formula For moment
00:06:36,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"to see what, intuitively, what we do in order to",00:06:32,8,see intuitively order
00:06:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,do set these probabilities to maximize the value of this function.,00:06:36,8,set probabilities maximize value function
00:06:44,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Okay, if we look into this further,",00:06:42,8,Okay look
00:06:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,then we see some interesting behavior of The two component models in that,00:06:44,8,see interesting behavior The two component models
00:06:54,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,they will be collaborating to maximize the probability of the observed data.,00:06:50,8,collaborating maximize probability observed data
00:06:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Which is dictated by the maximum likelihood estimator.,00:06:54,8,Which dictated maximum likelihood estimator
00:07:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"But they are also competing in some way, and in particular,",00:06:57,8,But also competing way particular
00:07:05,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,they would be competing on the words.,00:07:02,8,would competing words
00:07:09,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And they would tend to back high probabilities on different words,00:07:05,8,And would tend back high probabilities different words
00:07:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,to avoid this competition in some sense or to gain advantages in this competition.,00:07:09,8,avoid competition sense gain advantages competition
00:07:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So again, looking at this objective function and",00:07:14,8,So looking objective function
00:07:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,we have a constraint on the two probabilities.,00:07:16,8,constraint two probabilities
00:07:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Now, if you look at the formula intuitively,",00:07:21,8,Now look formula intuitively
00:07:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,you might feel that you want to set the probability of text to be somewhat larger.,00:07:25,8,might feel want set probability text somewhat larger
00:07:38,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"And this inducing can be work supported by mathematical fact, which is when",00:07:32,8,And inducing work supported mathematical fact
00:07:43,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,the sum of two variables is a constant then the product of them,00:07:38,8,sum two variables constant product
00:07:49,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"which is maximum when they are equal, and this is a fact we know from algebra.",00:07:43,8,maximum equal fact know algebra
00:07:52,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Now if we plug that [INAUDIBLE] It would mean that we have to make the two,00:07:49,8,Now plug INAUDIBLE It would mean make two
00:07:55,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,probabilities equal.,00:07:52,8,probabilities equal
00:07:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And when we make them equal and,00:07:56,8,And make equal
00:08:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"then if we consider the constraint it will be easy to solve this problem, and",00:07:57,8,consider constraint easy solve problem
00:08:09,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,the solution is the probability of tax will be .09 and probability is .01.,00:08:02,8,solution probability tax 09 probability 01
00:08:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"The probability of text is now much larger than probability of the, and",00:08:09,8,The probability text much larger probability
00:08:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,this is not the case when have just one distribution.,00:08:14,8,case one distribution
00:08:21,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"And this is clearly because of the use of the background model,",00:08:17,8,And clearly use background model
00:08:26,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,which assigned the very high probability to the and low probability to text.,00:08:21,8,assigned high probability low probability text
00:08:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And if you look at the equation you will see obviously,00:08:26,8,And look equation see obviously
00:08:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,some interaction of the two distributions here.,00:08:30,8,interaction two distributions
00:08:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"In particular, you will see in order to make them equal.",00:08:35,8,In particular see order make equal
00:08:46,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And then the probability assigned by theta sub d must be higher for,00:08:39,8,And probability assigned theta sub must higher
00:08:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,a word that has a smaller probability given by the background.,00:08:46,8,word smaller probability given background
00:08:56,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,This is obvious from examining this equation.,00:08:53,8,This obvious examining equation
00:08:59,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Because the background part is weak for text.,00:08:56,8,Because background part weak text
00:09:00,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,It's small.,00:08:59,8,It small
00:09:04,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"So in order to compensate for that, we must make the probability for",00:09:00,8,So order compensate must make probability
00:09:11,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"text given by theta sub D somewhat larger, so that the two sides can be balanced.",00:09:04,8,text given theta sub D somewhat larger two sides balanced
00:09:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,So this is in fact a very general behavior of this model.,00:09:11,8,So fact general behavior model
00:09:21,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"And that is, if one distribution assigns a high probability to one word than another,",00:09:17,8,And one distribution assigns high probability one word another
00:09:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,then the other distribution would tend to do the opposite.,00:09:21,8,distribution would tend opposite
00:09:28,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Basically it would discourage other distributions to do the same And,00:09:25,8,Basically would discourage distributions And
00:09:34,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,this is to balance them out so we can account for all kinds of words.,00:09:28,8,balance account kinds words
00:09:38,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,And this also means that by using a background model that is fixed into,00:09:34,8,And also means using background model fixed
00:09:42,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,assigned high probabilities through background words.,00:09:38,8,assigned high probabilities background words
00:09:47,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,We can indeed encourages the unknown topical one of this to assign smaller,00:09:42,8,We indeed encourages unknown topical one assign smaller
00:09:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,probabilities for such common words.,00:09:47,8,probabilities common words
00:09:54,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,"Instead put more probability than this on the content words,",00:09:50,8,Instead put probability content words
00:09:58,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,that cannot be explained well by the background model.,00:09:54,8,cannot explained well background model
00:10:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,Meaning that they have a very small probability from the background motor like,00:09:58,8,Meaning small probability background motor like
00:10:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 1,2.8,text here.,00:10:02,8,text
00:00:06,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,[MUSIC],00:00:01,4,MUSIC
00:00:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,This lecture is about the Latent Aspect Rating Analysis for Opinion Mining and,00:00:06,4,This lecture Latent Aspect Rating Analysis Opinion Mining
00:00:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Sentiment Analysis.,00:00:11,4,Sentiment Analysis
00:00:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In this lecture,",00:00:14,4,In lecture
00:00:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we're going to continue discussing Opinion Mining and Sentiment Analysis.,00:00:15,4,going continue discussing Opinion Mining Sentiment Analysis
00:00:25,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In particular, we're going to introduce Latent Aspect Rating Analysis",00:00:19,4,In particular going introduce Latent Aspect Rating Analysis
00:00:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,which allows us to perform detailed analysis of reviews with overall ratings.,00:00:25,4,allows us perform detailed analysis reviews overall ratings
00:00:35,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, first is motivation.",00:00:34,4,So first motivation
00:00:42,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Here are two reviews that you often see in the net about the hotel.,00:00:37,4,Here two reviews often see net hotel
00:00:44,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And you see some overall ratings.,00:00:42,4,And see overall ratings
00:00:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In this case, both reviewers have given five stars.",00:00:44,4,In case reviewers given five stars
00:00:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And, of course, there are also reviews that are in text.",00:00:49,4,And course also reviews text
00:00:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, if you just look at these reviews,",00:00:53,4,Now look reviews
00:01:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,it's not very clear whether the hotel is good for its location or for its service.,00:00:56,4,clear whether hotel good location service
00:01:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,It's also unclear why a reviewer liked this hotel.,00:01:01,4,It also unclear reviewer liked hotel
00:01:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,What we want to do is to decompose this overall rating into,00:01:06,4,What want decompose overall rating
00:01:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"ratings on different aspects such as value, rooms, location, and service.",00:01:11,4,ratings different aspects value rooms location service
00:01:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, if we can decompose the overall ratings,",00:01:18,4,So decompose overall ratings
00:01:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"the ratings on these different aspects, then, we",00:01:20,4,ratings different aspects
00:01:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,can obtain a more detailed understanding of the reviewer's opinionsabout the hotel.,00:01:24,4,obtain detailed understanding reviewer opinionsabout hotel
00:01:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And this would also allow us to rank hotels along different dimensions,00:01:30,4,And would also allow us rank hotels along different dimensions
00:01:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,such as value or rooms.,00:01:34,4,value rooms
00:01:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But, in general, such detailed understanding will reveal more information",00:01:36,4,But general detailed understanding reveal information
00:01:44,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"about the user's preferences, reviewer's preferences.",00:01:41,4,user preferences reviewer preferences
00:01:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And also, we can understand better how the reviewers view this",00:01:44,4,And also understand better reviewers view
00:01:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,hotel from different perspectives.,00:01:49,4,hotel different perspectives
00:01:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, not only do we want to infer these aspect ratings,",00:01:52,4,Now want infer aspect ratings
00:02:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we also want to infer the aspect weights.,00:01:59,4,also want infer aspect weights
00:02:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, some reviewers may care more about values as opposed to the service.",00:02:05,4,So reviewers may care values opposed service
00:02:10,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And that would be a case.,00:02:09,4,And would case
00:02:14,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"like what's shown on the left for the weight distribution,",00:02:10,4,like shown left weight distribution
00:02:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,where you can see a lot of weight is places on value.,00:02:14,4,see lot weight places value
00:02:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,But others care more for service.,00:02:18,4,But others care service
00:02:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And therefore, they might place more weight on service than value.",00:02:21,4,And therefore might place weight service value
00:02:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"The reason why this is also important is because,",00:02:25,4,The reason also important
00:02:32,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"do you think about a five star on value,",00:02:29,4,think five star value
00:02:38,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"it might still be very expensive if the reviewer cares a lot about service, right?",00:02:32,4,might still expensive reviewer cares lot service right
00:02:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"For this kind of service, this price is good, so",00:02:38,4,For kind service price good
00:02:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the reviewer might give it a five star.,00:02:41,4,reviewer might give five star
00:02:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But if a reviewer really cares about the value of the hotel,",00:02:45,4,But reviewer really cares value hotel
00:02:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"then the five star, most likely, would mean really cheap prices.",00:02:49,4,five star likely would mean really cheap prices
00:02:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, in order to interpret the ratings on different aspects accurately,",00:02:54,4,So order interpret ratings different aspects accurately
00:03:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we also need to know these aspect weights.,00:02:59,4,also need know aspect weights
00:03:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"When they're combined together,",00:03:02,4,When combined together
00:03:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we can have a more detailed understanding of the opinion.,00:03:04,4,detailed understanding opinion
00:03:14,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So the task here is to get these reviews and their overall ratings as input,",00:03:08,4,So task get reviews overall ratings input
00:03:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"and then, generate both the aspect ratings,",00:03:14,4,generate aspect ratings
00:03:22,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"the compose aspect ratings, and the aspect rates as output.",00:03:17,4,compose aspect ratings aspect rates output
00:03:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And this is a problem called Latent Aspect Rating Analysis.,00:03:22,4,And problem called Latent Aspect Rating Analysis
00:03:35,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So the task, in general, is given a set of review articles about",00:03:31,4,So task general given set review articles
00:03:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"the topic with overall ratings, and we hope to generate three things.",00:03:35,4,topic overall ratings hope generate three things
00:03:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,One is the major aspects commented on in the reviews.,00:03:41,4,One major aspects commented reviews
00:03:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Second is ratings on each aspect, such as value and room service.",00:03:46,4,Second ratings aspect value room service
00:03:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And third is the relative weights placed on different aspects by the reviewers.,00:03:53,4,And third relative weights placed different aspects reviewers
00:04:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And this task has a lot of applications, and if you can do this,",00:03:59,4,And task lot applications
00:04:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,and it will enable a lot of applications.,00:04:02,4,enable lot applications
00:04:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,I just listed some here.,00:04:05,4,I listed
00:04:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And later, I will show you some results.",00:04:07,4,And later I show results
00:04:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And, for example, we can do opinion based entity ranking.",00:04:08,4,And example opinion based entity ranking
00:04:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,We can generate an aspect-level opinion summary.,00:04:13,4,We generate aspect level opinion summary
00:04:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"We can also analyze reviewers preferences, compare them or",00:04:17,4,We also analyze reviewers preferences compare
00:04:25,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,compare their preferences on different hotels.,00:04:21,4,compare preferences different hotels
00:04:27,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And we can do personalized recommendations of products.,00:04:25,4,And personalized recommendations products
00:04:32,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, of course, the question is how can we solve this problem?",00:04:29,4,So course question solve problem
00:04:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, as in other cases of these advanced topics,",00:04:32,4,Now cases advanced topics
00:04:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we wont have time to really cover the technique in detail.,00:04:36,4,won time really cover technique detail
00:04:42,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But Im going to give you a brisk,",00:04:41,4,But I going give brisk
00:04:47,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,basic introduction to the technique development for this problem.,00:04:42,4,basic introduction technique development problem
00:04:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, first step, were going to talk about how to solve the problem in two stages.",00:04:47,4,So first step we going talk solve problem two stages
00:04:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Later, were going to also mention that we can do this in the unified model.",00:04:51,4,Later we going also mention unified model
00:05:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, take this review with the overall rating as input.",00:04:56,4,Now take review overall rating input
00:05:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"What we want to do is, first, we're going to segment the aspects.",00:05:00,4,What want first going segment aspects
00:05:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So we're going to pick out what words are talking about location, and",00:05:05,4,So going pick words talking location
00:05:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"what words are talking about room condition, etc.",00:05:09,4,words talking room condition etc
00:05:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So with this, we would be able to obtain aspect segments.",00:05:13,4,So would able obtain aspect segments
00:05:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In particular, we're going to obtain the counts of all the words",00:05:18,4,In particular going obtain counts words
00:05:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"in each segment, and this is denoted by C sub I of W and D.",00:05:23,4,segment denoted C sub I W D
00:05:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Now this can be done by using seed words like location and room or,00:05:28,4,Now done using seed words like location room
00:05:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,price to retrieve the [INAUDIBLE] in the segments.,00:05:33,4,price retrieve INAUDIBLE segments
00:05:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And then, from those segments, we can further mine correlated",00:05:36,4,And segments mine correlated
00:05:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,words with these seed words and that would allow us to segmented,00:05:41,4,words seed words would allow us segmented
00:05:51,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"the text into segments, discussing different aspects.",00:05:46,4,text segments discussing different aspects
00:05:52,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But, of course,",00:05:51,4,But course
00:05:57,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"later, as we will see, we can also use [INAUDIBLE] models to do the segmentation.",00:05:52,4,later see also use INAUDIBLE models segmentation
00:05:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But anyway, that's the first stage,",00:05:57,4,But anyway first stage
00:06:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,where the obtain the council of words in each segment.,00:05:59,4,obtain council words segment
00:06:06,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In the second stage,",00:06:05,4,In second stage
00:06:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"which is called Latent Rating Regression, we're going to use these words and",00:06:06,4,called Latent Rating Regression going use words
00:06:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,their frequencies in different aspects to predict the overall rate.,00:06:11,4,frequencies different aspects predict overall rate
00:06:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And this predicting happens in two stages.,00:06:15,4,And predicting happens two stages
00:06:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In the first stage, we're going to use the [INAUDIBLE] and",00:06:17,4,In first stage going use INAUDIBLE
00:06:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the weights of these words in each aspect to predict the aspect rating.,00:06:21,4,weights words aspect predict aspect rating
00:06:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, for example, if in your discussion of location, you see a word like,",00:06:26,4,So example discussion location see word like
00:06:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"amazing, mentioned many times, and it has a high weight.",00:06:31,4,amazing mentioned many times high weight
00:06:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"For example, here, 3.9.",00:06:36,4,For example 3 9
00:06:40,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Then, it will increase the Aspect Rating for location.",00:06:37,4,Then increase Aspect Rating location
00:06:44,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But, another word like, far, which is an acted weight,",00:06:40,4,But another word like far acted weight
00:06:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"if it's mentioned many times, and it will decrease the rating.",00:06:44,4,mentioned many times decrease rating
00:06:53,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So the aspect ratings, assume that it will be a weighted combination of these",00:06:49,4,So aspect ratings assume weighted combination
00:06:58,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,word frequencies where the weights are the sentiment weights of the words.,00:06:53,4,word frequencies weights sentiment weights words
00:07:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Of course, these sentimental weights might be different for different aspects.",00:06:58,4,Of course sentimental weights might different different aspects
00:07:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So we have, for each aspect, a set of term sentiment weights as shown here.",00:07:05,4,So aspect set term sentiment weights shown
00:07:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And that's in order by beta sub I and W.,00:07:11,4,And order beta sub I W
00:07:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In the second stage or second step, we're going to assume that the overall",00:07:18,4,In second stage second step going assume overall
00:07:27,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,rating is simply a weighted combination of these aspect ratings.,00:07:23,4,rating simply weighted combination aspect ratings
00:07:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So we're going to assume we have aspect weights to the [INAUDIBLE] sub i of d,",00:07:27,4,So going assume aspect weights INAUDIBLE sub
00:07:38,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"and this will be used to take a weighted average of the aspect ratings,",00:07:33,4,used take weighted average aspect ratings
00:07:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,which are denoted by r sub i of d.,00:07:38,4,denoted r sub
00:07:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And we're going to assume the overall rating is simply a weighted,00:07:42,4,And going assume overall rating simply weighted
00:07:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,average of these aspect ratings.,00:07:45,4,average aspect ratings
00:07:53,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,So this set up allows us to predict the overall rating based on,00:07:48,4,So set allows us predict overall rating based
00:07:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the observable frequencies.,00:07:53,4,observable frequencies
00:07:57,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So on the left side,",00:07:56,4,So left side
00:08:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"you will see all these observed information, the r sub d and the count.",00:07:57,4,see observed information r sub count
00:08:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But on the right side,",00:08:03,4,But right side
00:08:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,you see all the information in that range is actually latent.,00:08:04,4,see information range actually latent
00:08:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, we hope to discover that.",00:08:09,4,So hope discover
00:08:17,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, this is a typical case of a generating model where would embed",00:08:12,4,Now typical case generating model would embed
00:08:21,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the interesting variables in the generated model.,00:08:17,4,interesting variables generated model
00:08:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And then, we're going to set up a generation probability for",00:08:21,4,And going set generation probability
00:08:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the overall rating given the observed words.,00:08:26,4,overall rating given observed words
00:08:38,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And then, of course, we can adjust these parameter values including betas Rs and",00:08:31,4,And course adjust parameter values including betas Rs
00:08:44,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,alpha Is in order to maximize the probability of the data.,00:08:38,4,alpha Is order maximize probability data
00:08:49,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In this case, the conditional probability of the observed rating given the document.",00:08:44,4,In case conditional probability observed rating given document
00:08:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So we have seen such cases before in, for",00:08:49,4,So seen cases
00:08:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"example, PISA, where we predict a text data.",00:08:54,4,example PISA predict text data
00:09:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But here, we're predicting the rating, and the parameters,",00:08:59,4,But predicting rating parameters
00:09:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"of course, are very different.",00:09:02,4,course different
00:09:09,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But we can see, if we can uncover these parameters, it would be nice,",00:09:05,4,But see uncover parameters would nice
00:09:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,because r sub i of d is precise as the ratings that we want to get.,00:09:09,4,r sub precise ratings want get
00:09:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And these are the composer ratings on different aspects.,00:09:13,4,And composer ratings different aspects
00:09:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,[INAUDIBLE] sub I D is precisely the aspect weights that we,00:09:16,4,INAUDIBLE sub I D precisely aspect weights
00:09:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"hope to get as a byproduct, that we also get the beta factor, and",00:09:20,4,hope get byproduct also get beta factor
00:09:29,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"these are the [INAUDIBLE] factor, the sentiment weights of words.",00:09:24,4,INAUDIBLE factor sentiment weights words
00:09:32,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So more formally,",00:09:31,4,So formally
00:09:38,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the data we are modeling here is a set of review documents with overall ratings.,00:09:33,4,data modeling set review documents overall ratings
00:09:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And each review document denote by a d, and the overall ratings denote by r sub d.",00:09:38,4,And review document denote overall ratings denote r sub
00:09:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And d pre-segments turn into k aspect segments.,00:09:45,4,And pre segments turn k aspect segments
00:09:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And we're going to use ci(w,d) to denote the count of word w in aspect segment i.",00:09:48,4,And going use ci w denote count word w aspect segment
00:09:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Of course, it's zero if the word doesn't occur in the segment.",00:09:55,4,Of course zero word occur segment
00:10:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, the model is going to predict the rating based on d.",00:10:01,4,Now model going predict rating based
00:10:10,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, we're interested in the provisional problem of r sub-d given d.",00:10:04,4,So interested provisional problem r sub given
00:10:13,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And this model is set up as follows.,00:10:10,4,And model set follows
00:10:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,So r sub-d is assumed the two follow a normal distribution,00:10:13,4,So r sub assumed two follow normal distribution
00:10:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,doesn't mean that denotes actually await the average,00:10:18,4,mean denotes actually await average
00:10:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,of the aspect of ratings r Sub I of d as shown here.,00:10:23,4,aspect ratings r Sub I shown
00:10:30,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,This normal distribution is a variance of data squared.,00:10:28,4,This normal distribution variance data squared
00:10:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, of course, this is just our assumption.",00:10:30,4,Now course assumption
00:10:37,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,The actual rating is not necessarily anything thing this way.,00:10:34,4,The actual rating necessarily anything thing way
00:10:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But as always, when we make this assumption, we have a formal way to",00:10:37,4,But always make assumption formal way
00:10:45,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,model the problem and that allows us to compute the interest in quantities.,00:10:41,4,model problem allows us compute interest quantities
00:10:50,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In this case, the aspect ratings and the aspect weights.",00:10:45,4,In case aspect ratings aspect weights
00:10:56,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, the aspect rating as you see on the [INAUDIBLE]",00:10:52,4,Now aspect rating see INAUDIBLE
00:11:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,is assuming that will be a weight of sum of these weights.,00:10:56,4,assuming weight sum weights
00:11:03,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Where the weight is just the [INAUDIBLE] of the weight.,00:11:01,4,Where weight INAUDIBLE weight
00:11:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So as I said,",00:11:04,4,So I said
00:11:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,the overall rating is assumed to be a weighted average of aspect ratings.,00:11:08,4,overall rating assumed weighted average aspect ratings
00:11:20,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, these other values, r for sub I of D, or denoted together",00:11:15,4,Now values r sub I D denoted together
00:11:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,by other vector that depends on D is that the token of specific weights.,00:11:20,4,vector depends D token specific weights
00:11:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And were going to assume that this vector itself is drawn,00:11:26,4,And we going assume vector drawn
00:11:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"from another Multivariate Gaussian distribution,",00:11:31,4,another Multivariate Gaussian distribution
00:11:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"with mean denoted by a Mu factor, and covariance metrics sigma here.",00:11:36,4,mean denoted Mu factor covariance metrics sigma
00:11:48,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, so this means, when we generate our overall rating, we're going to first draw",00:11:43,4,Now means generate overall rating going first draw
00:11:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,a set of other values from this Multivariate Gaussian Prior distribution.,00:11:49,4,set values Multivariate Gaussian Prior distribution
00:12:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And once we get these other values, we're going to use then the weighted",00:11:54,4,And get values going use weighted
00:12:05,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,average of aspect ratings as the mean here to use the normal,00:12:00,4,average aspect ratings mean use normal
00:12:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,distribution to generate the overall rating.,00:12:05,4,distribution generate overall rating
00:12:18,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, the aspect rating, as I just said, is the sum of the sentiment weights of",00:12:13,4,Now aspect rating I said sum sentiment weights
00:12:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"words in aspect, note that here the sentiment weights are specific to aspect.",00:12:18,4,words aspect note sentiment weights specific aspect
00:12:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, beta is indexed by i, and that's for aspect.",00:12:24,4,So beta indexed aspect
00:12:33,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,And that gives us a way to model different segment of a word.,00:12:28,4,And gives us way model different segment word
00:12:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,This is neither because the same word might have,00:12:36,4,This neither word might
00:12:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,positive sentiment for another aspect.,00:12:41,4,positive sentiment another aspect
00:12:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,It's also used for see what parameters we have here beta sub i and,00:12:46,4,It also used see parameters beta sub
00:12:59,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,w gives us the aspect-specific sentiment of w.,00:12:54,4,w gives us aspect specific sentiment w
00:13:04,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, obviously, that's one of the important parameters.",00:12:59,4,So obviously one important parameters
00:13:08,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"But, in general, we can see we have these parameters, beta values, the delta,",00:13:04,4,But general see parameters beta values delta
00:13:11,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"and the Mu, and sigma.",00:13:08,4,Mu sigma
00:13:16,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, next, the question is, how can we estimate these parameters and, so",00:13:12,4,So next question estimate parameters
00:13:19,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,we collectively denote all the parameters by lambda here.,00:13:16,4,collectively denote parameters lambda
00:13:24,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Now, we can, as usual, use the maximum likelihood estimate, and",00:13:19,4,Now usual use maximum likelihood estimate
00:13:28,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"this will give us the settings of these parameters,",00:13:24,4,give us settings parameters
00:13:34,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,that with a maximized observed ratings condition of their respective reviews.,00:13:28,4,maximized observed ratings condition respective reviews
00:13:39,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And of, course, this would then give us all the useful",00:13:34,4,And course would give us useful
00:13:43,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,variables that we are interested in computing.,00:13:39,4,variables interested computing
00:13:50,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, more specifically, we can now, once we estimate the parameters,",00:13:45,4,So specifically estimate parameters
00:13:55,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"we can easily compute the aspect rating, for aspect the i or sub i of d.",00:13:50,4,easily compute aspect rating aspect sub
00:14:00,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And that's simply to take all of the words that occurred in the segment, i,",00:13:55,4,And simply take words occurred segment
00:14:02,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,and then take their counts and,00:14:00,4,take counts
00:14:07,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,then multiply that by the center of the weight of each word and take a sum.,00:14:02,4,multiply center weight word take sum
00:14:12,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So, of course, this time would be zero for words that are not occurring in and",00:14:07,4,So course time would zero words occurring
00:14:15,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,that's why were going to take the sum of all the words in the vocabulary.,00:14:12,4,going take sum words vocabulary
00:14:19,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Now what about the s factor weights?,00:14:17,4,Now factor weights
00:14:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Alpha sub i of d, well, it's not part of our parameter.",00:14:19,4,Alpha sub well part parameter
00:14:23,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,Right?,00:14:23,4,Right
00:14:26,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,So we have to use that to compute it.,00:14:23,4,So use compute
00:14:31,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"And in this case, we can use the Maximum",00:14:26,4,And case use Maximum
00:14:36,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,a Posteriori to compute this alpha value.,00:14:31,4,Posteriori compute alpha value
00:14:41,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"Basically, we're going to maximize the product of the prior of alpha according",00:14:36,4,Basically going maximize product prior alpha according
00:14:46,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,to our assumed Multivariate Gaussian Distribution and the likelihood.,00:14:41,4,assumed Multivariate Gaussian Distribution likelihood
00:14:50,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"In this case, the likelihood rate is the probability of",00:14:46,4,In case likelihood rate probability
00:14:54,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,generating this observed overall rating given this particular alpha value and,00:14:50,4,generating observed overall rating given particular alpha value
00:14:57,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"some other parameters, as you see here.",00:14:54,4,parameters see
00:15:01,5,Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1,4.4,"So for more details about this model, you can read this paper cited here.",00:14:57,4,So details model read paper cited
00:00:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,[SOUND] >> This,00:00:00,4,SOUND This
00:00:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"lecture is about the Overview of Statistical Language Models,",00:00:07,4,lecture Overview Statistical Language Models
00:00:12,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,which cover proper models as special cases.,00:00:09,4,cover proper models special cases
00:00:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,In this lecture we're going to give,00:00:12,4,In lecture going give
00:00:21,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,a overview of Statical Language Models.,00:00:15,4,overview Statical Language Models
00:00:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,These models are general models that cover,00:00:21,4,These models general models cover
00:00:28,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,probabilistic topic models as a special cases.,00:00:24,4,probabilistic topic models special cases
00:00:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So first off, what is a Statistical Language Model?",00:00:28,4,So first Statistical Language Model
00:00:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,A Statistical Language Model is basically a probability distribution,00:00:31,4,A Statistical Language Model basically probability distribution
00:00:37,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,over word sequences.,00:00:36,4,word sequences
00:00:41,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, for example, we might have a distribution that gives,",00:00:37,4,So example might distribution gives
00:00:44,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,today is Wednesday a probability of .001.,00:00:41,4,today Wednesday probability 001
00:00:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"It might give today Wednesday is, which",00:00:44,4,It might give today Wednesday
00:00:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"is a non-grammatical sentence, a very, very small probability as shown here.",00:00:49,4,non grammatical sentence small probability shown
00:00:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And similarly another sentence,",00:00:54,4,And similarly another sentence
00:01:01,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,the eigenvalue is positive might get the probability of .00001.,00:00:56,4,eigenvalue positive might get probability 00001
00:01:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,So as you can see such a distribution clearly is Context Dependent.,00:01:01,4,So see distribution clearly Context Dependent
00:01:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,It depends on the Context of Discussion.,00:01:06,4,It depends Context Discussion
00:01:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Some Word Sequences might have higher probabilities than others but the same,00:01:09,4,Some Word Sequences might higher probabilities others
00:01:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Sequence of Words might have different probability in different context.,00:01:15,4,Sequence Words might different probability different context
00:01:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And so this suggests that such a distribution can actually categorize topic,00:01:20,4,And suggests distribution actually categorize topic
00:01:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,such a model can also be regarded as Probabilistic Mechanism for,00:01:26,4,model also regarded Probabilistic Mechanism
00:01:32,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,generating text.,00:01:31,4,generating text
00:01:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And that just means we can view text data as data observed from such a model.,00:01:33,4,And means view text data data observed model
00:01:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"For this reason, we call such a model as Generating Model.",00:01:42,4,For reason call model Generating Model
00:01:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, now given a model we can then assemble sequences of words.",00:01:49,4,So given model assemble sequences words
00:01:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, for example, based on the distribution that I have shown here on this slide,",00:01:54,4,So example based distribution I shown slide
00:02:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,when matter it say assemble a sequence like today is Wednesday,00:01:59,4,matter say assemble sequence like today Wednesday
00:02:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,because it has a relative high probability.,00:02:04,4,relative high probability
00:02:10,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,We might often get such a sequence.,00:02:07,4,We might often get sequence
00:02:14,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,We might also get the item value as positive sometimes,00:02:10,4,We might also get item value positive sometimes
00:02:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"with a smaller probability and very, very occasionally we might",00:02:14,4,smaller probability occasionally might
00:02:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,get today is Wednesday because it's probability is so small.,00:02:19,4,get today Wednesday probability small
00:02:28,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So in general, in order to categorize such a distribution we must specify probability",00:02:24,4,So general order categorize distribution must specify probability
00:02:33,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,values for all these different sequences of words.,00:02:28,4,values different sequences words
00:02:37,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Obviously, it's impossible to specify that because it's",00:02:33,4,Obviously impossible specify
00:02:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,impossible to enumerate all of the possible sequences of words.,00:02:37,4,impossible enumerate possible sequences words
00:02:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So in practice, we will have to simplify the model in some way.",00:02:42,4,So practice simplify model way
00:02:52,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, the simplest language model is called the Unigram Language Model.",00:02:49,4,So simplest language model called Unigram Language Model
00:02:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"In such a case, it was simply a the text",00:02:52,4,In case simply text
00:03:01,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,is generated by generating each word independently.,00:02:57,4,generated generating word independently
00:03:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"But in general, the words may not be generated independently.",00:03:02,4,But general words may generated independently
00:03:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"But after we make this assumption, we can significantly simplify the language more.",00:03:06,4,But make assumption significantly simplify language
00:03:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Basically, now the probability of a sequence of words, w1 through wn,",00:03:12,4,Basically probability sequence words w1 wn
00:03:21,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,will be just the product of the probability of each word.,00:03:16,4,product probability word
00:03:26,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So for such a model,",00:03:24,4,So model
00:03:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,we have as many parameters as the number of words in our vocabulary.,00:03:26,4,many parameters number words vocabulary
00:03:35,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So here we assume we have n words, so we have n probabilities.",00:03:30,4,So assume n words n probabilities
00:03:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,One for each word.,00:03:35,4,One word
00:03:38,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And then some to 1.,00:03:36,4,And 1
00:03:43,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, now we assume that our text is a sample",00:03:38,4,So assume text sample
00:03:46,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,drawn according to this word distribution.,00:03:43,4,drawn according word distribution
00:03:50,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"That just means, we're going to draw a word each time and",00:03:46,4,That means going draw word time
00:03:52,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,then eventually we'll get a text.,00:03:50,4,eventually get text
00:03:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So for example, now again,",00:03:53,4,So example
00:04:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,we can try to assemble words according to a distribution.,00:03:56,4,try assemble words according distribution
00:04:05,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,We might get Wednesday often or today often.,00:04:02,4,We might get Wednesday often today often
00:04:11,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And some other words like eigenvalue might have a small probability, etcetera.",00:04:06,4,And words like eigenvalue might small probability etcetera
00:04:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"But with this, we actually can also compute the probability of",00:04:11,4,But actually also compute probability
00:04:25,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"every sequence, even though our model only specify the probabilities of words.",00:04:19,4,every sequence even though model specify probabilities words
00:04:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And this is because of the independence.,00:04:25,4,And independence
00:04:32,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So specifically, we can compute the probability of today is Wednesday.",00:04:27,4,So specifically compute probability today Wednesday
00:04:37,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Because it's just a product of the probability of today,",00:04:34,4,Because product probability today
00:04:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"the probability of is, and probability of Wednesday.",00:04:37,4,probability probability Wednesday
00:04:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"For example, I show some fake numbers here and when you",00:04:42,4,For example I show fake numbers
00:04:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,multiply these numbers together you get the probability that today's Wednesday.,00:04:45,4,multiply numbers together get probability today Wednesday
00:04:55,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So as you can see, with N probabilities, one for each word, we actually",00:04:49,4,So see N probabilities one word actually
00:05:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,can characterize the probability situation over all kinds of sequences of words.,00:04:55,4,characterize probability situation kinds sequences words
00:05:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And so, this is a very simple model.",00:05:02,4,And simple model
00:05:07,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Ignore the word order.,00:05:06,4,Ignore word order
00:05:12,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So it may not be, in fact, in some problems, such as for speech recognition,",00:05:07,4,So may fact problems speech recognition
00:05:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,where you may care about the order of words.,00:05:12,4,may care order words
00:05:18,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,But it turns out to be quite sufficient for,00:05:15,4,But turns quite sufficient
00:05:20,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,many tasks that involve topic analysis.,00:05:18,4,many tasks involve topic analysis
00:05:24,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And that's also what we're interested in here.,00:05:20,4,And also interested
00:05:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So when we have a model, we generally have two problems that we can think about.",00:05:24,4,So model generally two problems think
00:05:38,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"One is, given a model, how likely are we to observe a certain kind of data points?",00:05:31,4,One given model likely observe certain kind data points
00:05:41,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"That is, we are interested in the Sampling Process.",00:05:38,4,That interested Sampling Process
00:05:44,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,The other is the Estimation Process.,00:05:41,4,The Estimation Process
00:05:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And that, is to think of the parameters of a model given,",00:05:44,4,And think parameters model given
00:05:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,some observe the data and we're going to talk about that in a moment.,00:05:49,4,observe data going talk moment
00:05:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Let's first talk about the sampling.,00:05:53,4,Let first talk sampling
00:06:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So, here I show two examples of Water Distributions or Unigram Language Models.",00:05:56,4,So I show two examples Water Distributions Unigram Language Models
00:06:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,The first one has higher probabilities for,00:06:02,4,The first one higher probabilities
00:06:08,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"words like a text mining association, it's separate.",00:06:04,4,words like text mining association separate
00:06:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Now this signals a topic about text mining because when we assemble words from,00:06:10,4,Now signals topic text mining assemble words
00:06:21,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"such a distribution, we tend to see words that often occur in text mining contest.",00:06:16,4,distribution tend see words often occur text mining contest
00:06:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So in this case, if we ask the question about",00:06:23,4,So case ask question
00:06:30,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,what is the probability of generating a particular document.,00:06:27,4,probability generating particular document
00:06:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Then, we likely will see text that looks like a text mining paper.",00:06:30,4,Then likely see text looks like text mining paper
00:06:42,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Of course, the text that we generate by drawing words.",00:06:36,4,Of course text generate drawing words
00:06:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,This distribution is unlikely coherent.,00:06:42,4,This distribution unlikely coherent
00:06:49,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Although, the probability of generating attacks mine",00:06:45,4,Although probability generating attacks mine
00:06:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,[INAUDIBLE] publishing in the top conference is,00:06:49,4,INAUDIBLE publishing top conference
00:06:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,non-zero assuming that no word has a zero probability in the distribution.,00:06:53,4,non zero assuming word zero probability distribution
00:07:02,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And that just means, we can essentially generate all kinds of",00:06:59,4,And means essentially generate kinds
00:07:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,text documents including very meaningful text documents.,00:07:02,4,text documents including meaningful text documents
00:07:09,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Now, the second distribution show,",00:07:07,4,Now second distribution show
00:07:14,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"on the bottom, has different than what was high probabilities.",00:07:09,4,bottom different high probabilities
00:07:17,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So food [INAUDIBLE] healthy [INAUDIBLE], etcetera.",00:07:14,4,So food INAUDIBLE healthy INAUDIBLE etcetera
00:07:20,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,So this clearly indicates a different topic.,00:07:17,4,So clearly indicates different topic
00:07:23,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,In this case it's probably about health.,00:07:20,4,In case probably health
00:07:26,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So if we sample a word from such a distribution,",00:07:23,4,So sample word distribution
00:07:31,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"then the probability of observing a text mining paper would be very, very small.",00:07:26,4,probability observing text mining paper would small
00:07:37,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"On the other hand, the probability of observing a text that looks like a food",00:07:32,4,On hand probability observing text looks like food
00:07:40,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"nutrition paper would be high, relatively higher.",00:07:37,4,nutrition paper would high relatively higher
00:07:48,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"So that just means, given a particular distribution, different than the text.",00:07:41,4,So means given particular distribution different text
00:07:51,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Now let's look at the estimation problem now.,00:07:48,4,Now let look estimation problem
00:07:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"In this case, we're going to assume that we have observed the data.",00:07:51,4,In case going assume observed data
00:07:57,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,I will know exactly what the text data looks like.,00:07:54,4,I know exactly text data looks like
00:07:59,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"In this case, let's assume we have a text mining paper.",00:07:57,4,In case let assume text mining paper
00:08:06,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"In fact, it's abstract of the paper, so the total number of words is 100.",00:07:59,4,In fact abstract paper total number words 100
00:08:10,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And I've shown some counts of individual words here.,00:08:06,4,And I shown counts individual words
00:08:16,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Now, if we ask the question, what is the most likely",00:08:12,4,Now ask question likely
00:08:22,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,Language Model that has been used to generate this text data?,00:08:17,4,Language Model used generate text data
00:08:26,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Assuming that the text is observed from some Language Model,",00:08:22,4,Assuming text observed Language Model
00:08:28,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,what's our best guess of this Language Model?,00:08:26,4,best guess Language Model
00:08:35,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Okay, so the problem now is just to estimate the probabilities of these words.",00:08:30,4,Okay problem estimate probabilities words
00:08:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,As I've shown here.,00:08:35,4,As I shown
00:08:38,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,So what do you think?,00:08:37,4,So think
00:08:39,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,What would be your guess?,00:08:38,4,What would guess
00:08:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Would you guess text has a very small probability, or",00:08:40,4,Would guess text small probability
00:08:47,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,a relatively large probability?,00:08:45,4,relatively large probability
00:08:50,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,What about query?,00:08:48,4,What query
00:08:53,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Well, your guess probably would be dependent on",00:08:50,4,Well guess probably would dependent
00:08:56,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"how many times we have observed this word in the text data, right?",00:08:53,4,many times observed word text data right
00:09:00,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And if you think about it for a moment.,00:08:56,4,And think moment
00:09:04,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And if you are like many others, you would have guessed that,",00:09:00,4,And like many others would guessed
00:09:10,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"well, text has a probability of 10 out of 100 because I've observed",00:09:04,4,well text probability 10 100 I observed
00:09:15,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,the text 10 times in the text that has a total of 100 words.,00:09:10,4,text 10 times text total 100 words
00:09:19,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And similarly, mining has 5 out of 100.",00:09:15,4,And similarly mining 5 100
00:09:25,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And query has a relatively small probability, just observed for once.",00:09:19,4,And query relatively small probability observed
00:09:27,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,So it's 1 out of 100.,00:09:25,4,So 1 100
00:09:32,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Right, so that, intuitively, is a reasonable guess.",00:09:27,4,Right intuitively reasonable guess
00:09:36,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"But the question is, is this our best guess or best estimate of the parameters?",00:09:32,4,But question best guess best estimate parameters
00:09:40,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Of course, in order to answer this question,",00:09:37,4,Of course order answer question
00:09:45,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"we have to define what do we mean by best, in this case,",00:09:40,4,define mean best case
00:09:50,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,it turns out that our guesses are indeed the best.,00:09:45,4,turns guesses indeed best
00:09:54,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,In some sense and this is called Maximum Likelihood Estimate.,00:09:50,4,In sense called Maximum Likelihood Estimate
00:10:00,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"And it's the best thing that, it will give the observer data our maximum probability.",00:09:54,4,And best thing give observer data maximum probability
00:10:05,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,"Meaning that, if you change the estimate somehow, even slightly,",00:10:01,4,Meaning change estimate somehow even slightly
00:10:10,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,then the probability of the observed text data will be somewhat smaller.,00:10:05,4,probability observed text data somewhat smaller
00:10:13,3,Probabilistic Topic Models- Overview of Statistical Language Models- Part 1,2.4,And this is called a Maximum Likelihood Estimate.,00:10:10,4,And called Maximum Likelihood Estimate
00:00:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,[SOUND],00:00:00,10,SOUND
00:00:10,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,This lecture is about using a time series,00:00:07,10,This lecture using time series
00:00:14,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,as context to potentially discover causal topics in text.,00:00:10,10,context potentially discover causal topics text
00:00:18,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"In this lecture, we're going to continue discussing Contextual Text Mining.",00:00:14,10,In lecture going continue discussing Contextual Text Mining
00:00:23,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"In particular, we're going to look at the time series as a context for",00:00:18,10,In particular going look time series context
00:00:27,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"analyzing text, to potentially discover causal topics.",00:00:23,10,analyzing text potentially discover causal topics
00:00:29,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"As usual, it started with the motivation.",00:00:27,10,As usual started motivation
00:00:34,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"In this case, we hope to use text mining to understand a time series.",00:00:29,10,In case hope use text mining understand time series
00:00:39,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Here, what you are seeing is Dow Jones Industrial Average stock price curves.",00:00:34,10,Here seeing Dow Jones Industrial Average stock price curves
00:00:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And you'll see a sudden drop here.,00:00:39,10,And see sudden drop
00:00:42,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Right.,00:00:41,10,Right
00:00:45,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So one would be interested knowing what might have caused the stock,00:00:42,10,So one would interested knowing might caused stock
00:00:46,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,market to crash.,00:00:45,10,market crash
00:00:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Well, if you know the background, and you might be able to figure it out if you",00:00:48,10,Well know background might able figure
00:00:56,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"look at the time stamp, or there are other data that can help us think about.",00:00:51,10,look time stamp data help us think
00:01:00,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But the question here is can we get some clues about this,00:00:56,10,But question get clues
00:01:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,from the companion news stream?,00:01:00,10,companion news stream
00:01:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And we have a lot of news data that generated during that period.,00:01:02,10,And lot news data generated period
00:01:12,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So if you do that we might actually discover the crash.,00:01:08,10,So might actually discover crash
00:01:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"After it happened, at the time of the September 11 attack.",00:01:12,10,After happened time September 11 attack
00:01:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that's the time when there is a sudden rise of the topic,00:01:16,10,And time sudden rise topic
00:01:23,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,about September 11 happened in news articles.,00:01:21,10,September 11 happened news articles
00:01:32,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Here's another scenario where we want to analyze the Presidential Election.,00:01:26,10,Here another scenario want analyze Presidential Election
00:01:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And this is the time series that are from the Presidential Prediction Market.,00:01:32,10,And time series Presidential Prediction Market
00:01:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"For example, I write a trunk of market would have stocks for each candidate.",00:01:36,10,For example I write trunk market would stocks candidate
00:01:49,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And if you believe one candidate that will win then you tend to buy the stock for,00:01:44,10,And believe one candidate win tend buy stock
00:01:53,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"that candidate, causing the price of that candidate to increase.",00:01:49,10,candidate causing price candidate increase
00:01:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So, that's a nice way to actual do survey of people's opinions about",00:01:53,10,So nice way actual survey people opinions
00:01:59,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,these candidates.,00:01:58,10,candidates
00:02:05,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, suppose you see something drop of price for one candidate.",00:02:00,10,Now suppose see something drop price one candidate
00:02:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And you might also want to know what might have caused the sudden drop.,00:02:05,10,And might also want know might caused sudden drop
00:02:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Or in a social science study, you might be interested in knowing what method",00:02:10,10,Or social science study might interested knowing method
00:02:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"in this election, what issues really matter to people.",00:02:16,10,election issues really matter people
00:02:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now again in this case,",00:02:20,10,Now case
00:02:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we can look at the companion news stream and ask for the question.,00:02:21,10,look companion news stream ask question
00:02:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Are there any clues in the news stream that might provide insight about this?,00:02:25,10,Are clues news stream might provide insight
00:02:32,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So for example, we might discover the mention of tax cut",00:02:30,10,So example might discover mention tax cut
00:02:38,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,has been increasing since that point.,00:02:35,10,increasing since point
00:02:42,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So maybe, that's related to the drop of the price.",00:02:38,10,So maybe related drop price
00:02:47,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So all these cases are special cases of a general problem of joint,00:02:42,10,So cases special cases general problem joint
00:02:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,analysis of text and a time series data to discover causal topics.,00:02:47,10,analysis text time series data discover causal topics
00:02:56,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,The input in this case is time series plus,00:02:52,10,The input case time series plus
00:03:00,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"text data that are produced in the same time period, the companion text stream.",00:02:56,10,text data produced time period companion text stream
00:03:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And this is different from the standard topic models,",00:03:02,10,And different standard topic models
00:03:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,where we have just to text collection.,00:03:06,10,text collection
00:03:11,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"That's why we see time series here, it serves as context.",00:03:08,10,That see time series serves context
00:03:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, the output that we want to generate is the topics",00:03:13,10,Now output want generate topics
00:03:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,whose coverage in the text stream has strong correlations with the time series.,00:03:16,10,whose coverage text stream strong correlations time series
00:03:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"For example, whenever the topic is managing the price tends to go down, etc.",00:03:22,10,For example whenever topic managing price tends go etc
00:03:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Now we call these topics Causal Topics.,00:03:28,10,Now call topics Causal Topics
00:03:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Of course, they're not, strictly speaking, causal topics.",00:03:30,10,Of course strictly speaking causal topics
00:03:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"We are never going to be able to verify whether they are causal, or",00:03:35,10,We never going able verify whether causal
00:03:43,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,there's a true causal relationship here.,00:03:41,10,true causal relationship
00:03:47,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,That's why we put causal in quotation marks.,00:03:43,10,That put causal quotation marks
00:03:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But at least they are correlating topics that might potentially,00:03:47,10,But least correlating topics might potentially
00:03:53,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,explain the cause and,00:03:51,10,explain cause
00:03:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,humans can certainly further analyze such topics to understand the issue better.,00:03:53,10,humans certainly analyze topics understand issue better
00:04:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And the output would contain topics just like in topic modeling.,00:03:59,10,And output would contain topics like topic modeling
00:04:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But we hope that these topics are not just the regular topics with.,00:04:04,10,But hope topics regular topics
00:04:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"These topics certainly don't have to explain the data of the best in text, but",00:04:08,10,These topics certainly explain data best text
00:04:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,rather they have to explain the data in the text.,00:04:13,10,rather explain data text
00:04:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Meaning that they have to reprehend the meaningful topics in text.,00:04:17,10,Meaning reprehend meaningful topics text
00:04:23,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Cement but also more importantly,",00:04:21,10,Cement also importantly
00:04:29,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,they should be correlated with external hand series that's given as a context.,00:04:23,10,correlated external hand series given context
00:04:33,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So to understand how we solve this problem, let's first adjust to",00:04:29,10,So understand solve problem let first adjust
00:04:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"solve the problem with reactive topic model, for example PRSA.",00:04:33,10,solve problem reactive topic model example PRSA
00:04:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And we can apply this to text stream and,00:04:36,10,And apply text stream
00:04:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,with some extension like a CPRSA or Contextual PRSA.,00:04:40,10,extension like CPRSA Contextual PRSA
00:04:49,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Then we can discover these topics in the correlation and,00:04:44,10,Then discover topics correlation
00:04:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,also discover their coverage over time.,00:04:49,10,also discover coverage time
00:04:59,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So, one simple solution is, to choose the topics from",00:04:53,10,So one simple solution choose topics
00:05:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,this set that have the strongest correlation with the external time series.,00:04:59,10,set strongest correlation external time series
00:05:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But this approach is not going to be very good.,00:05:05,10,But approach going good
00:05:09,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Why? Because,00:05:08,10,Why Because
00:05:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,awareness pictured to the topics is that they will discover by PRSA or LDA.,00:05:09,10,awareness pictured topics discover PRSA LDA
00:05:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that means the choice of topics will be very limited.,00:05:13,10,And means choice topics limited
00:05:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And we know these models try to maximize the likelihood of the text data.,00:05:17,10,And know models try maximize likelihood text data
00:05:24,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So those topics tend to be the major topics that explain the text data well.,00:05:20,10,So topics tend major topics explain text data well
00:05:28,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,aAnd they are not necessarily correlated with time series.,00:05:24,10,aAnd necessarily correlated time series
00:05:33,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Even if we get the best one, the most correlated topics might still not be so",00:05:28,10,Even get best one correlated topics might still
00:05:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,interesting from causal perspective.,00:05:34,10,interesting causal perspective
00:05:42,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So here in this work site here, a better approach was proposed.",00:05:37,10,So work site better approach proposed
00:05:46,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And this approach is called Iterative Causal Topic Modeling.,00:05:42,10,And approach called Iterative Causal Topic Modeling
00:05:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"The idea is to do an iterative adjustment of topic,",00:05:46,10,The idea iterative adjustment topic
00:05:56,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,discovered by topic models using time series to induce a product.,00:05:50,10,discovered topic models using time series induce product
00:06:00,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So here's an illustration on how this work, how this works.",00:05:57,10,So illustration work works
00:06:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Take the text stream as input and,00:06:00,10,Take text stream input
00:06:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,then apply regular topic modeling to generate a number of topics.,00:06:02,10,apply regular topic modeling generate number topics
00:06:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Let's say four topics.,00:06:06,10,Let say four topics
00:06:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Shown here.,00:06:07,10,Shown
00:06:14,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And then we're going to use external time series to assess,00:06:09,10,And going use external time series assess
00:06:19,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,which topic is more causally related or correlated with the external time series.,00:06:14,10,topic causally related correlated external time series
00:06:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So we have something that rank them.,00:06:19,10,So something rank
00:06:24,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And we might think that topic one and topic four are more correlated and,00:06:21,10,And might think topic one topic four correlated
00:06:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,topic two and topic three are not.,00:06:24,10,topic two topic three
00:06:29,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Now we could have stopped here and,00:06:26,10,Now could stopped
00:06:33,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,that would be just like what the simple approached that I talked about earlier,00:06:29,10,would like simple approached I talked earlier
00:06:38,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,then we can get to these topics and call them causal topics.,00:06:33,10,get topics call causal topics
00:06:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But as I also explained that these topics are unlikely very good,00:06:38,10,But I also explained topics unlikely good
00:06:45,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,because they are general topics that explain the whole text connection.,00:06:41,10,general topics explain whole text connection
00:06:46,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,They are not necessary.,00:06:45,10,They necessary
00:06:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,The best topics are correlated with our time series.,00:06:46,10,The best topics correlated time series
00:06:57,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So what we can do in this approach is to first zoom into word level and,00:06:51,10,So approach first zoom word level
00:07:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we can look into each word and the top ranked word listed for each topic.,00:06:57,10,look word top ranked word listed topic
00:07:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Let's say we take Topic 1 as the target examined.,00:07:02,10,Let say take Topic 1 target examined
00:07:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,We know Topic 1 is correlated with the time series.,00:07:07,10,We know Topic 1 correlated time series
00:07:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Or is at least the best that we could get from this set of topics so far.,00:07:13,10,Or least best could get set topics far
00:07:22,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And we're going to look at the words in this topic, the top words.",00:07:18,10,And going look words topic top words
00:07:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And if the topic is correlated with the Time Series,",00:07:23,10,And topic correlated Time Series
00:07:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,there must be some words that are highly correlated with the Time Series.,00:07:26,10,must words highly correlated Time Series
00:07:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So here, for example, we might discover W1 and W3 are positively",00:07:30,10,So example might discover W1 W3 positively
00:07:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"correlated with Time Series, but W2 and W4 are negatively correlated.",00:07:35,10,correlated Time Series W2 W4 negatively correlated
00:07:47,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So, as a topic, and it's not good to mix these words with different correlations.",00:07:41,10,So topic good mix words different correlations
00:07:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So we can then for the separate of these words.,00:07:47,10,So separate words
00:07:54,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,We are going to get all the red words that indicate positive correlations.,00:07:50,10,We going get red words indicate positive correlations
00:07:55,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,W1 and W3. And,00:07:54,10,W1 W3 And
00:07:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we're going to also get another sub topic.,00:07:55,10,going also get another sub topic
00:08:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,If you want.,00:08:00,10,If want
00:08:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"That represents a negatively correlated words, W2 and W4.",00:08:02,10,That represents negatively correlated words W2 W4
00:08:14,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, these subtopics, or these variations of topics, based on the correlation",00:08:07,10,Now subtopics variations topics based correlation
00:08:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"analysis, are topics that are still quite related to the original topic, Topic 1.",00:08:14,10,analysis topics still quite related original topic Topic 1
00:08:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"But they are already deviating,",00:08:20,10,But already deviating
00:08:28,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,because of the use of time series information for bias selection of words.,00:08:21,10,use time series information bias selection words
00:08:33,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So then in some sense, well we should expect so, some sense",00:08:28,10,So sense well expect sense
00:08:37,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,more correlated with the time series than the original Topic 1.,00:08:33,10,correlated time series original Topic 1
00:08:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Because the Topic 1 has mixed words, here we separate them.",00:08:37,10,Because Topic 1 mixed words separate
00:08:45,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So each of these two subtopics,00:08:42,10,So two subtopics
00:08:49,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,can be expected to be better coherent in this time series.,00:08:46,10,expected better coherent time series
00:08:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"However, they may not be so coherent as it mention.",00:08:49,10,However may coherent mention
00:08:57,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So the idea here is to go back to topic model by using these,00:08:52,10,So idea go back topic model using
00:09:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,each as a prior to further guide the topic modeling.,00:08:57,10,prior guide topic modeling
00:09:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that's to say we ask our topic models now discover topics that,00:09:02,10,And say ask topic models discover topics
00:09:10,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,are very similar to each of these two subtopics.,00:09:06,10,similar two subtopics
00:09:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And this will cause a bias toward more correlate to the topics was a time series.,00:09:10,10,And cause bias toward correlate topics time series
00:09:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Of course then we can apply topic models to get another generation of topics.,00:09:17,10,Of course apply topic models get another generation topics
00:09:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that can be further ran to the base of the time series to set after the highly,00:09:21,10,And ran base time series set highly
00:09:27,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,correlated topics.,00:09:25,10,correlated topics
00:09:32,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And then we can further analyze the components at work in the topic and,00:09:27,10,And analyze components work topic
00:09:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,then try to analyze.word level correlation.,00:09:32,10,try analyze word level correlation
00:09:39,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And then get the even more correlated subtopics that can be,00:09:35,10,And get even correlated subtopics
00:09:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,further fed into the process as prior to drive the topic of model discovery.,00:09:39,10,fed process prior drive topic model discovery
00:09:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So this whole process is just a heuristic way of optimizing causality and,00:09:46,10,So whole process heuristic way optimizing causality
00:09:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"coherence, and that's our ultimate goal.",00:09:50,10,coherence ultimate goal
00:09:53,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Right?,00:09:52,10,Right
00:09:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So here you see the pure topic models will be very good at,00:09:53,10,So see pure topic models good
00:10:01,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"maximizing topic coherence, the topics will be all meaningful.",00:09:58,10,maximizing topic coherence topics meaningful
00:10:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"If we only use causality test, or correlation measure,",00:10:02,10,If use causality test correlation measure
00:10:12,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"then we might get a set words that are strongly correlate with time series,",00:10:07,10,might get set words strongly correlate time series
00:10:14,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,but they may not necessarily mean anything.,00:10:12,10,may necessarily mean anything
00:10:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,It might not be cementric connected.,00:10:14,10,It might cementric connected
00:10:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So, that would be at the other extreme, on the top.",00:10:17,10,So would extreme top
00:10:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, the ideal is to get the causal topic that's scored high,",00:10:21,10,Now ideal get causal topic scored high
00:10:29,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,both in topic coherence and also causal relation.,00:10:25,10,topic coherence also causal relation
00:10:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"In this approach,",00:10:29,10,In approach
00:10:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,it can be regarded as an alternate way to maximize both sine engines.,00:10:30,10,regarded alternate way maximize sine engines
00:10:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So when we apply the topic models we're maximizing the coherence.,00:10:35,10,So apply topic models maximizing coherence
00:10:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But when we decompose the topic model words into sets,00:10:40,10,But decompose topic model words sets
00:10:47,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,of words that are very strong correlated with the time series.,00:10:44,10,words strong correlated time series
00:10:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,We select the most strongly correlated words with the time series.,00:10:47,10,We select strongly correlated words time series
00:10:54,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,We are pushing the model back to the causal,00:10:51,10,We pushing model back causal
00:10:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,dimension to make it better in causal scoring.,00:10:54,10,dimension make better causal scoring
00:11:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And then, when we apply the selected words as a prior",00:10:58,10,And apply selected words prior
00:11:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"to guide a topic modeling, we again go back to optimize the coherence.",00:11:04,10,guide topic modeling go back optimize coherence
00:11:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Because topic models, we ensure the next generation of topics to be coherent and",00:11:08,10,Because topic models ensure next generation topics coherent
00:11:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we can iterate when they're optimized in this way as shown on this picture.,00:11:13,10,iterate optimized way shown picture
00:11:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So the only I think a component that you haven't seen such a framework is how,00:11:20,10,So I think component seen framework
00:11:27,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,to measure the causality.,00:11:25,10,measure causality
00:11:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Because the rest is just talking more on.,00:11:27,10,Because rest talking
00:11:33,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So let's have a little bit of discussion of that.,00:11:30,10,So let little bit discussion
00:11:34,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So here we show that.,00:11:33,10,So show
00:11:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And let's say we have a topic about government response here.,00:11:34,10,And let say topic government response
00:11:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And then we just talking more of we can get coverage of the topic over time.,00:11:36,10,And talking get coverage topic time
00:11:42,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So, we have a time series, X sub t.",00:11:40,10,So time series X sub
00:11:48,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, we also have, are give a time series that represents external information.",00:11:43,10,Now also give time series represents external information
00:11:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"It's a non text time series, Y sub t.",00:11:48,10,It non text time series Y sub
00:11:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,It's the stock prices.,00:11:50,10,It stock prices
00:11:57,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Now the the question here is does Xt cause Yt?,00:11:52,10,Now question Xt cause Yt
00:12:03,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Well in other words, we want to match the causality relation between the two.",00:11:58,10,Well words want match causality relation two
00:12:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Or maybe just measure the correlation of the two.,00:12:03,10,Or maybe measure correlation two
00:12:11,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,There are many measures that we can use in this framework.,00:12:08,10,There many measures use framework
00:12:14,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"For example, pairs in correlation is a common use measure.",00:12:11,10,For example pairs correlation common use measure
00:12:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And we got to consider time lag here so,00:12:14,10,And got consider time lag
00:12:19,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,that we can try to capture causal relation.,00:12:17,10,try capture causal relation
00:12:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Using somewhat past data and using the data in the past,00:12:19,10,Using somewhat past data using data past
00:12:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,to try to correlate with the data on,00:12:26,10,try correlate data
00:12:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"points of y that represents the future, for example.",00:12:30,10,points represents future example
00:12:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And by introducing such lag, we can hopefully capture some causal relation by",00:12:36,10,And introducing lag hopefully capture causal relation
00:12:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,even using correlation measures like person correlation.,00:12:41,10,even using correlation measures like person correlation
00:12:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"But a common use, the measure for causality here is Granger Causality Test.",00:12:45,10,But common use measure causality Granger Causality Test
00:12:55,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And the idea of this test is actually quite simple.,00:12:52,10,And idea test actually quite simple
00:12:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Basically you're going to have all the regressive model to,00:12:55,10,Basically going regressive model
00:13:03,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,use the history information of Y to predict itself.,00:12:58,10,use history information Y predict
00:13:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And this is the best we could without any other information.,00:13:03,10,And best could without information
00:13:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So we're going to build such a model.,00:13:06,10,So going build model
00:13:12,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And then we're going to add some history information of X into such model.,00:13:08,10,And going add history information X model
00:13:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,To see if we can improve the prediction of Y.,00:13:12,10,To see improve prediction Y
00:13:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,If we can do that with a statistically significant difference.,00:13:16,10,If statistically significant difference
00:13:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Then we just say X has some causal inference on Y,",00:13:21,10,Then say X causal inference Y
00:13:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,or otherwise it wouldn't have causal improvement of prediction of Y.,00:13:25,10,otherwise causal improvement prediction Y
00:13:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"If, on the other hand, the difference is insignificant and",00:13:32,10,If hand difference insignificant
00:13:39,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,that would mean X does not really have a cause or relation why.,00:13:35,10,would mean X really cause relation
00:13:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So that's the basic idea.,00:13:39,10,So basic idea
00:13:45,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, we don't have time to explain this in detail so you could read, but",00:13:40,10,Now time explain detail could read
00:13:49,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,you would read at this cited reference here to know more about this measure.,00:13:45,10,would read cited reference know measure
00:13:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,It's a very convenient used measure.,00:13:49,10,It convenient used measure
00:13:53,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Has many applications.,00:13:52,10,Has many applications
00:14:00,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So next, let's look at some simple results generated by this approach.",00:13:55,10,So next let look simple results generated approach
00:14:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And here the data is the New York Times and,00:14:00,10,And data New York Times
00:14:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,in the time period of June 2000 through December of 2011.,00:14:02,10,time period June 2000 December 2011
00:14:12,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And here the time series we used is stock prices of two companies.,00:14:06,10,And time series used stock prices two companies
00:14:15,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,American Airlines and Apple and,00:14:12,10,American Airlines Apple
00:14:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"the goal is to see if we inject the sum time series contest,",00:14:15,10,goal see inject sum time series contest
00:14:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,whether we can actually get topics that are wise for the time series.,00:14:21,10,whether actually get topics wise time series
00:14:29,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Imagine if we don't use any input, we don't use any context.",00:14:26,10,Imagine use input use context
00:14:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Then the topics from New York times discovered by PRSA would be,00:14:29,10,Then topics New York times discovered PRSA would
00:14:38,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,just general topics that people talk about in news.,00:14:35,10,general topics people talk news
00:14:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,All right. Those major topics in the news event.,00:14:38,10,All right Those major topics news event
00:14:47,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But here you see these topics are indeed biased toward each time series.,00:14:41,10,But see topics indeed biased toward time series
00:14:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And particularly if you look at the underlined words here,00:14:47,10,And particularly look underlined words
00:14:54,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"in the American Airlines result, and you see airlines,",00:14:51,10,American Airlines result see airlines
00:14:59,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"airport, air, united trade, or terrorism, etc.",00:14:54,10,airport air united trade terrorism etc
00:15:05,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So it clearly has topics that are more correlated with the external time series.,00:14:59,10,So clearly topics correlated external time series
00:15:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"On the right side,",00:15:05,10,On right side
00:15:11,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"you see that some of the topics are clearly related to Apple, right.",00:15:06,10,see topics clearly related Apple right
00:15:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So you can see computer, technology, software, internet, com, web, etc.",00:15:11,10,So see computer technology software internet com web etc
00:15:19,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So that just means the time series,00:15:17,10,So means time series
00:15:24,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,has effectively served as a context to bias the discovery of topics.,00:15:19,10,effectively served context bias discovery topics
00:15:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"From another perspective,",00:15:24,10,From another perspective
00:15:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,these results help us on what people have talked about in each case.,00:15:25,10,results help us people talked case
00:15:36,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So not just the people, what people have talked about,",00:15:30,10,So people people talked
00:15:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,but what are some topics that might be correlated with their stock prices.,00:15:36,10,topics might correlated stock prices
00:15:43,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And so these topics can serve as a starting point for,00:15:41,10,And topics serve starting point
00:15:48,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,people to further look into issues and you'll find the true causal relations.,00:15:43,10,people look issues find true causal relations
00:15:54,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Here are some other results from analyzing,00:15:48,10,Here results analyzing
00:15:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Presidential Election time series.,00:15:54,10,Presidential Election time series
00:16:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,The time series data here is from Iowa Electronic market.,00:15:58,10,The time series data Iowa Electronic market
00:16:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that's a prediction market.,00:16:02,10,And prediction market
00:16:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And the data is the same.,00:16:04,10,And data
00:16:09,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,New York Times from May 2000 to October 2000.,00:16:06,10,New York Times May 2000 October 2000
00:16:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,That's for 2000 presidential campaign election.,00:16:09,10,That 2000 presidential campaign election
00:16:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, what you see here",00:16:13,10,Now see
00:16:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,are the top three words in significant topics from New York Times.,00:16:16,10,top three words significant topics New York Times
00:16:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And if you look at these topics, and they are indeed quite related to the campaign.",00:16:21,10,And look topics indeed quite related campaign
00:16:30,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Actually the issues are very much related to,00:16:26,10,Actually issues much related
00:16:35,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,the important issues of this presidential election.,00:16:30,10,important issues presidential election
00:16:40,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Now here I should mention that the text data has been filtered by using,00:16:35,10,Now I mention text data filtered using
00:16:43,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,only the articles that mention these candidate names.,00:16:40,10,articles mention candidate names
00:16:50,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,It's a subset of these news articles.,00:16:45,10,It subset news articles
00:16:51,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Very different from the previous experiment.,00:16:50,10,Very different previous experiment
00:16:58,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,But the results here clearly show that the approach can uncover some,00:16:53,10,But results clearly show approach uncover
00:17:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,important issues in that presidential election.,00:16:58,10,important issues presidential election
00:17:07,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So tax cut, oil energy, abortion and gun control are all known",00:17:02,10,So tax cut oil energy abortion gun control known
00:17:11,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,to be important issues in that presidential election.,00:17:07,10,important issues presidential election
00:17:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And that was supported by some literature in political science.,00:17:11,10,And supported literature political science
00:17:21,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And also I was discussing Wikipedia, right.",00:17:17,10,And also I discussing Wikipedia right
00:17:26,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So basically the results show that the approach can effectively,00:17:21,10,So basically results show approach effectively
00:17:31,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,discover possibly causal topics based on the time series data.,00:17:26,10,discover possibly causal topics based time series data
00:17:37,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,So there are two suggested readings here.,00:17:35,10,So two suggested readings
00:17:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,One is the paper about this iterative topic modeling with time series feedback.,00:17:37,10,One paper iterative topic modeling time series feedback
00:17:48,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Where you can find more details about how this approach works.,00:17:44,10,Where find details approach works
00:17:52,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And the second one is reading about Granger Casuality text.,00:17:48,10,And second one reading Granger Casuality text
00:18:02,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So in the end, let's summarize the discussion of Text-based Prediction.",00:17:55,10,So end let summarize discussion Text based Prediction
00:18:06,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now, Text-based prediction is generally very useful for",00:18:02,10,Now Text based prediction generally useful
00:18:09,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,big data applications that involve text.,00:18:06,10,big data applications involve text
00:18:13,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Because they can help us inform new knowledge about the world.,00:18:09,10,Because help us inform new knowledge world
00:18:16,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And the knowledge can go beyond what's discussed in the text.,00:18:13,10,And knowledge go beyond discussed text
00:18:23,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,As a result can also support optimizing of our decision making.,00:18:17,10,As result also support optimizing decision making
00:18:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And this has a wider spread application.,00:18:23,10,And wider spread application
00:18:31,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,Text data is often combined with non-text data for prediction.,00:18:28,10,Text data often combined non text data prediction
00:18:34,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"because, for this purpose, the prediction purpose,",00:18:31,10,purpose prediction purpose
00:18:37,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"we generally would like to combine non-text data and text data together,",00:18:34,10,generally would like combine non text data text data together
00:18:41,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,as much cruel as possible for prediction.,00:18:37,10,much cruel possible prediction
00:18:44,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And so as a result during the analysis of text and,00:18:41,10,And result analysis text
00:18:49,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,non-text is very necessary and it's also very useful.,00:18:44,10,non text necessary also useful
00:18:53,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"Now when we analyze text data together with non-text data,",00:18:49,10,Now analyze text data together non text data
00:18:56,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we can see they can help each other.,00:18:53,10,see help
00:19:00,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"So non-text data, provide a context for mining text data, and",00:18:56,10,So non text data provide context mining text data
00:19:04,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,we discussed a number of techniques for contextual text mining.,00:19:00,10,discussed number techniques contextual text mining
00:19:08,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"And on the other hand, a text data can also help interpret",00:19:04,10,And hand text data also help interpret
00:19:12,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"patterns discovered from non-text data, and this is called a pattern annotation.",00:19:08,10,patterns discovered non text data called pattern annotation
00:19:17,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,"In general, this is a very active research topic, and",00:19:14,10,In general active research topic
00:19:20,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,there are new papers being published.,00:19:17,10,new papers published
00:19:25,5,Contextual Text Mining- Mining Casual Topics with Time Series Supervision,4.10,And there are also many open challenges that have to be solved.,00:19:20,10,And also many open challenges solved
00:00:05,3,Probabilistic Topic Models- Mining One Topic,2.6,[SOUND] This lecture is a continued,00:00:00,6,SOUND This lecture continued
00:00:13,3,Probabilistic Topic Models- Mining One Topic,2.6,discussion of probabilistic topic models.,00:00:05,6,discussion probabilistic topic models
00:00:19,3,Probabilistic Topic Models- Mining One Topic,2.6,"In this lecture, we're going to continue discussing probabilistic models.",00:00:13,6,In lecture going continue discussing probabilistic models
00:00:24,3,Probabilistic Topic Models- Mining One Topic,2.6,We're going to talk about a very simple case where we,00:00:19,6,We going talk simple case
00:00:28,3,Probabilistic Topic Models- Mining One Topic,2.6,are interested in just mining one topic from one document.,00:00:24,6,interested mining one topic one document
00:00:35,3,Probabilistic Topic Models- Mining One Topic,2.6,"So in this simple setup, we are interested in analyzing",00:00:30,6,So simple setup interested analyzing
00:00:41,3,Probabilistic Topic Models- Mining One Topic,2.6,one document and trying to discover just one topic.,00:00:35,6,one document trying discover one topic
00:00:44,3,Probabilistic Topic Models- Mining One Topic,2.6,So this is the simplest case of topic model.,00:00:41,6,So simplest case topic model
00:00:49,3,Probabilistic Topic Models- Mining One Topic,2.6,"The input now no longer has k, which is the number of topics because we",00:00:44,6,The input longer k number topics
00:00:55,3,Probabilistic Topic Models- Mining One Topic,2.6,"know there is only one topic and the collection has only one document, also.",00:00:49,6,know one topic collection one document also
00:01:00,3,Probabilistic Topic Models- Mining One Topic,2.6,"In the output, we also no longer have coverage because",00:00:55,6,In output also longer coverage
00:01:06,3,Probabilistic Topic Models- Mining One Topic,2.6,we assumed that the document covers this topic 100%.,00:01:00,6,assumed document covers topic 100
00:01:10,3,Probabilistic Topic Models- Mining One Topic,2.6,So the main goal is just to discover the world of probabilities for,00:01:06,6,So main goal discover world probabilities
00:01:12,3,Probabilistic Topic Models- Mining One Topic,2.6,"this single topic, as shown here.",00:01:10,6,single topic shown
00:01:19,3,Probabilistic Topic Models- Mining One Topic,2.6,"As always, when we think about using a generating model to solve such a problem,",00:01:14,6,As always think using generating model solve problem
00:01:24,3,Probabilistic Topic Models- Mining One Topic,2.6,we start with thinking about what kind of data we are going to model or,00:01:19,6,start thinking kind data going model
00:01:28,3,Probabilistic Topic Models- Mining One Topic,2.6,from what perspective we're going to model the data or data representation.,00:01:24,6,perspective going model data data representation
00:01:32,3,Probabilistic Topic Models- Mining One Topic,2.6,And then we're going to design a specific model for,00:01:28,6,And going design specific model
00:01:36,3,Probabilistic Topic Models- Mining One Topic,2.6,"the generating of the data, from our perspective.",00:01:32,6,generating data perspective
00:01:41,3,Probabilistic Topic Models- Mining One Topic,2.6,Where our perspective just means we want to take a particular angle of looking at,00:01:36,6,Where perspective means want take particular angle looking
00:01:45,3,Probabilistic Topic Models- Mining One Topic,2.6,"the data, so that the model will have the right parameters for",00:01:41,6,data model right parameters
00:01:48,3,Probabilistic Topic Models- Mining One Topic,2.6,discovering the knowledge that we want.,00:01:45,6,discovering knowledge want
00:01:54,3,Probabilistic Topic Models- Mining One Topic,2.6,And then we'll be thinking about the microfunction or,00:01:48,6,And thinking microfunction
00:02:00,3,Probabilistic Topic Models- Mining One Topic,2.6,write down the microfunction to capture more formally how likely,00:01:54,6,write microfunction capture formally likely
00:02:04,3,Probabilistic Topic Models- Mining One Topic,2.6,a data point will be obtained from this model.,00:02:00,6,data point obtained model
00:02:10,3,Probabilistic Topic Models- Mining One Topic,2.6,And the likelihood function will have some parameters in the function.,00:02:05,6,And likelihood function parameters function
00:02:15,3,Probabilistic Topic Models- Mining One Topic,2.6,"And then we argue our interest in estimating those parameters for example,",00:02:10,6,And argue interest estimating parameters example
00:02:21,3,Probabilistic Topic Models- Mining One Topic,2.6,by maximizing the likelihood which will lead to maximum likelihood estimated.,00:02:15,6,maximizing likelihood lead maximum likelihood estimated
00:02:26,3,Probabilistic Topic Models- Mining One Topic,2.6,These estimator parameters will then become the output,00:02:21,6,These estimator parameters become output
00:02:31,3,Probabilistic Topic Models- Mining One Topic,2.6,"of the mining hours, which means we'll take the estimating",00:02:26,6,mining hours means take estimating
00:02:35,3,Probabilistic Topic Models- Mining One Topic,2.6,parameters as the knowledge that we discover from the text.,00:02:31,6,parameters knowledge discover text
00:02:39,3,Probabilistic Topic Models- Mining One Topic,2.6,So let's look at these steps for this very simple case.,00:02:35,6,So let look steps simple case
00:02:45,3,Probabilistic Topic Models- Mining One Topic,2.6,Later we'll look at this procedure for some more complicated cases.,00:02:39,6,Later look procedure complicated cases
00:02:50,3,Probabilistic Topic Models- Mining One Topic,2.6,"So our data, in this case is, just a document which is a sequence of words.",00:02:45,6,So data case document sequence words
00:02:52,3,Probabilistic Topic Models- Mining One Topic,2.6,Each word here is denoted by x sub i.,00:02:50,6,Each word denoted x sub
00:02:56,3,Probabilistic Topic Models- Mining One Topic,2.6,Our model is a Unigram language model.,00:02:52,6,Our model Unigram language model
00:03:03,3,Probabilistic Topic Models- Mining One Topic,2.6,A word distribution that we hope to denote a topic and that's our goal.,00:02:56,6,A word distribution hope denote topic goal
00:03:08,3,Probabilistic Topic Models- Mining One Topic,2.6,"So we will have as many parameters as many words in our vocabulary, in this case M.",00:03:03,6,So many parameters many words vocabulary case M
00:03:14,3,Probabilistic Topic Models- Mining One Topic,2.6,And for convenience we're going to use theta sub i to,00:03:09,6,And convenience going use theta sub
00:03:18,3,Probabilistic Topic Models- Mining One Topic,2.6,denote the probability of word w sub i.,00:03:14,6,denote probability word w sub
00:03:23,3,Probabilistic Topic Models- Mining One Topic,2.6,And obviously these theta sub i's will sum to 1.,00:03:20,6,And obviously theta sub sum 1
00:03:27,3,Probabilistic Topic Models- Mining One Topic,2.6,Now what does a likelihood function look like?,00:03:24,6,Now likelihood function look like
00:03:30,3,Probabilistic Topic Models- Mining One Topic,2.6,"Well, this is just the probability of generating this whole document,",00:03:27,6,Well probability generating whole document
00:03:31,3,Probabilistic Topic Models- Mining One Topic,2.6,that given such a model.,00:03:30,6,given model
00:03:36,3,Probabilistic Topic Models- Mining One Topic,2.6,Because we assume the independence in generating each word so the probability of,00:03:31,6,Because assume independence generating word probability
00:03:41,3,Probabilistic Topic Models- Mining One Topic,2.6,the document will be just a product of the probability of each word.,00:03:36,6,document product probability word
00:03:46,3,Probabilistic Topic Models- Mining One Topic,2.6,And since some word might have repeated occurrences.,00:03:42,6,And since word might repeated occurrences
00:03:51,3,Probabilistic Topic Models- Mining One Topic,2.6,So we can also rewrite this product in a different form.,00:03:46,6,So also rewrite product different form
00:03:58,3,Probabilistic Topic Models- Mining One Topic,2.6,"So in this line, we have rewritten the formula into a product",00:03:52,6,So line rewritten formula product
00:04:05,3,Probabilistic Topic Models- Mining One Topic,2.6,"over all the unique words in the vocabulary, w sub 1 through w sub M.",00:03:58,6,unique words vocabulary w sub 1 w sub M
00:04:09,3,Probabilistic Topic Models- Mining One Topic,2.6,Now this is different from the previous line.,00:04:05,6,Now different previous line
00:04:13,3,Probabilistic Topic Models- Mining One Topic,2.6,"Well, the product is over different positions of words in the document.",00:04:09,6,Well product different positions words document
00:04:19,3,Probabilistic Topic Models- Mining One Topic,2.6,"Now when we do this transformation, we then would need to",00:04:15,6,Now transformation would need
00:04:24,3,Probabilistic Topic Models- Mining One Topic,2.6,introduce a counter function here.,00:04:19,6,introduce counter function
00:04:29,3,Probabilistic Topic Models- Mining One Topic,2.6,This denotes the count of word one in document and,00:04:24,6,This denotes count word one document
00:04:33,3,Probabilistic Topic Models- Mining One Topic,2.6,similarly this is the count of words of n in the document,00:04:29,6,similarly count words n document
00:04:37,3,Probabilistic Topic Models- Mining One Topic,2.6,because these words might have repeated occurrences.,00:04:33,6,words might repeated occurrences
00:04:40,3,Probabilistic Topic Models- Mining One Topic,2.6,You can also see if a word did not occur in the document.,00:04:37,6,You also see word occur document
00:04:46,3,Probabilistic Topic Models- Mining One Topic,2.6,"It will have a zero count, therefore that corresponding term will disappear.",00:04:41,6,It zero count therefore corresponding term disappear
00:04:50,3,Probabilistic Topic Models- Mining One Topic,2.6,So this is a very useful form of,00:04:46,6,So useful form
00:04:55,3,Probabilistic Topic Models- Mining One Topic,2.6,writing down the likelihood function that we will often use later.,00:04:50,6,writing likelihood function often use later
00:05:01,3,Probabilistic Topic Models- Mining One Topic,2.6,"So I want you to pay attention to this, just get familiar with this notation.",00:04:55,6,So I want pay attention get familiar notation
00:05:07,3,Probabilistic Topic Models- Mining One Topic,2.6,It's just to change the product over all the different words in the vocabulary.,00:05:01,6,It change product different words vocabulary
00:05:12,3,Probabilistic Topic Models- Mining One Topic,2.6,"So in the end, of course, we'll use theta sub i to express this likelihood",00:05:07,6,So end course use theta sub express likelihood
00:05:14,3,Probabilistic Topic Models- Mining One Topic,2.6,function and it would look like this.,00:05:12,6,function would look like
00:05:19,3,Probabilistic Topic Models- Mining One Topic,2.6,"Next, we're going to find the theta values or probabilities",00:05:14,6,Next going find theta values probabilities
00:05:24,3,Probabilistic Topic Models- Mining One Topic,2.6,of these words that would maximize this likelihood function.,00:05:19,6,words would maximize likelihood function
00:05:30,3,Probabilistic Topic Models- Mining One Topic,2.6,So now lets take a look at the maximum likelihood estimate problem more closely.,00:05:24,6,So lets take look maximum likelihood estimate problem closely
00:05:35,3,Probabilistic Topic Models- Mining One Topic,2.6,This line is copied from the previous slide.,00:05:32,6,This line copied previous slide
00:05:37,3,Probabilistic Topic Models- Mining One Topic,2.6,It's just our likelihood function.,00:05:35,6,It likelihood function
00:05:43,3,Probabilistic Topic Models- Mining One Topic,2.6,So our goal is to maximize this likelihood function.,00:05:38,6,So goal maximize likelihood function
00:05:46,3,Probabilistic Topic Models- Mining One Topic,2.6,We will find it often easy to,00:05:43,6,We find often easy
00:05:51,3,Probabilistic Topic Models- Mining One Topic,2.6,maximize the local likelihood instead of the original likelihood.,00:05:47,6,maximize local likelihood instead original likelihood
00:05:56,3,Probabilistic Topic Models- Mining One Topic,2.6,And this is purely for mathematical convenience because after,00:05:51,6,And purely mathematical convenience
00:06:03,3,Probabilistic Topic Models- Mining One Topic,2.6,the logarithm transformation our function will becomes a sum instead of product.,00:05:56,6,logarithm transformation function becomes sum instead product
00:06:10,3,Probabilistic Topic Models- Mining One Topic,2.6,And we also have constraints over these these probabilities.,00:06:03,6,And also constraints probabilities
00:06:16,3,Probabilistic Topic Models- Mining One Topic,2.6,"The sum makes it easier to take derivative, which is often needed for",00:06:10,6,The sum makes easier take derivative often needed
00:06:21,3,Probabilistic Topic Models- Mining One Topic,2.6,finding the optimal solution of this function.,00:06:16,6,finding optimal solution function
00:06:27,3,Probabilistic Topic Models- Mining One Topic,2.6,"So please take a look at this sum again, here.",00:06:21,6,So please take look sum
00:06:32,3,Probabilistic Topic Models- Mining One Topic,2.6,And this is a form of a function that you will often,00:06:27,6,And form function often
00:06:38,3,Probabilistic Topic Models- Mining One Topic,2.6,"see later also, the more general topic models.",00:06:32,6,see later also general topic models
00:06:42,3,Probabilistic Topic Models- Mining One Topic,2.6,So it's a sum over all the words in the vocabulary.,00:06:38,6,So sum words vocabulary
00:06:48,3,Probabilistic Topic Models- Mining One Topic,2.6,And inside the sum there is a count of a word in the document.,00:06:42,6,And inside sum count word document
00:06:54,3,Probabilistic Topic Models- Mining One Topic,2.6,And this is macroed by the logarithm of a probability.,00:06:48,6,And macroed logarithm probability
00:06:57,3,Probabilistic Topic Models- Mining One Topic,2.6,So let's see how we can solve this problem.,00:06:55,6,So let see solve problem
00:07:04,3,Probabilistic Topic Models- Mining One Topic,2.6,Now at this point the problem is purely a mathematical problem because we are going,00:06:58,6,Now point problem purely mathematical problem going
00:07:11,3,Probabilistic Topic Models- Mining One Topic,2.6,to just the find the optimal solution of a constrained maximization problem.,00:07:04,6,find optimal solution constrained maximization problem
00:07:14,3,Probabilistic Topic Models- Mining One Topic,2.6,The objective function is the likelihood function and,00:07:11,6,The objective function likelihood function
00:07:18,3,Probabilistic Topic Models- Mining One Topic,2.6,the constraint is that all these probabilities must sum to one.,00:07:14,6,constraint probabilities must sum one
00:07:23,3,Probabilistic Topic Models- Mining One Topic,2.6,"So, one way to solve the problem is to use Lagrange multiplier approace.",00:07:18,6,So one way solve problem use Lagrange multiplier approace
00:07:29,3,Probabilistic Topic Models- Mining One Topic,2.6,Now this command is beyond the scope of this course but,00:07:24,6,Now command beyond scope course
00:07:33,3,Probabilistic Topic Models- Mining One Topic,2.6,"since Lagrange multiplier is a very useful approach, I also would like",00:07:29,6,since Lagrange multiplier useful approach I also would like
00:07:37,3,Probabilistic Topic Models- Mining One Topic,2.6,"to just give a brief introduction to this, for those of you who are interested.",00:07:33,6,give brief introduction interested
00:07:43,3,Probabilistic Topic Models- Mining One Topic,2.6,"So in this approach we will construct a Lagrange function, here.",00:07:39,6,So approach construct Lagrange function
00:07:49,3,Probabilistic Topic Models- Mining One Topic,2.6,And this function will combine our objective function,00:07:43,6,And function combine objective function
00:07:55,3,Probabilistic Topic Models- Mining One Topic,2.6,with another term that encodes our constraint and,00:07:49,6,another term encodes constraint
00:07:59,3,Probabilistic Topic Models- Mining One Topic,2.6,"we introduce Lagrange multiplier here,",00:07:55,6,introduce Lagrange multiplier
00:08:04,3,Probabilistic Topic Models- Mining One Topic,2.6,"lambda, so it's an additional parameter.",00:07:59,6,lambda additional parameter
00:08:10,3,Probabilistic Topic Models- Mining One Topic,2.6,"Now, the idea of this approach is just to turn the constraint optimization into,",00:08:04,6,Now idea approach turn constraint optimization
00:08:14,3,Probabilistic Topic Models- Mining One Topic,2.6,"in some sense, an unconstrained optimizing problem.",00:08:10,6,sense unconstrained optimizing problem
00:08:18,3,Probabilistic Topic Models- Mining One Topic,2.6,Now we are just interested in optimizing this Lagrange function.,00:08:14,6,Now interested optimizing Lagrange function
00:08:24,3,Probabilistic Topic Models- Mining One Topic,2.6,"As you may recall from calculus, an optimal point",00:08:19,6,As may recall calculus optimal point
00:08:29,3,Probabilistic Topic Models- Mining One Topic,2.6,would be achieved when the derivative is set to zero.,00:08:24,6,would achieved derivative set zero
00:08:31,3,Probabilistic Topic Models- Mining One Topic,2.6,This is a necessary condition.,00:08:29,6,This necessary condition
00:08:33,3,Probabilistic Topic Models- Mining One Topic,2.6,"It's not sufficient, though.",00:08:31,6,It sufficient though
00:08:38,3,Probabilistic Topic Models- Mining One Topic,2.6,"So if we do that you will see the partial derivative,",00:08:33,6,So see partial derivative
00:08:42,3,Probabilistic Topic Models- Mining One Topic,2.6,"with respect to theta i here ,is equal to this.",00:08:38,6,respect theta equal
00:08:50,3,Probabilistic Topic Models- Mining One Topic,2.6,And this part comes from the derivative of the logarithm function and,00:08:42,6,And part comes derivative logarithm function
00:08:55,3,Probabilistic Topic Models- Mining One Topic,2.6,this lambda is simply taken from here.,00:08:50,6,lambda simply taken
00:09:00,3,Probabilistic Topic Models- Mining One Topic,2.6,And when we set it to zero we can,00:08:55,6,And set zero
00:09:05,3,Probabilistic Topic Models- Mining One Topic,2.6,easily see theta sub i is related to lambda in this way.,00:09:00,6,easily see theta sub related lambda way
00:09:09,3,Probabilistic Topic Models- Mining One Topic,2.6,Since we know all the theta i's must a sum to one,00:09:06,6,Since know theta must sum one
00:09:12,3,Probabilistic Topic Models- Mining One Topic,2.6,"we can plug this into this constraint, here.",00:09:09,6,plug constraint
00:09:15,3,Probabilistic Topic Models- Mining One Topic,2.6,And this will allow us to solve for lambda.,00:09:12,6,And allow us solve lambda
00:09:20,3,Probabilistic Topic Models- Mining One Topic,2.6,And this is just a net sum of all the counts.,00:09:16,6,And net sum counts
00:09:27,3,Probabilistic Topic Models- Mining One Topic,2.6,"And this further allows us to then solve the optimization problem,",00:09:20,6,And allows us solve optimization problem
00:09:31,3,Probabilistic Topic Models- Mining One Topic,2.6,"eventually, to find the optimal setting for theta sub i.",00:09:27,6,eventually find optimal setting theta sub
00:09:37,3,Probabilistic Topic Models- Mining One Topic,2.6,And if you look at this formula it turns out that it's actually very intuitive,00:09:31,6,And look formula turns actually intuitive
00:09:43,3,Probabilistic Topic Models- Mining One Topic,2.6,"because this is just the normalized count of these words by the document ns,",00:09:37,6,normalized count words document ns
00:09:47,3,Probabilistic Topic Models- Mining One Topic,2.6,which is also a sum of all the counts of words in the document.,00:09:43,6,also sum counts words document
00:09:52,3,Probabilistic Topic Models- Mining One Topic,2.6,"So, after all this mess, after all,",00:09:47,6,So mess
00:09:59,3,Probabilistic Topic Models- Mining One Topic,2.6,we have just obtained something that's very intuitive and,00:09:52,6,obtained something intuitive
00:10:04,3,Probabilistic Topic Models- Mining One Topic,2.6,this will be just our intuition where we want to,00:09:59,6,intuition want
00:10:10,3,Probabilistic Topic Models- Mining One Topic,2.6,maximize the data by assigning as much probability,00:10:04,6,maximize data assigning much probability
00:10:16,3,Probabilistic Topic Models- Mining One Topic,2.6,mass as possible to all the observed the words here.,00:10:10,6,mass possible observed words
00:10:21,3,Probabilistic Topic Models- Mining One Topic,2.6,And you might also notice that this is the general result of maximum likelihood,00:10:16,6,And might also notice general result maximum likelihood
00:10:23,3,Probabilistic Topic Models- Mining One Topic,2.6,raised estimator.,00:10:21,6,raised estimator
00:10:29,3,Probabilistic Topic Models- Mining One Topic,2.6,"In general, the estimator would be to normalize counts and it's just sometimes",00:10:23,6,In general estimator would normalize counts sometimes
00:10:35,3,Probabilistic Topic Models- Mining One Topic,2.6,"the counts have to be done in a particular way, as you will also see later.",00:10:29,6,counts done particular way also see later
00:10:41,3,Probabilistic Topic Models- Mining One Topic,2.6,So this is basically an analytical solution to our optimization problem.,00:10:35,6,So basically analytical solution optimization problem
00:10:46,3,Probabilistic Topic Models- Mining One Topic,2.6,"In general though, when the likelihood function is very complicated, we're not",00:10:41,6,In general though likelihood function complicated
00:10:50,3,Probabilistic Topic Models- Mining One Topic,2.6,going to be able to solve the optimization problem by having a closed form formula.,00:10:46,6,going able solve optimization problem closed form formula
00:10:55,3,Probabilistic Topic Models- Mining One Topic,2.6,Instead we have to use some numerical algorithms and,00:10:50,6,Instead use numerical algorithms
00:10:58,3,Probabilistic Topic Models- Mining One Topic,2.6,"we're going to see such cases later, also.",00:10:55,6,going see cases later also
00:11:02,3,Probabilistic Topic Models- Mining One Topic,2.6,So if you imagine what would we get if we use such a maximum,00:10:58,6,So imagine would get use maximum
00:11:07,3,Probabilistic Topic Models- Mining One Topic,2.6,likelihood estimator to estimate one topic for a single document d here?,00:11:02,6,likelihood estimator estimate one topic single document
00:11:09,3,Probabilistic Topic Models- Mining One Topic,2.6,Let's imagine this document is a text mining paper.,00:11:07,6,Let imagine document text mining paper
00:11:16,3,Probabilistic Topic Models- Mining One Topic,2.6,"Now, what you might see is something that looks like this.",00:11:09,6,Now might see something looks like
00:11:20,3,Probabilistic Topic Models- Mining One Topic,2.6,"On the top, you will see the high probability words tend to be those very",00:11:16,6,On top see high probability words tend
00:11:23,3,Probabilistic Topic Models- Mining One Topic,2.6,"common words, often functional words in English.",00:11:20,6,common words often functional words English
00:11:27,3,Probabilistic Topic Models- Mining One Topic,2.6,And this will be followed by some content words that really,00:11:23,6,And followed content words really
00:11:31,3,Probabilistic Topic Models- Mining One Topic,2.6,"characterize the topic well like text, mining, etc.",00:11:27,6,characterize topic well like text mining etc
00:11:36,3,Probabilistic Topic Models- Mining One Topic,2.6,"And then in the end, you also see there is more probability of",00:11:31,6,And end also see probability
00:11:40,3,Probabilistic Topic Models- Mining One Topic,2.6,words that are not really related to the topic but,00:11:36,6,words really related topic
00:11:44,3,Probabilistic Topic Models- Mining One Topic,2.6,they might be extraneously mentioned in the document.,00:11:40,6,might extraneously mentioned document
00:11:49,3,Probabilistic Topic Models- Mining One Topic,2.6,"As a topic representation, you will see this is not ideal, right?",00:11:44,6,As topic representation see ideal right
00:11:52,3,Probabilistic Topic Models- Mining One Topic,2.6,"That because the high probability words are functional words,",00:11:49,6,That high probability words functional words
00:11:55,3,Probabilistic Topic Models- Mining One Topic,2.6,they are not really characterizing the topic.,00:11:52,6,really characterizing topic
00:11:58,3,Probabilistic Topic Models- Mining One Topic,2.6,So my question is how can we get rid of such common words?,00:11:55,6,So question get rid common words
00:12:02,3,Probabilistic Topic Models- Mining One Topic,2.6,Now this is the topic of the next module.,00:11:59,6,Now topic next module
00:12:06,3,Probabilistic Topic Models- Mining One Topic,2.6,We're going to talk about how to use probabilistic models to somehow get rid of,00:12:02,6,We going talk use probabilistic models somehow get rid
00:12:08,3,Probabilistic Topic Models- Mining One Topic,2.6,these common words.,00:12:06,6,common words
00:00:04,2,Word Association Mining and Analysis,1.7,[SOUND] This lecture is,00:00:00,7,SOUND This lecture
00:00:10,2,Word Association Mining and Analysis,1.7,about the word association,00:00:04,7,word association
00:00:15,2,Word Association Mining and Analysis,1.7,mining and analysis.,00:00:10,7,mining analysis
00:00:19,2,Word Association Mining and Analysis,1.7,"In this lecture, we're going to talk about how to mine",00:00:15,7,In lecture going talk mine
00:00:22,2,Word Association Mining and Analysis,1.7,associations of words from text.,00:00:19,7,associations words text
00:00:27,2,Word Association Mining and Analysis,1.7,Now this is an example of knowledge about the natural language that,00:00:22,7,Now example knowledge natural language
00:00:29,2,Word Association Mining and Analysis,1.7,we can mine from text data.,00:00:27,7,mine text data
00:00:35,2,Word Association Mining and Analysis,1.7,Here's the outline.,00:00:33,7,Here outline
00:00:39,2,Word Association Mining and Analysis,1.7,We're going to first talk about what is word association and,00:00:35,7,We going first talk word association
00:00:45,2,Word Association Mining and Analysis,1.7,then explain why discovering such relations is useful and finally,00:00:39,7,explain discovering relations useful finally
00:00:50,2,Word Association Mining and Analysis,1.7,we're going to talk about some general ideas about how to mine word associations.,00:00:45,7,going talk general ideas mine word associations
00:00:55,2,Word Association Mining and Analysis,1.7,In general there are two word relations and these are quite basic.,00:00:50,7,In general two word relations quite basic
00:00:58,2,Word Association Mining and Analysis,1.7,One is called a paradigmatic relation.,00:00:56,7,One called paradigmatic relation
00:01:03,2,Word Association Mining and Analysis,1.7,The other is syntagmatic relation.,00:00:58,7,The syntagmatic relation
00:01:07,2,Word Association Mining and Analysis,1.7,A and B have paradigmatic relation,00:01:03,7,A B paradigmatic relation
00:01:11,2,Word Association Mining and Analysis,1.7,if they can be substituted for each other.,00:01:07,7,substituted
00:01:17,2,Word Association Mining and Analysis,1.7,That means the two words that have paradigmatic relation,00:01:11,7,That means two words paradigmatic relation
00:01:23,2,Word Association Mining and Analysis,1.7,"would be in the same semantic class, or syntactic class.",00:01:17,7,would semantic class syntactic class
00:01:26,2,Word Association Mining and Analysis,1.7,And we can in general replace one by the other,00:01:23,7,And general replace one
00:01:30,2,Word Association Mining and Analysis,1.7,without affecting the understanding of the sentence.,00:01:26,7,without affecting understanding sentence
00:01:33,2,Word Association Mining and Analysis,1.7,That means we would still have a valid sentence.,00:01:30,7,That means would still valid sentence
00:01:41,2,Word Association Mining and Analysis,1.7,"For example, cat and dog, these two words have a paradigmatic relation",00:01:33,7,For example cat dog two words paradigmatic relation
00:01:47,2,Word Association Mining and Analysis,1.7,because they are in the same class of animal.,00:01:41,7,class animal
00:01:51,2,Word Association Mining and Analysis,1.7,"And in general, if you replace cat with dog in a sentence,",00:01:47,7,And general replace cat dog sentence
00:01:56,2,Word Association Mining and Analysis,1.7,the sentence would still be a valid sentence that you can make sense of.,00:01:51,7,sentence would still valid sentence make sense
00:02:01,2,Word Association Mining and Analysis,1.7,Similarly Monday and Tuesday have paradigmatical relation.,00:01:58,7,Similarly Monday Tuesday paradigmatical relation
00:02:09,2,Word Association Mining and Analysis,1.7,The second kind of relation is called syntagmatical relation.,00:02:04,7,The second kind relation called syntagmatical relation
00:02:17,2,Word Association Mining and Analysis,1.7,"In this case, the two words that have this relation, can be combined with each other.",00:02:10,7,In case two words relation combined
00:02:22,2,Word Association Mining and Analysis,1.7,So A and B have syntagmatic relation if they can be combined with each other in,00:02:17,7,So A B syntagmatic relation combined
00:02:29,2,Word Association Mining and Analysis,1.7,"a sentence, that means these two words are semantically related.",00:02:22,7,sentence means two words semantically related
00:02:36,2,Word Association Mining and Analysis,1.7,"So for example, cat and sit are related because a cat can sit somewhere.",00:02:30,7,So example cat sit related cat sit somewhere
00:02:43,2,Word Association Mining and Analysis,1.7,"Similarly, car and drive are related semantically and",00:02:38,7,Similarly car drive related semantically
00:02:47,2,Word Association Mining and Analysis,1.7,they can be combined with each other to convey meaning.,00:02:43,7,combined convey meaning
00:02:54,2,Word Association Mining and Analysis,1.7,"However, in general, we can not replace cat with sit in a sentence or",00:02:47,7,However general replace cat sit sentence
00:02:59,2,Word Association Mining and Analysis,1.7,"car with drive in the sentence to still get a valid sentence,",00:02:54,7,car drive sentence still get valid sentence
00:03:03,2,Word Association Mining and Analysis,1.7,"meaning that if we do that, the sentence will become somewhat meaningless.",00:02:59,7,meaning sentence become somewhat meaningless
00:03:10,2,Word Association Mining and Analysis,1.7,So this is different from paradigmatic relation.,00:03:03,7,So different paradigmatic relation
00:03:15,2,Word Association Mining and Analysis,1.7,And these two relations are in fact so fundamental that they can be,00:03:10,7,And two relations fact fundamental
00:03:24,2,Word Association Mining and Analysis,1.7,generalized to capture basic relations between units in arbitrary sequences.,00:03:17,7,generalized capture basic relations units arbitrary sequences
00:03:27,2,Word Association Mining and Analysis,1.7,And definitely they can be generalized to describe,00:03:24,7,And definitely generalized describe
00:03:31,2,Word Association Mining and Analysis,1.7,relations of any items in a language.,00:03:27,7,relations items language
00:03:36,2,Word Association Mining and Analysis,1.7,"So, A and B don't have to be words and they can be phrases, for example.",00:03:31,7,So A B words phrases example
00:03:44,2,Word Association Mining and Analysis,1.7,And they can even be more complex phrases than just a non-phrase.,00:03:37,7,And even complex phrases non phrase
00:03:48,2,Word Association Mining and Analysis,1.7,If you think about the general problem of the sequence mining,00:03:44,7,If think general problem sequence mining
00:03:53,2,Word Association Mining and Analysis,1.7,then we can think about the units being and the sequence data.,00:03:48,7,think units sequence data
00:03:58,2,Word Association Mining and Analysis,1.7,Then we think of paradigmatic relation as relations that,00:03:53,7,Then think paradigmatic relation relations
00:04:05,2,Word Association Mining and Analysis,1.7,"are applied to units that tend to occur in a singular locations in a sentence,",00:03:58,7,applied units tend occur singular locations sentence
00:04:11,2,Word Association Mining and Analysis,1.7,or in a sequence of data elements in general.,00:04:05,7,sequence data elements general
00:04:20,2,Word Association Mining and Analysis,1.7,So they occur in similar locations relative to the neighbors in the sequence.,00:04:11,7,So occur similar locations relative neighbors sequence
00:04:25,2,Word Association Mining and Analysis,1.7,Syntagmatical relation on the other hand is related to,00:04:20,7,Syntagmatical relation hand related
00:04:30,2,Word Association Mining and Analysis,1.7,co-occurrent elements that tend to show up in the same sequence.,00:04:25,7,co occurrent elements tend show sequence
00:04:38,2,Word Association Mining and Analysis,1.7,So these two are complimentary and are basic relations of words.,00:04:33,7,So two complimentary basic relations words
00:04:42,2,Word Association Mining and Analysis,1.7,And we're interested in discovering them automatically from text data.,00:04:38,7,And interested discovering automatically text data
00:04:46,2,Word Association Mining and Analysis,1.7,Discovering such worded relations has many applications.,00:04:42,7,Discovering worded relations many applications
00:04:52,2,Word Association Mining and Analysis,1.7,"First, such relations can be directly useful for improving accuracy of many NLP",00:04:47,7,First relations directly useful improving accuracy many NLP
00:04:58,2,Word Association Mining and Analysis,1.7,"tasks, and this is because this is part of our knowledge about a language.",00:04:52,7,tasks part knowledge language
00:05:02,2,Word Association Mining and Analysis,1.7,"So if you know these two words are synonyms, for example,",00:04:58,7,So know two words synonyms example
00:05:04,2,Word Association Mining and Analysis,1.7,and then you can help a lot of tasks.,00:05:02,7,help lot tasks
00:05:10,2,Word Association Mining and Analysis,1.7,And grammar learning can be also done by using such techniques.,00:05:05,7,And grammar learning also done using techniques
00:05:15,2,Word Association Mining and Analysis,1.7,"Because if we can learn paradigmatic relations,",00:05:10,7,Because learn paradigmatic relations
00:05:20,2,Word Association Mining and Analysis,1.7,"then we form classes of words, syntactic classes for example.",00:05:15,7,form classes words syntactic classes example
00:05:25,2,Word Association Mining and Analysis,1.7,"And if we learn syntagmatic relations, then we would be able to know",00:05:20,7,And learn syntagmatic relations would able know
00:05:32,2,Word Association Mining and Analysis,1.7,the rules for putting together a larger expression based on component expressions.,00:05:25,7,rules putting together larger expression based component expressions
00:05:37,2,Word Association Mining and Analysis,1.7,So we learn the structure and what can go with what else.,00:05:32,7,So learn structure go else
00:05:43,2,Word Association Mining and Analysis,1.7,Word relations can be also very useful for,00:05:39,7,Word relations also useful
00:05:46,2,Word Association Mining and Analysis,1.7,many applications in text retrieval and mining.,00:05:43,7,many applications text retrieval mining
00:05:50,2,Word Association Mining and Analysis,1.7,"For example, in search and text retrieval, we can use word",00:05:46,7,For example search text retrieval use word
00:05:55,2,Word Association Mining and Analysis,1.7,"associations to modify a query, and this can be used to",00:05:50,7,associations modify query used
00:06:00,2,Word Association Mining and Analysis,1.7,introduce additional related words into a query and make the query more effective.,00:05:55,7,introduce additional related words query make query effective
00:06:03,2,Word Association Mining and Analysis,1.7,It's often called a query expansion.,00:06:01,7,It often called query expansion
00:06:10,2,Word Association Mining and Analysis,1.7,Or you can use related words to suggest related queries to the user,00:06:05,7,Or use related words suggest related queries user
00:06:11,2,Word Association Mining and Analysis,1.7,to explore the information space.,00:06:10,7,explore information space
00:06:15,2,Word Association Mining and Analysis,1.7,Another application is to use word associations to,00:06:12,7,Another application use word associations
00:06:19,2,Word Association Mining and Analysis,1.7,automatically construct the top of the map for browsing.,00:06:15,7,automatically construct top map browsing
00:06:24,2,Word Association Mining and Analysis,1.7,We can have words as nodes and associations as edges.,00:06:19,7,We words nodes associations edges
00:06:27,2,Word Association Mining and Analysis,1.7,A user could navigate from one word to another to,00:06:24,7,A user could navigate one word another
00:06:31,2,Word Association Mining and Analysis,1.7,find information in the information space.,00:06:28,7,find information information space
00:06:40,2,Word Association Mining and Analysis,1.7,"Finally, such word associations can also be used to compare and summarize opinions.",00:06:33,7,Finally word associations also used compare summarize opinions
00:06:45,2,Word Association Mining and Analysis,1.7,"For example, we might be interested in understanding positive and",00:06:40,7,For example might interested understanding positive
00:06:48,2,Word Association Mining and Analysis,1.7,negative opinions about the iPhone 6.,00:06:45,7,negative opinions iPhone 6
00:06:55,2,Word Association Mining and Analysis,1.7,"In order to do that, we can look at what words are most strongly associated with",00:06:48,7,In order look words strongly associated
00:07:01,2,Word Association Mining and Analysis,1.7,a feature word like battery in positive versus negative reviews.,00:06:55,7,feature word like battery positive versus negative reviews
00:07:05,2,Word Association Mining and Analysis,1.7,Such a syntagmatical relations would help us,00:07:01,7,Such syntagmatical relations would help us
00:07:08,2,Word Association Mining and Analysis,1.7,show the detailed opinions about the product.,00:07:05,7,show detailed opinions product
00:07:20,2,Word Association Mining and Analysis,1.7,"So, how can we discover such associations automatically?",00:07:16,7,So discover associations automatically
00:07:24,2,Word Association Mining and Analysis,1.7,"Now, here are some intuitions about how to do that.",00:07:20,7,Now intuitions
00:07:27,2,Word Association Mining and Analysis,1.7,Now let's first look at the paradigmatic relation.,00:07:24,7,Now let first look paradigmatic relation
00:07:32,2,Word Association Mining and Analysis,1.7,Here we essentially can take advantage of similar context.,00:07:29,7,Here essentially take advantage similar context
00:07:38,2,Word Association Mining and Analysis,1.7,So here you see some simple sentences about cat and dog.,00:07:34,7,So see simple sentences cat dog
00:07:43,2,Word Association Mining and Analysis,1.7,"You can see they generally occur in similar context,",00:07:38,7,You see generally occur similar context
00:07:48,2,Word Association Mining and Analysis,1.7,and that after all is the definition of paradigmatic relation.,00:07:43,7,definition paradigmatic relation
00:07:54,2,Word Association Mining and Analysis,1.7,On the right side you can kind of see I extracted expressly,00:07:49,7,On right side kind see I extracted expressly
00:07:59,2,Word Association Mining and Analysis,1.7,the context of cat and dog from this small sample of text data.,00:07:54,7,context cat dog small sample text data
00:08:05,2,Word Association Mining and Analysis,1.7,"I've taken away cat and dog from these sentences, so",00:08:00,7,I taken away cat dog sentences
00:08:07,2,Word Association Mining and Analysis,1.7,that you can see just the context.,00:08:05,7,see context
00:08:12,2,Word Association Mining and Analysis,1.7,"Now, of course we can have different perspectives to look at the context.",00:08:08,7,Now course different perspectives look context
00:08:19,2,Word Association Mining and Analysis,1.7,"For example, we can look at what words occur in the left",00:08:13,7,For example look words occur left
00:08:24,2,Word Association Mining and Analysis,1.7,part of this context.,00:08:19,7,part context
00:08:28,2,Word Association Mining and Analysis,1.7,So we can call this left context.,00:08:24,7,So call left context
00:08:34,2,Word Association Mining and Analysis,1.7,What words occur before we see cat or dog?,00:08:28,7,What words occur see cat dog
00:08:39,2,Word Association Mining and Analysis,1.7,"So, you can see in this case, clearly dog and cat have similar left context.",00:08:34,7,So see case clearly dog cat similar left context
00:08:47,2,Word Association Mining and Analysis,1.7,"You generally say his cat or my cat and you say also, my dog and his dog.",00:08:41,7,You generally say cat cat say also dog dog
00:08:52,2,Word Association Mining and Analysis,1.7,So that makes them similar in the left context.,00:08:47,7,So makes similar left context
00:08:58,2,Word Association Mining and Analysis,1.7,"Similarly, if you look at the words that occur after cat and dog,",00:08:53,7,Similarly look words occur cat dog
00:09:03,2,Word Association Mining and Analysis,1.7,"which we can call right context, they are also very similar in this case.",00:08:58,7,call right context also similar case
00:09:07,2,Word Association Mining and Analysis,1.7,"Of course, it's an extreme case, where you only see eats.",00:09:03,7,Of course extreme case see eats
00:09:12,2,Word Association Mining and Analysis,1.7,"And in general, you'll see many other words, of course,",00:09:08,7,And general see many words course
00:09:15,2,Word Association Mining and Analysis,1.7,that can't follow cat and dog.,00:09:12,7,follow cat dog
00:09:21,2,Word Association Mining and Analysis,1.7,You can also even look at the general context.,00:09:17,7,You also even look general context
00:09:24,2,Word Association Mining and Analysis,1.7,And that might include all the words in the sentence or,00:09:21,7,And might include words sentence
00:09:26,2,Word Association Mining and Analysis,1.7,in sentences around this word.,00:09:24,7,sentences around word
00:09:34,2,Word Association Mining and Analysis,1.7,"And even in the general context, you also see similarity between the two words.",00:09:27,7,And even general context also see similarity two words
00:09:41,2,Word Association Mining and Analysis,1.7,So this was just a suggestion that we can discover paradigmatic,00:09:35,7,So suggestion discover paradigmatic
00:09:47,2,Word Association Mining and Analysis,1.7,relation by looking at the similarity of context of words.,00:09:41,7,relation looking similarity context words
00:09:50,2,Word Association Mining and Analysis,1.7,"So, for example, if we think about the following questions.",00:09:47,7,So example think following questions
00:09:54,2,Word Association Mining and Analysis,1.7,How similar are context of cat and context of dog?,00:09:50,7,How similar context cat context dog
00:10:01,2,Word Association Mining and Analysis,1.7,In contrast how similar are context of cat and context of computer?,00:09:56,7,In contrast similar context cat context computer
00:10:07,2,Word Association Mining and Analysis,1.7,"Now, intuitively, we're to imagine the context of cat and",00:10:02,7,Now intuitively imagine context cat
00:10:11,2,Word Association Mining and Analysis,1.7,the context of dog would be more similar than,00:10:07,7,context dog would similar
00:10:16,2,Word Association Mining and Analysis,1.7,the context of cat and context of the computer.,00:10:11,7,context cat context computer
00:10:20,2,Word Association Mining and Analysis,1.7,"That means, in the first case the similarity value would be high,",00:10:16,7,That means first case similarity value would high
00:10:25,2,Word Association Mining and Analysis,1.7,"between the context of cat and dog, where as in the second,",00:10:21,7,context cat dog second
00:10:30,2,Word Association Mining and Analysis,1.7,the similarity between context of cat and computer would be low,00:10:25,7,similarity context cat computer would low
00:10:35,2,Word Association Mining and Analysis,1.7,because they all not having a paradigmatic,00:10:30,7,paradigmatic
00:10:40,2,Word Association Mining and Analysis,1.7,relationship and imagine what words occur after computer in general.,00:10:35,7,relationship imagine words occur computer general
00:10:44,2,Word Association Mining and Analysis,1.7,It would be very different from what words occur after cat.,00:10:40,7,It would different words occur cat
00:10:50,2,Word Association Mining and Analysis,1.7,"So this is the basic idea of what this covering, paradigmatic relation.",00:10:46,7,So basic idea covering paradigmatic relation
00:10:54,2,Word Association Mining and Analysis,1.7,What about the syntagmatic relation?,00:10:52,7,What syntagmatic relation
00:10:58,2,Word Association Mining and Analysis,1.7,"Well, here we're going to explore the correlated occurrences,",00:10:54,7,Well going explore correlated occurrences
00:11:02,2,Word Association Mining and Analysis,1.7,again based on the definition of syntagmatic relation.,00:10:58,7,based definition syntagmatic relation
00:11:05,2,Word Association Mining and Analysis,1.7,Here you see the same sample of text.,00:11:03,7,Here see sample text
00:11:10,2,Word Association Mining and Analysis,1.7,But here we're interested in knowing what other words are correlated,00:11:06,7,But interested knowing words correlated
00:11:14,2,Word Association Mining and Analysis,1.7,with the verb eats and what words can go with eats.,00:11:10,7,verb eats words go eats
00:11:20,2,Word Association Mining and Analysis,1.7,And if you look at the right side of this slide and,00:11:16,7,And look right side slide
00:11:25,2,Word Association Mining and Analysis,1.7,"you see, I've taken away the two words around eats.",00:11:20,7,see I taken away two words around eats
00:11:30,2,Word Association Mining and Analysis,1.7,I've taken away the word to its left and,00:11:27,7,I taken away word left
00:11:33,2,Word Association Mining and Analysis,1.7,also the word to its right in each sentence.,00:11:30,7,also word right sentence
00:11:41,2,Word Association Mining and Analysis,1.7,"And then we ask the question, what words tend to occur to the left of eats?",00:11:35,7,And ask question words tend occur left eats
00:11:47,2,Word Association Mining and Analysis,1.7,And what words tend to occur to the right of eats?,00:11:43,7,And words tend occur right eats
00:11:54,2,Word Association Mining and Analysis,1.7,Now thinking about this question would help us discover syntagmatic,00:11:49,7,Now thinking question would help us discover syntagmatic
00:12:00,2,Word Association Mining and Analysis,1.7,relations because syntagmatic relations essentially captures such correlations.,00:11:54,7,relations syntagmatic relations essentially captures correlations
00:12:07,2,Word Association Mining and Analysis,1.7,"So the important question to ask for syntagmatical relation is,",00:12:03,7,So important question ask syntagmatical relation
00:12:14,2,Word Association Mining and Analysis,1.7,"whenever eats occurs, what other words also tend to occur?",00:12:07,7,whenever eats occurs words also tend occur
00:12:19,2,Word Association Mining and Analysis,1.7,So the question here has to do with whether there,00:12:16,7,So question whether
00:12:23,2,Word Association Mining and Analysis,1.7,are some other words that tend to co-occur together with each.,00:12:19,7,words tend co occur together
00:12:28,2,Word Association Mining and Analysis,1.7,Meaning that whenever you see eats you tend to see the other words.,00:12:23,7,Meaning whenever see eats tend see words
00:12:34,2,Word Association Mining and Analysis,1.7,"And if you don't see eats, probably, you don't see other words often either.",00:12:29,7,And see eats probably see words often either
00:12:40,2,Word Association Mining and Analysis,1.7,So this intuition can help discover syntagmatic relations.,00:12:36,7,So intuition help discover syntagmatic relations
00:12:43,2,Word Association Mining and Analysis,1.7,"Now again, consider example.",00:12:41,7,Now consider example
00:12:48,2,Word Association Mining and Analysis,1.7,How helpful is occurrence of eats for predicting occurrence of meat?,00:12:44,7,How helpful occurrence eats predicting occurrence meat
00:12:53,2,Word Association Mining and Analysis,1.7,"Right. All right, so knowing whether eats occurs",00:12:49,7,Right All right knowing whether eats occurs
00:12:58,2,Word Association Mining and Analysis,1.7,in a sentence would generally help us predict whether meat also occurs indeed.,00:12:53,7,sentence would generally help us predict whether meat also occurs indeed
00:13:01,2,Word Association Mining and Analysis,1.7,"And if we see eats occur in the sentence, and",00:12:58,7,And see eats occur sentence
00:13:05,2,Word Association Mining and Analysis,1.7,that should increase the chance that meat would also occur.,00:13:01,7,increase chance meat would also occur
00:13:12,2,Word Association Mining and Analysis,1.7,"In contrast, if you look at the question in the bottom,",00:13:08,7,In contrast look question bottom
00:13:15,2,Word Association Mining and Analysis,1.7,how helpful is the occurrence of eats for predicting of occurrence of text?,00:13:12,7,helpful occurrence eats predicting occurrence text
00:13:20,2,Word Association Mining and Analysis,1.7,"Because eats and text are not really related, so",00:13:17,7,Because eats text really related
00:13:24,2,Word Association Mining and Analysis,1.7,knowing whether eats occurred in the sentence doesn't,00:13:20,7,knowing whether eats occurred sentence
00:13:30,2,Word Association Mining and Analysis,1.7,"really help us predict the weather, text also occurs in the sentence.",00:13:24,7,really help us predict weather text also occurs sentence
00:13:34,2,Word Association Mining and Analysis,1.7,So this is in contrast to the question about eats and meat.,00:13:30,7,So contrast question eats meat
00:13:38,2,Word Association Mining and Analysis,1.7,This also helps explain that intuition,00:13:35,7,This also helps explain intuition
00:13:43,2,Word Association Mining and Analysis,1.7,behind the methods of what discovering syntagmatic relations.,00:13:38,7,behind methods discovering syntagmatic relations
00:13:49,2,Word Association Mining and Analysis,1.7,Mainly we need to capture the correlation between the occurrences of two words.,00:13:43,7,Mainly need capture correlation occurrences two words
00:13:52,2,Word Association Mining and Analysis,1.7,So to summarize the general ideas for,00:13:50,7,So summarize general ideas
00:13:55,2,Word Association Mining and Analysis,1.7,discovering word associations are the following.,00:13:52,7,discovering word associations following
00:14:02,2,Word Association Mining and Analysis,1.7,"For paradigmatic relation, we present each word by its context.",00:13:56,7,For paradigmatic relation present word context
00:14:04,2,Word Association Mining and Analysis,1.7,And then compute its context similarity.,00:14:02,7,And compute context similarity
00:14:09,2,Word Association Mining and Analysis,1.7,We're going to assume the words that have high context similarity,00:14:04,7,We going assume words high context similarity
00:14:12,2,Word Association Mining and Analysis,1.7,to have paradigmatic relation.,00:14:09,7,paradigmatic relation
00:14:19,2,Word Association Mining and Analysis,1.7,"For syntagmatic relation, we will count how many times two words occur together",00:14:14,7,For syntagmatic relation count many times two words occur together
00:14:25,2,Word Association Mining and Analysis,1.7,"in a context, which can be a sentence, a paragraph, or a document even.",00:14:19,7,context sentence paragraph document even
00:14:28,2,Word Association Mining and Analysis,1.7,And we're going to compare,00:14:25,7,And going compare
00:14:31,2,Word Association Mining and Analysis,1.7,their co-occurrences with their individual occurrences.,00:14:28,7,co occurrences individual occurrences
00:14:36,2,Word Association Mining and Analysis,1.7,We're going to assume words with high co-occurrences but,00:14:33,7,We going assume words high co occurrences
00:14:42,2,Word Association Mining and Analysis,1.7,relatively low individual occurrences to have syntagmatic relations,00:14:36,7,relatively low individual occurrences syntagmatic relations
00:14:46,2,Word Association Mining and Analysis,1.7,because they attempt to occur together and they don't usually occur alone.,00:14:42,7,attempt occur together usually occur alone
00:14:51,2,Word Association Mining and Analysis,1.7,Note that the paradigmatic relation and the syntagmatic relation,00:14:46,7,Note paradigmatic relation syntagmatic relation
00:14:57,2,Word Association Mining and Analysis,1.7,are actually closely related in that paradigmatically,00:14:51,7,actually closely related paradigmatically
00:15:02,2,Word Association Mining and Analysis,1.7,related words tend to have syntagmatic relation with the same word.,00:14:57,7,related words tend syntagmatic relation word
00:15:05,2,Word Association Mining and Analysis,1.7,"They tend to be associated with the same word, and",00:15:02,7,They tend associated word
00:15:10,2,Word Association Mining and Analysis,1.7,that suggests that we can also do join the discovery of the two relations.,00:15:05,7,suggests also join discovery two relations
00:15:15,2,Word Association Mining and Analysis,1.7,So these general ideas can be implemented in many different ways.,00:15:10,7,So general ideas implemented many different ways
00:15:19,2,Word Association Mining and Analysis,1.7,"And the course won't cover all of them, but",00:15:15,7,And course cover
00:15:24,2,Word Association Mining and Analysis,1.7,we will cover at least some of the methods that are effective for,00:15:19,7,cover least methods effective
00:15:27,2,Word Association Mining and Analysis,1.7,discovering these relations.,00:15:24,7,discovering relations
00:00:05,2,Natural Language Content Analysis- Part 2,1.4,[SOUND],00:00:00,4,SOUND
00:00:13,2,Natural Language Content Analysis- Part 2,1.4,So here are some specific examples of what,00:00:10,4,So specific examples
00:00:15,2,Natural Language Content Analysis- Part 2,1.4,we can't do today and,00:00:13,4,today
00:00:21,2,Natural Language Content Analysis- Part 2,1.4,part of speech tagging is still not easy to do 100% correctly.,00:00:15,4,part speech tagging still easy 100 correctly
00:00:27,2,Natural Language Content Analysis- Part 2,1.4,"So in the example, he turned off the highway verses he turned off the fan and",00:00:21,4,So example turned highway verses turned fan
00:00:33,2,Natural Language Content Analysis- Part 2,1.4,the two offs actually have somewhat a differentness in their active,00:00:27,4,two offs actually somewhat differentness active
00:00:39,2,Natural Language Content Analysis- Part 2,1.4,categories and also its very difficult to get a complete the parsing correct.,00:00:33,4,categories also difficult get complete parsing correct
00:00:44,2,Natural Language Content Analysis- Part 2,1.4,"Again, the example, a man saw a boy with a telescope can actually",00:00:39,4,Again example man saw boy telescope actually
00:00:48,2,Natural Language Content Analysis- Part 2,1.4,be very difficult to parse depending on the context.,00:00:44,4,difficult parse depending context
00:00:52,2,Natural Language Content Analysis- Part 2,1.4,Precise deep semantic analysis is also very hard.,00:00:48,4,Precise deep semantic analysis also hard
00:00:55,2,Natural Language Content Analysis- Part 2,1.4,"For example, to define the meaning of own,",00:00:52,4,For example define meaning
00:01:01,2,Natural Language Content Analysis- Part 2,1.4,"precisely is very difficult in the sentence, like John owns a restaurant.",00:00:55,4,precisely difficult sentence like John owns restaurant
00:01:04,2,Natural Language Content Analysis- Part 2,1.4,So the state of the off can be summarized as follows.,00:01:01,4,So state summarized follows
00:01:05,2,Natural Language Content Analysis- Part 2,1.4,Robust and,00:01:04,4,Robust
00:01:11,2,Natural Language Content Analysis- Part 2,1.4,general NLP tends to be shallow while a deep understanding does not scale up.,00:01:05,4,general NLP tends shallow deep understanding scale
00:01:18,2,Natural Language Content Analysis- Part 2,1.4,"For this reason in this course, the techniques that we cover are in",00:01:12,4,For reason course techniques cover
00:01:23,2,Natural Language Content Analysis- Part 2,1.4,"general, shallow techniques for analyzing text data and",00:01:18,4,general shallow techniques analyzing text data
00:01:29,2,Natural Language Content Analysis- Part 2,1.4,mining text data and they are generally based on statistical analysis.,00:01:23,4,mining text data generally based statistical analysis
00:01:34,2,Natural Language Content Analysis- Part 2,1.4,So there are robust and general and they are in,00:01:29,4,So robust general
00:01:39,2,Natural Language Content Analysis- Part 2,1.4,the in category of shallow analysis.,00:01:36,4,category shallow analysis
00:01:44,2,Natural Language Content Analysis- Part 2,1.4,So such techniques have the advantage of being able to be,00:01:39,4,So techniques advantage able
00:01:49,2,Natural Language Content Analysis- Part 2,1.4,applied to any text data in any natural about any topic.,00:01:44,4,applied text data natural topic
00:01:55,2,Natural Language Content Analysis- Part 2,1.4,"But the downside is that, they don't give use a deeper understanding of text.",00:01:49,4,But downside give use deeper understanding text
00:01:59,2,Natural Language Content Analysis- Part 2,1.4,"For that, we have to rely on deeper natural language analysis.",00:01:55,4,For rely deeper natural language analysis
00:02:05,2,Natural Language Content Analysis- Part 2,1.4,That typically would require a human effort to annotate,00:02:00,4,That typically would require human effort annotate
00:02:10,2,Natural Language Content Analysis- Part 2,1.4,a lot of examples of analysis that would like to do and then computers can use,00:02:05,4,lot examples analysis would like computers use
00:02:16,2,Natural Language Content Analysis- Part 2,1.4,machine learning techniques and learn from these training examples to do the task.,00:02:10,4,machine learning techniques learn training examples task
00:02:21,2,Natural Language Content Analysis- Part 2,1.4,"So in practical applications, we generally combine the two kinds of techniques",00:02:16,4,So practical applications generally combine two kinds techniques
00:02:29,2,Natural Language Content Analysis- Part 2,1.4,with the general statistical and methods as a backbone as the basis.,00:02:21,4,general statistical methods backbone basis
00:02:32,2,Natural Language Content Analysis- Part 2,1.4,These can be applied to any text data.,00:02:29,4,These applied text data
00:02:37,2,Natural Language Content Analysis- Part 2,1.4,"And on top of that, we're going to use humans to, and you take more data and",00:02:32,4,And top going use humans take data
00:02:42,2,Natural Language Content Analysis- Part 2,1.4,"to use supervised machine learning to do some tasks as well as we can,",00:02:37,4,use supervised machine learning tasks well
00:02:48,2,Natural Language Content Analysis- Part 2,1.4,especially for those important tasks to bring humans into the loop,00:02:42,4,especially important tasks bring humans loop
00:02:55,2,Natural Language Content Analysis- Part 2,1.4,to analyze text data more precisely.,00:02:48,4,analyze text data precisely
00:03:00,2,Natural Language Content Analysis- Part 2,1.4,But this course will cover the general statistical approaches,00:02:55,4,But course cover general statistical approaches
00:03:04,2,Natural Language Content Analysis- Part 2,1.4,"that generally, don't require much human effort.",00:03:00,4,generally require much human effort
00:03:09,2,Natural Language Content Analysis- Part 2,1.4,"So they're practically, more useful that some of the deeper",00:03:04,4,So practically useful deeper
00:03:16,2,Natural Language Content Analysis- Part 2,1.4,analysis techniques that require a lot of human effort to annotate the text today.,00:03:09,4,analysis techniques require lot human effort annotate text today
00:03:21,2,Natural Language Content Analysis- Part 2,1.4,"So to summarize, the main points we take are first NLP",00:03:16,4,So summarize main points take first NLP
00:03:24,2,Natural Language Content Analysis- Part 2,1.4,is the foundation for text mining.,00:03:21,4,foundation text mining
00:03:27,2,Natural Language Content Analysis- Part 2,1.4,"So obviously, the better we can understand the text data,",00:03:24,4,So obviously better understand text data
00:03:29,2,Natural Language Content Analysis- Part 2,1.4,the better we can do text mining.,00:03:27,4,better text mining
00:03:34,2,Natural Language Content Analysis- Part 2,1.4,Computers today are far from being able to understand the natural language.,00:03:30,4,Computers today far able understand natural language
00:03:38,2,Natural Language Content Analysis- Part 2,1.4,Deep NLP requires common sense knowledge and inferences.,00:03:34,4,Deep NLP requires common sense knowledge inferences
00:03:42,2,Natural Language Content Analysis- Part 2,1.4,"Thus, only working for very limited domains not feasible for",00:03:38,4,Thus working limited domains feasible
00:03:44,2,Natural Language Content Analysis- Part 2,1.4,large scale text mining.,00:03:42,4,large scale text mining
00:03:50,2,Natural Language Content Analysis- Part 2,1.4,Shallow NLP based on statistical methods can be done in large scale and,00:03:44,4,Shallow NLP based statistical methods done large scale
00:03:52,2,Natural Language Content Analysis- Part 2,1.4,is the main topic of this course and,00:03:50,4,main topic course
00:03:56,2,Natural Language Content Analysis- Part 2,1.4,they are generally applicable to a lot of applications.,00:03:52,4,generally applicable lot applications
00:04:02,2,Natural Language Content Analysis- Part 2,1.4,"They are in some sense also, more useful techniques.",00:03:56,4,They sense also useful techniques
00:04:06,2,Natural Language Content Analysis- Part 2,1.4,"In practice, we use statistical NLP as the basis and",00:04:02,4,In practice use statistical NLP basis
00:04:11,2,Natural Language Content Analysis- Part 2,1.4,we'll have humans for help as needed in various ways.,00:04:06,4,humans help needed various ways
00:00:06,2,Overview Text Mining and Analytics- Part 1,1.1,[SOUND] In,00:00:00,1,SOUND In
00:00:11,2,Overview Text Mining and Analytics- Part 1,1.1,this lecture we give an overview of Text Mining and Analytics.,00:00:06,1,lecture give overview Text Mining Analytics
00:00:19,2,Overview Text Mining and Analytics- Part 1,1.1,"First, let's define the term text mining, and the term text analytics.",00:00:13,1,First let define term text mining term text analytics
00:00:24,2,Overview Text Mining and Analytics- Part 1,1.1,The title of this course is called Text Mining and Analytics.,00:00:19,1,The title course called Text Mining Analytics
00:00:31,2,Overview Text Mining and Analytics- Part 1,1.1,"But the two terms text mining, and text analytics are actually roughly the same.",00:00:25,1,But two terms text mining text analytics actually roughly
00:00:36,2,Overview Text Mining and Analytics- Part 1,1.1,"So we are not really going to really distinguish them, and",00:00:32,1,So really going really distinguish
00:00:38,2,Overview Text Mining and Analytics- Part 1,1.1,we're going to use them interchangeably.,00:00:36,1,going use interchangeably
00:00:42,2,Overview Text Mining and Analytics- Part 1,1.1,But the reason that we have chosen to use,00:00:38,1,But reason chosen use
00:00:47,2,Overview Text Mining and Analytics- Part 1,1.1,"both terms in the title is because there is also some subtle difference,",00:00:42,1,terms title also subtle difference
00:00:51,2,Overview Text Mining and Analytics- Part 1,1.1,if you look at the two phrases literally.,00:00:47,1,look two phrases literally
00:00:55,2,Overview Text Mining and Analytics- Part 1,1.1,Mining emphasizes more on the process.,00:00:52,1,Mining emphasizes process
00:01:01,2,Overview Text Mining and Analytics- Part 1,1.1,So it gives us a error rate medical view of the problem.,00:00:55,1,So gives us error rate medical view problem
00:01:06,2,Overview Text Mining and Analytics- Part 1,1.1,"Analytics, on the other hand emphasizes more on the result,",00:01:01,1,Analytics hand emphasizes result
00:01:09,2,Overview Text Mining and Analytics- Part 1,1.1,or having a problem in mind.,00:01:07,1,problem mind
00:01:14,2,Overview Text Mining and Analytics- Part 1,1.1,We are going to look at text data to help us solve a problem.,00:01:09,1,We going look text data help us solve problem
00:01:19,2,Overview Text Mining and Analytics- Part 1,1.1,"But again as I said, we can treat these two terms roughly the same.",00:01:16,1,But I said treat two terms roughly
00:01:24,2,Overview Text Mining and Analytics- Part 1,1.1,And I think in the literature you probably will find the same.,00:01:21,1,And I think literature probably find
00:01:27,2,Overview Text Mining and Analytics- Part 1,1.1,So we're not going to really distinguish that in the course.,00:01:24,1,So going really distinguish course
00:01:35,2,Overview Text Mining and Analytics- Part 1,1.1,Both text mining and text analytics mean that we,00:01:29,1,Both text mining text analytics mean
00:01:40,2,Overview Text Mining and Analytics- Part 1,1.1,"want to turn text data into high quality information, or actionable knowledge.",00:01:35,1,want turn text data high quality information actionable knowledge
00:01:44,2,Overview Text Mining and Analytics- Part 1,1.1,"So in both cases, we",00:01:42,1,So cases
00:01:50,2,Overview Text Mining and Analytics- Part 1,1.1,have the problem of dealing with a lot of text data and we hope to.,00:01:45,1,problem dealing lot text data hope
00:01:56,2,Overview Text Mining and Analytics- Part 1,1.1,Turn these text data into something more useful to us than the raw text data.,00:01:50,1,Turn text data something useful us raw text data
00:02:00,2,Overview Text Mining and Analytics- Part 1,1.1,And here we distinguish two different results.,00:01:57,1,And distinguish two different results
00:02:04,2,Overview Text Mining and Analytics- Part 1,1.1,"One is high-quality information, the other is actionable knowledge.",00:02:00,1,One high quality information actionable knowledge
00:02:08,2,Overview Text Mining and Analytics- Part 1,1.1,Sometimes the boundary between the two is not so clear.,00:02:05,1,Sometimes boundary two clear
00:02:11,2,Overview Text Mining and Analytics- Part 1,1.1,But I also want to say a little bit about,00:02:09,1,But I also want say little bit
00:02:17,2,Overview Text Mining and Analytics- Part 1,1.1,these two different angles of the result of text field mining.,00:02:12,1,two different angles result text field mining
00:02:22,2,Overview Text Mining and Analytics- Part 1,1.1,"In the case of high quality information, we refer to more",00:02:19,1,In case high quality information refer
00:02:27,2,Overview Text Mining and Analytics- Part 1,1.1,concise information about the topic.,00:02:22,1,concise information topic
00:02:34,2,Overview Text Mining and Analytics- Part 1,1.1,Which might be much easier for humans to digest than the raw text data.,00:02:28,1,Which might much easier humans digest raw text data
00:02:37,2,Overview Text Mining and Analytics- Part 1,1.1,"For example, you might face a lot of reviews of a product.",00:02:34,1,For example might face lot reviews product
00:02:42,2,Overview Text Mining and Analytics- Part 1,1.1,A more concise form of information would be a very concise summary,00:02:38,1,A concise form information would concise summary
00:02:46,2,Overview Text Mining and Analytics- Part 1,1.1,of the major opinions about the features of the product.,00:02:42,1,major opinions features product
00:02:50,2,Overview Text Mining and Analytics- Part 1,1.1,"Positive about, let's say battery life of a laptop.",00:02:46,1,Positive let say battery life laptop
00:02:58,2,Overview Text Mining and Analytics- Part 1,1.1,Now this kind of results are very useful to help people digest the text data.,00:02:53,1,Now kind results useful help people digest text data
00:03:05,2,Overview Text Mining and Analytics- Part 1,1.1,And so this is to minimize a human effort in consuming text data in some sense.,00:02:59,1,And minimize human effort consuming text data sense
00:03:09,2,Overview Text Mining and Analytics- Part 1,1.1,The other kind of output is actually more knowledge.,00:03:06,1,The kind output actually knowledge
00:03:15,2,Overview Text Mining and Analytics- Part 1,1.1,Here we emphasize the utility of the information or,00:03:09,1,Here emphasize utility information
00:03:17,2,Overview Text Mining and Analytics- Part 1,1.1,knowledge we discover from text data.,00:03:15,1,knowledge discover text data
00:03:23,2,Overview Text Mining and Analytics- Part 1,1.1,"It's actionable knowledge for some decision problem, or some actions to take.",00:03:18,1,It actionable knowledge decision problem actions take
00:03:31,2,Overview Text Mining and Analytics- Part 1,1.1,"For example, we might be able to determine which product is more appealing to us,",00:03:24,1,For example might able determine product appealing us
00:03:36,2,Overview Text Mining and Analytics- Part 1,1.1,or a better choice for a shocking decision.,00:03:31,1,better choice shocking decision
00:03:43,2,Overview Text Mining and Analytics- Part 1,1.1,"Now, such an outcome could be called actionable knowledge,",00:03:38,1,Now outcome could called actionable knowledge
00:03:49,2,Overview Text Mining and Analytics- Part 1,1.1,"because a consumer can take the knowledge and make a decision, and act on it.",00:03:43,1,consumer take knowledge make decision act
00:03:55,2,Overview Text Mining and Analytics- Part 1,1.1,"So, in this case text mining supplies knowledge for optimal decision making.",00:03:49,1,So case text mining supplies knowledge optimal decision making
00:03:59,2,Overview Text Mining and Analytics- Part 1,1.1,"But again, the two are not so clearly distinguished, so",00:03:55,1,But two clearly distinguished
00:04:03,2,Overview Text Mining and Analytics- Part 1,1.1,we don't necessarily have to make a distinction.,00:03:59,1,necessarily make distinction
00:04:09,2,Overview Text Mining and Analytics- Part 1,1.1,"Text mining is also related to text retrieval,",00:04:06,1,Text mining also related text retrieval
00:04:14,2,Overview Text Mining and Analytics- Part 1,1.1,which is a essential component in many text mining systems.,00:04:09,1,essential component many text mining systems
00:04:20,2,Overview Text Mining and Analytics- Part 1,1.1,"Now, text retrieval refers to finding relevant information from",00:04:15,1,Now text retrieval refers finding relevant information
00:04:22,2,Overview Text Mining and Analytics- Part 1,1.1,a large amount of text data.,00:04:20,1,large amount text data
00:04:30,2,Overview Text Mining and Analytics- Part 1,1.1,So I've taught another separate book on text retrieval and search engines.,00:04:24,1,So I taught another separate book text retrieval search engines
00:04:34,2,Overview Text Mining and Analytics- Part 1,1.1,Where we discussed various techniques for text retrieval.,00:04:31,1,Where discussed various techniques text retrieval
00:04:41,2,Overview Text Mining and Analytics- Part 1,1.1,"If you have taken that book, and you will find some overlap.",00:04:36,1,If taken book find overlap
00:04:46,2,Overview Text Mining and Analytics- Part 1,1.1,And it will be useful To know the background of text retrieval,00:04:42,1,And useful To know background text retrieval
00:04:50,2,Overview Text Mining and Analytics- Part 1,1.1,of understanding some of the topics in text mining.,00:04:46,1,understanding topics text mining
00:04:54,2,Overview Text Mining and Analytics- Part 1,1.1,"But, if you have not taken that book,",00:04:51,1,But taken book
00:04:59,2,Overview Text Mining and Analytics- Part 1,1.1,"it's also fine because in this book on text mining and analytics, we're",00:04:54,1,also fine book text mining analytics
00:05:03,2,Overview Text Mining and Analytics- Part 1,1.1,going to repeat some of the key concepts that are relevant for text mining.,00:04:59,1,going repeat key concepts relevant text mining
00:05:06,2,Overview Text Mining and Analytics- Part 1,1.1,But they're at the high level and,00:05:03,1,But high level
00:05:10,2,Overview Text Mining and Analytics- Part 1,1.1,they also explain the relation between text retrieval and text mining.,00:05:06,1,also explain relation text retrieval text mining
00:05:18,2,Overview Text Mining and Analytics- Part 1,1.1,Text retrieval is very useful for text mining in two ways.,00:05:12,1,Text retrieval useful text mining two ways
00:05:23,2,Overview Text Mining and Analytics- Part 1,1.1,"First, text retrieval can be a preprocessor for text mining.",00:05:18,1,First text retrieval preprocessor text mining
00:05:27,2,Overview Text Mining and Analytics- Part 1,1.1,Meaning that it can help us turn big text data into,00:05:23,1,Meaning help us turn big text data
00:05:32,2,Overview Text Mining and Analytics- Part 1,1.1,a relatively small amount of most relevant text data.,00:05:27,1,relatively small amount relevant text data
00:05:35,2,Overview Text Mining and Analytics- Part 1,1.1,Which is often what's needed for solving a particular problem.,00:05:32,1,Which often needed solving particular problem
00:05:41,2,Overview Text Mining and Analytics- Part 1,1.1,"And in this sense, text retrieval also helps minimize human effort.",00:05:36,1,And sense text retrieval also helps minimize human effort
00:05:46,2,Overview Text Mining and Analytics- Part 1,1.1,Text retrieval is also needed for knowledge provenance.,00:05:43,1,Text retrieval also needed knowledge provenance
00:05:50,2,Overview Text Mining and Analytics- Part 1,1.1,And this roughly corresponds to the interpretation of text,00:05:46,1,And roughly corresponds interpretation text
00:05:56,2,Overview Text Mining and Analytics- Part 1,1.1,mining as turning text data into actionable knowledge.,00:05:50,1,mining turning text data actionable knowledge
00:05:58,2,Overview Text Mining and Analytics- Part 1,1.1,"Once we find the patterns in text data, or",00:05:56,1,Once find patterns text data
00:06:04,2,Overview Text Mining and Analytics- Part 1,1.1,"actionable knowledge, we generally would have to verify the knowledge.",00:05:58,1,actionable knowledge generally would verify knowledge
00:06:06,2,Overview Text Mining and Analytics- Part 1,1.1,By looking at the original text data.,00:06:04,1,By looking original text data
00:06:11,2,Overview Text Mining and Analytics- Part 1,1.1,"So the users would have to have some text retrieval support, go back to the original",00:06:06,1,So users would text retrieval support go back original
00:06:16,2,Overview Text Mining and Analytics- Part 1,1.1,text data to interpret the pattern or to better understand an analogy or,00:06:11,1,text data interpret pattern better understand analogy
00:06:19,2,Overview Text Mining and Analytics- Part 1,1.1,to verify whether a pattern is really reliable.,00:06:16,1,verify whether pattern really reliable
00:06:23,2,Overview Text Mining and Analytics- Part 1,1.1,"So this is a high level introduction to the concept of text mining,",00:06:19,1,So high level introduction concept text mining
00:06:29,2,Overview Text Mining and Analytics- Part 1,1.1,and the relationship between text mining and retrieval.,00:06:23,1,relationship text mining retrieval
00:06:36,2,Overview Text Mining and Analytics- Part 1,1.1,"Next, let's talk about text data as a special kind of data.",00:06:32,1,Next let talk text data special kind data
00:06:45,2,Overview Text Mining and Analytics- Part 1,1.1,Now it's interesting to view text data as data,00:06:39,1,Now interesting view text data data
00:06:51,2,Overview Text Mining and Analytics- Part 1,1.1,generated by humans as subjective sensors.,00:06:45,1,generated humans subjective sensors
00:07:03,2,Overview Text Mining and Analytics- Part 1,1.1,"So, this slide shows an analogy between text data and non-text data.",00:06:53,1,So slide shows analogy text data non text data
00:07:07,2,Overview Text Mining and Analytics- Part 1,1.1,And between humans as subjective sensors and,00:07:03,1,And humans subjective sensors
00:07:13,2,Overview Text Mining and Analytics- Part 1,1.1,"physical sensors, such as a network sensor or a thermometer.",00:07:07,1,physical sensors network sensor thermometer
00:07:21,2,Overview Text Mining and Analytics- Part 1,1.1,So in general a sensor would monitor the real world in some way.,00:07:16,1,So general sensor would monitor real world way
00:07:26,2,Overview Text Mining and Analytics- Part 1,1.1,"It would sense some signal from the real world, and",00:07:21,1,It would sense signal real world
00:07:32,2,Overview Text Mining and Analytics- Part 1,1.1,"then would report the signal as data, in various forms.",00:07:26,1,would report signal data various forms
00:07:38,2,Overview Text Mining and Analytics- Part 1,1.1,"For example, a thermometer would watch the temperature of real world and",00:07:32,1,For example thermometer would watch temperature real world
00:07:43,2,Overview Text Mining and Analytics- Part 1,1.1,then we report the temperature being a particular format.,00:07:38,1,report temperature particular format
00:07:49,2,Overview Text Mining and Analytics- Part 1,1.1,"Similarly, a geo sensor would sense the location and then report.",00:07:44,1,Similarly geo sensor would sense location report
00:07:53,2,Overview Text Mining and Analytics- Part 1,1.1,"The location specification, for",00:07:49,1,The location specification
00:07:57,2,Overview Text Mining and Analytics- Part 1,1.1,"example, in the form of longitude value and latitude value.",00:07:53,1,example form longitude value latitude value
00:08:02,2,Overview Text Mining and Analytics- Part 1,1.1,"A network sends over the monitor network traffic,",00:07:57,1,A network sends monitor network traffic
00:08:04,2,Overview Text Mining and Analytics- Part 1,1.1,or activities in the network and are reported.,00:08:02,1,activities network reported
00:08:09,2,Overview Text Mining and Analytics- Part 1,1.1,Some digital format of data.,00:08:04,1,Some digital format data
00:08:16,2,Overview Text Mining and Analytics- Part 1,1.1,Similarly we can think of humans as subjective sensors.,00:08:09,1,Similarly think humans subjective sensors
00:08:22,2,Overview Text Mining and Analytics- Part 1,1.1,That will observe the real world and from some perspective.,00:08:16,1,That observe real world perspective
00:08:28,2,Overview Text Mining and Analytics- Part 1,1.1,And then humans will express what they have observed in the form of text data.,00:08:22,1,And humans express observed form text data
00:08:33,2,Overview Text Mining and Analytics- Part 1,1.1,"So, in this sense, human is actually a subjective sensor that would also",00:08:28,1,So sense human actually subjective sensor would also
00:08:36,2,Overview Text Mining and Analytics- Part 1,1.1,sense what's happening in the world and,00:08:33,1,sense happening world
00:08:43,2,Overview Text Mining and Analytics- Part 1,1.1,"then express what's observed in the form of data, in this case, text data.",00:08:36,1,express observed form data case text data
00:08:47,2,Overview Text Mining and Analytics- Part 1,1.1,"Now, looking at the text data in this way has an advantage of being",00:08:43,1,Now looking text data way advantage
00:08:50,2,Overview Text Mining and Analytics- Part 1,1.1,able to integrate all types of data together.,00:08:47,1,able integrate types data together
00:08:54,2,Overview Text Mining and Analytics- Part 1,1.1,And that's indeed needed in most data mining problems.,00:08:50,1,And indeed needed data mining problems
00:09:01,2,Overview Text Mining and Analytics- Part 1,1.1,So here we are looking at the general problem of data mining.,00:08:56,1,So looking general problem data mining
00:09:07,2,Overview Text Mining and Analytics- Part 1,1.1,And in general we would Be dealing with a lot of data,00:09:02,1,And general would Be dealing lot data
00:09:11,2,Overview Text Mining and Analytics- Part 1,1.1,about our world that are related to a problem.,00:09:07,1,world related problem
00:09:17,2,Overview Text Mining and Analytics- Part 1,1.1,And in general it will be dealing with both non-text data and text data.,00:09:11,1,And general dealing non text data text data
00:09:21,2,Overview Text Mining and Analytics- Part 1,1.1,And of course the non-text data are usually produced by physical senses.,00:09:17,1,And course non text data usually produced physical senses
00:09:26,2,Overview Text Mining and Analytics- Part 1,1.1,And those non-text data can be also of different formats.,00:09:21,1,And non text data also different formats
00:09:30,2,Overview Text Mining and Analytics- Part 1,1.1,"Numerical data, categorical, or relational data,",00:09:27,1,Numerical data categorical relational data
00:09:33,2,Overview Text Mining and Analytics- Part 1,1.1,or multi-media data like video or speech.,00:09:30,1,multi media data like video speech
00:09:41,2,Overview Text Mining and Analytics- Part 1,1.1,"So, these non text data are often very important in some problems.",00:09:36,1,So non text data often important problems
00:09:45,2,Overview Text Mining and Analytics- Part 1,1.1,"But text data is also very important,",00:09:41,1,But text data also important
00:09:50,2,Overview Text Mining and Analytics- Part 1,1.1,mostly because they contain a lot of symmetrical content.,00:09:45,1,mostly contain lot symmetrical content
00:09:55,2,Overview Text Mining and Analytics- Part 1,1.1,"And they often contain knowledge about the users,",00:09:50,1,And often contain knowledge users
00:09:58,2,Overview Text Mining and Analytics- Part 1,1.1,especially preferences and opinions of users.,00:09:55,1,especially preferences opinions users
00:10:07,2,Overview Text Mining and Analytics- Part 1,1.1,"So, but by treating text data as the data observed from human sensors,",00:10:01,1,So treating text data data observed human sensors
00:10:14,2,Overview Text Mining and Analytics- Part 1,1.1,we can treat all this data together in the same framework.,00:10:07,1,treat data together framework
00:10:18,2,Overview Text Mining and Analytics- Part 1,1.1,"So the data mining problem is basically to turn such data,",00:10:14,1,So data mining problem basically turn data
00:10:22,2,Overview Text Mining and Analytics- Part 1,1.1,turn all the data in your actionable knowledge to that we can take advantage,00:10:18,1,turn data actionable knowledge take advantage
00:10:26,2,Overview Text Mining and Analytics- Part 1,1.1,of it to change the real world of course for better.,00:10:22,1,change real world course better
00:10:31,2,Overview Text Mining and Analytics- Part 1,1.1,So this means the data mining problem is,00:10:26,1,So means data mining problem
00:10:37,2,Overview Text Mining and Analytics- Part 1,1.1,basically taking a lot of data as input and giving actionable knowledge as output.,00:10:31,1,basically taking lot data input giving actionable knowledge output
00:10:42,2,Overview Text Mining and Analytics- Part 1,1.1,"Inside of the data mining module, you can also see",00:10:37,1,Inside data mining module also see
00:10:46,2,Overview Text Mining and Analytics- Part 1,1.1,we have a number of different kind of mining algorithms.,00:10:42,1,number different kind mining algorithms
00:10:49,2,Overview Text Mining and Analytics- Part 1,1.1,"And this is because, for different kinds of data,",00:10:46,1,And different kinds data
00:10:55,2,Overview Text Mining and Analytics- Part 1,1.1,we generally need different algorithms for mining the data.,00:10:49,1,generally need different algorithms mining data
00:10:57,2,Overview Text Mining and Analytics- Part 1,1.1,"For example,",00:10:56,1,For example
00:11:01,2,Overview Text Mining and Analytics- Part 1,1.1,video data might require computer vision to understand video content.,00:10:57,1,video data might require computer vision understand video content
00:11:06,2,Overview Text Mining and Analytics- Part 1,1.1,And that would facilitate the more effective mining.,00:11:01,1,And would facilitate effective mining
00:11:11,2,Overview Text Mining and Analytics- Part 1,1.1,And we also have a lot of general algorithms that are applicable,00:11:06,1,And also lot general algorithms applicable
00:11:16,2,Overview Text Mining and Analytics- Part 1,1.1,"to all kinds of data and those algorithms, of course, are very useful.",00:11:11,1,kinds data algorithms course useful
00:11:19,2,Overview Text Mining and Analytics- Part 1,1.1,"Although, for a particular kind of data,",00:11:16,1,Although particular kind data
00:11:23,2,Overview Text Mining and Analytics- Part 1,1.1,we generally want to also develop a special algorithm.,00:11:19,1,generally want also develop special algorithm
00:11:27,2,Overview Text Mining and Analytics- Part 1,1.1,So this course will cover specialized algorithms that,00:11:23,1,So course cover specialized algorithms
00:11:31,2,Overview Text Mining and Analytics- Part 1,1.1,are particularly useful for mining text data.,00:11:27,1,particularly useful mining text data
00:00:03,1,Text Mining and Analytics,,[SOUND],00:00:00,1,SOUND
00:00:07,1,Text Mining and Analytics,,Hello.,00:00:03,1,Hello
00:00:11,1,Text Mining and Analytics,,Welcome to the course Text Mining and Analytics.,00:00:07,1,Welcome course Text Mining Analytics
00:00:13,1,Text Mining and Analytics,,My name is ChengXiang Zhai.,00:00:11,1,My name ChengXiang Zhai
00:00:15,1,Text Mining and Analytics,,"I have a nickname, Cheng.",00:00:13,1,I nickname Cheng
00:00:19,1,Text Mining and Analytics,,I am a professor of the Department of Computer Science at the University of,00:00:15,1,I professor Department Computer Science University
00:00:24,1,Text Mining and Analytics,,Illinois at Urbana-Champaign.,00:00:19,1,Illinois Urbana Champaign
00:00:27,1,Text Mining and Analytics,,This course is a part of a data mining specialization,00:00:24,1,This course part data mining specialization
00:00:32,1,Text Mining and Analytics,,offered by the University of Illinois at Urbana-Champaign.,00:00:27,1,offered University Illinois Urbana Champaign
00:00:37,1,Text Mining and Analytics,,"In addition to this course, there are four other courses offered by",00:00:32,1,In addition course four courses offered
00:00:44,1,Text Mining and Analytics,,"Professor Jiawei Han, Professor John Hart and me, followed by",00:00:39,1,Professor Jiawei Han Professor John Hart followed
00:00:49,1,Text Mining and Analytics,,a capstone project course that all of us will teach together.,00:00:44,1,capstone project course us teach together
00:00:58,1,Text Mining and Analytics,,"This course is particularly related to another course in the specialization,",00:00:51,1,This course particularly related another course specialization
00:01:04,1,Text Mining and Analytics,,mainly text retrieval and search engines in that both courses are about text data.,00:00:58,1,mainly text retrieval search engines courses text data
00:01:12,1,Text Mining and Analytics,,"In contrast, pattern discovery and cluster analysis are about",00:01:07,1,In contrast pattern discovery cluster analysis
00:01:18,1,Text Mining and Analytics,,algorithms more applicable to all kinds of data in general.,00:01:12,1,algorithms applicable kinds data general
00:01:23,1,Text Mining and Analytics,,The visualization course is also relatively general in that the techniques,00:01:18,1,The visualization course also relatively general techniques
00:01:25,1,Text Mining and Analytics,,can be applied to all kinds of data.,00:01:23,1,applied kinds data
00:01:33,1,Text Mining and Analytics,,This course addresses a pressing need for harnessing big text data.,00:01:28,1,This course addresses pressing need harnessing big text data
00:01:39,1,Text Mining and Analytics,,"Text data has been growing dramatically recently,",00:01:35,1,Text data growing dramatically recently
00:01:44,1,Text Mining and Analytics,,mostly because of the advance of technologies deployed on the web,00:01:39,1,mostly advance technologies deployed web
00:01:48,1,Text Mining and Analytics,,that would enable people to quickly generate text data.,00:01:44,1,would enable people quickly generate text data
00:01:55,1,Text Mining and Analytics,,"So, I listed some of the examples on this slide",00:01:50,1,So I listed examples slide
00:02:01,1,Text Mining and Analytics,,that can show a variety of text data that are available today.,00:01:57,1,show variety text data available today
00:02:05,1,Text Mining and Analytics,,"For example, if you think about the data on the internet, on the web,",00:02:01,1,For example think data internet web
00:02:11,1,Text Mining and Analytics,,everyday we are seeing many web pages being created.,00:02:07,1,everyday seeing many web pages created
00:02:17,1,Text Mining and Analytics,,Blogs are another kind of new text data that,00:02:13,1,Blogs another kind new text data
00:02:19,1,Text Mining and Analytics,,are being generated quickly by people.,00:02:17,1,generated quickly people
00:02:23,1,Text Mining and Analytics,,Anyone can write a blog article on the web.,00:02:19,1,Anyone write blog article web
00:02:25,1,Text Mining and Analytics,,New articles of course have always been,00:02:23,1,New articles course always
00:02:30,1,Text Mining and Analytics,,a main kind of text data that being generated everyday.,00:02:25,1,main kind text data generated everyday
00:02:34,1,Text Mining and Analytics,,Emails are yet another kind of text data.,00:02:31,1,Emails yet another kind text data
00:02:41,1,Text Mining and Analytics,,And literature is also representing a large portion of text data.,00:02:34,1,And literature also representing large portion text data
00:02:46,1,Text Mining and Analytics,,It's also especially very important because of the high quality,00:02:41,1,It also especially important high quality
00:02:50,1,Text Mining and Analytics,,in the data.,00:02:46,1,data
00:02:55,1,Text Mining and Analytics,,"That is, we encode our knowledge about the word",00:02:50,1,That encode knowledge word
00:03:03,1,Text Mining and Analytics,,using text data represented by all the literature articles.,00:02:55,1,using text data represented literature articles
00:03:06,1,Text Mining and Analytics,,It's a vast amount of knowledge of,00:03:03,1,It vast amount knowledge
00:03:12,1,Text Mining and Analytics,,all the text and data in these literature articles.,00:03:08,1,text data literature articles
00:03:21,1,Text Mining and Analytics,,Twitter is another representative text data representing social media.,00:03:14,1,Twitter another representative text data representing social media
00:03:23,1,Text Mining and Analytics,,Of course there are forums as well.,00:03:21,1,Of course forums well
00:03:27,1,Text Mining and Analytics,,People are generating tweets very quickly,00:03:24,1,People generating tweets quickly
00:03:32,1,Text Mining and Analytics,,indeed as we are speaking perhaps many people have already written many tweets.,00:03:27,1,indeed speaking perhaps many people already written many tweets
00:03:35,1,Text Mining and Analytics,,"So, as you can see there are all kinds of text data",00:03:32,1,So see kinds text data
00:03:37,1,Text Mining and Analytics,,that are being generated very quickly.,00:03:35,1,generated quickly
00:03:42,1,Text Mining and Analytics,,Now these text data present some challenges for people.,00:03:38,1,Now text data present challenges people
00:03:49,1,Text Mining and Analytics,,It's very hard for anyone to digest all the text data quickly.,00:03:43,1,It hard anyone digest text data quickly
00:03:55,1,Text Mining and Analytics,,"In particular, it's impossible for scientists to read all of the for",00:03:49,1,In particular impossible scientists read
00:03:58,1,Text Mining and Analytics,,example or for anyone to read all the tweets.,00:03:55,1,example anyone read tweets
00:04:06,1,Text Mining and Analytics,,So there's a need for tools to help people digest text data more efficiently.,00:04:01,1,So need tools help people digest text data efficiently
00:04:12,1,Text Mining and Analytics,,There is also another interesting opportunity,00:04:09,1,There also another interesting opportunity
00:04:16,1,Text Mining and Analytics,,"provided by such big text data, and that is it's possible to leverage",00:04:12,1,provided big text data possible leverage
00:04:20,1,Text Mining and Analytics,,the amount of text data to discover interesting patterns to,00:04:16,1,amount text data discover interesting patterns
00:04:25,1,Text Mining and Analytics,,turn text data into actionable knowledge that can be useful for decision making.,00:04:20,1,turn text data actionable knowledge useful decision making
00:04:32,1,Text Mining and Analytics,,"So for example, product managers may be interested",00:04:27,1,So example product managers may interested
00:04:37,1,Text Mining and Analytics,,"in knowing the feedback of customers about their products,",00:04:32,1,knowing feedback customers products
00:04:42,1,Text Mining and Analytics,,knowing how well their products are being received as,00:04:37,1,knowing well products received
00:04:46,1,Text Mining and Analytics,,compared with the products of competitors.,00:04:42,1,compared products competitors
00:04:48,1,Text Mining and Analytics,,This can be a good opportunity for,00:04:46,1,This good opportunity
00:04:54,1,Text Mining and Analytics,,leveraging text data as we have seen a lot of reviews of product on the web.,00:04:48,1,leveraging text data seen lot reviews product web
00:04:59,1,Text Mining and Analytics,,So if we can develop a master text mining techniques to tap into such,00:04:54,1,So develop master text mining techniques tap
00:05:07,1,Text Mining and Analytics,,"a [INAUDIBLE] to extract the knowledge and opinions of people about these products,",00:04:59,1,INAUDIBLE extract knowledge opinions people products
00:05:12,1,Text Mining and Analytics,,then we can help these product managers to gain business intelligence or,00:05:07,1,help product managers gain business intelligence
00:05:16,1,Text Mining and Analytics,,to essentially feedback from their customers.,00:05:12,1,essentially feedback customers
00:05:20,1,Text Mining and Analytics,,"In scientific research, for example,",00:05:18,1,In scientific research example
00:05:26,1,Text Mining and Analytics,,"scientists are interested in knowing the trends of research topics, knowing",00:05:20,1,scientists interested knowing trends research topics knowing
00:05:33,1,Text Mining and Analytics,,about what related fields have discovered.,00:05:29,1,related fields discovered
00:05:37,1,Text Mining and Analytics,,This problem is especially important in biology research as well.,00:05:33,1,This problem especially important biology research well
00:05:41,1,Text Mining and Analytics,,"Different communities tend to use different terminologies, yet",00:05:37,1,Different communities tend use different terminologies yet
00:05:44,1,Text Mining and Analytics,,they're starting very similar problems.,00:05:41,1,starting similar problems
00:05:49,1,Text Mining and Analytics,,So how can we integrate the knowledge that is covered in different communities,00:05:44,1,So integrate knowledge covered different communities
00:05:52,1,Text Mining and Analytics,,to help study a particular problem?,00:05:49,1,help study particular problem
00:05:56,1,Text Mining and Analytics,,"It's very important, and it can speed up scientific discovery.",00:05:52,1,It important speed scientific discovery
00:06:02,1,Text Mining and Analytics,,So there are many such examples where we can leverage the text data,00:05:57,1,So many examples leverage text data
00:06:05,1,Text Mining and Analytics,,to discover useable knowledge to optimize our decision.,00:06:02,1,discover useable knowledge optimize decision
00:06:08,1,Text Mining and Analytics,,The main techniques for,00:06:06,1,The main techniques
00:06:13,1,Text Mining and Analytics,,harnessing big text data are text retrieval and text mining.,00:06:08,1,harnessing big text data text retrieval text mining
00:06:17,1,Text Mining and Analytics,,"So these are two very much related technologies.Yet,",00:06:13,1,So two much related technologies Yet
00:06:20,1,Text Mining and Analytics,,they have somewhat different purposes.,00:06:17,1,somewhat different purposes
00:06:29,1,Text Mining and Analytics,,These two kinds of techniques are covered in the tool in this specialization.,00:06:20,1,These two kinds techniques covered tool specialization
00:06:34,1,Text Mining and Analytics,,"So, text retrieval on search engines covers text retrieval,",00:06:29,1,So text retrieval search engines covers text retrieval
00:06:39,1,Text Mining and Analytics,,and this is necessary to turn big text data into,00:06:34,1,necessary turn big text data
00:06:45,1,Text Mining and Analytics,,"a much smaller but more relevant text data, which are often the data that",00:06:39,1,much smaller relevant text data often data
00:06:51,1,Text Mining and Analytics,,we need to handle a particular problem or to optimize a particular decision.,00:06:45,1,need handle particular problem optimize particular decision
00:06:57,1,Text Mining and Analytics,,This course covers text mining which is a second step in this pipeline,00:06:51,1,This course covers text mining second step pipeline
00:07:04,1,Text Mining and Analytics,,that can be used to further process the small amount of relevant data,00:06:57,1,used process small amount relevant data
00:07:10,1,Text Mining and Analytics,,to extract the knowledge or to help people digest the text data easily.,00:07:04,1,extract knowledge help people digest text data easily
00:07:13,1,Text Mining and Analytics,,"So the two courses are clearly related, in fact,",00:07:10,1,So two courses clearly related fact
00:07:19,1,Text Mining and Analytics,,some of the techniques are shared by both text retrieval and text mining.,00:07:13,1,techniques shared text retrieval text mining
00:07:24,1,Text Mining and Analytics,,"If you have already taken the text retrieval course, then you might see",00:07:19,1,If already taken text retrieval course might see
00:07:31,1,Text Mining and Analytics,,"some of the content being repeated in this text mining course, although",00:07:24,1,content repeated text mining course although
00:07:35,1,Text Mining and Analytics,,we'll be talking about the techniques from a very different perspective.,00:07:31,1,talking techniques different perspective
00:07:39,1,Text Mining and Analytics,,"If you have not taken the text retrieval course,",00:07:35,1,If taken text retrieval course
00:07:45,1,Text Mining and Analytics,,it's also fine because this course is self-contained and,00:07:39,1,also fine course self contained
00:07:50,1,Text Mining and Analytics,,you can certainly understand all of the materials without a problem.,00:07:45,1,certainly understand materials without problem
00:07:55,1,Text Mining and Analytics,,"Of course, you might find it beneficial to take both courses and",00:07:50,1,Of course might find beneficial take courses
00:08:00,1,Text Mining and Analytics,,that will give you a very complete set of skills to handle big text data.,00:07:55,1,give complete set skills handle big text data
00:00:03,2,Natural Language Content Analysis- Part 1,1.3,[SOUND],00:00:00,3,SOUND
00:00:13,2,Natural Language Content Analysis- Part 1,1.3,This lecture is about natural language,00:00:09,3,This lecture natural language
00:00:16,2,Natural Language Content Analysis- Part 1,1.3,content analysis.,00:00:13,3,content analysis
00:00:21,2,Natural Language Content Analysis- Part 1,1.3,Natural language content analysis is the foundation of text mining.,00:00:16,3,Natural language content analysis foundation text mining
00:00:23,2,Natural Language Content Analysis- Part 1,1.3,So we're going to first talk about this.,00:00:21,3,So going first talk
00:00:26,2,Natural Language Content Analysis- Part 1,1.3,"And in particular,",00:00:24,3,And particular
00:00:31,2,Natural Language Content Analysis- Part 1,1.3,natural language processing with a factor how we can present text data.,00:00:26,3,natural language processing factor present text data
00:00:38,2,Natural Language Content Analysis- Part 1,1.3,And this determines what algorithms can be used to analyze and mine text data.,00:00:33,3,And determines algorithms used analyze mine text data
00:00:44,2,Natural Language Content Analysis- Part 1,1.3,We're going to take a look at the basic concepts in natural language first.,00:00:40,3,We going take look basic concepts natural language first
00:00:48,2,Natural Language Content Analysis- Part 1,1.3,And I'm going to explain these concepts,00:00:46,3,And I going explain concepts
00:00:52,2,Natural Language Content Analysis- Part 1,1.3,using a similar example that you've all seen here.,00:00:48,3,using similar example seen
00:00:55,2,Natural Language Content Analysis- Part 1,1.3,A dog is chasing a boy on the playground.,00:00:52,3,A dog chasing boy playground
00:00:58,2,Natural Language Content Analysis- Part 1,1.3,Now this is a very simple sentence.,00:00:55,3,Now simple sentence
00:01:01,2,Natural Language Content Analysis- Part 1,1.3,When we read such a sentence we don't have to think,00:00:58,3,When read sentence think
00:01:05,2,Natural Language Content Analysis- Part 1,1.3,about it to get the meaning of it.,00:01:01,3,get meaning
00:01:09,2,Natural Language Content Analysis- Part 1,1.3,"But when a computer has to understand the sentence,",00:01:05,3,But computer understand sentence
00:01:12,2,Natural Language Content Analysis- Part 1,1.3,the computer has to go through several steps.,00:01:09,3,computer go several steps
00:01:16,2,Natural Language Content Analysis- Part 1,1.3,"First, the computer needs to know what are the words,",00:01:13,3,First computer needs know words
00:01:18,2,Natural Language Content Analysis- Part 1,1.3,how to segment the words in English.,00:01:16,3,segment words English
00:01:22,2,Natural Language Content Analysis- Part 1,1.3,"And this is very easy, we can just look at the space.",00:01:18,3,And easy look space
00:01:26,2,Natural Language Content Analysis- Part 1,1.3,"And then the computer will need the know the categories of these words,",00:01:22,3,And computer need know categories words
00:01:27,2,Natural Language Content Analysis- Part 1,1.3,syntactical categories.,00:01:26,3,syntactical categories
00:01:34,2,Natural Language Content Analysis- Part 1,1.3,"So for example, dog is a noun, chasing's a verb, boy is another noun etc.",00:01:27,3,So example dog noun chasing verb boy another noun etc
00:01:37,2,Natural Language Content Analysis- Part 1,1.3,And this is called a Lexical analysis.,00:01:34,3,And called Lexical analysis
00:01:41,2,Natural Language Content Analysis- Part 1,1.3,"In particular, tagging these words with these syntactic categories",00:01:37,3,In particular tagging words syntactic categories
00:01:43,2,Natural Language Content Analysis- Part 1,1.3,is called a part-of-speech tagging.,00:01:41,3,called part speech tagging
00:01:48,2,Natural Language Content Analysis- Part 1,1.3,After that the computer also needs to figure out the relationship between,00:01:45,3,After computer also needs figure relationship
00:01:49,2,Natural Language Content Analysis- Part 1,1.3,these words.,00:01:48,3,words
00:01:53,2,Natural Language Content Analysis- Part 1,1.3,So a and dog would form a noun phrase.,00:01:49,3,So dog would form noun phrase
00:01:57,2,Natural Language Content Analysis- Part 1,1.3,"On the playground would be a prepositional phrase, etc.",00:01:53,3,On playground would prepositional phrase etc
00:02:01,2,Natural Language Content Analysis- Part 1,1.3,And there is certain way for them to be connected together in order for,00:01:57,3,And certain way connected together order
00:02:03,2,Natural Language Content Analysis- Part 1,1.3,them to create meaning.,00:02:01,3,create meaning
00:02:06,2,Natural Language Content Analysis- Part 1,1.3,Some other combinations may not make sense.,00:02:03,3,Some combinations may make sense
00:02:12,2,Natural Language Content Analysis- Part 1,1.3,"And this is called syntactical parsing, or",00:02:07,3,And called syntactical parsing
00:02:17,2,Natural Language Content Analysis- Part 1,1.3,"syntactical analysis, parsing of a natural language sentence.",00:02:12,3,syntactical analysis parsing natural language sentence
00:02:21,2,Natural Language Content Analysis- Part 1,1.3,The outcome is a parse tree that you are seeing here.,00:02:17,3,The outcome parse tree seeing
00:02:24,2,Natural Language Content Analysis- Part 1,1.3,"That tells us the structure of the sentence, so",00:02:21,3,That tells us structure sentence
00:02:27,2,Natural Language Content Analysis- Part 1,1.3,that we know how we can interpret this sentence.,00:02:24,3,know interpret sentence
00:02:29,2,Natural Language Content Analysis- Part 1,1.3,But this is not semantics yet.,00:02:27,3,But semantics yet
00:02:34,2,Natural Language Content Analysis- Part 1,1.3,So in order to get the meaning we would have to map these phrases and,00:02:29,3,So order get meaning would map phrases
00:02:39,2,Natural Language Content Analysis- Part 1,1.3,these structures into some real world antithesis that we have in our mind.,00:02:34,3,structures real world antithesis mind
00:02:45,2,Natural Language Content Analysis- Part 1,1.3,"So dog is a concept that we know, and boy is a concept that we know.",00:02:39,3,So dog concept know boy concept know
00:02:50,2,Natural Language Content Analysis- Part 1,1.3,So connecting these phrases that we know is understanding.,00:02:45,3,So connecting phrases know understanding
00:02:58,2,Natural Language Content Analysis- Part 1,1.3,"Now for a computer, would have to formally represent these entities by using symbols.",00:02:52,3,Now computer would formally represent entities using symbols
00:03:03,2,Natural Language Content Analysis- Part 1,1.3,"So dog, d1 means d1 is a dog.",00:02:58,3,So dog d1 means d1 dog
00:03:09,2,Natural Language Content Analysis- Part 1,1.3,"Boy, b1 means b1 refers to a boy etc.",00:03:04,3,Boy b1 means b1 refers boy etc
00:03:13,2,Natural Language Content Analysis- Part 1,1.3,And also represents the chasing action as a predicate.,00:03:09,3,And also represents chasing action predicate
00:03:18,2,Natural Language Content Analysis- Part 1,1.3,"So, chasing is a predicate here with",00:03:13,3,So chasing predicate
00:03:23,2,Natural Language Content Analysis- Part 1,1.3,"three arguments, d1, b1, and p1.",00:03:18,3,three arguments d1 b1 p1
00:03:25,2,Natural Language Content Analysis- Part 1,1.3,Which is playground.,00:03:23,3,Which playground
00:03:31,2,Natural Language Content Analysis- Part 1,1.3,So this formal rendition of the semantics of this sentence.,00:03:25,3,So formal rendition semantics sentence
00:03:35,2,Natural Language Content Analysis- Part 1,1.3,"Once we reach that level of understanding, we might also make inferences.",00:03:31,3,Once reach level understanding might also make inferences
00:03:42,2,Natural Language Content Analysis- Part 1,1.3,"For example, if we assume there's a rule that says if someone's being chased then",00:03:35,3,For example assume rule says someone chased
00:03:48,2,Natural Language Content Analysis- Part 1,1.3,"the person can get scared, then we can infer this boy might be scared.",00:03:42,3,person get scared infer boy might scared
00:03:52,2,Natural Language Content Analysis- Part 1,1.3,"This is the inferred meaning, based on additional knowledge.",00:03:48,3,This inferred meaning based additional knowledge
00:03:58,2,Natural Language Content Analysis- Part 1,1.3,"And finally, we might even further infer",00:03:52,3,And finally might even infer
00:04:06,2,Natural Language Content Analysis- Part 1,1.3,"what this sentence is requesting,",00:03:58,3,sentence requesting
00:04:12,2,Natural Language Content Analysis- Part 1,1.3,"or why the person who say it in a sentence, is saying the sentence.",00:04:06,3,person say sentence saying sentence
00:04:18,2,Natural Language Content Analysis- Part 1,1.3,"And so, this has to do with purpose of saying the sentence.",00:04:12,3,And purpose saying sentence
00:04:24,2,Natural Language Content Analysis- Part 1,1.3,This is called speech act analysis or pragmatic analysis.,00:04:18,3,This called speech act analysis pragmatic analysis
00:04:27,2,Natural Language Content Analysis- Part 1,1.3,Which first to the use of language.,00:04:24,3,Which first use language
00:04:32,2,Natural Language Content Analysis- Part 1,1.3,"So, in this case a person saying this may be reminding another person to",00:04:27,3,So case person saying may reminding another person
00:04:34,2,Natural Language Content Analysis- Part 1,1.3,bring back the dog.,00:04:32,3,bring back dog
00:04:42,2,Natural Language Content Analysis- Part 1,1.3,"So this means when saying a sentence, the person actually takes an action.",00:04:35,3,So means saying sentence person actually takes action
00:04:44,2,Natural Language Content Analysis- Part 1,1.3,So the action here is to make a request.,00:04:42,3,So action make request
00:04:51,2,Natural Language Content Analysis- Part 1,1.3,"Now, this slide clearly shows that in order to really understand",00:04:46,3,Now slide clearly shows order really understand
00:04:55,2,Natural Language Content Analysis- Part 1,1.3,a sentence there are a lot of things that a computer has to do.,00:04:51,3,sentence lot things computer
00:05:00,2,Natural Language Content Analysis- Part 1,1.3,"Now, in general it's very hard for a computer will do everything,",00:04:55,3,Now general hard computer everything
00:05:04,2,Natural Language Content Analysis- Part 1,1.3,especially if you would want it to do everything correctly.,00:05:00,3,especially would want everything correctly
00:05:06,2,Natural Language Content Analysis- Part 1,1.3,This is very difficult.,00:05:04,3,This difficult
00:05:11,2,Natural Language Content Analysis- Part 1,1.3,"Now, the main reason why natural language processing is very difficult,",00:05:08,3,Now main reason natural language processing difficult
00:05:14,2,Natural Language Content Analysis- Part 1,1.3,it's because it's designed it will make human communications efficient.,00:05:11,3,designed make human communications efficient
00:05:20,2,Natural Language Content Analysis- Part 1,1.3,"As a result, for example, with only a lot of common sense knowledge.",00:05:15,3,As result example lot common sense knowledge
00:05:25,2,Natural Language Content Analysis- Part 1,1.3,"Because we assume all of us have this knowledge,",00:05:21,3,Because assume us knowledge
00:05:28,2,Natural Language Content Analysis- Part 1,1.3,there's no need to encode this knowledge.,00:05:25,3,need encode knowledge
00:05:31,2,Natural Language Content Analysis- Part 1,1.3,That makes communication efficient.,00:05:29,3,That makes communication efficient
00:05:37,2,Natural Language Content Analysis- Part 1,1.3,"We also keep a lot of ambiguities, like, ambiguities of words.",00:05:32,3,We also keep lot ambiguities like ambiguities words
00:05:45,2,Natural Language Content Analysis- Part 1,1.3,"And this is again, because we assume we have the ability to disambiguate the word.",00:05:39,3,And assume ability disambiguate word
00:05:48,2,Natural Language Content Analysis- Part 1,1.3,"So, there's no problem with having the same word to mean",00:05:45,3,So problem word mean
00:05:50,2,Natural Language Content Analysis- Part 1,1.3,possibly different things in different context.,00:05:48,3,possibly different things different context
00:05:55,2,Natural Language Content Analysis- Part 1,1.3,Yet for a computer this would be very difficult,00:05:52,3,Yet computer would difficult
00:06:00,2,Natural Language Content Analysis- Part 1,1.3,because a computer does not have the common sense knowledge that we do.,00:05:55,3,computer common sense knowledge
00:06:03,2,Natural Language Content Analysis- Part 1,1.3,So the computer will be confused indeed.,00:06:00,3,So computer confused indeed
00:06:06,2,Natural Language Content Analysis- Part 1,1.3,And this makes it hard for natural language processing.,00:06:03,3,And makes hard natural language processing
00:06:09,2,Natural Language Content Analysis- Part 1,1.3,"Indeed, it makes it very hard for",00:06:06,3,Indeed makes hard
00:06:15,2,Natural Language Content Analysis- Part 1,1.3,every step in the slide that I showed you earlier.,00:06:09,3,every step slide I showed earlier
00:06:19,2,Natural Language Content Analysis- Part 1,1.3,Ambiguity is a main killer.,00:06:16,3,Ambiguity main killer
00:06:22,2,Natural Language Content Analysis- Part 1,1.3,"Meaning that in every step there are multiple choices,",00:06:19,3,Meaning every step multiple choices
00:06:26,2,Natural Language Content Analysis- Part 1,1.3,and the computer would have to decide whats the right choice and,00:06:22,3,computer would decide whats right choice
00:06:30,2,Natural Language Content Analysis- Part 1,1.3,that decision can be very difficult as you will see also in a moment.,00:06:26,3,decision difficult see also moment
00:06:32,2,Natural Language Content Analysis- Part 1,1.3,"And in general,",00:06:31,3,And general
00:06:37,2,Natural Language Content Analysis- Part 1,1.3,we need common sense reasoning in order to fully understand the natural language.,00:06:32,3,need common sense reasoning order fully understand natural language
00:06:40,2,Natural Language Content Analysis- Part 1,1.3,And computers today don't yet have that.,00:06:37,3,And computers today yet
00:06:42,2,Natural Language Content Analysis- Part 1,1.3,That's why it's very hard for,00:06:40,3,That hard
00:06:47,2,Natural Language Content Analysis- Part 1,1.3,computers to precisely understand the natural language at this point.,00:06:42,3,computers precisely understand natural language point
00:06:51,2,Natural Language Content Analysis- Part 1,1.3,So here are some specific examples of challenges.,00:06:48,3,So specific examples challenges
00:06:53,2,Natural Language Content Analysis- Part 1,1.3,Think about the world-level ambiguity.,00:06:51,3,Think world level ambiguity
00:06:56,2,Natural Language Content Analysis- Part 1,1.3,"A word like design can be a noun or a verb, so",00:06:53,3,A word like design noun verb
00:06:59,2,Natural Language Content Analysis- Part 1,1.3,we've got ambiguous part of speech tag.,00:06:56,3,got ambiguous part speech tag
00:07:06,2,Natural Language Content Analysis- Part 1,1.3,"Root also has multiple meanings, it can be of mathematical sense,",00:07:00,3,Root also multiple meanings mathematical sense
00:07:10,2,Natural Language Content Analysis- Part 1,1.3,"like in the square of, or can be root of a plant.",00:07:06,3,like square root plant
00:07:17,2,Natural Language Content Analysis- Part 1,1.3,Syntactic ambiguity refers to different interpretations,00:07:12,3,Syntactic ambiguity refers different interpretations
00:07:21,2,Natural Language Content Analysis- Part 1,1.3,of a sentence in terms structures.,00:07:19,3,sentence terms structures
00:07:23,2,Natural Language Content Analysis- Part 1,1.3,"So for example,",00:07:21,3,So example
00:07:26,2,Natural Language Content Analysis- Part 1,1.3,natural language processing can actually be interpreted in two ways.,00:07:23,3,natural language processing actually interpreted two ways
00:07:33,2,Natural Language Content Analysis- Part 1,1.3,So one is the ordinary meaning that we,00:07:28,3,So one ordinary meaning
00:07:38,2,Natural Language Content Analysis- Part 1,1.3,will be getting as we're talking about this topic.,00:07:33,3,getting talking topic
00:07:41,2,Natural Language Content Analysis- Part 1,1.3,"So, it's processing of natural language.",00:07:38,3,So processing natural language
00:07:44,2,Natural Language Content Analysis- Part 1,1.3,But there's is also another possible interpretation,00:07:41,3,But also another possible interpretation
00:07:47,2,Natural Language Content Analysis- Part 1,1.3,which is to say language processing is natural.,00:07:44,3,say language processing natural
00:07:53,2,Natural Language Content Analysis- Part 1,1.3,"Now we don't generally have this problem, but imagine for the computer to determine",00:07:48,3,Now generally problem imagine computer determine
00:07:56,2,Natural Language Content Analysis- Part 1,1.3,"the structure, the computer would have to make a choice between the two.",00:07:53,3,structure computer would make choice two
00:08:03,2,Natural Language Content Analysis- Part 1,1.3,Another classic example is a man saw a boy with a telescope.,00:07:59,3,Another classic example man saw boy telescope
00:08:10,2,Natural Language Content Analysis- Part 1,1.3,And this ambiguity lies in the question who had the telescope?,00:08:03,3,And ambiguity lies question telescope
00:08:13,2,Natural Language Content Analysis- Part 1,1.3,This is called a prepositional phrase attachment ambiguity.,00:08:10,3,This called prepositional phrase attachment ambiguity
00:08:20,2,Natural Language Content Analysis- Part 1,1.3,Meaning where to attach this prepositional phrase with the telescope.,00:08:14,3,Meaning attach prepositional phrase telescope
00:08:22,2,Natural Language Content Analysis- Part 1,1.3,Should it modify the boy?,00:08:20,3,Should modify boy
00:08:28,2,Natural Language Content Analysis- Part 1,1.3,"Or should it be modifying, saw, the verb.",00:08:22,3,Or modifying saw verb
00:08:31,2,Natural Language Content Analysis- Part 1,1.3,Another problem is anaphora resolution.,00:08:28,3,Another problem anaphora resolution
00:08:35,2,Natural Language Content Analysis- Part 1,1.3,In John persuaded Bill to buy a TV for himself.,00:08:31,3,In John persuaded Bill buy TV
00:08:37,2,Natural Language Content Analysis- Part 1,1.3,Does himself refer to John or Bill?,00:08:35,3,Does refer John Bill
00:08:41,2,Natural Language Content Analysis- Part 1,1.3,Presupposition is another difficulty.,00:08:39,3,Presupposition another difficulty
00:08:45,2,Natural Language Content Analysis- Part 1,1.3,"He has quit smoking implies that he smoked before, and",00:08:41,3,He quit smoking implies smoked
00:08:50,2,Natural Language Content Analysis- Part 1,1.3,we need to have such a knowledge in order to understand the languages.,00:08:45,3,need knowledge order understand languages
00:08:57,2,Natural Language Content Analysis- Part 1,1.3,"Because of these problems, the state of the art natural language processing",00:08:52,3,Because problems state art natural language processing
00:09:01,2,Natural Language Content Analysis- Part 1,1.3,techniques can not do anything perfectly.,00:08:57,3,techniques anything perfectly
00:09:04,2,Natural Language Content Analysis- Part 1,1.3,"Even for the simplest part of speech tagging,",00:09:01,3,Even simplest part speech tagging
00:09:07,2,Natural Language Content Analysis- Part 1,1.3,we still can not solve the whole problem.,00:09:04,3,still solve whole problem
00:09:12,2,Natural Language Content Analysis- Part 1,1.3,"The accuracy that are listed here, which is about 97%,",00:09:07,3,The accuracy listed 97
00:09:16,2,Natural Language Content Analysis- Part 1,1.3,was just taken from some studies earlier.,00:09:12,3,taken studies earlier
00:09:22,2,Natural Language Content Analysis- Part 1,1.3,And these studies obviously have to be using particular data sets so,00:09:17,3,And studies obviously using particular data sets
00:09:27,2,Natural Language Content Analysis- Part 1,1.3,the numbers here are not really meaningful if you,00:09:22,3,numbers really meaningful
00:09:33,2,Natural Language Content Analysis- Part 1,1.3,take it out of the context of the data set that are used for evaluation.,00:09:27,3,take context data set used evaluation
00:09:39,2,Natural Language Content Analysis- Part 1,1.3,"But I show these numbers mainly to give you some sense about the accuracy,",00:09:33,3,But I show numbers mainly give sense accuracy
00:09:42,2,Natural Language Content Analysis- Part 1,1.3,or how well we can do things like this.,00:09:39,3,well things like
00:09:47,2,Natural Language Content Analysis- Part 1,1.3,It doesn't mean any data set accuracy would be precisely 97%.,00:09:42,3,It mean data set accuracy would precisely 97
00:09:52,2,Natural Language Content Analysis- Part 1,1.3,"But, in general, we can do parsing speech tagging fairly well although not perfect.",00:09:47,3,But general parsing speech tagging fairly well although perfect
00:09:59,2,Natural Language Content Analysis- Part 1,1.3,"Parsing would be more difficult, but for partial parsing, meaning to get some",00:09:53,3,Parsing would difficult partial parsing meaning get
00:10:04,2,Natural Language Content Analysis- Part 1,1.3,"phrases correct, we can probably achieve 90% or better accuracy.",00:09:59,3,phrases correct probably achieve 90 better accuracy
00:10:12,2,Natural Language Content Analysis- Part 1,1.3,"But to get the complete parse tree correctly is still very, very difficult.",00:10:06,3,But get complete parse tree correctly still difficult
00:10:18,2,Natural Language Content Analysis- Part 1,1.3,"For semantic analysis, we can also do some aspects of semantic analysis,",00:10:13,3,For semantic analysis also aspects semantic analysis
00:10:22,2,Natural Language Content Analysis- Part 1,1.3,"particularly, extraction of entities and relations.",00:10:18,3,particularly extraction entities relations
00:10:27,2,Natural Language Content Analysis- Part 1,1.3,"For example, recognizing this is the person, that's a location, and",00:10:22,3,For example recognizing person location
00:10:33,2,Natural Language Content Analysis- Part 1,1.3,this person and that person met in some place etc.,00:10:27,3,person person met place etc
00:10:36,2,Natural Language Content Analysis- Part 1,1.3,We can also do word sense to some extent.,00:10:33,3,We also word sense extent
00:10:45,2,Natural Language Content Analysis- Part 1,1.3,The occurrence of root in this sentence refers to the mathematical sense etc.,00:10:38,3,The occurrence root sentence refers mathematical sense etc
00:10:49,2,Natural Language Content Analysis- Part 1,1.3,Sentiment analysis is another aspect of semantic analysis that we can do.,00:10:45,3,Sentiment analysis another aspect semantic analysis
00:10:55,2,Natural Language Content Analysis- Part 1,1.3,That means we can tag the senses as generally positive when,00:10:50,3,That means tag senses generally positive
00:11:00,2,Natural Language Content Analysis- Part 1,1.3,it's talking about the product or talking about the person.,00:10:55,3,talking product talking person
00:11:08,2,Natural Language Content Analysis- Part 1,1.3,"Inference, however, is very hard, and we generally cannot do that for",00:11:02,3,Inference however hard generally cannot
00:11:14,2,Natural Language Content Analysis- Part 1,1.3,any big domain and if it's only feasible for a very limited domain.,00:11:08,3,big domain feasible limited domain
00:11:18,2,Natural Language Content Analysis- Part 1,1.3,And that's a generally difficult problem in artificial intelligence.,00:11:14,3,And generally difficult problem artificial intelligence
00:11:21,2,Natural Language Content Analysis- Part 1,1.3,Speech act analysis is also very difficult and,00:11:18,3,Speech act analysis also difficult
00:11:26,2,Natural Language Content Analysis- Part 1,1.3,we can only do this probably for very specialized cases.,00:11:21,3,probably specialized cases
00:11:32,2,Natural Language Content Analysis- Part 1,1.3,And with a lot of help from humans to annotate enough data for,00:11:26,3,And lot help humans annotate enough data
00:11:34,2,Natural Language Content Analysis- Part 1,1.3,the computers to learn from.,00:11:32,3,computers learn
00:11:38,2,Natural Language Content Analysis- Part 1,1.3,So the slide also shows that,00:11:36,3,So slide also shows
00:11:44,2,Natural Language Content Analysis- Part 1,1.3,computers are far from being able to understand natural language precisely.,00:11:38,3,computers far able understand natural language precisely
00:11:50,2,Natural Language Content Analysis- Part 1,1.3,And that also explains why the text mining problem is difficult.,00:11:44,3,And also explains text mining problem difficult
00:11:54,2,Natural Language Content Analysis- Part 1,1.3,Because we cannot rely on mechanical approaches or,00:11:50,3,Because cannot rely mechanical approaches
00:11:58,2,Natural Language Content Analysis- Part 1,1.3,computational methods to understand the language precisely.,00:11:54,3,computational methods understand language precisely
00:12:04,2,Natural Language Content Analysis- Part 1,1.3,"Therefore, we have to use whatever we have today.",00:11:58,3,Therefore use whatever today
00:12:10,2,Natural Language Content Analysis- Part 1,1.3,A particular statistical machine learning method of statistical analysis methods,00:12:04,3,A particular statistical machine learning method statistical analysis methods
00:12:16,2,Natural Language Content Analysis- Part 1,1.3,to try to get as much meaning out from the text as possible.,00:12:10,3,try get much meaning text possible
00:12:19,2,Natural Language Content Analysis- Part 1,1.3,"And, later you will see that there are actually",00:12:16,3,And later see actually
00:12:25,2,Natural Language Content Analysis- Part 1,1.3,many such algorithms that can indeed extract,00:12:20,3,many algorithms indeed extract
00:12:30,2,Natural Language Content Analysis- Part 1,1.3,interesting model from text even though we cannot really fully understand it.,00:12:25,3,interesting model text even though cannot really fully understand
00:12:36,2,Natural Language Content Analysis- Part 1,1.3,Meaning of all the natural language sentences precisely.,00:12:30,3,Meaning natural language sentences precisely
00:00:02,3,Topic Mining and Analysis- Term as Topic,2.2,[MUSIC],00:00:00,2,MUSIC
00:00:11,3,Topic Mining and Analysis- Term as Topic,2.2,This lecture is about topic mining and analysis.,00:00:07,2,This lecture topic mining analysis
00:00:17,3,Topic Mining and Analysis- Term as Topic,2.2,We're going to talk about using a term as topic.,00:00:12,2,We going talk using term topic
00:00:20,3,Topic Mining and Analysis- Term as Topic,2.2,This is a slide that you have seen in a earlier lecture,00:00:17,2,This slide seen earlier lecture
00:00:25,3,Topic Mining and Analysis- Term as Topic,2.2,where we define the task of topic mining and analysis.,00:00:20,2,define task topic mining analysis
00:00:30,3,Topic Mining and Analysis- Term as Topic,2.2,"We also raised the question, how do we exactly define the topic of theta?",00:00:25,2,We also raised question exactly define topic theta
00:00:36,3,Topic Mining and Analysis- Term as Topic,2.2,"So in this lecture, we're going to offer one way to define it, and",00:00:31,2,So lecture going offer one way define
00:00:37,3,Topic Mining and Analysis- Term as Topic,2.2,that's our initial idea.,00:00:36,2,initial idea
00:00:40,3,Topic Mining and Analysis- Term as Topic,2.2,Our idea here is defining a topic simply as a term.,00:00:37,2,Our idea defining topic simply term
00:00:44,3,Topic Mining and Analysis- Term as Topic,2.2,A term can be a word or a phrase.,00:00:42,2,A term word phrase
00:00:49,3,Topic Mining and Analysis- Term as Topic,2.2,"And in general, we can use these terms to describe topics.",00:00:45,2,And general use terms describe topics
00:00:54,3,Topic Mining and Analysis- Term as Topic,2.2,So our first thought is just to define a topic as one term.,00:00:49,2,So first thought define topic one term
00:00:58,3,Topic Mining and Analysis- Term as Topic,2.2,"For example, we might have terms like sports, travel, or science,",00:00:54,2,For example might terms like sports travel science
00:00:59,3,Topic Mining and Analysis- Term as Topic,2.2,as you see here.,00:00:58,2,see
00:01:02,3,Topic Mining and Analysis- Term as Topic,2.2,"Now if we define a topic in this way,",00:00:59,2,Now define topic way
00:01:09,3,Topic Mining and Analysis- Term as Topic,2.2,we can then analyze the coverage of such topics in each document.,00:01:02,2,analyze coverage topics document
00:01:10,3,Topic Mining and Analysis- Term as Topic,2.2,"Here for example,",00:01:09,2,Here example
00:01:15,3,Topic Mining and Analysis- Term as Topic,2.2,we might want to discover to what extent document one covers sports.,00:01:10,2,might want discover extent document one covers sports
00:01:21,3,Topic Mining and Analysis- Term as Topic,2.2,And we found that 30% of the content of document one is about sports.,00:01:15,2,And found 30 content document one sports
00:01:23,3,Topic Mining and Analysis- Term as Topic,2.2,"And 12% is about the travel, etc.",00:01:21,2,And 12 travel etc
00:01:28,3,Topic Mining and Analysis- Term as Topic,2.2,We might also discover document two does not cover sports at all.,00:01:23,2,We might also discover document two cover sports
00:01:31,3,Topic Mining and Analysis- Term as Topic,2.2,"So the coverage is zero, etc.",00:01:28,2,So coverage zero etc
00:01:39,3,Topic Mining and Analysis- Term as Topic,2.2,"So now, of course, as we discussed in the task definition for",00:01:32,2,So course discussed task definition
00:01:42,3,Topic Mining and Analysis- Term as Topic,2.2,"topic mining and analysis, we have two tasks.",00:01:39,2,topic mining analysis two tasks
00:01:44,3,Topic Mining and Analysis- Term as Topic,2.2,One is to discover the topics.,00:01:42,2,One discover topics
00:01:48,3,Topic Mining and Analysis- Term as Topic,2.2,And the second is to analyze coverage.,00:01:44,2,And second analyze coverage
00:01:51,3,Topic Mining and Analysis- Term as Topic,2.2,So let's first think about how we can discover,00:01:48,2,So let first think discover
00:01:55,3,Topic Mining and Analysis- Term as Topic,2.2,topics if we represent each topic by a term.,00:01:51,2,topics represent topic term
00:01:59,3,Topic Mining and Analysis- Term as Topic,2.2,So that means we need to mine k topical terms from a collection.,00:01:55,2,So means need mine k topical terms collection
00:02:04,3,Topic Mining and Analysis- Term as Topic,2.2,"Now there are, of course, many different ways of doing that.",00:02:01,2,Now course many different ways
00:02:08,3,Topic Mining and Analysis- Term as Topic,2.2,"And we're going to talk about a natural way of doing that,",00:02:05,2,And going talk natural way
00:02:10,3,Topic Mining and Analysis- Term as Topic,2.2,which is also likely effective.,00:02:08,2,also likely effective
00:02:11,3,Topic Mining and Analysis- Term as Topic,2.2,"So first of all,",00:02:10,2,So first
00:02:16,3,Topic Mining and Analysis- Term as Topic,2.2,we're going to parse the text data in the collection to obtain candidate terms.,00:02:11,2,going parse text data collection obtain candidate terms
00:02:20,3,Topic Mining and Analysis- Term as Topic,2.2,Here candidate terms can be words or phrases.,00:02:16,2,Here candidate terms words phrases
00:02:25,3,Topic Mining and Analysis- Term as Topic,2.2,Let's say the simplest solution is to just take each word as a term.,00:02:20,2,Let say simplest solution take word term
00:02:29,3,Topic Mining and Analysis- Term as Topic,2.2,These words then become candidate topics.,00:02:25,2,These words become candidate topics
00:02:32,3,Topic Mining and Analysis- Term as Topic,2.2,Then we're going to design a scoring function to match how good each term,00:02:29,2,Then going design scoring function match good term
00:02:33,3,Topic Mining and Analysis- Term as Topic,2.2,is as a topic.,00:02:32,2,topic
00:02:37,3,Topic Mining and Analysis- Term as Topic,2.2,So how can we design such a function?,00:02:35,2,So design function
00:02:40,3,Topic Mining and Analysis- Term as Topic,2.2,Well there are many things that we can consider.,00:02:37,2,Well many things consider
00:02:44,3,Topic Mining and Analysis- Term as Topic,2.2,"For example, we can use pure statistics to design such a scoring function.",00:02:40,2,For example use pure statistics design scoring function
00:02:48,3,Topic Mining and Analysis- Term as Topic,2.2,"Intuitively, we would like to favor representative terms,",00:02:45,2,Intuitively would like favor representative terms
00:02:53,3,Topic Mining and Analysis- Term as Topic,2.2,meaning terms that can represent a lot of content in the collection.,00:02:48,2,meaning terms represent lot content collection
00:02:58,3,Topic Mining and Analysis- Term as Topic,2.2,So that would mean we want to favor a frequent term.,00:02:53,2,So would mean want favor frequent term
00:03:03,3,Topic Mining and Analysis- Term as Topic,2.2,"However, if we simply use the frequency to design the scoring function,",00:02:58,2,However simply use frequency design scoring function
00:03:07,3,Topic Mining and Analysis- Term as Topic,2.2,then the highest scored terms would be general terms or,00:03:03,2,highest scored terms would general terms
00:03:10,3,Topic Mining and Analysis- Term as Topic,2.2,"functional terms like the, etc.",00:03:07,2,functional terms like etc
00:03:13,3,Topic Mining and Analysis- Term as Topic,2.2,Those terms occur very frequently English.,00:03:10,2,Those terms occur frequently English
00:03:19,3,Topic Mining and Analysis- Term as Topic,2.2,So we also want to avoid having such words on the top so,00:03:14,2,So also want avoid words top
00:03:22,3,Topic Mining and Analysis- Term as Topic,2.2,we want to penalize such words.,00:03:19,2,want penalize words
00:03:26,3,Topic Mining and Analysis- Term as Topic,2.2,"But in general, we would like to favor terms that are fairly frequent but",00:03:22,2,But general would like favor terms fairly frequent
00:03:28,3,Topic Mining and Analysis- Term as Topic,2.2,not so frequent.,00:03:26,2,frequent
00:03:34,3,Topic Mining and Analysis- Term as Topic,2.2,So a particular approach could be based on TF-IDF weighting from retrieval.,00:03:28,2,So particular approach could based TF IDF weighting retrieval
00:03:37,3,Topic Mining and Analysis- Term as Topic,2.2,And TF stands for term frequency.,00:03:35,2,And TF stands term frequency
00:03:40,3,Topic Mining and Analysis- Term as Topic,2.2,IDF stands for inverse document frequency.,00:03:37,2,IDF stands inverse document frequency
00:03:43,3,Topic Mining and Analysis- Term as Topic,2.2,We talked about some of these,00:03:40,2,We talked
00:03:48,3,Topic Mining and Analysis- Term as Topic,2.2,ideas in the lectures about the discovery of word associations.,00:03:43,2,ideas lectures discovery word associations
00:03:50,3,Topic Mining and Analysis- Term as Topic,2.2,"So these are statistical methods,",00:03:48,2,So statistical methods
00:03:56,3,Topic Mining and Analysis- Term as Topic,2.2,meaning that the function is defined mostly based on statistics.,00:03:50,2,meaning function defined mostly based statistics
00:03:59,3,Topic Mining and Analysis- Term as Topic,2.2,So the scoring function would be very general.,00:03:56,2,So scoring function would general
00:04:02,3,Topic Mining and Analysis- Term as Topic,2.2,"It can be applied to any language, any text.",00:03:59,2,It applied language text
00:04:06,3,Topic Mining and Analysis- Term as Topic,2.2,"But when we apply such a approach to a particular problem,",00:04:02,2,But apply approach particular problem
00:04:12,3,Topic Mining and Analysis- Term as Topic,2.2,we might also be able to leverage some domain-specific heuristics.,00:04:06,2,might also able leverage domain specific heuristics
00:04:16,3,Topic Mining and Analysis- Term as Topic,2.2,"For example, in news we might favor title words actually general.",00:04:12,2,For example news might favor title words actually general
00:04:21,3,Topic Mining and Analysis- Term as Topic,2.2,We might want to favor title words because the authors tend to,00:04:16,2,We might want favor title words authors tend
00:04:26,3,Topic Mining and Analysis- Term as Topic,2.2,use the title to describe the topic of an article.,00:04:21,2,use title describe topic article
00:04:32,3,Topic Mining and Analysis- Term as Topic,2.2,"If we're dealing with tweets, we could also favor hashtags,",00:04:27,2,If dealing tweets could also favor hashtags
00:04:37,3,Topic Mining and Analysis- Term as Topic,2.2,which are invented to denote topics.,00:04:32,2,invented denote topics
00:04:43,3,Topic Mining and Analysis- Term as Topic,2.2,"So naturally, hashtags can be good candidates for representing topics.",00:04:37,2,So naturally hashtags good candidates representing topics
00:04:50,3,Topic Mining and Analysis- Term as Topic,2.2,"Anyway, after we have this design scoring function, then we can discover",00:04:44,2,Anyway design scoring function discover
00:04:55,3,Topic Mining and Analysis- Term as Topic,2.2,the k topical terms by simply picking k terms with the highest scores.,00:04:50,2,k topical terms simply picking k terms highest scores
00:04:57,3,Topic Mining and Analysis- Term as Topic,2.2,"Now, of course,",00:04:55,2,Now course
00:05:02,3,Topic Mining and Analysis- Term as Topic,2.2,we might encounter situation where the highest scored terms are all very similar.,00:04:57,2,might encounter situation highest scored terms similar
00:05:06,3,Topic Mining and Analysis- Term as Topic,2.2,"They're semantically similar, or closely related, or even synonyms.",00:05:02,2,They semantically similar closely related even synonyms
00:05:08,3,Topic Mining and Analysis- Term as Topic,2.2,So that's not desirable.,00:05:06,2,So desirable
00:05:12,3,Topic Mining and Analysis- Term as Topic,2.2,So we also want to have coverage over all the content in the collection.,00:05:08,2,So also want coverage content collection
00:05:15,3,Topic Mining and Analysis- Term as Topic,2.2,So we would like to remove redundancy.,00:05:12,2,So would like remove redundancy
00:05:19,3,Topic Mining and Analysis- Term as Topic,2.2,"And one way to do that is to do a greedy algorithm,",00:05:15,2,And one way greedy algorithm
00:05:24,3,Topic Mining and Analysis- Term as Topic,2.2,which is sometimes called a maximal marginal relevance ranking.,00:05:19,2,sometimes called maximal marginal relevance ranking
00:05:29,3,Topic Mining and Analysis- Term as Topic,2.2,"Basically, the idea is to go down the list based on our scoring",00:05:24,2,Basically idea go list based scoring
00:05:34,3,Topic Mining and Analysis- Term as Topic,2.2,function and gradually take terms to collect the k topical terms.,00:05:29,2,function gradually take terms collect k topical terms
00:05:36,3,Topic Mining and Analysis- Term as Topic,2.2,"The first term, of course, will be picked.",00:05:34,2,The first term course picked
00:05:40,3,Topic Mining and Analysis- Term as Topic,2.2,"When we pick the next term, we're going to look at what terms have already",00:05:36,2,When pick next term going look terms already
00:05:45,3,Topic Mining and Analysis- Term as Topic,2.2,been picked and try to avoid picking a term that's too similar.,00:05:40,2,picked try avoid picking term similar
00:05:50,3,Topic Mining and Analysis- Term as Topic,2.2,"So while we are considering the ranking of a term in the list,",00:05:45,2,So considering ranking term list
00:05:54,3,Topic Mining and Analysis- Term as Topic,2.2,we are also considering the redundancy of the candidate term,00:05:50,2,also considering redundancy candidate term
00:05:56,3,Topic Mining and Analysis- Term as Topic,2.2,with respect to the terms that we already picked.,00:05:54,2,respect terms already picked
00:06:02,3,Topic Mining and Analysis- Term as Topic,2.2,"And with some thresholding, then we can get a balance of",00:05:58,2,And thresholding get balance
00:06:08,3,Topic Mining and Analysis- Term as Topic,2.2,the redundancy removal and also high score of a term.,00:06:02,2,redundancy removal also high score term
00:06:11,3,Topic Mining and Analysis- Term as Topic,2.2,"Okay, so after this that will get k topical terms.",00:06:08,2,Okay get k topical terms
00:06:17,3,Topic Mining and Analysis- Term as Topic,2.2,And those can be regarded as the topics that we discovered from the connection.,00:06:11,2,And regarded topics discovered connection
00:06:21,3,Topic Mining and Analysis- Term as Topic,2.2,"Next, let's think about how we're going to compute the topic coverage pi sub ij.",00:06:17,2,Next let think going compute topic coverage pi sub ij
00:06:26,3,Topic Mining and Analysis- Term as Topic,2.2,"So looking at this picture, we have sports, travel and science and",00:06:23,2,So looking picture sports travel science
00:06:28,3,Topic Mining and Analysis- Term as Topic,2.2,these topics.,00:06:26,2,topics
00:06:31,3,Topic Mining and Analysis- Term as Topic,2.2,And now suppose you are give a document.,00:06:28,2,And suppose give document
00:06:35,3,Topic Mining and Analysis- Term as Topic,2.2,How should we pick out coverage of each topic in the document?,00:06:31,2,How pick coverage topic document
00:06:42,3,Topic Mining and Analysis- Term as Topic,2.2,"Well, one approach can be to simply count occurrences of these terms.",00:06:36,2,Well one approach simply count occurrences terms
00:06:46,3,Topic Mining and Analysis- Term as Topic,2.2,"So for example, sports might have occurred four times in this this document and",00:06:42,2,So example sports might occurred four times document
00:06:49,3,Topic Mining and Analysis- Term as Topic,2.2,"travel occurred twice, etc.",00:06:46,2,travel occurred twice etc
00:06:54,3,Topic Mining and Analysis- Term as Topic,2.2,And then we can just normalize these counts as our estimate of the coverage,00:06:49,2,And normalize counts estimate coverage
00:06:56,3,Topic Mining and Analysis- Term as Topic,2.2,probability for each topic.,00:06:54,2,probability topic
00:07:01,3,Topic Mining and Analysis- Term as Topic,2.2,"So in general, the formula would be to collect the counts of",00:06:56,2,So general formula would collect counts
00:07:05,3,Topic Mining and Analysis- Term as Topic,2.2,all the terms that represent the topics.,00:07:01,2,terms represent topics
00:07:10,3,Topic Mining and Analysis- Term as Topic,2.2,And then simply normalize them so that the coverage of each,00:07:05,2,And simply normalize coverage
00:07:13,3,Topic Mining and Analysis- Term as Topic,2.2,topic in the document would add to one.,00:07:10,2,topic document would add one
00:07:21,3,Topic Mining and Analysis- Term as Topic,2.2,This forms a distribution of the topics for the document to characterize coverage,00:07:15,2,This forms distribution topics document characterize coverage
00:07:26,3,Topic Mining and Analysis- Term as Topic,2.2,of different topics in the document.,00:07:21,2,different topics document
00:07:30,3,Topic Mining and Analysis- Term as Topic,2.2,"Now, as always, when we think about idea for",00:07:26,2,Now always think idea
00:07:34,3,Topic Mining and Analysis- Term as Topic,2.2,"solving problem, we have to ask the question, how good is this one?",00:07:30,2,solving problem ask question good one
00:07:37,3,Topic Mining and Analysis- Term as Topic,2.2,Or is this the best way of solving problem?,00:07:34,2,Or best way solving problem
00:07:41,3,Topic Mining and Analysis- Term as Topic,2.2,So now let's examine this approach.,00:07:38,2,So let examine approach
00:07:44,3,Topic Mining and Analysis- Term as Topic,2.2,"In general, we have to do some empirical evaluation",00:07:41,2,In general empirical evaluation
00:07:50,3,Topic Mining and Analysis- Term as Topic,2.2,by using actual data sets and to see how well it works.,00:07:46,2,using actual data sets see well works
00:07:57,3,Topic Mining and Analysis- Term as Topic,2.2,"Well, in this case let's take a look at a simple example here.",00:07:52,2,Well case let take look simple example
00:08:03,3,Topic Mining and Analysis- Term as Topic,2.2,And we have a text document that's about a NBA basketball game.,00:07:57,2,And text document NBA basketball game
00:08:07,3,Topic Mining and Analysis- Term as Topic,2.2,"So in terms of the content, it's about sports.",00:08:04,2,So terms content sports
00:08:14,3,Topic Mining and Analysis- Term as Topic,2.2,"But if we simply count these words that represent our topics,",00:08:08,2,But simply count words represent topics
00:08:19,3,Topic Mining and Analysis- Term as Topic,2.2,"we will find that the word sports actually did not occur in the article,",00:08:14,2,find word sports actually occur article
00:08:21,3,Topic Mining and Analysis- Term as Topic,2.2,even though the content is about the sports.,00:08:19,2,even though content sports
00:08:25,3,Topic Mining and Analysis- Term as Topic,2.2,So the count of sports is zero.,00:08:22,2,So count sports zero
00:08:31,3,Topic Mining and Analysis- Term as Topic,2.2,That means the coverage of sports would be estimated as zero.,00:08:25,2,That means coverage sports would estimated zero
00:08:36,3,Topic Mining and Analysis- Term as Topic,2.2,"Now of course, the term science also did not occur in",00:08:31,2,Now course term science also occur
00:08:40,3,Topic Mining and Analysis- Term as Topic,2.2,the document and it's estimate is also zero.,00:08:36,2,document estimate also zero
00:08:42,3,Topic Mining and Analysis- Term as Topic,2.2,And that's okay.,00:08:40,2,And okay
00:08:47,3,Topic Mining and Analysis- Term as Topic,2.2,But sports certainly is not okay because we know the content is about sports.,00:08:42,2,But sports certainly okay know content sports
00:08:49,3,Topic Mining and Analysis- Term as Topic,2.2,So this estimate has problem.,00:08:47,2,So estimate problem
00:08:56,3,Topic Mining and Analysis- Term as Topic,2.2,"What's worse, the term travel actually occurred in the document.",00:08:50,2,What worse term travel actually occurred document
00:08:59,3,Topic Mining and Analysis- Term as Topic,2.2,"So when we estimate the coverage of the topic travel,",00:08:56,2,So estimate coverage topic travel
00:09:02,3,Topic Mining and Analysis- Term as Topic,2.2,we have got a non-zero count.,00:08:59,2,got non zero count
00:09:05,3,Topic Mining and Analysis- Term as Topic,2.2,So its estimated coverage will be non-zero.,00:09:02,2,So estimated coverage non zero
00:09:07,3,Topic Mining and Analysis- Term as Topic,2.2,So this obviously is also not desirable.,00:09:05,2,So obviously also desirable
00:09:13,3,Topic Mining and Analysis- Term as Topic,2.2,So this simple example illustrates some problems of this approach.,00:09:08,2,So simple example illustrates problems approach
00:09:17,3,Topic Mining and Analysis- Term as Topic,2.2,"First, when we count what words belong to to the topic,",00:09:13,2,First count words belong topic
00:09:20,3,Topic Mining and Analysis- Term as Topic,2.2,we also need to consider related words.,00:09:17,2,also need consider related words
00:09:24,3,Topic Mining and Analysis- Term as Topic,2.2,We can't simply just count the topic word sports.,00:09:20,2,We simply count topic word sports
00:09:26,3,Topic Mining and Analysis- Term as Topic,2.2,"In this case, it did not occur at all.",00:09:24,2,In case occur
00:09:31,3,Topic Mining and Analysis- Term as Topic,2.2,"But there are many related words like basketball, game, etc.",00:09:26,2,But many related words like basketball game etc
00:09:33,3,Topic Mining and Analysis- Term as Topic,2.2,So we need to count the related words also.,00:09:31,2,So need count related words also
00:09:38,3,Topic Mining and Analysis- Term as Topic,2.2,The second problem is that a word like star can be actually ambiguous.,00:09:33,2,The second problem word like star actually ambiguous
00:09:42,3,Topic Mining and Analysis- Term as Topic,2.2,"So here it probably means a basketball star, but",00:09:38,2,So probably means basketball star
00:09:47,3,Topic Mining and Analysis- Term as Topic,2.2,we can imagine it might also mean a star on the sky.,00:09:42,2,imagine might also mean star sky
00:09:53,3,Topic Mining and Analysis- Term as Topic,2.2,"So in that case, the star might actually suggest, perhaps, a topic of science.",00:09:47,2,So case star might actually suggest perhaps topic science
00:09:56,3,Topic Mining and Analysis- Term as Topic,2.2,So we need to deal with that as well.,00:09:54,2,So need deal well
00:10:02,3,Topic Mining and Analysis- Term as Topic,2.2,"Finally, a main restriction of this approach is that we have only one",00:09:56,2,Finally main restriction approach one
00:10:08,3,Topic Mining and Analysis- Term as Topic,2.2,"term to describe the topic, so it cannot really describe complicated topics.",00:10:02,2,term describe topic cannot really describe complicated topics
00:10:12,3,Topic Mining and Analysis- Term as Topic,2.2,"For example, a very specialized topic in sports would be harder to",00:10:08,2,For example specialized topic sports would harder
00:10:15,3,Topic Mining and Analysis- Term as Topic,2.2,describe by using just a word or one phrase.,00:10:12,2,describe using word one phrase
00:10:17,3,Topic Mining and Analysis- Term as Topic,2.2,We need to use more words.,00:10:15,2,We need use words
00:10:20,3,Topic Mining and Analysis- Term as Topic,2.2,So this example illustrates some general problems with,00:10:17,2,So example illustrates general problems
00:10:23,3,Topic Mining and Analysis- Term as Topic,2.2,this approach of treating a term as topic.,00:10:20,2,approach treating term topic
00:10:26,3,Topic Mining and Analysis- Term as Topic,2.2,"First, it lacks expressive power.",00:10:23,2,First lacks expressive power
00:10:30,3,Topic Mining and Analysis- Term as Topic,2.2,"Meaning that it can only represent the simple general topics, but",00:10:26,2,Meaning represent simple general topics
00:10:36,3,Topic Mining and Analysis- Term as Topic,2.2,it cannot represent the complicated topics that might require more words to describe.,00:10:30,2,cannot represent complicated topics might require words describe
00:10:40,3,Topic Mining and Analysis- Term as Topic,2.2,"Second, it's incomplete in vocabulary coverage,",00:10:37,2,Second incomplete vocabulary coverage
00:10:44,3,Topic Mining and Analysis- Term as Topic,2.2,meaning that the topic itself is only represented as one term.,00:10:40,2,meaning topic represented one term
00:10:48,3,Topic Mining and Analysis- Term as Topic,2.2,It does not suggest what other terms are related to the topic.,00:10:44,2,It suggest terms related topic
00:10:52,3,Topic Mining and Analysis- Term as Topic,2.2,"Even if we're talking about sports, there are many terms that are related.",00:10:48,2,Even talking sports many terms related
00:10:57,3,Topic Mining and Analysis- Term as Topic,2.2,"So it does not allow us to easily count related terms to order,",00:10:52,2,So allow us easily count related terms order
00:10:59,3,Topic Mining and Analysis- Term as Topic,2.2,conversion to coverage of this topic.,00:10:57,2,conversion coverage topic
00:11:02,3,Topic Mining and Analysis- Term as Topic,2.2,"Finally, there is this problem of word sense disintegration.",00:10:59,2,Finally problem word sense disintegration
00:11:05,3,Topic Mining and Analysis- Term as Topic,2.2,A topical term or related term can be ambiguous.,00:11:02,2,A topical term related term ambiguous
00:11:08,3,Topic Mining and Analysis- Term as Topic,2.2,"For example, basketball star versus star in the sky.",00:11:05,2,For example basketball star versus star sky
00:11:14,3,Topic Mining and Analysis- Term as Topic,2.2,"So in the next lecture, we're going to talk",00:11:10,2,So next lecture going talk
00:11:18,3,Topic Mining and Analysis- Term as Topic,2.2,about how to solve the problem with of a topic.,00:11:14,2,solve problem topic
00:00:06,2,Paradigmatic Relation Discovery Part 2,1.9,[SOUND] In this lecture we continue discussing,00:00:00,9,SOUND In lecture continue discussing
00:00:11,2,Paradigmatic Relation Discovery Part 2,1.9,Paradigmatic Relation Discovery.,00:00:06,9,Paradigmatic Relation Discovery
00:00:17,2,Paradigmatic Relation Discovery Part 2,1.9,Earlier we introduced a method called Expected Overlap of Words in Context.,00:00:11,9,Earlier introduced method called Expected Overlap Words Context
00:00:22,2,Paradigmatic Relation Discovery Part 2,1.9,In this method we represent each context by a word of vector,00:00:17,9,In method represent context word vector
00:00:26,2,Paradigmatic Relation Discovery Part 2,1.9,that represents the probability of a word in the context.,00:00:22,9,represents probability word context
00:00:32,2,Paradigmatic Relation Discovery Part 2,1.9,And we measure the similarity by using the dot product which can be interpreted as,00:00:26,9,And measure similarity using dot product interpreted
00:00:38,2,Paradigmatic Relation Discovery Part 2,1.9,the probability that two randomly picked words from the two contexts are identical.,00:00:32,9,probability two randomly picked words two contexts identical
00:00:42,2,Paradigmatic Relation Discovery Part 2,1.9,We also discussed the two problems of this method.,00:00:38,9,We also discussed two problems method
00:00:47,2,Paradigmatic Relation Discovery Part 2,1.9,The first is that it favors matching one frequent term,00:00:42,9,The first favors matching one frequent term
00:00:49,2,Paradigmatic Relation Discovery Part 2,1.9,very well over matching more distinct terms.,00:00:47,9,well matching distinct terms
00:00:54,2,Paradigmatic Relation Discovery Part 2,1.9,It put too much emphasis on matching one term very well.,00:00:51,9,It put much emphasis matching one term well
00:00:59,2,Paradigmatic Relation Discovery Part 2,1.9,The second is that it treats every word equally.,00:00:55,9,The second treats every word equally
00:01:05,2,Paradigmatic Relation Discovery Part 2,1.9,Even a common word like the would contribute equally,00:01:01,9,Even common word like would contribute equally
00:01:09,2,Paradigmatic Relation Discovery Part 2,1.9,as content word like eats.,00:01:05,9,content word like eats
00:01:14,2,Paradigmatic Relation Discovery Part 2,1.9,So now we are going to talk about how to solve this problems.,00:01:09,9,So going talk solve problems
00:01:18,2,Paradigmatic Relation Discovery Part 2,1.9,More specifically we're going to introduce some retrieval heuristics,00:01:14,9,More specifically going introduce retrieval heuristics
00:01:23,2,Paradigmatic Relation Discovery Part 2,1.9,used in text retrieval and these heuristics can effectively solve these,00:01:18,9,used text retrieval heuristics effectively solve
00:01:28,2,Paradigmatic Relation Discovery Part 2,1.9,problems as these problems also occur in text retrieval when we match a query,00:01:23,9,problems problems also occur text retrieval match query
00:01:33,2,Paradigmatic Relation Discovery Part 2,1.9,"with a document, so to address the first problem,",00:01:28,9,document address first problem
00:01:36,2,Paradigmatic Relation Discovery Part 2,1.9,we can use a sublinear transformation of term frequency.,00:01:33,9,use sublinear transformation term frequency
00:01:41,2,Paradigmatic Relation Discovery Part 2,1.9,"That is, we don't have to use raw frequency count of the term to represent",00:01:36,9,That use raw frequency count term represent
00:01:42,2,Paradigmatic Relation Discovery Part 2,1.9,the context.,00:01:41,9,context
00:01:47,2,Paradigmatic Relation Discovery Part 2,1.9,We can transform it into some form that wouldn't emphasize so much on the raw,00:01:42,9,We transform form emphasize much raw
00:01:53,2,Paradigmatic Relation Discovery Part 2,1.9,"frequency to address the problem, we can put more weight on rare terms.",00:01:47,9,frequency address problem put weight rare terms
00:01:55,2,Paradigmatic Relation Discovery Part 2,1.9,"And that is, we ran reward a matching a rare word.",00:01:53,9,And ran reward matching rare word
00:02:01,2,Paradigmatic Relation Discovery Part 2,1.9,And this heuristic is called IDF term weighting in text retrieval.,00:01:55,9,And heuristic called IDF term weighting text retrieval
00:02:04,2,Paradigmatic Relation Discovery Part 2,1.9,IDF stands for inverse document frequency.,00:02:01,9,IDF stands inverse document frequency
00:02:10,2,Paradigmatic Relation Discovery Part 2,1.9,So now we're going to talk about the two heuristics in more detail.,00:02:05,9,So going talk two heuristics detail
00:02:14,2,Paradigmatic Relation Discovery Part 2,1.9,"First, let's talk about the TF transformation.",00:02:10,9,First let talk TF transformation
00:02:19,2,Paradigmatic Relation Discovery Part 2,1.9,"That is, it'll convert the raw count of a word in the document into some weight",00:02:14,9,That convert raw count word document weight
00:02:25,2,Paradigmatic Relation Discovery Part 2,1.9,that reflects our belief about how important this wording.,00:02:19,9,reflects belief important wording
00:02:26,2,Paradigmatic Relation Discovery Part 2,1.9,The document.,00:02:25,9,The document
00:02:31,2,Paradigmatic Relation Discovery Part 2,1.9,"And so, that would be denoted by TF of w and d.",00:02:27,9,And would denoted TF w
00:02:36,2,Paradigmatic Relation Discovery Part 2,1.9,That's shown in the Y axis.,00:02:31,9,That shown Y axis
00:02:40,2,Paradigmatic Relation Discovery Part 2,1.9,"Now, in general, there are many ways to map that.",00:02:36,9,Now general many ways map
00:02:43,2,Paradigmatic Relation Discovery Part 2,1.9,And let's first look at the the simple way of mapping.,00:02:40,9,And let first look simple way mapping
00:02:51,2,Paradigmatic Relation Discovery Part 2,1.9,"In this case, we're going to say, well, any non zero counts will be mapped to one.",00:02:44,9,In case going say well non zero counts mapped one
00:02:55,2,Paradigmatic Relation Discovery Part 2,1.9,And the zero count will be mapped to zero.,00:02:53,9,And zero count mapped zero
00:03:01,2,Paradigmatic Relation Discovery Part 2,1.9,"So with this mapping, all the frequencies will be mapped to only two values,",00:02:55,9,So mapping frequencies mapped two values
00:03:03,2,Paradigmatic Relation Discovery Part 2,1.9,zero or one.,00:03:01,9,zero one
00:03:07,2,Paradigmatic Relation Discovery Part 2,1.9,And the mapping function is,00:03:03,9,And mapping function
00:03:13,2,Paradigmatic Relation Discovery Part 2,1.9,shown here as a flat line here.,00:03:07,9,shown flat line
00:03:17,2,Paradigmatic Relation Discovery Part 2,1.9,"This is naive because in order the frequency of words, however,",00:03:13,9,This naive order frequency words however
00:03:21,2,Paradigmatic Relation Discovery Part 2,1.9,"this actually has advantage of emphasizing,",00:03:17,9,actually advantage emphasizing
00:03:26,2,Paradigmatic Relation Discovery Part 2,1.9,matching all the words in the context.,00:03:23,9,matching words context
00:03:31,2,Paradigmatic Relation Discovery Part 2,1.9,It does not allow a frequent word to dominate the match now,00:03:26,9,It allow frequent word dominate match
00:03:36,2,Paradigmatic Relation Discovery Part 2,1.9,the approach that we have taken earlier in the overlap account approach,00:03:31,9,approach taken earlier overlap account approach
00:03:42,2,Paradigmatic Relation Discovery Part 2,1.9,is a linear transformation we basically take y as the same as x so,00:03:36,9,linear transformation basically take x
00:03:45,2,Paradigmatic Relation Discovery Part 2,1.9,we use the raw count as a representation and,00:03:42,9,use raw count representation
00:03:49,2,Paradigmatic Relation Discovery Part 2,1.9,that created the problem that we just talked about.,00:03:45,9,created problem talked
00:03:55,2,Paradigmatic Relation Discovery Part 2,1.9,"Namely, it emphasizes too much on matching one frequent term.",00:03:49,9,Namely emphasizes much matching one frequent term
00:03:59,2,Paradigmatic Relation Discovery Part 2,1.9,Matching one frequent term can contribute a lot.,00:03:55,9,Matching one frequent term contribute lot
00:04:05,2,Paradigmatic Relation Discovery Part 2,1.9,We can have a lot of other interesting transformations in between,00:04:01,9,We lot interesting transformations
00:04:07,2,Paradigmatic Relation Discovery Part 2,1.9,the two extremes.,00:04:05,9,two extremes
00:04:10,2,Paradigmatic Relation Discovery Part 2,1.9,And they generally form a sub linear transformation.,00:04:07,9,And generally form sub linear transformation
00:04:16,2,Paradigmatic Relation Discovery Part 2,1.9,"So for example, one a logarithm of the row count.",00:04:11,9,So example one logarithm row count
00:04:20,2,Paradigmatic Relation Discovery Part 2,1.9,And this will give us curve that looks like this that you are seeing here.,00:04:16,9,And give us curve looks like seeing
00:04:25,2,Paradigmatic Relation Discovery Part 2,1.9,"In this case, you can see the high frequency counts.",00:04:21,9,In case see high frequency counts
00:04:29,2,Paradigmatic Relation Discovery Part 2,1.9,"The high counts are penalized a little bit all right,",00:04:25,9,The high counts penalized little bit right
00:04:32,2,Paradigmatic Relation Discovery Part 2,1.9,so the curve is a sub linear curve.,00:04:29,9,curve sub linear curve
00:04:37,2,Paradigmatic Relation Discovery Part 2,1.9,And it brings down the weight of those really high counts.,00:04:32,9,And brings weight really high counts
00:04:42,2,Paradigmatic Relation Discovery Part 2,1.9,And this what we want because it prevents,00:04:37,9,And want prevents
00:04:46,2,Paradigmatic Relation Discovery Part 2,1.9,that kind of terms from dominating the scoring function.,00:04:42,9,kind terms dominating scoring function
00:04:54,2,Paradigmatic Relation Discovery Part 2,1.9,"Now, there is also another interesting transformation called a BM25",00:04:49,9,Now also another interesting transformation called BM25
00:05:00,2,Paradigmatic Relation Discovery Part 2,1.9,"transformation, which as been shown to be very effective for retrieval.",00:04:54,9,transformation shown effective retrieval
00:05:07,2,Paradigmatic Relation Discovery Part 2,1.9,And in this transformation we have a form that looks like this.,00:05:00,9,And transformation form looks like
00:05:11,2,Paradigmatic Relation Discovery Part 2,1.9,"So it's k plus one multiplies by x, divided by x plus k.",00:05:07,9,So k plus one multiplies x divided x plus k
00:05:13,2,Paradigmatic Relation Discovery Part 2,1.9,Where k is a parameter.,00:05:11,9,Where k parameter
00:05:16,2,Paradigmatic Relation Discovery Part 2,1.9,X is the count.,00:05:14,9,X count
00:05:18,2,Paradigmatic Relation Discovery Part 2,1.9,The raw count of a word.,00:05:16,9,The raw count word
00:05:24,2,Paradigmatic Relation Discovery Part 2,1.9,"Now the transformation is very interesting, in that it can actually",00:05:19,9,Now transformation interesting actually
00:05:29,2,Paradigmatic Relation Discovery Part 2,1.9,"kind of go from one extreme to the other extreme by varying k,",00:05:24,9,kind go one extreme extreme varying k
00:05:37,2,Paradigmatic Relation Discovery Part 2,1.9,"and it also is interesting that it has upper bound, k + 1 in this case.",00:05:31,9,also interesting upper bound k 1 case
00:05:43,2,Paradigmatic Relation Discovery Part 2,1.9,"So, this puts a very strict constraint on high frequency terms,",00:05:37,9,So puts strict constraint high frequency terms
00:05:46,2,Paradigmatic Relation Discovery Part 2,1.9,because their weight will never exceed k + 1.,00:05:43,9,weight never exceed k 1
00:05:51,2,Paradigmatic Relation Discovery Part 2,1.9,"As we vary k, we can simulate the two extremes.",00:05:46,9,As vary k simulate two extremes
00:05:54,2,Paradigmatic Relation Discovery Part 2,1.9,"So, when is set to zero, we roughly have the zero one vector.",00:05:51,9,So set zero roughly zero one vector
00:05:59,2,Paradigmatic Relation Discovery Part 2,1.9,"Whereas, when we set the k to a very large value,",00:05:56,9,Whereas set k large value
00:06:01,2,Paradigmatic Relation Discovery Part 2,1.9,"it will behave more like, immediate transformation.",00:05:59,9,behave like immediate transformation
00:06:07,2,Paradigmatic Relation Discovery Part 2,1.9,So this transformation function is by far the most effective transformation function,00:06:02,9,So transformation function far effective transformation function
00:06:14,2,Paradigmatic Relation Discovery Part 2,1.9,"for tax and retrieval, and it also makes sense for our problem set up.",00:06:07,9,tax retrieval also makes sense problem set
00:06:19,2,Paradigmatic Relation Discovery Part 2,1.9,"So we just talked about how to solve the problem of overemphasizing a frequently,",00:06:14,9,So talked solve problem overemphasizing frequently
00:06:21,2,Paradigmatic Relation Discovery Part 2,1.9,a frequently tongue.,00:06:19,9,frequently tongue
00:06:26,2,Paradigmatic Relation Discovery Part 2,1.9,"Now let's look at the second problem, and that is how we can penalize popular terms,",00:06:21,9,Now let look second problem penalize popular terms
00:06:30,2,Paradigmatic Relation Discovery Part 2,1.9,matching the is not surprising because the occurs everywhere.,00:06:27,9,matching surprising occurs everywhere
00:06:35,2,Paradigmatic Relation Discovery Part 2,1.9,But matching eats would count a lot so how can we address that problem.,00:06:30,9,But matching eats would count lot address problem
00:06:38,2,Paradigmatic Relation Discovery Part 2,1.9,In this case we can use the IDF weight.,00:06:35,9,In case use IDF weight
00:06:42,2,Paradigmatic Relation Discovery Part 2,1.9,Pop that's commonly used in retrieval.,00:06:39,9,Pop commonly used retrieval
00:06:45,2,Paradigmatic Relation Discovery Part 2,1.9,IDF stands for inverse document frequency.,00:06:42,9,IDF stands inverse document frequency
00:06:48,2,Paradigmatic Relation Discovery Part 2,1.9,Now frequency means the count of,00:06:45,9,Now frequency means count
00:06:51,2,Paradigmatic Relation Discovery Part 2,1.9,the total number of documents that contain a particular word.,00:06:48,9,total number documents contain particular word
00:06:59,2,Paradigmatic Relation Discovery Part 2,1.9,So here we show that the IDF measure is defined as a logarithm function,00:06:53,9,So show IDF measure defined logarithm function
00:07:04,2,Paradigmatic Relation Discovery Part 2,1.9,of the number of documents that match a term or document frequency.,00:06:59,9,number documents match term document frequency
00:07:09,2,Paradigmatic Relation Discovery Part 2,1.9,"So, k is the number of documents containing a word, or document frequency.",00:07:05,9,So k number documents containing word document frequency
00:07:13,2,Paradigmatic Relation Discovery Part 2,1.9,And M here is the total number of documents in the collection.,00:07:10,9,And M total number documents collection
00:07:20,2,Paradigmatic Relation Discovery Part 2,1.9,The IDF function is giving a higher value for,00:07:15,9,The IDF function giving higher value
00:07:25,2,Paradigmatic Relation Discovery Part 2,1.9,"a lower k, meaning that it rewards a rare term, and",00:07:20,9,lower k meaning rewards rare term
00:07:28,2,Paradigmatic Relation Discovery Part 2,1.9,the maximum value is log of M+1.,00:07:25,9,maximum value log M 1
00:07:36,2,Paradigmatic Relation Discovery Part 2,1.9,"That's when the word occurred just once in the context, so that's a very rare term.",00:07:28,9,That word occurred context rare term
00:07:40,2,Paradigmatic Relation Discovery Part 2,1.9,The rarest term in the whole collection.,00:07:37,9,The rarest term whole collection
00:07:48,2,Paradigmatic Relation Discovery Part 2,1.9,"The lowest value you can see here is when K reaches its maximum, which would be M.",00:07:41,9,The lowest value see K reaches maximum would M
00:07:52,2,Paradigmatic Relation Discovery Part 2,1.9,"All right so, that would be a very low value,",00:07:48,9,All right would low value
00:07:59,2,Paradigmatic Relation Discovery Part 2,1.9,close to zero in fact.,00:07:55,9,close zero fact
00:08:04,2,Paradigmatic Relation Discovery Part 2,1.9,"So, this of course measure is used in search.",00:07:59,9,So course measure used search
00:08:06,2,Paradigmatic Relation Discovery Part 2,1.9,Where we naturally have a collection.,00:08:04,9,Where naturally collection
00:08:10,2,Paradigmatic Relation Discovery Part 2,1.9,"In our case, what would be our collection?",00:08:07,9,In case would collection
00:08:14,2,Paradigmatic Relation Discovery Part 2,1.9,"Well, we can also use the context that we had collected for",00:08:10,9,Well also use context collected
00:08:16,2,Paradigmatic Relation Discovery Part 2,1.9,all the words as our collection.,00:08:14,9,words collection
00:08:21,2,Paradigmatic Relation Discovery Part 2,1.9,"And that is to say, a word that's populating the collection in general.",00:08:16,9,And say word populating collection general
00:08:27,2,Paradigmatic Relation Discovery Part 2,1.9,Would also have a low IDF because depending,00:08:22,9,Would also low IDF depending
00:08:35,2,Paradigmatic Relation Discovery Part 2,1.9,on the dataset we can Construct the context vectors in the different ways.,00:08:27,9,dataset Construct context vectors different ways
00:08:41,2,Paradigmatic Relation Discovery Part 2,1.9,"But in the end, if a term is very frequently original data set.",00:08:35,9,But end term frequently original data set
00:08:46,2,Paradigmatic Relation Discovery Part 2,1.9,Then it will still be frequenting the collective context documents.,00:08:41,9,Then still frequenting collective context documents
00:08:53,2,Paradigmatic Relation Discovery Part 2,1.9,So how can we add these heuristics to improve our,00:08:48,9,So add heuristics improve
00:08:58,2,Paradigmatic Relation Discovery Part 2,1.9,similarity function well here's one way.,00:08:53,9,similarity function well one way
00:09:01,2,Paradigmatic Relation Discovery Part 2,1.9,And there are many other ways that are possible.,00:08:58,9,And many ways possible
00:09:02,2,Paradigmatic Relation Discovery Part 2,1.9,But this is a reasonable way.,00:09:01,9,But reasonable way
00:09:06,2,Paradigmatic Relation Discovery Part 2,1.9,Where we can adapt the BM25 retrieval model for,00:09:02,9,Where adapt BM25 retrieval model
00:09:10,2,Paradigmatic Relation Discovery Part 2,1.9,paradigmatic relation mining.,00:09:06,9,paradigmatic relation mining
00:09:15,2,Paradigmatic Relation Discovery Part 2,1.9,"So here, we define,",00:09:10,9,So define
00:09:20,2,Paradigmatic Relation Discovery Part 2,1.9,in this case we define the document vector as,00:09:15,9,case define document vector
00:09:25,2,Paradigmatic Relation Discovery Part 2,1.9,containing elements representing normalized BM25 values.,00:09:20,9,containing elements representing normalized BM25 values
00:09:34,2,Paradigmatic Relation Discovery Part 2,1.9,"So in this normalization function, we see, we take a sum over, sum of all the words.",00:09:27,9,So normalization function see take sum sum words
00:09:40,2,Paradigmatic Relation Discovery Part 2,1.9,And we normalize the weight,00:09:34,9,And normalize weight
00:09:47,2,Paradigmatic Relation Discovery Part 2,1.9,of each word by the sum of the weights of all the words.,00:09:40,9,word sum weights words
00:09:54,2,Paradigmatic Relation Discovery Part 2,1.9,"And this is to, again, ensure all the xi's will sum to 1 in this vector.",00:09:48,9,And ensure xi sum 1 vector
00:09:57,2,Paradigmatic Relation Discovery Part 2,1.9,"So this would be very similar to what we had before,",00:09:54,9,So would similar
00:10:04,2,Paradigmatic Relation Discovery Part 2,1.9,in that this vector is actually something similar to a word distribution.,00:09:57,9,vector actually something similar word distribution
00:10:08,2,Paradigmatic Relation Discovery Part 2,1.9,Or the xis with sum to 1.,00:10:04,9,Or xis sum 1
00:10:12,2,Paradigmatic Relation Discovery Part 2,1.9,Now the weight of BM25 for each word is defined here.,00:10:08,9,Now weight BM25 word defined
00:10:20,2,Paradigmatic Relation Discovery Part 2,1.9,And if you compare this with our old definition where we just have a normalized,00:10:15,9,And compare old definition normalized
00:10:24,2,Paradigmatic Relation Discovery Part 2,1.9,"count, of this one so we only have this one and",00:10:20,9,count one one
00:10:29,2,Paradigmatic Relation Discovery Part 2,1.9,the document lens of the total counts of words.,00:10:24,9,document lens total counts words
00:10:33,2,Paradigmatic Relation Discovery Part 2,1.9,Being that context document and that's what we had before.,00:10:29,9,Being context document
00:10:37,2,Paradigmatic Relation Discovery Part 2,1.9,"But now with the BM25 transformation, we're introduced to something else.",00:10:33,9,But BM25 transformation introduced something else
00:10:43,2,Paradigmatic Relation Discovery Part 2,1.9,"First off, because this extra occurrence of this count is just to achieve",00:10:39,9,First extra occurrence count achieve
00:10:45,2,Paradigmatic Relation Discovery Part 2,1.9,the of normalization.,00:10:43,9,normalization
00:10:49,2,Paradigmatic Relation Discovery Part 2,1.9,But we also see we introduced the parameter k here.,00:10:46,9,But also see introduced parameter k
00:10:57,2,Paradigmatic Relation Discovery Part 2,1.9,And this parameter is generally non active number although zero is also possible.,00:10:50,9,And parameter generally non active number although zero also possible
00:11:04,2,Paradigmatic Relation Discovery Part 2,1.9,This controls the upper bound and the kind of all,00:10:59,9,This controls upper bound kind
00:11:11,2,Paradigmatic Relation Discovery Part 2,1.9,to what extent it simulates the linear transformation.,00:11:05,9,extent simulates linear transformation
00:11:18,2,Paradigmatic Relation Discovery Part 2,1.9,"And so this is one parameter, but we also see there was another parameter here, B.",00:11:11,9,And one parameter also see another parameter B
00:11:20,2,Paradigmatic Relation Discovery Part 2,1.9,And this would be within 0 an 1.,00:11:18,9,And would within 0 1
00:11:24,2,Paradigmatic Relation Discovery Part 2,1.9,And this is a parameter to control length] normalization.,00:11:20,9,And parameter control length normalization
00:11:31,2,Paradigmatic Relation Discovery Part 2,1.9,"And in this case, the normalization formula has average document length here.",00:11:25,9,And case normalization formula average document length
00:11:36,2,Paradigmatic Relation Discovery Part 2,1.9,And this is computed by taking the average of,00:11:32,9,And computed taking average
00:11:40,2,Paradigmatic Relation Discovery Part 2,1.9,the lengths of all the documents in the collection.,00:11:37,9,lengths documents collection
00:11:43,2,Paradigmatic Relation Discovery Part 2,1.9,"In this case, all the lengths of all the context documents.",00:11:40,9,In case lengths context documents
00:11:44,2,Paradigmatic Relation Discovery Part 2,1.9,That we are considering.,00:11:43,9,That considering
00:11:50,2,Paradigmatic Relation Discovery Part 2,1.9,So this average document will be a constant for any given collection.,00:11:46,9,So average document constant given collection
00:11:55,2,Paradigmatic Relation Discovery Part 2,1.9,So it actually is only affecting the factor of,00:11:50,9,So actually affecting factor
00:12:00,2,Paradigmatic Relation Discovery Part 2,1.9,the parameter b here because this is a constant.,00:11:55,9,parameter b constant
00:12:08,2,Paradigmatic Relation Discovery Part 2,1.9,But I kept it here because it's constant and that's useful,00:12:01,9,But I kept constant useful
00:12:15,2,Paradigmatic Relation Discovery Part 2,1.9,in retrieval where it would give us a stabilized interpretation of parameter B.,00:12:08,9,retrieval would give us stabilized interpretation parameter B
00:12:17,2,Paradigmatic Relation Discovery Part 2,1.9,"But, for our purpose it would be a constant.",00:12:15,9,But purpose would constant
00:12:22,2,Paradigmatic Relation Discovery Part 2,1.9,So it would only be affecting the length,00:12:17,9,So would affecting length
00:12:27,2,Paradigmatic Relation Discovery Part 2,1.9,normalization together with parameter b.,00:12:22,9,normalization together parameter b
00:12:37,2,Paradigmatic Relation Discovery Part 2,1.9,"Now with this definition then, we have a new way to define our document of vectors.",00:12:30,9,Now definition new way define document vectors
00:12:42,2,Paradigmatic Relation Discovery Part 2,1.9,And we can compute the vector d2 in the same way.,00:12:37,9,And compute vector d2 way
00:12:45,2,Paradigmatic Relation Discovery Part 2,1.9,The difference is that the high frequency terms will now have a somewhat,00:12:42,9,The difference high frequency terms somewhat
00:12:46,2,Paradigmatic Relation Discovery Part 2,1.9,lower weight.,00:12:45,9,lower weight
00:12:54,2,Paradigmatic Relation Discovery Part 2,1.9,And this would help us control the influence of these high frequency terms.,00:12:46,9,And would help us control influence high frequency terms
00:12:58,2,Paradigmatic Relation Discovery Part 2,1.9,"Now, the idea can be added here in the scoring function.",00:12:54,9,Now idea added scoring function
00:13:02,2,Paradigmatic Relation Discovery Part 2,1.9,That means we will introduce a way for matching each time.,00:12:58,9,That means introduce way matching time
00:13:06,2,Paradigmatic Relation Discovery Part 2,1.9,"You may recall, this is sum that indicates",00:13:02,9,You may recall sum indicates
00:13:11,2,Paradigmatic Relation Discovery Part 2,1.9,all the possible words that can be overlapped between the two contacts.,00:13:06,9,possible words overlapped two contacts
00:13:15,2,Paradigmatic Relation Discovery Part 2,1.9,And the Xi and the Yi are probabilities,00:13:11,9,And Xi Yi probabilities
00:13:20,2,Paradigmatic Relation Discovery Part 2,1.9,"of picking the word from both context, therefore,",00:13:17,9,picking word context therefore
00:13:25,2,Paradigmatic Relation Discovery Part 2,1.9,it indicates how likely we'll see a match on this word.,00:13:20,9,indicates likely see match word
00:13:29,2,Paradigmatic Relation Discovery Part 2,1.9,"Now, IDF would give us the importance of matching this word.",00:13:25,9,Now IDF would give us importance matching word
00:13:33,2,Paradigmatic Relation Discovery Part 2,1.9,"A common word will be worth less than a rare word, and so",00:13:29,9,A common word worth less rare word
00:13:37,2,Paradigmatic Relation Discovery Part 2,1.9,we emphasize more on matching rare words now.,00:13:33,9,emphasize matching rare words
00:13:40,2,Paradigmatic Relation Discovery Part 2,1.9,"So, with this modification, then the new function.",00:13:37,9,So modification new function
00:13:43,2,Paradigmatic Relation Discovery Part 2,1.9,When likely to address those two problems.,00:13:40,9,When likely address two problems
00:13:44,2,Paradigmatic Relation Discovery Part 2,1.9,"Now interestingly,",00:13:43,9,Now interestingly
00:13:48,2,Paradigmatic Relation Discovery Part 2,1.9,we can also use this approach to discover syntagmatical relations.,00:13:44,9,also use approach discover syntagmatical relations
00:13:57,2,Paradigmatic Relation Discovery Part 2,1.9,"In general, when we represent a term vector to replant",00:13:50,9,In general represent term vector replant
00:14:01,2,Paradigmatic Relation Discovery Part 2,1.9,"a context with a term vector we would likely see,",00:13:57,9,context term vector would likely see
00:14:06,2,Paradigmatic Relation Discovery Part 2,1.9,"some terms have higher weights, and other terms have lower weights.",00:14:01,9,terms higher weights terms lower weights
00:14:09,2,Paradigmatic Relation Discovery Part 2,1.9,"Depending on how we assign weights to these terms,",00:14:06,9,Depending assign weights terms
00:14:12,2,Paradigmatic Relation Discovery Part 2,1.9,we might be able to use these weights to discover,00:14:09,9,might able use weights discover
00:14:17,2,Paradigmatic Relation Discovery Part 2,1.9,the words that are strongly associated with a candidate of word in the context.,00:14:12,9,words strongly associated candidate word context
00:14:22,2,Paradigmatic Relation Discovery Part 2,1.9,It's interesting that we can also use this context for,00:14:18,9,It interesting also use context
00:14:27,2,Paradigmatic Relation Discovery Part 2,1.9,similarity function based on BM25 to discover syntagmatic relations.,00:14:22,9,similarity function based BM25 discover syntagmatic relations
00:14:34,2,Paradigmatic Relation Discovery Part 2,1.9,"So, the idea is to use the converted implantation of the context.",00:14:28,9,So idea use converted implantation context
00:14:37,2,Paradigmatic Relation Discovery Part 2,1.9,To see which terms are scored high.,00:14:34,9,To see terms scored high
00:14:39,2,Paradigmatic Relation Discovery Part 2,1.9,"And if a term has high weight,",00:14:37,9,And term high weight
00:14:44,2,Paradigmatic Relation Discovery Part 2,1.9,then that term might be more strongly related to the candidate word.,00:14:39,9,term might strongly related candidate word
00:14:49,2,Paradigmatic Relation Discovery Part 2,1.9,So let's take a look at the vector in more detail here.,00:14:45,9,So let take look vector detail
00:14:53,2,Paradigmatic Relation Discovery Part 2,1.9,And we have,00:14:49,9,And
00:14:59,2,Paradigmatic Relation Discovery Part 2,1.9,each Xi defined as a normalized weight of BM25.,00:14:53,9,Xi defined normalized weight BM25
00:15:07,2,Paradigmatic Relation Discovery Part 2,1.9,Now this weight alone only reflects how frequent the word occurs in the context.,00:15:01,9,Now weight alone reflects frequent word occurs context
00:15:12,2,Paradigmatic Relation Discovery Part 2,1.9,"But, we can't just say an infrequent term in the context would be",00:15:08,9,But say infrequent term context would
00:15:14,2,Paradigmatic Relation Discovery Part 2,1.9,correlated with the candidate word,00:15:12,9,correlated candidate word
00:15:20,2,Paradigmatic Relation Discovery Part 2,1.9,because many common words like the will occur frequently out of context.,00:15:15,9,many common words like occur frequently context
00:15:27,2,Paradigmatic Relation Discovery Part 2,1.9,"But if we apply IDF weighting as you see here,",00:15:22,9,But apply IDF weighting see
00:15:34,2,Paradigmatic Relation Discovery Part 2,1.9,we can then re weigh these terms based on IDF.,00:15:27,9,weigh terms based IDF
00:15:38,2,Paradigmatic Relation Discovery Part 2,1.9,"That means the words that are common, like the, will get penalized.",00:15:34,9,That means words common like get penalized
00:15:43,2,Paradigmatic Relation Discovery Part 2,1.9,so now the highest weighted terms will not be those common terms because they have,00:15:38,9,highest weighted terms common terms
00:15:45,2,Paradigmatic Relation Discovery Part 2,1.9,lower IDFs.,00:15:43,9,lower IDFs
00:15:51,2,Paradigmatic Relation Discovery Part 2,1.9,"Instead, those terms would be the terms that are frequently in the context but",00:15:45,9,Instead terms would terms frequently context
00:15:52,2,Paradigmatic Relation Discovery Part 2,1.9,not frequent in the collection.,00:15:51,9,frequent collection
00:15:57,2,Paradigmatic Relation Discovery Part 2,1.9,So those are clearly the words that tend to occur in the context,00:15:52,9,So clearly words tend occur context
00:16:00,2,Paradigmatic Relation Discovery Part 2,1.9,"of the candidate word, for example, cat.",00:15:57,9,candidate word example cat
00:16:05,2,Paradigmatic Relation Discovery Part 2,1.9,"So, for this reason, the highly weighted terms in this idea of weighted vector",00:16:00,9,So reason highly weighted terms idea weighted vector
00:16:12,2,Paradigmatic Relation Discovery Part 2,1.9,can also be assumed to be candidates for syntagmatic relations.,00:16:07,9,also assumed candidates syntagmatic relations
00:16:17,2,Paradigmatic Relation Discovery Part 2,1.9,"Now, of course, this is only a byproduct of how approach is for",00:16:12,9,Now course byproduct approach
00:16:19,2,Paradigmatic Relation Discovery Part 2,1.9,discovering parathmatic relations.,00:16:17,9,discovering parathmatic relations
00:16:22,2,Paradigmatic Relation Discovery Part 2,1.9,"And in the next lecture,",00:16:20,9,And next lecture
00:16:27,2,Paradigmatic Relation Discovery Part 2,1.9,we're going to talk more about how to discover syntagmatic relations.,00:16:22,9,going talk discover syntagmatic relations
00:16:35,2,Paradigmatic Relation Discovery Part 2,1.9,But it clearly shows the relation between discovering the two relations.,00:16:29,9,But clearly shows relation discovering two relations
00:16:38,2,Paradigmatic Relation Discovery Part 2,1.9,And indeed they can be discussed.,00:16:35,9,And indeed discussed
00:16:43,2,Paradigmatic Relation Discovery Part 2,1.9,Discovered in a joined manner by leveraging,00:16:38,9,Discovered joined manner leveraging
00:16:48,2,Paradigmatic Relation Discovery Part 2,1.9,"such associations, namely syntactical",00:16:43,9,associations namely syntactical
00:16:53,2,Paradigmatic Relation Discovery Part 2,1.9,"relation words that are similar in,",00:16:48,9,relation words similar
00:16:58,2,Paradigmatic Relation Discovery Part 2,1.9,yeah it also shows the relation between,00:16:53,9,yeah also shows relation
00:17:03,2,Paradigmatic Relation Discovery Part 2,1.9,syntagmatic relation discovery and,00:16:58,9,syntagmatic relation discovery
00:17:08,2,Paradigmatic Relation Discovery Part 2,1.9,the paradgratical relations discovery.,00:17:03,9,paradgratical relations discovery
00:17:12,2,Paradigmatic Relation Discovery Part 2,1.9,We may be able to leverage the relation to,00:17:08,9,We may able leverage relation
00:17:17,2,Paradigmatic Relation Discovery Part 2,1.9,join the discovery of two kinds of relations.,00:17:14,9,join discovery two kinds relations
00:17:22,2,Paradigmatic Relation Discovery Part 2,1.9,This also shows some interesting connections between the discovery of,00:17:18,9,This also shows interesting connections discovery
00:17:26,2,Paradigmatic Relation Discovery Part 2,1.9,syntagmatic relation and the paradigmatic relation.,00:17:22,9,syntagmatic relation paradigmatic relation
00:17:34,2,Paradigmatic Relation Discovery Part 2,1.9,Specifically those words that are paradigmatic related tend to be,00:17:28,9,Specifically words paradigmatic related tend
00:17:40,2,Paradigmatic Relation Discovery Part 2,1.9,having a syntagmatic relation with the same word.,00:17:36,9,syntagmatic relation word
00:17:49,2,Paradigmatic Relation Discovery Part 2,1.9,So to summarize the main idea of what is covering paradigmatic relations,00:17:43,9,So summarize main idea covering paradigmatic relations
00:17:54,2,Paradigmatic Relation Discovery Part 2,1.9,"is to collect the context of a candidate word to form a pseudo document,",00:17:49,9,collect context candidate word form pseudo document
00:17:58,2,Paradigmatic Relation Discovery Part 2,1.9,and this is typically represented as a bag of words.,00:17:54,9,typically represented bag words
00:18:02,2,Paradigmatic Relation Discovery Part 2,1.9,And then compute similarity of the corresponding context documents,00:17:58,9,And compute similarity corresponding context documents
00:18:04,2,Paradigmatic Relation Discovery Part 2,1.9,of two candidate words.,00:18:02,9,two candidate words
00:18:11,2,Paradigmatic Relation Discovery Part 2,1.9,And then we can take the highly similar word pairs and,00:18:05,9,And take highly similar word pairs
00:18:14,2,Paradigmatic Relation Discovery Part 2,1.9,treat them as having paradigmatic relations.,00:18:11,9,treat paradigmatic relations
00:18:17,2,Paradigmatic Relation Discovery Part 2,1.9,These are the words that share similar contexts.,00:18:15,9,These words share similar contexts
00:18:23,2,Paradigmatic Relation Discovery Part 2,1.9,"There are many different ways to implement this general idea,",00:18:18,9,There many different ways implement general idea
00:18:27,2,Paradigmatic Relation Discovery Part 2,1.9,"and we just talked about some of the approaches, and",00:18:23,9,talked approaches
00:18:32,2,Paradigmatic Relation Discovery Part 2,1.9,more specifically we talked about using text retrieval models to help,00:18:27,9,specifically talked using text retrieval models help
00:18:38,2,Paradigmatic Relation Discovery Part 2,1.9,us design effective similarity function to compute the paradigmatic relations.,00:18:32,9,us design effective similarity function compute paradigmatic relations
00:18:46,2,Paradigmatic Relation Discovery Part 2,1.9,More specifically we have used the BM25 and,00:18:41,9,More specifically used BM25
00:18:52,2,Paradigmatic Relation Discovery Part 2,1.9,IDF weighting to discover paradigmatic relation.,00:18:46,9,IDF weighting discover paradigmatic relation
00:18:56,2,Paradigmatic Relation Discovery Part 2,1.9,And these approaches also represent the state of the art.,00:18:52,9,And approaches also represent state art
00:18:58,2,Paradigmatic Relation Discovery Part 2,1.9,In text retrieval techniques.,00:18:56,9,In text retrieval techniques
00:19:02,2,Paradigmatic Relation Discovery Part 2,1.9,"Finally, syntagmatic relations can also be discovered as a by",00:18:58,9,Finally syntagmatic relations also discovered
00:19:05,2,Paradigmatic Relation Discovery Part 2,1.9,product when we discover paradigmatic relations.,00:19:02,9,product discover paradigmatic relations
00:00:05,5,Text-Based Prediction,4.6,[SOUND] This lecture is about,00:00:00,6,SOUND This lecture
00:00:11,5,Text-Based Prediction,4.6,the Text-Based Prediction.,00:00:05,6,Text Based Prediction
00:00:15,5,Text-Based Prediction,4.6,"In this lecture, we're going to start talking about the mining",00:00:11,6,In lecture going start talking mining
00:00:21,5,Text-Based Prediction,4.6,"a different kind of knowledge, as you can see here on this slide.",00:00:15,6,different kind knowledge see slide
00:00:27,5,Text-Based Prediction,4.6,Namely we're going to use text data to infer values of some other,00:00:21,6,Namely going use text data infer values
00:00:33,5,Text-Based Prediction,4.6,variables in the real world that may not be directly related to the text.,00:00:27,6,variables real world may directly related text
00:00:36,5,Text-Based Prediction,4.6,Or only remotely related to text data.,00:00:33,6,Or remotely related text data
00:00:39,5,Text-Based Prediction,4.6,So this is very different from content analysis or,00:00:36,6,So different content analysis
00:00:44,5,Text-Based Prediction,4.6,topic mining where we directly characterize the content of text.,00:00:39,6,topic mining directly characterize content text
00:00:48,5,Text-Based Prediction,4.6,"It's also different from opinion mining or sentiment analysis,",00:00:44,6,It also different opinion mining sentiment analysis
00:00:54,5,Text-Based Prediction,4.6,which still have to do is characterizing mostly the content.,00:00:48,6,still characterizing mostly content
00:00:59,5,Text-Based Prediction,4.6,Only that we focus more on the subject of content,00:00:54,6,Only focus subject content
00:01:03,5,Text-Based Prediction,4.6,which reflects what we know about the opinion holder.,00:00:59,6,reflects know opinion holder
00:01:08,5,Text-Based Prediction,4.6,But this only provides limited review of what we can predict.,00:01:05,6,But provides limited review predict
00:01:15,5,Text-Based Prediction,4.6,"In this lecture and the following lectures, we're going to talk more about",00:01:10,6,In lecture following lectures going talk
00:01:20,5,Text-Based Prediction,4.6,how we can predict more Information about the world.,00:01:15,6,predict Information world
00:01:26,5,Text-Based Prediction,4.6,How can we get the sophisticated patterns of text together with other kind of data?,00:01:20,6,How get sophisticated patterns text together kind data
00:01:32,5,Text-Based Prediction,4.6,"It would be useful first to take a look at the big picture of prediction, and",00:01:28,6,It would useful first take look big picture prediction
00:01:36,5,Text-Based Prediction,4.6,"data mining in general, and I call this data mining loop.",00:01:32,6,data mining general I call data mining loop
00:01:41,5,Text-Based Prediction,4.6,"So the picture that you are seeing right now is that there are multiple sensors,",00:01:36,6,So picture seeing right multiple sensors
00:01:43,5,Text-Based Prediction,4.6,"including human sensors,",00:01:41,6,including human sensors
00:01:46,5,Text-Based Prediction,4.6,to report what we have seen in the real world in the form of data.,00:01:43,6,report seen real world form data
00:01:50,5,Text-Based Prediction,4.6,"Of course the data in the form of non-text data, and text data.",00:01:46,6,Of course data form non text data text data
00:01:56,5,Text-Based Prediction,4.6,And our goal is to see if we can predict some values of,00:01:51,6,And goal see predict values
00:01:59,5,Text-Based Prediction,4.6,important real world variables that matter to us.,00:01:56,6,important real world variables matter us
00:02:05,5,Text-Based Prediction,4.6,"For example, someone's house condition, or the weather, or etc.",00:01:59,6,For example someone house condition weather etc
00:02:11,5,Text-Based Prediction,4.6,And so these variables would be important because we might want to act on that.,00:02:05,6,And variables would important might want act
00:02:14,5,Text-Based Prediction,4.6,We might want to make decisions based on that.,00:02:11,6,We might want make decisions based
00:02:18,5,Text-Based Prediction,4.6,So how can we get from the data to these predicted values?,00:02:14,6,So get data predicted values
00:02:22,5,Text-Based Prediction,4.6,Well in general we'll first have to do data mining and analysis of the data.,00:02:18,6,Well general first data mining analysis data
00:02:28,5,Text-Based Prediction,4.6,"Because we, in general, should treat all the data that we collected",00:02:23,6,Because general treat data collected
00:02:32,5,Text-Based Prediction,4.6,in such a prediction problem set up.,00:02:30,6,prediction problem set
00:02:37,5,Text-Based Prediction,4.6,We are very much interested in joint mining of non-text and,00:02:32,6,We much interested joint mining non text
00:02:39,5,Text-Based Prediction,4.6,"text data, which should combine all the data together.",00:02:37,6,text data combine data together
00:02:45,5,Text-Based Prediction,4.6,"And then, through analysis, generally there",00:02:41,6,And analysis generally
00:02:50,5,Text-Based Prediction,4.6,are multiple predictors of this interesting variable to us.,00:02:45,6,multiple predictors interesting variable us
00:02:52,5,Text-Based Prediction,4.6,And we call these features.,00:02:50,6,And call features
00:02:56,5,Text-Based Prediction,4.6,"And these features can then be put into a predictive model,",00:02:52,6,And features put predictive model
00:03:01,5,Text-Based Prediction,4.6,to actually predict the value of any interesting variable.,00:02:56,6,actually predict value interesting variable
00:03:06,5,Text-Based Prediction,4.6,So this then allows us to change the world.,00:03:02,6,So allows us change world
00:03:12,5,Text-Based Prediction,4.6,And so this basically is the general process for,00:03:06,6,And basically general process
00:03:15,5,Text-Based Prediction,4.6,"making a prediction based on data, including the test data.",00:03:12,6,making prediction based data including test data
00:03:20,5,Text-Based Prediction,4.6,Now it's important to emphasize that a human actually,00:03:17,6,Now important emphasize human actually
00:03:23,5,Text-Based Prediction,4.6,plays a very important role in this process.,00:03:20,6,plays important role process
00:03:27,5,Text-Based Prediction,4.6,Especially because of the involvement of text data.,00:03:24,6,Especially involvement text data
00:03:32,5,Text-Based Prediction,4.6,So human first would be involved in the mining of the data.,00:03:27,6,So human first would involved mining data
00:03:36,5,Text-Based Prediction,4.6,It would control the generation of these features.,00:03:32,6,It would control generation features
00:03:39,5,Text-Based Prediction,4.6,"And it would also help us understand the text data,",00:03:36,6,And would also help us understand text data
00:03:43,5,Text-Based Prediction,4.6,because text data are created to be consumed by humans.,00:03:39,6,text data created consumed humans
00:03:46,5,Text-Based Prediction,4.6,Humans are the best in consuming or interpreting text data.,00:03:43,6,Humans best consuming interpreting text data
00:03:52,5,Text-Based Prediction,4.6,"But when there are, of course, a lot of text data then machines have to help and",00:03:48,6,But course lot text data machines help
00:03:54,5,Text-Based Prediction,4.6,that's why we need to do text data mining.,00:03:52,6,need text data mining
00:04:00,5,Text-Based Prediction,4.6,Sometimes machines can see patterns in a lot of data that humans may not see.,00:03:55,6,Sometimes machines see patterns lot data humans may see
00:04:03,5,Text-Based Prediction,4.6,But in general human would play an important role in,00:04:00,6,But general human would play important role
00:04:07,5,Text-Based Prediction,4.6,"analyzing some text data, or applications.",00:04:03,6,analyzing text data applications
00:04:11,5,Text-Based Prediction,4.6,"Next, human also must be involved in predictive model building and",00:04:07,6,Next human also must involved predictive model building
00:04:13,5,Text-Based Prediction,4.6,adjusting or testing.,00:04:11,6,adjusting testing
00:04:17,5,Text-Based Prediction,4.6,"So in particular, we will have a lot of domain knowledge about the problem",00:04:13,6,So particular lot domain knowledge problem
00:04:22,5,Text-Based Prediction,4.6,of prediction that we can build into this predictive model.,00:04:17,6,prediction build predictive model
00:04:28,5,Text-Based Prediction,4.6,"And then next, of course, when we have predictive values for the variables,",00:04:22,6,And next course predictive values variables
00:04:32,5,Text-Based Prediction,4.6,then humans would be involved in taking actions to change a word or,00:04:28,6,humans would involved taking actions change word
00:04:35,5,Text-Based Prediction,4.6,make decisions based on these particular values.,00:04:32,6,make decisions based particular values
00:04:41,5,Text-Based Prediction,4.6,And finally it's interesting that a human could be involved,00:04:36,6,And finally interesting human could involved
00:04:42,5,Text-Based Prediction,4.6,in controlling the sensors.,00:04:41,6,controlling sensors
00:04:48,5,Text-Based Prediction,4.6,And this is so that we can adjust to the sensors to collect,00:04:43,6,And adjust sensors collect
00:04:50,5,Text-Based Prediction,4.6,the most useful data for prediction.,00:04:48,6,useful data prediction
00:04:54,5,Text-Based Prediction,4.6,So that's why I call this data mining loop.,00:04:52,6,So I call data mining loop
00:04:58,5,Text-Based Prediction,4.6,"Because as we perturb the sensors, it'll collect the new data and",00:04:54,6,Because perturb sensors collect new data
00:05:03,5,Text-Based Prediction,4.6,more useful data then we will obtain more data for prediction.,00:04:58,6,useful data obtain data prediction
00:05:07,5,Text-Based Prediction,4.6,And this data generally will help us improve the predicting accuracy.,00:05:03,6,And data generally help us improve predicting accuracy
00:05:08,5,Text-Based Prediction,4.6,"And in this loop,",00:05:07,6,And loop
00:05:12,5,Text-Based Prediction,4.6,humans will recognize what additional data will need to be collected.,00:05:08,6,humans recognize additional data need collected
00:05:14,5,Text-Based Prediction,4.6,"And machines, of course,",00:05:12,6,And machines course
00:05:19,5,Text-Based Prediction,4.6,help humans identify what data should be collected next.,00:05:14,6,help humans identify data collected next
00:05:23,5,Text-Based Prediction,4.6,"In general, we want to collect data that is most useful for learning.",00:05:19,6,In general want collect data useful learning
00:05:28,5,Text-Based Prediction,4.6,And there was actually a subarea in machine learning called active learning,00:05:23,6,And actually subarea machine learning called active learning
00:05:29,5,Text-Based Prediction,4.6,that has to do with this.,00:05:28,6,
00:05:31,5,Text-Based Prediction,4.6,How do you identify data,00:05:29,6,How identify data
00:05:36,5,Text-Based Prediction,4.6,points that would be most helpful in machine learning programs?,00:05:32,6,points would helpful machine learning programs
00:05:37,5,Text-Based Prediction,4.6,"If you can label them, right?",00:05:36,6,If label right
00:05:39,5,Text-Based Prediction,4.6,"So, in general,",00:05:38,6,So general
00:05:43,5,Text-Based Prediction,4.6,you can see there is a loop here from data acquisition to data analysis.,00:05:39,6,see loop data acquisition data analysis
00:05:46,5,Text-Based Prediction,4.6,Or data mining to prediction of values.,00:05:43,6,Or data mining prediction values
00:05:50,5,Text-Based Prediction,4.6,"And to take actions to change the word, and then observe what happens.",00:05:46,6,And take actions change word observe happens
00:05:54,5,Text-Based Prediction,4.6,And then you can then decide what additional data,00:05:50,6,And decide additional data
00:05:58,5,Text-Based Prediction,4.6,have to be collected by adjusting the sensors.,00:05:54,6,collected adjusting sensors
00:06:02,5,Text-Based Prediction,4.6,"Or from the prediction arrows, you can also note what additional data",00:05:58,6,Or prediction arrows also note additional data
00:06:06,5,Text-Based Prediction,4.6,we need to acquire in order to improve the accuracy of prediction.,00:06:02,6,need acquire order improve accuracy prediction
00:06:09,5,Text-Based Prediction,4.6,And this big picture is actually very general and,00:06:06,6,And big picture actually general
00:06:14,5,Text-Based Prediction,4.6,it's reflecting a lot of important applications of big data.,00:06:09,6,reflecting lot important applications big data
00:06:19,5,Text-Based Prediction,4.6,"So, it's useful to keep that in mind while we are looking at some text",00:06:16,6,So useful keep mind looking text
00:06:20,5,Text-Based Prediction,4.6,mining techniques.,00:06:19,6,mining techniques
00:06:26,5,Text-Based Prediction,4.6,So from text mining perspective and we're interested in text based prediction.,00:06:22,6,So text mining perspective interested text based prediction
00:06:29,5,Text-Based Prediction,4.6,"Of course, sometimes texts alone can make predictions.",00:06:26,6,Of course sometimes texts alone make predictions
00:06:31,5,Text-Based Prediction,4.6,And this is most useful for,00:06:29,6,And useful
00:06:36,5,Text-Based Prediction,4.6,prediction about human behavior or human preferences or opinions.,00:06:31,6,prediction human behavior human preferences opinions
00:06:40,5,Text-Based Prediction,4.6,But in general text data will be put together as non-text data.,00:06:36,6,But general text data put together non text data
00:06:43,5,Text-Based Prediction,4.6,"So the interesting questions here would be, first,",00:06:40,6,So interesting questions would first
00:06:47,5,Text-Based Prediction,4.6,how can we design effective predictors?,00:06:43,6,design effective predictors
00:06:52,5,Text-Based Prediction,4.6,And how do we generate such effective predictors from text?,00:06:47,6,And generate effective predictors text
00:06:57,5,Text-Based Prediction,4.6,And this question has been addressed to some extent in some previous lectures,00:06:53,6,And question addressed extent previous lectures
00:07:03,5,Text-Based Prediction,4.6,where we talked about what kind of features we can design for text data.,00:06:57,6,talked kind features design text data
00:07:06,5,Text-Based Prediction,4.6,And it has also been addressed to some extent by,00:07:03,6,And also addressed extent
00:07:10,5,Text-Based Prediction,4.6,talking about the other knowledge that we can mine from text.,00:07:06,6,talking knowledge mine text
00:07:15,5,Text-Based Prediction,4.6,"So, for example, topic mining can be very useful to generate the patterns or topic",00:07:10,6,So example topic mining useful generate patterns topic
00:07:22,5,Text-Based Prediction,4.6,based indicators or predictors that can be further fed into a predictive model.,00:07:15,6,based indicators predictors fed predictive model
00:07:26,5,Text-Based Prediction,4.6,So topics can be intermediate recognition of text.,00:07:22,6,So topics intermediate recognition text
00:07:29,5,Text-Based Prediction,4.6,That would allow us to do design high level features or,00:07:26,6,That would allow us design high level features
00:07:35,5,Text-Based Prediction,4.6,predictors that are useful for prediction of some other variable.,00:07:29,6,predictors useful prediction variable
00:07:40,5,Text-Based Prediction,4.6,"It may be also generated from original text data, it provides a much better",00:07:35,6,It may also generated original text data provides much better
00:07:45,5,Text-Based Prediction,4.6,implementation of the problem and it serves as more effective predictors.,00:07:40,6,implementation problem serves effective predictors
00:07:50,5,Text-Based Prediction,4.6,"And similarly similar analysis can lead to such predictors, as well.",00:07:46,6,And similarly similar analysis lead predictors well
00:07:52,5,Text-Based Prediction,4.6,"So, those other data mining or",00:07:50,6,So data mining
00:07:56,5,Text-Based Prediction,4.6,text mining algorithms can be used to generate predictors.,00:07:52,6,text mining algorithms used generate predictors
00:08:03,5,Text-Based Prediction,4.6,"The other question is, how can we join the mine text and non-text data together?",00:07:58,6,The question join mine text non text data together
00:08:06,5,Text-Based Prediction,4.6,"Now, this is a question that we have not addressed yet.",00:08:03,6,Now question addressed yet
00:08:07,5,Text-Based Prediction,4.6,"So, in this lecture,",00:08:06,6,So lecture
00:08:11,5,Text-Based Prediction,4.6,"and in the following lectures, we're going to address this problem.",00:08:07,6,following lectures going address problem
00:08:16,5,Text-Based Prediction,4.6,Because this is where we can generate much more enriched features for prediction.,00:08:11,6,Because generate much enriched features prediction
00:08:21,5,Text-Based Prediction,4.6,And allows us to review a lot of interesting knowledge about the world.,00:08:16,6,And allows us review lot interesting knowledge world
00:08:23,5,Text-Based Prediction,4.6,These patterns that are generated from text and,00:08:21,6,These patterns generated text
00:08:30,5,Text-Based Prediction,4.6,"non-text data themselves can sometimes, already be useful for prediction.",00:08:23,6,non text data sometimes already useful prediction
00:08:34,5,Text-Based Prediction,4.6,"But, when they are put together with many other predictors",00:08:30,6,But put together many predictors
00:08:37,5,Text-Based Prediction,4.6,they can really help improving the prediction.,00:08:34,6,really help improving prediction
00:08:42,5,Text-Based Prediction,4.6,"Basically, you can see text-based prediction can actually serve as a unified",00:08:39,6,Basically see text based prediction actually serve unified
00:08:47,5,Text-Based Prediction,4.6,framework to combine many text mining and analysis techniques.,00:08:42,6,framework combine many text mining analysis techniques
00:08:54,5,Text-Based Prediction,4.6,Including topic mining and any content mining techniques or segment analysis.,00:08:47,6,Including topic mining content mining techniques segment analysis
00:09:01,5,Text-Based Prediction,4.6,The goal here is mainly to evoke values of real-world variables.,00:08:55,6,The goal mainly evoke values real world variables
00:09:07,5,Text-Based Prediction,4.6,But in order to achieve the goal we can do some other preparations.,00:09:01,6,But order achieve goal preparations
00:09:08,5,Text-Based Prediction,4.6,And these are subtasks.,00:09:07,6,And subtasks
00:09:14,5,Text-Based Prediction,4.6,"So one subtask could mine the content of text data, like topic mining.",00:09:08,6,So one subtask could mine content text data like topic mining
00:09:18,5,Text-Based Prediction,4.6,And the other could be to mine knowledge about the observer.,00:09:14,6,And could mine knowledge observer
00:09:20,5,Text-Based Prediction,4.6,"So sentiment analysis, opinion.",00:09:18,6,So sentiment analysis opinion
00:09:26,5,Text-Based Prediction,4.6,And both can help provide predictors for the prediction problem.,00:09:21,6,And help provide predictors prediction problem
00:09:31,5,Text-Based Prediction,4.6,"And of course we can also add non-text data directly to the predicted model, but",00:09:27,6,And course also add non text data directly predicted model
00:09:36,5,Text-Based Prediction,4.6,then non-text data also helps provide a context for text analyst.,00:09:31,6,non text data also helps provide context text analyst
00:09:42,5,Text-Based Prediction,4.6,And that further improves the topic mining and the opinion analysis.,00:09:36,6,And improves topic mining opinion analysis
00:09:48,5,Text-Based Prediction,4.6,And such improvement often leads to more effective predictors for our problems.,00:09:42,6,And improvement often leads effective predictors problems
00:09:53,5,Text-Based Prediction,4.6,It would enlarge the space of patterns of opinions of topics that we can,00:09:48,6,It would enlarge space patterns opinions topics
00:09:58,5,Text-Based Prediction,4.6,mine from text and that we'll discuss more later.,00:09:53,6,mine text discuss later
00:10:00,5,Text-Based Prediction,4.6,So the joint analysis of text and,00:09:58,6,So joint analysis text
00:10:04,5,Text-Based Prediction,4.6,non-text data can be actually understood from two perspectives.,00:10:00,6,non text data actually understood two perspectives
00:10:10,5,Text-Based Prediction,4.6,"One perspective, we have non-text can help with testimony.",00:10:05,6,One perspective non text help testimony
00:10:15,5,Text-Based Prediction,4.6,Because non-text data can provide a context for,00:10:11,6,Because non text data provide context
00:10:19,5,Text-Based Prediction,4.6,mining text data provide a way to partition data in different ways.,00:10:15,6,mining text data provide way partition data different ways
00:10:24,5,Text-Based Prediction,4.6,And this leads to a number of type of techniques for contextual types of mining.,00:10:19,6,And leads number type techniques contextual types mining
00:10:29,5,Text-Based Prediction,4.6,And that's the mine text in the context defined by non-text data.,00:10:24,6,And mine text context defined non text data
00:10:34,5,Text-Based Prediction,4.6,"And you see this reference here, for a large body of work, in this direction.",00:10:29,6,And see reference large body work direction
00:10:37,5,Text-Based Prediction,4.6,"And I will need to highlight some of them, in the next lectures.",00:10:34,6,And I need highlight next lectures
00:10:42,5,Text-Based Prediction,4.6,"Now, the other perspective is text data",00:10:39,6,Now perspective text data
00:10:46,5,Text-Based Prediction,4.6,can help with non-text data mining as well.,00:10:42,6,help non text data mining well
00:10:49,5,Text-Based Prediction,4.6,And this is because text data can help interpret,00:10:46,6,And text data help interpret
00:10:52,5,Text-Based Prediction,4.6,patterns discovered from non-text data.,00:10:49,6,patterns discovered non text data
00:10:56,5,Text-Based Prediction,4.6,Let's say you discover some frequent patterns from non-text data.,00:10:52,6,Let say discover frequent patterns non text data
00:11:01,5,Text-Based Prediction,4.6,Now we can use the text data associated with instances,00:10:56,6,Now use text data associated instances
00:11:06,5,Text-Based Prediction,4.6,where the pattern occurs as well as text data that is associated with,00:11:01,6,pattern occurs well text data associated
00:11:09,5,Text-Based Prediction,4.6,instances where the pattern doesn't look up.,00:11:06,6,instances pattern look
00:11:11,5,Text-Based Prediction,4.6,And this gives us two sets of text data.,00:11:09,6,And gives us two sets text data
00:11:13,5,Text-Based Prediction,4.6,And then we can see what's the difference.,00:11:11,6,And see difference
00:11:18,5,Text-Based Prediction,4.6,And this difference in text data is interpretable because text content is,00:11:13,6,And difference text data interpretable text content
00:11:19,5,Text-Based Prediction,4.6,easy to digest.,00:11:18,6,easy digest
00:11:23,5,Text-Based Prediction,4.6,And that difference might suggest some meaning for,00:11:19,6,And difference might suggest meaning
00:11:26,5,Text-Based Prediction,4.6,this pattern that we found from non-text data.,00:11:23,6,pattern found non text data
00:11:29,5,Text-Based Prediction,4.6,"So, it helps interpret such patterns.",00:11:26,6,So helps interpret patterns
00:11:31,5,Text-Based Prediction,4.6,And this technique is called pattern annotation.,00:11:29,6,And technique called pattern annotation
00:11:37,5,Text-Based Prediction,4.6,And you can see this reference listed here for more detail.,00:11:32,6,And see reference listed detail
00:11:40,5,Text-Based Prediction,4.6,So here are the references that I just mentioned.,00:11:38,6,So references I mentioned
00:11:43,5,Text-Based Prediction,4.6,The first is reference for pattern annotation.,00:11:40,6,The first reference pattern annotation
00:11:49,5,Text-Based Prediction,4.6,"The second is, Qiaozhu Mei's dissertation on contextual text mining.",00:11:43,6,The second Qiaozhu Mei dissertation contextual text mining
00:11:53,5,Text-Based Prediction,4.6,It contains a large body of work on contextual text mining techniques.,00:11:49,6,It contains large body work contextual text mining techniques
00:00:10,2,Text Representation- Part 1,1.5,[SOUND] This lecture is about Text Representation.,00:00:07,5,SOUND This lecture Text Representation
00:00:17,2,Text Representation- Part 1,1.5,In this lecture we're going to discuss text representation and discuss how,00:00:12,5,In lecture going discuss text representation discuss
00:00:23,2,Text Representation- Part 1,1.5,natural language processing can allow us to represent text in many different ways.,00:00:17,5,natural language processing allow us represent text many different ways
00:00:27,2,Text Representation- Part 1,1.5,Let's take a look at this example sentence again.,00:00:25,5,Let take look example sentence
00:00:33,2,Text Representation- Part 1,1.5,We can represent this sentence in many different ways.,00:00:29,5,We represent sentence many different ways
00:00:41,2,Text Representation- Part 1,1.5,"First, we can always represent such a sentence as a string of characters.",00:00:34,5,First always represent sentence string characters
00:00:45,2,Text Representation- Part 1,1.5,This is true for all the languages.,00:00:42,5,This true languages
00:00:48,2,Text Representation- Part 1,1.5,When we store them in the computer.,00:00:45,5,When store computer
00:00:56,2,Text Representation- Part 1,1.5,When we store a natural language sentence as a string of characters.,00:00:50,5,When store natural language sentence string characters
00:01:01,2,Text Representation- Part 1,1.5,We have perhaps the most general way of representing text since,00:00:56,5,We perhaps general way representing text since
00:01:04,2,Text Representation- Part 1,1.5,we can always use this approach to represent any text data.,00:01:01,5,always use approach represent text data
00:01:11,2,Text Representation- Part 1,1.5,But unfortunately using such a representation will not help us to,00:01:06,5,But unfortunately using representation help us
00:01:17,2,Text Representation- Part 1,1.5,"semantic analysis, which is often needed for many applications of text mining.",00:01:11,5,semantic analysis often needed many applications text mining
00:01:21,2,Text Representation- Part 1,1.5,The reason is because we're not even recognizing words.,00:01:18,5,The reason even recognizing words
00:01:29,2,Text Representation- Part 1,1.5,So as a string we are going to keep all of the spaces and these ascii symbols.,00:01:21,5,So string going keep spaces ascii symbols
00:01:33,2,Text Representation- Part 1,1.5,We can perhaps count out what's the most frequent character in,00:01:29,5,We perhaps count frequent character
00:01:39,2,Text Representation- Part 1,1.5,the English text or the correlation between those characters.,00:01:33,5,English text correlation characters
00:01:44,2,Text Representation- Part 1,1.5,"But we can't really analyze semantics, yet",00:01:39,5,But really analyze semantics yet
00:01:48,2,Text Representation- Part 1,1.5,this is the most general way of representing text because we,00:01:44,5,general way representing text
00:01:53,2,Text Representation- Part 1,1.5,hadn't used this to represent any natural language or text.,00:01:48,5,used represent natural language text
00:01:58,2,Text Representation- Part 1,1.5,If we try to do a little bit more natural language processing by,00:01:53,5,If try little bit natural language processing
00:02:03,2,Text Representation- Part 1,1.5,"doing word segmentation, then we can obtain a representation",00:01:58,5,word segmentation obtain representation
00:02:07,2,Text Representation- Part 1,1.5,"of the same text, but in the form of a sequence of words.",00:02:03,5,text form sequence words
00:02:15,2,Text Representation- Part 1,1.5,"So here we see that we can identify words, like a dog is chasing, etc.",00:02:08,5,So see identify words like dog chasing etc
00:02:24,2,Text Representation- Part 1,1.5,Now with this level of representation we suddenly can do a lot of things.,00:02:18,5,Now level representation suddenly lot things
00:02:29,2,Text Representation- Part 1,1.5,And this is mainly because words are the basic units of human communication and,00:02:24,5,And mainly words basic units human communication
00:02:30,2,Text Representation- Part 1,1.5,natural language.,00:02:29,5,natural language
00:02:32,2,Text Representation- Part 1,1.5,So they are very powerful.,00:02:30,5,So powerful
00:02:38,2,Text Representation- Part 1,1.5,"By identifying words, we can for example, easily count what",00:02:33,5,By identifying words example easily count
00:02:44,2,Text Representation- Part 1,1.5,"are the most frequent words in this document or in the whole collection, etc.",00:02:38,5,frequent words document whole collection etc
00:02:48,2,Text Representation- Part 1,1.5,And these words can be used to form topics.,00:02:45,5,And words used form topics
00:02:53,2,Text Representation- Part 1,1.5,When we combine related words together and some words positive and,00:02:48,5,When combine related words together words positive
00:02:57,2,Text Representation- Part 1,1.5,some words are negatives or we can also do analysis.,00:02:53,5,words negatives also analysis
00:03:04,2,Text Representation- Part 1,1.5,So representing text data as a sequence of words opens up a lot of interesting,00:02:59,5,So representing text data sequence words opens lot interesting
00:03:05,2,Text Representation- Part 1,1.5,analysis possibilities.,00:03:04,5,analysis possibilities
00:03:12,2,Text Representation- Part 1,1.5,"However, this level of representation is slightly less general than string of",00:03:07,5,However level representation slightly less general string
00:03:13,2,Text Representation- Part 1,1.5,characters.,00:03:12,5,characters
00:03:20,2,Text Representation- Part 1,1.5,"Because in some languages, such as Chinese, it's actually not that easy to",00:03:13,5,Because languages Chinese actually easy
00:03:26,2,Text Representation- Part 1,1.5,"identified all the word boundaries, because in such a language you see",00:03:20,5,identified word boundaries language see
00:03:30,2,Text Representation- Part 1,1.5,text as a sequence of characters with no space in between.,00:03:26,5,text sequence characters space
00:03:35,2,Text Representation- Part 1,1.5,So you have to rely on some special techniques to identify words.,00:03:31,5,So rely special techniques identify words
00:03:43,2,Text Representation- Part 1,1.5,In such a language of course then we might make mistakes in segmenting words.,00:03:37,5,In language course might make mistakes segmenting words
00:03:50,2,Text Representation- Part 1,1.5,So the sequence of words representation is not as robust as string of characters.,00:03:43,5,So sequence words representation robust string characters
00:03:56,2,Text Representation- Part 1,1.5,"But in English, it's very easy to obtain this level of representation.",00:03:50,5,But English easy obtain level representation
00:03:58,2,Text Representation- Part 1,1.5,So we can do that all the time.,00:03:56,5,So time
00:04:06,2,Text Representation- Part 1,1.5,Now if we go further to do in that round of processing we can add a part of,00:04:01,5,Now go round processing add part
00:04:07,2,Text Representation- Part 1,1.5,these text.,00:04:06,5,text
00:04:13,2,Text Representation- Part 1,1.5,"Now once we do that we can count, for example, the most frequent nouns or",00:04:08,5,Now count example frequent nouns
00:04:18,2,Text Representation- Part 1,1.5,"what kind of nouns are associated with what kind of verbs, etc.",00:04:13,5,kind nouns associated kind verbs etc
00:04:22,2,Text Representation- Part 1,1.5,"So, this opens up a little bit more interesting opportunities for",00:04:18,5,So opens little bit interesting opportunities
00:04:25,2,Text Representation- Part 1,1.5,further analysis.,00:04:22,5,analysis
00:04:28,2,Text Representation- Part 1,1.5,Note that I use a plus sign here because,00:04:25,5,Note I use plus sign
00:04:32,2,Text Representation- Part 1,1.5,"by representing text as a sequence of part of speech tags,",00:04:28,5,representing text sequence part speech tags
00:04:38,2,Text Representation- Part 1,1.5,we don't necessarily replace the original word sequence written.,00:04:32,5,necessarily replace original word sequence written
00:04:44,2,Text Representation- Part 1,1.5,"Instead, we add this as an additional way or representing text data.",00:04:38,5,Instead add additional way representing text data
00:04:48,2,Text Representation- Part 1,1.5,So now the data is represented as both a sequence of words and,00:04:44,5,So data represented sequence words
00:04:51,2,Text Representation- Part 1,1.5,a sequence of part of speech tags.,00:04:48,5,sequence part speech tags
00:04:54,2,Text Representation- Part 1,1.5,"This enriches the representation of text data, and,",00:04:51,5,This enriches representation text data
00:04:58,2,Text Representation- Part 1,1.5,thus also enables a more interesting analysis.,00:04:54,5,thus also enables interesting analysis
00:05:02,2,Text Representation- Part 1,1.5,"If we go further,",00:05:01,5,If go
00:05:07,2,Text Representation- Part 1,1.5,then we'll be pausing the sentence to obtain a syntactic structure.,00:05:02,5,pausing sentence obtain syntactic structure
00:05:13,2,Text Representation- Part 1,1.5,Now this of course will further open up more,00:05:08,5,Now course open
00:05:17,2,Text Representation- Part 1,1.5,"interesting analysis of, for example,",00:05:13,5,interesting analysis example
00:05:23,2,Text Representation- Part 1,1.5,the writing styles or correcting grammar mistakes.,00:05:17,5,writing styles correcting grammar mistakes
00:05:26,2,Text Representation- Part 1,1.5,If we go further for semantic analysis.,00:05:23,5,If go semantic analysis
00:05:32,2,Text Representation- Part 1,1.5,Then we might be able to recognize dog as an animal.,00:05:26,5,Then might able recognize dog animal
00:05:37,2,Text Representation- Part 1,1.5,"And we also can recognize boy as a person, and playground as a location.",00:05:32,5,And also recognize boy person playground location
00:05:40,2,Text Representation- Part 1,1.5,And we can further analyse their relations.,00:05:38,5,And analyse relations
00:05:45,2,Text Representation- Part 1,1.5,"For example, dog was chasing the boy, and boy is on the playground.",00:05:40,5,For example dog chasing boy boy playground
00:05:52,2,Text Representation- Part 1,1.5,"This will add more entities and relations, through entity relation recreation.",00:05:46,5,This add entities relations entity relation recreation
00:05:57,2,Text Representation- Part 1,1.5,"At this level, we can do even more interesting things.",00:05:52,5,At level even interesting things
00:06:02,2,Text Representation- Part 1,1.5,"For example, now we can counter easily the most frequent person",00:05:57,5,For example counter easily frequent person
00:06:06,2,Text Representation- Part 1,1.5,that's managing this whole collection of news articles.,00:06:02,5,managing whole collection news articles
00:06:08,2,Text Representation- Part 1,1.5,Or whenever you mention this person,00:06:06,5,Or whenever mention person
00:06:12,2,Text Representation- Part 1,1.5,"you also tend to see mentioning of another person, etc.",00:06:09,5,also tend see mentioning another person etc
00:06:19,2,Text Representation- Part 1,1.5,So this is very a useful representation.,00:06:14,5,So useful representation
00:06:25,2,Text Representation- Part 1,1.5,And it's also related to the knowledge graph that some of you may have heard of,00:06:19,5,And also related knowledge graph may heard
00:06:30,2,Text Representation- Part 1,1.5,that Google is doing as a more semantic way of representing text data.,00:06:25,5,Google semantic way representing text data
00:06:39,2,Text Representation- Part 1,1.5,However it's also less robust sequence of words.,00:06:32,5,However also less robust sequence words
00:06:43,2,Text Representation- Part 1,1.5,"Or even syntactical analysis, because it's not always easy",00:06:39,5,Or even syntactical analysis always easy
00:06:48,2,Text Representation- Part 1,1.5,to identify all the entities with the right types and we might make mistakes.,00:06:43,5,identify entities right types might make mistakes
00:06:53,2,Text Representation- Part 1,1.5,And relations are even harder to find and we might make mistakes.,00:06:48,5,And relations even harder find might make mistakes
00:06:57,2,Text Representation- Part 1,1.5,"This makes this level of representation less robust, yet it's very useful.",00:06:53,5,This makes level representation less robust yet useful
00:07:03,2,Text Representation- Part 1,1.5,Now if we move further to logic group condition then we have predicates and,00:06:59,5,Now move logic group condition predicates
00:07:04,2,Text Representation- Part 1,1.5,inference rules.,00:07:03,5,inference rules
00:07:13,2,Text Representation- Part 1,1.5,With inference rules we can infer interesting derived facts from the text.,00:07:06,5,With inference rules infer interesting derived facts text
00:07:17,2,Text Representation- Part 1,1.5,"So that's very useful but unfortunately, this level of",00:07:13,5,So useful unfortunately level
00:07:22,2,Text Representation- Part 1,1.5,representation is even less robust and we can make mistakes.,00:07:17,5,representation even less robust make mistakes
00:07:29,2,Text Representation- Part 1,1.5,And we can't do that all the time for all kinds of sentences.,00:07:24,5,And time kinds sentences
00:07:32,2,Text Representation- Part 1,1.5,And finally speech acts would add a yet,00:07:29,5,And finally speech acts would add yet
00:07:39,2,Text Representation- Part 1,1.5,another level of rendition of the intent of saying this sentence.,00:07:32,5,another level rendition intent saying sentence
00:07:41,2,Text Representation- Part 1,1.5,So in this case it might be a request.,00:07:39,5,So case might request
00:07:46,2,Text Representation- Part 1,1.5,So knowing that would allow us to you know analyze more even more interesting,00:07:41,5,So knowing would allow us know analyze even interesting
00:07:51,2,Text Representation- Part 1,1.5,things about the observer or the author of this sentence.,00:07:46,5,things observer author sentence
00:07:54,2,Text Representation- Part 1,1.5,What's the intention of saying that?,00:07:51,5,What intention saying
00:07:57,2,Text Representation- Part 1,1.5,What scenarios or what kind of actions will be made?,00:07:54,5,What scenarios kind actions made
00:08:06,2,Text Representation- Part 1,1.5,"So this is, Another role of analysis that would be very interesting.",00:07:57,5,So Another role analysis would interesting
00:08:11,2,Text Representation- Part 1,1.5,"So this picture shows that if we move down, we generally see",00:08:06,5,So picture shows move generally see
00:08:14,2,Text Representation- Part 1,1.5,more sophisticated and natural language processing techniques will be used.,00:08:11,5,sophisticated natural language processing techniques used
00:08:19,2,Text Representation- Part 1,1.5,And unfortunately such techniques would require more human effort.,00:08:16,5,And unfortunately techniques would require human effort
00:08:23,2,Text Representation- Part 1,1.5,And they are less accurate.,00:08:20,5,And less accurate
00:08:27,2,Text Representation- Part 1,1.5,That means there are mistakes.,00:08:23,5,That means mistakes
00:08:33,2,Text Representation- Part 1,1.5,So if we analyze our text at the levels that are representing,00:08:27,5,So analyze text levels representing
00:08:38,2,Text Representation- Part 1,1.5,deeper analysis of language then we have to tolerate errors.,00:08:33,5,deeper analysis language tolerate errors
00:08:43,2,Text Representation- Part 1,1.5,So that also means it's still necessary to combine such deep analysis,00:08:38,5,So also means still necessary combine deep analysis
00:08:48,2,Text Representation- Part 1,1.5,"with shallow analysis based on, for example, sequence of words.",00:08:43,5,shallow analysis based example sequence words
00:08:55,2,Text Representation- Part 1,1.5,"On the right side, you see the arrow points down to indicate that",00:08:48,5,On right side see arrow points indicate
00:09:01,2,Text Representation- Part 1,1.5,"as we go down, with our representation of text is closer to knowledge representation",00:08:55,5,go representation text closer knowledge representation
00:09:07,2,Text Representation- Part 1,1.5,in our mind and need for solving a lot of problems.,00:09:01,5,mind need solving lot problems
00:09:15,2,Text Representation- Part 1,1.5,"Now, this is desirable because as we can represent text as a level of knowledge,",00:09:08,5,Now desirable represent text level knowledge
00:09:17,2,Text Representation- Part 1,1.5,we can easily extract the knowledge.,00:09:15,5,easily extract knowledge
00:09:19,2,Text Representation- Part 1,1.5,That's the purpose of text mining.,00:09:17,5,That purpose text mining
00:09:22,2,Text Representation- Part 1,1.5,"So, there was a trade off here.",00:09:19,5,So trade
00:09:26,2,Text Representation- Part 1,1.5,Between doing deeper analysis that might have errors but,00:09:22,5,Between deeper analysis might errors
00:09:30,2,Text Representation- Part 1,1.5,would give us direct knowledge that can be extracted from text.,00:09:26,5,would give us direct knowledge extracted text
00:09:35,2,Text Representation- Part 1,1.5,And doing shadow analysis which is more robust but,00:09:30,5,And shadow analysis robust
00:09:43,2,Text Representation- Part 1,1.5,wouldn't actually give us the necessary deeper representation of knowledge.,00:09:35,5,actually give us necessary deeper representation knowledge
00:09:46,2,Text Representation- Part 1,1.5,"I should also say that text data are generated by humans,",00:09:43,5,I also say text data generated humans
00:09:48,2,Text Representation- Part 1,1.5,and are meant to be consumed by humans.,00:09:46,5,meant consumed humans
00:09:52,2,Text Representation- Part 1,1.5,"So as a result, in text data analysis,",00:09:49,5,So result text data analysis
00:09:56,2,Text Representation- Part 1,1.5,"text mining, humans play a very important role.",00:09:52,5,text mining humans play important role
00:09:58,2,Text Representation- Part 1,1.5,"They are always in the loop,",00:09:56,5,They always loop
00:10:03,2,Text Representation- Part 1,1.5,meaning that we should optimize a collaboration of humans and computers.,00:09:58,5,meaning optimize collaboration humans computers
00:10:08,2,Text Representation- Part 1,1.5,"So, in that sense it's okay that computers may not be able to",00:10:04,5,So sense okay computers may able
00:10:14,2,Text Representation- Part 1,1.5,have completely accurate representation of text data.,00:10:09,5,completely accurate representation text data
00:10:22,2,Text Representation- Part 1,1.5,And patterns that are extracted from text data can be interpreted by humans.,00:10:14,5,And patterns extracted text data interpreted humans
00:10:27,2,Text Representation- Part 1,1.5,And then humans can guide the computers to do more accurate analysis by annotating,00:10:22,5,And humans guide computers accurate analysis annotating
00:10:31,2,Text Representation- Part 1,1.5,"more data, by providing features to guide machine learning programs,",00:10:27,5,data providing features guide machine learning programs
00:10:33,2,Text Representation- Part 1,1.5,to make them work more effectively.,00:10:31,5,make work effectively
00:00:07,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,[NOISE] This,00:00:00,2,NOISE This
00:00:09,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,lecture is about the sentiment classification.,00:00:07,2,lecture sentiment classification
00:00:12,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,If we assume that,00:00:11,2,If assume
00:00:17,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"most of the elements in the opinion representation are all ready known,",00:00:13,2,elements opinion representation ready known
00:00:23,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"then our only task may be just a sentiment classification, as shown in this case.",00:00:17,2,task may sentiment classification shown case
00:00:28,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So suppose we know who's the opinion holder and what's the opinion target,",00:00:23,2,So suppose know opinion holder opinion target
00:00:33,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"and also know the content and the context of the opinion, then we mainly need to",00:00:28,2,also know content context opinion mainly need
00:00:38,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,decide the opinion sentiment of the review.,00:00:33,2,decide opinion sentiment review
00:00:45,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So this is a case of just using sentiment classification for understanding opinion.,00:00:38,2,So case using sentiment classification understanding opinion
00:00:51,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,Sentiment classification can be defined more specifically as follows.,00:00:46,2,Sentiment classification defined specifically follows
00:00:57,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"The input is opinionated text object, the output is typically a sentiment label,",00:00:51,2,The input opinionated text object output typically sentiment label
00:01:02,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"or a sentiment tag, and that can be designed in two ways.",00:00:57,2,sentiment tag designed two ways
00:01:07,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"One is polarity analysis, where we have categories such as positive, negative,",00:01:02,2,One polarity analysis categories positive negative
00:01:07,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,or neutral.,00:01:07,2,neutral
00:01:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,The other is emotion analysis that can go beyond,00:01:08,2,The emotion analysis go beyond
00:01:20,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,a polarity to characterize the feeling of the opinion holder.,00:01:14,2,polarity characterize feeling opinion holder
00:01:24,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"In the case of polarity analysis, we sometimes",00:01:21,2,In case polarity analysis sometimes
00:01:29,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,also have numerical ratings as you often see in some reviews on the web.,00:01:24,2,also numerical ratings often see reviews web
00:01:37,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Five might denote the most positive, and one maybe the most negative, for example.",00:01:30,2,Five might denote positive one maybe negative example
00:01:42,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"In general, you have just disk holder categories to characterize the sentiment.",00:01:37,2,In general disk holder categories characterize sentiment
00:01:46,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"In emotion analysis, of course, there are also different ways for",00:01:43,2,In emotion analysis course also different ways
00:01:48,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,design the categories.,00:01:46,2,design categories
00:01:52,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"The six most frequently used categories are happy,",00:01:49,2,The six frequently used categories happy
00:01:57,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"sad, fearful, angry, surprised, and disgusted.",00:01:52,2,sad fearful angry surprised disgusted
00:02:04,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So as you can see, the task is essentially a classification task, or categorization",00:01:59,2,So see task essentially classification task categorization
00:02:08,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"task, as we've seen before, so it's a special case of text categorization.",00:02:04,2,task seen special case text categorization
00:02:13,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,This also means any textual categorization method can be used to do sentiment,00:02:08,2,This also means textual categorization method used sentiment
00:02:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,classification.,00:02:13,2,classification
00:02:18,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Now of course if you just do that, the accuracy may not be good",00:02:15,2,Now course accuracy may good
00:02:24,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,because sentiment classification does requires some improvement over,00:02:18,2,sentiment classification requires improvement
00:02:29,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"regular text categorization technique, or simple text categorization technique.",00:02:24,2,regular text categorization technique simple text categorization technique
00:02:33,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"In particular, it needs two kind of improvements.",00:02:29,2,In particular needs two kind improvements
00:02:37,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,One is to use more sophisticated features that may be more appropriate for,00:02:33,2,One use sophisticated features may appropriate
00:02:40,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,sentiment tagging as I will discuss in a moment.,00:02:37,2,sentiment tagging I discuss moment
00:02:45,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"The other is to consider the order of these categories, and",00:02:41,2,The consider order categories
00:02:51,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"especially in polarity analysis, it's very clear there's an order here,",00:02:45,2,especially polarity analysis clear order
00:02:56,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,and so these categories are not all that independent.,00:02:51,2,categories independent
00:03:00,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"There's order among them, and so it's useful to consider the order.",00:02:56,2,There order among useful consider order
00:03:03,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"For example, we could use ordinal regression to do that,",00:03:00,2,For example could use ordinal regression
00:03:06,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,and that's something that we'll talk more about later.,00:03:03,2,something talk later
00:03:11,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So now, let's talk about some features that are often very useful for",00:03:06,2,So let talk features often useful
00:03:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"text categorization and text mining in general, but",00:03:11,2,text categorization text mining general
00:03:17,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,some of them are especially also needed for sentiment analysis.,00:03:14,2,especially also needed sentiment analysis
00:03:23,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So let's start from the simplest one, which is character n-grams.",00:03:18,2,So let start simplest one character n grams
00:03:26,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"You can just have a sequence of characters as a unit,",00:03:23,2,You sequence characters unit
00:03:32,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"and they can be mixed with different n's, different lengths.",00:03:26,2,mixed different n different lengths
00:03:35,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"All right, and this is a very general way and",00:03:32,2,All right general way
00:03:38,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,very robust way to represent the text data.,00:03:35,2,robust way represent text data
00:03:41,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And you could do that for any language, pretty much.",00:03:38,2,And could language pretty much
00:03:46,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And this is also robust to spelling errors or recognition errors, right?",00:03:42,2,And also robust spelling errors recognition errors right
00:03:50,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So if you misspell a word by one character and this representation actually would,00:03:46,2,So misspell word one character representation actually would
00:03:55,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,allow you to match this word when it occurs in the text correctly.,00:03:50,2,allow match word occurs text correctly
00:04:00,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Right, so misspell the word and the correct form can be matched because",00:03:55,2,Right misspell word correct form matched
00:04:04,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,they contain some common n-grams of characters.,00:04:00,2,contain common n grams characters
00:04:08,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,But of course such a recommendation would not be as discriminating as words.,00:04:04,2,But course recommendation would discriminating words
00:04:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So next, we have word n-grams, a sequence of words and again,",00:04:10,2,So next word n grams sequence words
00:04:17,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,we can mix them with different n's.,00:04:14,2,mix different n
00:04:23,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,Unigram's are actually often very effective for a lot of text processing,00:04:17,2,Unigram actually often effective lot text processing
00:04:29,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"tasks, and it's mostly because words are word designed features by humans for",00:04:23,2,tasks mostly words word designed features humans
00:04:34,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"communication, and so they are often good enough for many tasks.",00:04:29,2,communication often good enough many tasks
00:04:38,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"But it's not good, or not sufficient for sentiment analysis clearly.",00:04:34,2,But good sufficient sentiment analysis clearly
00:04:42,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"For example, we might see a sentence like,",00:04:38,2,For example might see sentence like
00:04:47,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"it's not good or it's not as good as something else, right?",00:04:42,2,good good something else right
00:04:49,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So in such a case if you just take a good and,00:04:47,2,So case take good
00:04:54,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"that would suggest positive that's not good, all right so it's not accurate.",00:04:49,2,would suggest positive good right accurate
00:04:59,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"But if you take a bigram, not good together, and then it's more accurate.",00:04:54,2,But take bigram good together accurate
00:05:03,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So longer n-grams are generally more discriminative, and they're more specific.",00:04:59,2,So longer n grams generally discriminative specific
00:05:07,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"If you match it, and it says a lot, and",00:05:03,2,If match says lot
00:05:11,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"it's accurate it's unlikely, very ambiguous.",00:05:07,2,accurate unlikely ambiguous
00:05:16,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,But it may cause overfitting because with such very unique features that machine,00:05:11,2,But may cause overfitting unique features machine
00:05:21,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,oriented program can easily pick up such features from the training set and,00:05:16,2,oriented program easily pick features training set
00:05:26,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,to rely on such unique features to distinguish the categories.,00:05:21,2,rely unique features distinguish categories
00:05:30,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And obviously, that kind of classify, one would generalize word to future there when",00:05:26,2,And obviously kind classify one would generalize word future
00:05:34,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,such discriminative features will not necessarily occur.,00:05:30,2,discriminative features necessarily occur
00:05:39,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So that's a problem of overfitting that's not desirable.,00:05:34,2,So problem overfitting desirable
00:05:43,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"We can also consider part of speech tag, n-grams if we can do part of",00:05:39,2,We also consider part speech tag n grams part
00:05:49,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"speech tagging an, for example, adjective noun could form a pair.",00:05:43,2,speech tagging example adjective noun could form pair
00:05:55,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,We can also mix n-grams of words and n-grams of part of speech tags.,00:05:49,2,We also mix n grams words n grams part speech tags
00:05:59,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"For example, the word great might be followed by a noun, and this could become",00:05:55,2,For example word great might followed noun could become
00:06:05,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"a feature, a hybrid feature, that could be useful for sentiment analysis.",00:05:59,2,feature hybrid feature could useful sentiment analysis
00:06:09,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So next we can also have word classes.,00:06:06,2,So next also word classes
00:06:15,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So these classes can be syntactic like a part of speech tags, or could be semantic,",00:06:09,2,So classes syntactic like part speech tags could semantic
00:06:20,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"and they might represent concepts in the thesaurus or ontology, like WordNet.",00:06:15,2,might represent concepts thesaurus ontology like WordNet
00:06:25,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Or they can be recognized the name entities, like people or place, and",00:06:20,2,Or recognized name entities like people place
00:06:31,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,these categories can be used to enrich the presentation as additional features.,00:06:25,2,categories used enrich presentation additional features
00:06:35,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"We can also learn word clusters and parodically, for example,",00:06:31,2,We also learn word clusters parodically example
00:06:40,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,we've talked about the mining associations of words.,00:06:35,2,talked mining associations words
00:06:43,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And so we can have cluster of paradigmatically related words or,00:06:40,2,And cluster paradigmatically related words
00:06:45,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"syntaxmatically related words, and",00:06:43,2,syntaxmatically related words
00:06:50,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,these clusters can be features to supplement the word base representation.,00:06:45,2,clusters features supplement word base representation
00:06:54,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Furthermore, we can also have frequent pattern syntax, and",00:06:50,2,Furthermore also frequent pattern syntax
00:06:57,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"these could be frequent word set, the words that",00:06:54,2,could frequent word set words
00:07:01,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,form the pattern do not necessarily occur together or next to each other.,00:06:57,2,form pattern necessarily occur together next
00:07:04,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,But we'll also have locations where,00:07:01,2,But also locations
00:07:09,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"the words my occur more closely together, and such",00:07:04,2,words occur closely together
00:07:13,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,patterns provide a more discriminative features than words obviously.,00:07:09,2,patterns provide discriminative features words obviously
00:07:18,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And they may also generalize better than just regular n-grams because they,00:07:14,2,And may also generalize better regular n grams
00:07:18,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,are frequent.,00:07:18,2,frequent
00:07:22,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So you expected them to occur also in tested data.,00:07:18,2,So expected occur also tested data
00:07:27,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So they have a lot of advantages, but they might still face the problem",00:07:22,2,So lot advantages might still face problem
00:07:31,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,of overfeeding as the features become more complex.,00:07:27,2,overfeeding features become complex
00:07:37,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"This is a problem in general, and the same is true for parse tree-based features,",00:07:31,2,This problem general true parse tree based features
00:07:42,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"when you can use a parse tree to derive features such as frequent subtrees, or",00:07:37,2,use parse tree derive features frequent subtrees
00:07:46,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"paths, and those are even more discriminating, but",00:07:42,2,paths even discriminating
00:07:51,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,they're also are more likely to cause over fitting.,00:07:46,2,also likely cause fitting
00:07:55,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And in general, pattern discovery algorithm's are very useful for",00:07:51,2,And general pattern discovery algorithm useful
00:07:59,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,feature construction because they allow us to search in a large space of possible,00:07:55,2,feature construction allow us search large space possible
00:08:04,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,features that are more complex than words that are sometimes useful.,00:07:59,2,features complex words sometimes useful
00:08:08,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So in general, natural language processing is very important that",00:08:04,2,So general natural language processing important
00:08:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"they derive complex features, and they can enrich text representation.",00:08:08,2,derive complex features enrich text representation
00:08:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So for example,",00:08:14,2,So example
00:08:21,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,this is a simple sentence that I showed you a long time ago in another lecture.,00:08:14,2,simple sentence I showed long time ago another lecture
00:08:26,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So from these words we can only derive simple word n-grams,",00:08:21,2,So words derive simple word n grams
00:08:29,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,representations or character n-grams.,00:08:26,2,representations character n grams
00:08:32,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"But with NLP, we can enrich the representation",00:08:29,2,But NLP enrich representation
00:08:37,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"with a lot of other information such as part of speech tags, parse trees or",00:08:32,2,lot information part speech tags parse trees
00:08:40,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"entities, or even speech act.",00:08:37,2,entities even speech act
00:08:45,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Now with such enriching information of course, then we can generate a lot",00:08:40,2,Now enriching information course generate lot
00:08:50,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"of other features, more complex features like a mixed grams of a word and",00:08:45,2,features complex features like mixed grams word
00:08:54,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"the part of speech tags, or even a part of a parse tree.",00:08:50,2,part speech tags even part parse tree
00:09:00,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So in general, feature design actually affects categorization accuracy",00:08:55,2,So general feature design actually affects categorization accuracy
00:09:05,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"significantly, and it's a very important part of any machine learning application.",00:09:00,2,significantly important part machine learning application
00:09:10,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"In general, I think it would be most effective if you can combine",00:09:05,2,In general I think would effective combine
00:09:15,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"machine learning, error analysis, and domain knowledge in design features.",00:09:10,2,machine learning error analysis domain knowledge design features
00:09:18,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So first you want to use the main knowledge,",00:09:15,2,So first want use main knowledge
00:09:22,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"your understanding of the problem, the design seed features, and",00:09:18,2,understanding problem design seed features
00:09:27,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,you can also define a basic feature space with a lot of possible features for,00:09:22,2,also define basic feature space lot possible features
00:09:32,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"the machine learning program to work on, and machine can be applied to select",00:09:27,2,machine learning program work machine applied select
00:09:35,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,the most effective features or construct the new features.,00:09:32,2,effective features construct new features
00:09:37,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"That's feature learning, and",00:09:35,2,That feature learning
00:09:43,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,these features can then be further analyzed by humans through error analysis.,00:09:37,2,features analyzed humans error analysis
00:09:46,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And you can look at the categorization errors, and",00:09:43,2,And look categorization errors
00:09:50,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"then further analyze what features can help you recover from those errors,",00:09:46,2,analyze features help recover errors
00:09:54,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,or what features cause overfitting and cause those errors.,00:09:50,2,features cause overfitting cause errors
00:09:58,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And so this can lead into feature validation that will,00:09:54,2,And lead feature validation
00:10:01,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"revised the feature set, and then you can iterate.",00:09:58,2,revised feature set iterate
00:10:05,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And we might consider using a different features space.,00:10:01,2,And might consider using different features space
00:10:11,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So NLP enriches text recognition as I just said, and",00:10:07,2,So NLP enriches text recognition I said
00:10:14,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"because it enriches the feature space,",00:10:11,2,enriches feature space
00:10:19,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"it allows much larger such a space of features and there are also many,",00:10:14,2,allows much larger space features also many
00:10:23,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,many more features that can be very useful for a lot of tasks.,00:10:19,2,many features useful lot tasks
00:10:28,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,But be careful not to use a lot of category features because,00:10:23,2,But careful use lot category features
00:10:33,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"it can cause overfitting, or otherwise you would",00:10:28,2,cause overfitting otherwise would
00:10:38,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,have to training careful not to let overflow happen.,00:10:33,2,training careful let overflow happen
00:10:41,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"So a main challenge in design features,",00:10:38,2,So main challenge design features
00:10:46,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,a common challenge is to optimize a trade off between exhaustivity and,00:10:41,2,common challenge optimize trade exhaustivity
00:10:51,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"the specificity, and this trade off turns out to be very difficult.",00:10:46,2,specificity trade turns difficult
00:10:56,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,Now exhaustivity means we want the features to actually have,00:10:51,2,Now exhaustivity means want features actually
00:10:59,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,high coverage of a lot of documents.,00:10:56,2,high coverage lot documents
00:11:04,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"And so in that sense, you want the features to be frequent.",00:10:59,2,And sense want features frequent
00:11:08,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"Specifity requires the feature to be discriminative, so",00:11:04,2,Specifity requires feature discriminative
00:11:13,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,naturally infrequent the features tend to be more discriminative.,00:11:08,2,naturally infrequent features tend discriminative
00:11:17,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,So this really cause a trade off between,00:11:13,2,So really cause trade
00:11:22,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,frequent versus infrequent features.,00:11:17,2,frequent versus infrequent features
00:11:22,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And that's why a featured design is usually odd.,00:11:22,2,And featured design usually odd
00:11:27,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,And that's probably the most important part in machine learning any,00:11:22,2,And probably important part machine learning
00:11:32,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,"problem in particularly in our case, for text categoration or",00:11:27,2,problem particularly case text categoration
00:11:35,5,Opinion Mining and Sentiment Analysis- Sentiment Classification,4.2,more specifically the senitment classification.,00:11:32,2,specifically senitment classification
00:00:09,5,Course Summary,4.11,This lecture is a summary of this whole course.,00:00:06,11,This lecture summary whole course
00:00:14,5,Course Summary,4.11,"First, let's revisit the topics that we covered in this course.",00:00:10,11,First let revisit topics covered course
00:00:18,5,Course Summary,4.11,"In the beginning, we talked about the natural language processing and",00:00:14,11,In beginning talked natural language processing
00:00:21,5,Course Summary,4.11,how it can enrich text representation.,00:00:18,11,enrich text representation
00:00:26,5,Course Summary,4.11,"We then talked about how to mine knowledge about the language,",00:00:21,11,We talked mine knowledge language
00:00:29,5,Course Summary,4.11,"natural language used to express the,",00:00:26,11,natural language used express
00:00:33,5,Course Summary,4.11,what's observing the world in text and data.,00:00:29,11,observing world text data
00:00:38,5,Course Summary,4.11,"In particular, we talked about how to mine word associations.",00:00:34,11,In particular talked mine word associations
00:00:42,5,Course Summary,4.11,We then talked about how to analyze topics in text.,00:00:38,11,We talked analyze topics text
00:00:45,5,Course Summary,4.11,How to discover topics and analyze them.,00:00:42,11,How discover topics analyze
00:00:51,5,Course Summary,4.11,"This can be regarded as knowledge about observed world,",00:00:47,11,This regarded knowledge observed world
00:00:55,5,Course Summary,4.11,and then we talked about how to mine knowledge about the observer and,00:00:51,11,talked mine knowledge observer
00:01:00,5,Course Summary,4.11,"particularly talk about the, how to mine opinions and do sentiment analysis.",00:00:55,11,particularly talk mine opinions sentiment analysis
00:01:06,5,Course Summary,4.11,"And finally, we will talk about the text-based prediction, which has to",00:01:00,11,And finally talk text based prediction
00:01:11,5,Course Summary,4.11,do with predicting values of other real world variables based on text data.,00:01:06,11,predicting values real world variables based text data
00:01:16,5,Course Summary,4.11,"And in discussing this, we will also discuss the role of non-text data,",00:01:11,11,And discussing also discuss role non text data
00:01:21,5,Course Summary,4.11,"which can contribute additional predictors for the prediction problem,",00:01:16,11,contribute additional predictors prediction problem
00:01:25,5,Course Summary,4.11,"and also can provide context for analyzing text data, and",00:01:21,11,also provide context analyzing text data
00:01:30,5,Course Summary,4.11,in particular we talked about how to use context to analyze topics.,00:01:25,11,particular talked use context analyze topics
00:01:39,5,Course Summary,4.11,So here are the key high-level take away messages from this cost.,00:01:33,11,So key high level take away messages cost
00:01:41,5,Course Summary,4.11,I going to go over these major topics and,00:01:39,11,I going go major topics
00:01:46,5,Course Summary,4.11,point out what are the key take-away messages that you should remember.,00:01:41,11,point key take away messages remember
00:01:50,5,Course Summary,4.11,First the NLP and text representation.,00:01:47,11,First NLP text representation
00:01:56,5,Course Summary,4.11,You should realize that NLP is always very important for,00:01:53,11,You realize NLP always important
00:02:01,5,Course Summary,4.11,any text replication because it enriches text representation.,00:01:56,11,text replication enriches text representation
00:02:05,5,Course Summary,4.11,The more NLP the better text representation we can have.,00:02:01,11,The NLP better text representation
00:02:08,5,Course Summary,4.11,"And this further enables more accurate knowledge discovery,",00:02:05,11,And enables accurate knowledge discovery
00:02:11,5,Course Summary,4.11,"to discover deeper knowledge, buried in text.",00:02:08,11,discover deeper knowledge buried text
00:02:17,5,Course Summary,4.11,"However, the current estate of art of natural energy processing is,",00:02:12,11,However current estate art natural energy processing
00:02:19,5,Course Summary,4.11,still not robust enough.,00:02:17,11,still robust enough
00:02:23,5,Course Summary,4.11,"So, as an result, the robust text mining technologies today,",00:02:19,11,So result robust text mining technologies today
00:02:26,5,Course Summary,4.11,tend to be based on world [INAUDIBLE].,00:02:23,11,tend based world INAUDIBLE
00:02:30,5,Course Summary,4.11,"And tend to rely a lot on statistical analysis,",00:02:26,11,And tend rely lot statistical analysis
00:02:33,5,Course Summary,4.11,as we've discussed in this course.,00:02:30,11,discussed course
00:02:39,5,Course Summary,4.11,And you may recall we've mostly used word based representations.,00:02:33,11,And may recall mostly used word based representations
00:02:42,5,Course Summary,4.11,"And we've relied a lot on statistical techniques,",00:02:39,11,And relied lot statistical techniques
00:02:45,5,Course Summary,4.11,statistical learning techniques particularly.,00:02:42,11,statistical learning techniques particularly
00:02:52,5,Course Summary,4.11,"In word-association mining and analysis the important points first,",00:02:47,11,In word association mining analysis important points first
00:02:56,5,Course Summary,4.11,we are introduced the two concepts for two basic and,00:02:52,11,introduced two concepts two basic
00:03:02,5,Course Summary,4.11,"complementary relations of words, paradigmatic and syntagmatic relations.",00:02:56,11,complementary relations words paradigmatic syntagmatic relations
00:03:08,5,Course Summary,4.11,These are actually very general relations between elements sequences.,00:03:02,11,These actually general relations elements sequences
00:03:14,5,Course Summary,4.11,If you take it as meaning elements that occur in similar,00:03:08,11,If take meaning elements occur similar
00:03:18,5,Course Summary,4.11,context in the sequence and elements that tend to co-occur with each other.,00:03:14,11,context sequence elements tend co occur
00:03:24,5,Course Summary,4.11,And these relations might be also meaningful for other sequences of data.,00:03:18,11,And relations might also meaningful sequences data
00:03:29,5,Course Summary,4.11,We also talked a lot about test the similarity then we,00:03:25,11,We also talked lot test similarity
00:03:34,5,Course Summary,4.11,discuss how to discover paradynamic similarities compare,00:03:29,11,discuss discover paradynamic similarities compare
00:03:38,5,Course Summary,4.11,the context of words discover words that share similar context.,00:03:34,11,context words discover words share similar context
00:03:39,5,Course Summary,4.11,"At that point level,",00:03:38,11,At point level
00:03:44,5,Course Summary,4.11,we talked about representing text data with a vector space model.,00:03:39,11,talked representing text data vector space model
00:03:48,5,Course Summary,4.11,And we talked about some retrieval techniques such as BM25 for,00:03:44,11,And talked retrieval techniques BM25
00:03:52,5,Course Summary,4.11,"measuring similarity of text and for assigning weights to terms,",00:03:48,11,measuring similarity text assigning weights terms
00:03:55,5,Course Summary,4.11,"tf-idf weighting, et cetera.",00:03:52,11,tf idf weighting et cetera
00:03:59,5,Course Summary,4.11,And this part is well-connected to text retrieval.,00:03:55,11,And part well connected text retrieval
00:04:02,5,Course Summary,4.11,There are other techniques that can be relevant here also.,00:03:59,11,There techniques relevant also
00:04:08,5,Course Summary,4.11,"The next point is about co-occurrence analysis of text, and",00:04:03,11,The next point co occurrence analysis text
00:04:12,5,Course Summary,4.11,"we introduce some information theory concepts such as entropy,",00:04:08,11,introduce information theory concepts entropy
00:04:15,5,Course Summary,4.11,"conditional entropy, and mutual information.",00:04:12,11,conditional entropy mutual information
00:04:18,5,Course Summary,4.11,These are not only very useful for,00:04:15,11,These useful
00:04:23,5,Course Summary,4.11,"measuring the co-occurrences of words, they are also very useful for",00:04:18,11,measuring co occurrences words also useful
00:04:26,5,Course Summary,4.11,"analyzing other kind of data, and they are useful for, for example, for",00:04:23,11,analyzing kind data useful example
00:04:29,5,Course Summary,4.11,feature selection in text categorization as well.,00:04:26,11,feature selection text categorization well
00:04:34,5,Course Summary,4.11,"So this is another important concept, good to know.",00:04:30,11,So another important concept good know
00:04:38,5,Course Summary,4.11,"And then we talked about the topic mining and analysis, and",00:04:35,11,And talked topic mining analysis
00:04:41,5,Course Summary,4.11,that's where we introduce in the probabilistic topic model.,00:04:38,11,introduce probabilistic topic model
00:04:45,5,Course Summary,4.11,"We spent a lot of time to explain the basic topic model,",00:04:41,11,We spent lot time explain basic topic model
00:04:52,5,Course Summary,4.11,"PLSA in detail and this is, those are the basics for understanding LDA which is.",00:04:45,11,PLSA detail basics understanding LDA
00:04:56,5,Course Summary,4.11,"Theoretically, a more opinion model, but",00:04:52,11,Theoretically opinion model
00:05:01,5,Course Summary,4.11,we did not have enough time to really go in depth in introducing LDA.,00:04:56,11,enough time really go depth introducing LDA
00:05:06,5,Course Summary,4.11,"But in practice, PLSA seems as effective as LDA and",00:05:02,11,But practice PLSA seems effective LDA
00:05:09,5,Course Summary,4.11,it's simpler to implement and it's also more efficient.,00:05:06,11,simpler implement also efficient
00:05:15,5,Course Summary,4.11,In this part of Wilson videos is some general concepts that would be useful to,00:05:11,11,In part Wilson videos general concepts would useful
00:05:20,5,Course Summary,4.11,"know, one is generative model, and this is a general method for",00:05:15,11,know one generative model general method
00:05:23,5,Course Summary,4.11,modeling text data and modeling other kinds of data as well.,00:05:20,11,modeling text data modeling kinds data well
00:05:30,5,Course Summary,4.11,"And we talked about the maximum life erase data, the EM algorithm for",00:05:24,11,And talked maximum life erase data EM algorithm
00:05:35,5,Course Summary,4.11,solving the problem of computing maximum estimator.,00:05:30,11,solving problem computing maximum estimator
00:05:38,5,Course Summary,4.11,"So, these are all general techniques that tend to be very useful",00:05:35,11,So general techniques tend useful
00:05:39,5,Course Summary,4.11,in other scenarios as well.,00:05:38,11,scenarios well
00:05:45,5,Course Summary,4.11,Then we talked about the text clustering and the text categorization.,00:05:40,11,Then talked text clustering text categorization
00:05:50,5,Course Summary,4.11,Those are two important building blocks in any text mining application systems.,00:05:45,11,Those two important building blocks text mining application systems
00:05:56,5,Course Summary,4.11,In text with clustering we talked about how we can solve the problem by,00:05:50,11,In text clustering talked solve problem
00:06:02,5,Course Summary,4.11,using a slightly different mixture module than the probabilistic topic model.,00:05:56,11,using slightly different mixture module probabilistic topic model
00:06:07,5,Course Summary,4.11,and we then also prefer to view the similarity based,00:06:02,11,also prefer view similarity based
00:06:10,5,Course Summary,4.11,approaches to test for cuss word.,00:06:07,11,approaches test cuss word
00:06:15,5,Course Summary,4.11,In categorization we also talk about the two kinds of approaches.,00:06:11,11,In categorization also talk two kinds approaches
00:06:19,5,Course Summary,4.11,One is generative classifies that rely on to base word to,00:06:15,11,One generative classifies rely base word
00:06:24,5,Course Summary,4.11,"infer the condition of or probability of a category given text data,",00:06:20,11,infer condition probability category given text data
00:06:28,5,Course Summary,4.11,in deeper we'll introduce you should use [INAUDIBLE] base in detail.,00:06:24,11,deeper introduce use INAUDIBLE base detail
00:06:36,5,Course Summary,4.11,"This is the practical use for technique, for a lot of text, capitalization tasks.",00:06:29,11,This practical use technique lot text capitalization tasks
00:06:41,5,Course Summary,4.11,"We also introduce the some discriminative classifiers,",00:06:37,11,We also introduce discriminative classifiers
00:06:45,5,Course Summary,4.11,"particularly logistical regression, can nearest labor and SBN.",00:06:41,11,particularly logistical regression nearest labor SBN
00:06:49,5,Course Summary,4.11,"They also very important, they are very popular, they are very useful for",00:06:45,11,They also important popular useful
00:06:50,5,Course Summary,4.11,text capitalization as well.,00:06:49,11,text capitalization well
00:06:57,5,Course Summary,4.11,"In both parts, we'll also discuss how to evaluate the results.",00:06:52,11,In parts also discuss evaluate results
00:07:03,5,Course Summary,4.11,Evaluation is quite important because if the matches that you use don't really,00:06:57,11,Evaluation quite important matches use really
00:07:07,5,Course Summary,4.11,reflect the volatility of the method then it would give you misleading results so,00:07:03,11,reflect volatility method would give misleading results
00:07:10,5,Course Summary,4.11,its very important to get the variation right.,00:07:07,11,important get variation right
00:07:15,5,Course Summary,4.11,And we talked about variation of categorization in detail was a lot of,00:07:10,11,And talked variation categorization detail lot
00:07:16,5,Course Summary,4.11,specific measures.,00:07:15,11,specific measures
00:07:21,5,Course Summary,4.11,Then we talked about the sentiment analysis and the paradigm and,00:07:18,11,Then talked sentiment analysis paradigm
00:07:25,5,Course Summary,4.11,that's where we introduced sentiment classification problem.,00:07:21,11,introduced sentiment classification problem
00:07:29,5,Course Summary,4.11,"And although it's a special case of text recalculation, but",00:07:25,11,And although special case text recalculation
00:07:34,5,Course Summary,4.11,we talked about how to extend or improve the text recalculation method,00:07:29,11,talked extend improve text recalculation method
00:07:41,5,Course Summary,4.11,by using more sophisticated features that would be needed for sentiment analysis.,00:07:34,11,using sophisticated features would needed sentiment analysis
00:07:46,5,Course Summary,4.11,"We did a review of some common use for complex features for text analysis, and",00:07:41,11,We review common use complex features text analysis
00:07:50,5,Course Summary,4.11,"then we also talked about how to capture the order of these categories,",00:07:46,11,also talked capture order categories
00:07:55,5,Course Summary,4.11,"in sentiment classification, and in particular we introduced ordinal",00:07:50,11,sentiment classification particular introduced ordinal
00:08:00,5,Course Summary,4.11,logistical regression then we also talked about Latent Aspect Rating Analysis.,00:07:55,11,logistical regression also talked Latent Aspect Rating Analysis
00:08:05,5,Course Summary,4.11,This is an unsupervised way of using a generative model to understand and,00:08:00,11,This unsupervised way using generative model understand
00:08:07,5,Course Summary,4.11,review data in more detail.,00:08:05,11,review data detail
00:08:12,5,Course Summary,4.11,"In particular, it allows us to understand the composed ratings of",00:08:07,11,In particular allows us understand composed ratings
00:08:18,5,Course Summary,4.11,a reviewer on different aspects of a topic.,00:08:14,11,reviewer different aspects topic
00:08:20,5,Course Summary,4.11,"So given text reviews with overall ratings,",00:08:18,11,So given text reviews overall ratings
00:08:24,5,Course Summary,4.11,the method allows even further ratings on different aspects.,00:08:20,11,method allows even ratings different aspects
00:08:26,5,Course Summary,4.11,"And it also allows us to infer,",00:08:24,11,And also allows us infer
00:08:30,5,Course Summary,4.11,the viewers laying their weights on these aspects or,00:08:26,11,viewers laying weights aspects
00:08:35,5,Course Summary,4.11,which aspects are more important to a viewer can be revealed as well.,00:08:30,11,aspects important viewer revealed well
00:08:39,5,Course Summary,4.11,And this enables a lot of interesting applications.,00:08:35,11,And enables lot interesting applications
00:08:46,5,Course Summary,4.11,"Finally, in the discussion of prediction, we mainly talk about the joint mining",00:08:41,11,Finally discussion prediction mainly talk joint mining
00:08:50,5,Course Summary,4.11,"of text and non text data, as they are both very important for prediction.",00:08:46,11,text non text data important prediction
00:08:57,5,Course Summary,4.11,We particularly talked about how text data can help non-text data and vice versa.,00:08:51,11,We particularly talked text data help non text data vice versa
00:09:01,5,Course Summary,4.11,"In the case of using non-text data to help text data analysis,",00:08:58,11,In case using non text data help text data analysis
00:09:04,5,Course Summary,4.11,we talked about the contextual text mining.,00:09:01,11,talked contextual text mining
00:09:08,5,Course Summary,4.11,We introduced the contextual PLSA as a generalizing or generalized model of PLSA,00:09:04,11,We introduced contextual PLSA generalizing generalized model PLSA
00:09:13,5,Course Summary,4.11,"to allows us to incorporate the context of variables, such as time and location.",00:09:08,11,allows us incorporate context variables time location
00:09:18,5,Course Summary,4.11,And this is a general way to allow us to reveal a lot of interesting topic,00:09:13,11,And general way allow us reveal lot interesting topic
00:09:20,5,Course Summary,4.11,of patterns in text data.,00:09:18,11,patterns text data
00:09:24,5,Course Summary,4.11,"We also introduced the net PLSA, in this case we used social network or",00:09:20,11,We also introduced net PLSA case used social network
00:09:30,5,Course Summary,4.11,network in general of text data to help analyze puppets.,00:09:24,11,network general text data help analyze puppets
00:09:36,5,Course Summary,4.11,And finally we talk about how can be used as context to,00:09:31,11,And finally talk used context
00:09:40,5,Course Summary,4.11,mine potentially causal Topics in text layer.,00:09:36,11,mine potentially causal Topics text layer
00:09:46,5,Course Summary,4.11,"Now, in the other way of using text to",00:09:43,11,Now way using text
00:09:51,5,Course Summary,4.11,"help interpret patterns discovered from LAM text data,",00:09:47,11,help interpret patterns discovered LAM text data
00:09:57,5,Course Summary,4.11,we did not really discuss anything in detail but just provide a reference but,00:09:51,11,really discuss anything detail provide reference
00:10:02,5,Course Summary,4.11,"I should stress that that's after a very important direction to know about,",00:09:57,11,I stress important direction know
00:10:06,5,Course Summary,4.11,"if you want to build a practical text mining systems,",00:10:02,11,want build practical text mining systems
00:10:10,5,Course Summary,4.11,because understanding and interpreting patterns is quite important.,00:10:06,11,understanding interpreting patterns quite important
00:10:18,5,Course Summary,4.11,"So this is a summary of the key take away messages, and",00:10:13,11,So summary key take away messages
00:10:22,5,Course Summary,4.11,I hope these will be very useful to you for building any,00:10:18,11,I hope useful building
00:10:27,5,Course Summary,4.11,text mining applications or to you for the starting of these algorithms.,00:10:22,11,text mining applications starting algorithms
00:10:31,5,Course Summary,4.11,"And this should provide a good basis for you to read from your research papers,",00:10:27,11,And provide good basis read research papers
00:10:33,5,Course Summary,4.11,to know more about more of allowance for,00:10:31,11,know allowance
00:10:37,5,Course Summary,4.11,other organisms or to invent new hours in yourself.,00:10:33,11,organisms invent new hours
00:10:43,5,Course Summary,4.11,"So to know more about this topic,",00:10:40,11,So know topic
00:10:47,5,Course Summary,4.11,I would suggest you to look into other areas in more depth.,00:10:43,11,I would suggest look areas depth
00:10:51,5,Course Summary,4.11,"And during this short period of time of this course,",00:10:48,11,And short period time course
00:10:57,5,Course Summary,4.11,"we could only touch the basic concepts, basic principles, of text mining and",00:10:51,11,could touch basic concepts basic principles text mining
00:11:03,5,Course Summary,4.11,we emphasize the coverage of practical algorithms.,00:10:57,11,emphasize coverage practical algorithms
00:11:09,5,Course Summary,4.11,And this is after the cost of covering algorithms and,00:11:03,11,And cost covering algorithms
00:11:15,5,Course Summary,4.11,in many cases we omit the discussion of a lot of algorithms.,00:11:09,11,many cases omit discussion lot algorithms
00:11:19,5,Course Summary,4.11,So to learn more about the subject you should definitely learn more,00:11:15,11,So learn subject definitely learn
00:11:22,5,Course Summary,4.11,about the natural language process because this is foundation for,00:11:19,11,natural language process foundation
00:11:24,5,Course Summary,4.11,all text based applications.,00:11:22,11,text based applications
00:11:28,5,Course Summary,4.11,"The more NLP you can do, the better the additional text that you can get, and",00:11:24,11,The NLP better additional text get
00:11:32,5,Course Summary,4.11,then the deeper knowledge you can discover.,00:11:28,11,deeper knowledge discover
00:11:34,5,Course Summary,4.11,So this is very important.,00:11:32,11,So important
00:11:39,5,Course Summary,4.11,The second area you should look into is the Statistical Machine Learning.,00:11:37,11,The second area look Statistical Machine Learning
00:11:45,5,Course Summary,4.11,And these techniques are now the backbone techniques for,00:11:41,11,And techniques backbone techniques
00:11:49,5,Course Summary,4.11,not just text analysis applications but also for NLP.,00:11:46,11,text analysis applications also NLP
00:11:55,5,Course Summary,4.11,A lot of NLP techniques are nowadays actually based on supervised machinery.,00:11:49,11,A lot NLP techniques nowadays actually based supervised machinery
00:12:00,5,Course Summary,4.11,"So, they are very important because they are a key",00:11:56,11,So important key
00:12:04,5,Course Summary,4.11,to also understanding some advancing NLP techniques and,00:12:00,11,also understanding advancing NLP techniques
00:12:08,5,Course Summary,4.11,naturally they will provide more tools for doing text analysis in general.,00:12:04,11,naturally provide tools text analysis general
00:12:13,5,Course Summary,4.11,"Now, a particularly interesting area,",00:12:09,11,Now particularly interesting area
00:12:17,5,Course Summary,4.11,called deep learning has attracted a lot of attention recently.,00:12:13,11,called deep learning attracted lot attention recently
00:12:21,5,Course Summary,4.11,"It has also shown promise in many application areas,",00:12:17,11,It also shown promise many application areas
00:12:26,5,Course Summary,4.11,"especially in speech and vision, and has been applied to text data as well.",00:12:21,11,especially speech vision applied text data well
00:12:30,5,Course Summary,4.11,"So, for example, recently there has work on using deep learning to do",00:12:26,11,So example recently work using deep learning
00:12:34,5,Course Summary,4.11,segment analysis to achieve better accuracy.,00:12:30,11,segment analysis achieve better accuracy
00:12:38,5,Course Summary,4.11,"So that's one example of [INAUDIBLE] techniques that we weren't able to cover,",00:12:34,11,So one example INAUDIBLE techniques able cover
00:12:40,5,Course Summary,4.11,but that's also very important.,00:12:38,11,also important
00:12:45,5,Course Summary,4.11,And the other area that has emerged in status learning is the water and,00:12:41,11,And area emerged status learning water
00:12:50,5,Course Summary,4.11,"baring technique, where they can learn better recognition of words.",00:12:45,11,baring technique learn better recognition words
00:12:55,5,Course Summary,4.11,And then these better recognitions will allow you confuse similarity of words.,00:12:50,11,And better recognitions allow confuse similarity words
00:12:55,5,Course Summary,4.11,"As you can see,",00:12:55,11,As see
00:13:01,5,Course Summary,4.11,this provides directly a way to discover the paradigmatic relations of words.,00:12:55,11,provides directly way discover paradigmatic relations words
00:13:06,5,Course Summary,4.11,"And results that people have got, so far, are very impressive.",00:13:01,11,And results people got far impressive
00:13:10,5,Course Summary,4.11,"That's another promising technique that we did not have time to touch,",00:13:06,11,That another promising technique time touch
00:13:16,5,Course Summary,4.11,"but, of course, whether these new techniques",00:13:12,11,course whether new techniques
00:13:20,5,Course Summary,4.11,would lead to practical useful techniques that work much better than the current,00:13:16,11,would lead practical useful techniques work much better current
00:13:25,5,Course Summary,4.11,technologies is still an open question that has to be examined.,00:13:20,11,technologies still open question examined
00:13:28,5,Course Summary,4.11,And no serious evaluation has been done yet.,00:13:25,11,And serious evaluation done yet
00:13:32,5,Course Summary,4.11,"In, for example, examining the practical value of word embedding,",00:13:28,11,In example examining practical value word embedding
00:13:34,5,Course Summary,4.11,other than word similarity and basic evaluation.,00:13:32,11,word similarity basic evaluation
00:13:39,5,Course Summary,4.11,"But nevertheless, these are advanced techniques",00:13:36,11,But nevertheless advanced techniques
00:13:43,5,Course Summary,4.11,that surely will make impact in text mining in the future.,00:13:39,11,surely make impact text mining future
00:13:46,5,Course Summary,4.11,So its very important to know more about these.,00:13:43,11,So important know
00:13:50,5,Course Summary,4.11,Statistical learning is also the key to predictive modeling which is very crucial,00:13:46,11,Statistical learning also key predictive modeling crucial
00:13:55,5,Course Summary,4.11,for many big data applications and we did not talk about that predictive modeling,00:13:50,11,many big data applications talk predictive modeling
00:13:59,5,Course Summary,4.11,component but this is mostly about the regression or categorization,00:13:55,11,component mostly regression categorization
00:14:05,5,Course Summary,4.11,techniques and this is another reason why statistical learning is important.,00:13:59,11,techniques another reason statistical learning important
00:14:11,5,Course Summary,4.11,"We also suggest that you learn more about data mining, and that's simply because",00:14:07,11,We also suggest learn data mining simply
00:14:16,5,Course Summary,4.11,"general data mining algorithms can always be applied to text data, which can be",00:14:11,11,general data mining algorithms always applied text data
00:14:21,5,Course Summary,4.11,regarded as as special case of general data.,00:14:16,11,regarded special case general data
00:14:26,5,Course Summary,4.11,So there are many applications of data mining techniques.,00:14:23,11,So many applications data mining techniques
00:14:30,5,Course Summary,4.11,"In particular for example, pattern discovery would be very useful to generate",00:14:26,11,In particular example pattern discovery would useful generate
00:14:35,5,Course Summary,4.11,the interesting features for test analysis and the reason that an information network,00:14:30,11,interesting features test analysis reason information network
00:14:40,5,Course Summary,4.11,that mining techniques can also be used to analyze text information at work.,00:14:35,11,mining techniques also used analyze text information work
00:14:44,5,Course Summary,4.11,So these are all good to know.,00:14:42,11,So good know
00:14:49,5,Course Summary,4.11,In order to develop effective text analysis techniques.,00:14:44,11,In order develop effective text analysis techniques
00:14:52,5,Course Summary,4.11,"And finally, we also recommend you to learn more about the text retrieval,",00:14:49,11,And finally also recommend learn text retrieval
00:14:55,5,Course Summary,4.11,"information retrieval, of search engines.",00:14:52,11,information retrieval search engines
00:15:00,5,Course Summary,4.11,This is especially important if you are interested in building practical text,00:14:55,11,This especially important interested building practical text
00:15:02,5,Course Summary,4.11,application systems.,00:15:00,11,application systems
00:15:05,5,Course Summary,4.11,And a search ending would be an essential system,00:15:02,11,And search ending would essential system
00:15:08,5,Course Summary,4.11,component in any text-based applications.,00:15:05,11,component text based applications
00:15:13,5,Course Summary,4.11,And that's because texts data are created for humans to consume.,00:15:08,11,And texts data created humans consume
00:15:19,5,Course Summary,4.11,So humans are at the best position to understand text data and,00:15:13,11,So humans best position understand text data
00:15:24,5,Course Summary,4.11,"it's important to have human in the loop in big text data applications, so",00:15:19,11,important human loop big text data applications
00:15:29,5,Course Summary,4.11,it can in particular help text mining systems in two ways.,00:15:24,11,particular help text mining systems two ways
00:15:35,5,Course Summary,4.11,One is through effectively reduce the data size from a large collection to,00:15:29,11,One effectively reduce data size large collection
00:15:40,5,Course Summary,4.11,a small collection with the most relevant text data that only matter for,00:15:35,11,small collection relevant text data matter
00:15:42,5,Course Summary,4.11,the particular interpretation.,00:15:40,11,particular interpretation
00:15:47,5,Course Summary,4.11,"So the other is to provide a way to annotate it, to explain parents,",00:15:42,11,So provide way annotate explain parents
00:15:51,5,Course Summary,4.11,and this has to do with knowledge providence.,00:15:47,11,knowledge providence
00:15:54,5,Course Summary,4.11,"Once we discover some knowledge, we have to figure out whether or",00:15:51,11,Once discover knowledge figure whether
00:15:57,5,Course Summary,4.11,not the discovery is really reliable.,00:15:54,11,discovery really reliable
00:16:00,5,Course Summary,4.11,So we need to go back to the original text to verify that.,00:15:57,11,So need go back original text verify
00:16:02,5,Course Summary,4.11,And that is why the search engine is very important.,00:16:00,11,And search engine important
00:16:08,5,Course Summary,4.11,"Moreover, some techniques of information retrieval,",00:16:04,11,Moreover techniques information retrieval
00:16:13,5,Course Summary,4.11,"for example BM25, vector space and are also very useful for text data mining.",00:16:08,11,example BM25 vector space also useful text data mining
00:16:16,5,Course Summary,4.11,"We only mention some of them, but if you know more about",00:16:13,11,We mention know
00:16:20,5,Course Summary,4.11,text retrieval you'll see that there are many techniques that are used for it.,00:16:16,11,text retrieval see many techniques used
00:16:25,5,Course Summary,4.11,Another technique that it's used for is indexing technique that enables quick,00:16:20,11,Another technique used indexing technique enables quick
00:16:28,5,Course Summary,4.11,"response of search engine to a user's query, and such techniques can be",00:16:25,11,response search engine user query techniques
00:16:32,5,Course Summary,4.11,very useful for building efficient text mining systems as well.,00:16:28,11,useful building efficient text mining systems well
00:16:39,5,Course Summary,4.11,"So, finally, I want to remind you of this big picture for",00:16:35,11,So finally I want remind big picture
00:16:43,5,Course Summary,4.11,harnessing big text data that I showed you at your beginning of the semester.,00:16:39,11,harnessing big text data I showed beginning semester
00:16:48,5,Course Summary,4.11,"So in general, to deal with a big text application system,",00:16:45,11,So general deal big text application system
00:16:51,5,Course Summary,4.11,"we need two kinds text, text retrieval and text mining.",00:16:48,11,need two kinds text text retrieval text mining
00:16:58,5,Course Summary,4.11,"And text retrieval, as I explained, is to help convert big text data into",00:16:53,11,And text retrieval I explained help convert big text data
00:17:02,5,Course Summary,4.11,"a small amount of most relevant data for a particular problem, and can also help",00:16:58,11,small amount relevant data particular problem also help
00:17:07,5,Course Summary,4.11,"providing knowledge provenance, help interpreting patterns later.",00:17:02,11,providing knowledge provenance help interpreting patterns later
00:17:12,5,Course Summary,4.11,Text mining has to do with further analyzing the relevant data to discover,00:17:07,11,Text mining analyzing relevant data discover
00:17:16,5,Course Summary,4.11,the actionable knowledge that can be directly useful for decision making or,00:17:12,11,actionable knowledge directly useful decision making
00:17:18,5,Course Summary,4.11,many other tasks.,00:17:16,11,many tasks
00:17:20,5,Course Summary,4.11,So this course covers text mining.,00:17:18,11,So course covers text mining
00:17:24,5,Course Summary,4.11,And there's a companion course called Text Retrieval and,00:17:20,11,And companion course called Text Retrieval
00:17:27,5,Course Summary,4.11,Search Engines that covers text retrieval.,00:17:24,11,Search Engines covers text retrieval
00:17:32,5,Course Summary,4.11,"If you haven't taken that course, it would be useful for you to take it,",00:17:27,11,If taken course would useful take
00:17:37,5,Course Summary,4.11,especially if you are interested in building a text caching system.,00:17:32,11,especially interested building text caching system
00:17:42,5,Course Summary,4.11,And taking both courses will give you a complete set of practical skills for,00:17:37,11,And taking courses give complete set practical skills
00:17:43,5,Course Summary,4.11,building such a system.,00:17:42,11,building system
00:17:49,5,Course Summary,4.11,So in [INAUDIBLE] I just would like to thank you for,00:17:43,11,So INAUDIBLE I would like thank
00:17:51,5,Course Summary,4.11,taking this course.,00:17:49,11,taking course
00:17:57,5,Course Summary,4.11,I hope you have learned useful knowledge and skills in test mining and [INAUDIBLE].,00:17:51,11,I hope learned useful knowledge skills test mining INAUDIBLE
00:18:02,5,Course Summary,4.11,As you see from our discussions there are a lot of opportunities for,00:17:57,11,As see discussions lot opportunities
00:18:06,5,Course Summary,4.11,this kind of techniques and there are also a lot of open channels.,00:18:02,11,kind techniques also lot open channels
00:18:10,5,Course Summary,4.11,So I hope you can use what you have learned to build a lot of use for,00:18:06,11,So I hope use learned build lot use
00:18:15,5,Course Summary,4.11,applications will benefit society and to also join,00:18:10,11,applications benefit society also join
00:18:20,5,Course Summary,4.11,the research community to discover new techniques for text mining and benefits.,00:18:15,11,research community discover new techniques text mining benefits
00:18:21,5,Course Summary,4.11,Thank you.,00:18:20,11,Thank
00:00:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,[MUSIC],00:00:00,10,MUSIC
00:00:10,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,This lecture is about the expectation maximization algorithms or,00:00:06,10,This lecture expectation maximization algorithms
00:00:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,also called the EM algorithms.,00:00:10,10,also called EM algorithms
00:00:14,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"In this lecture,",00:00:13,10,In lecture
00:00:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,we're going to continue the discussion of probabilistic topic models.,00:00:14,10,going continue discussion probabilistic topic models
00:00:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"In particular, we're going to introduce the EM algorithm.",00:00:18,10,In particular going introduce EM algorithm
00:00:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Which is a family of useful algorithms for computing the maximum life or,00:00:22,10,Which family useful algorithms computing maximum life
00:00:28,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,estimate of mixture models.,00:00:27,10,estimate mixture models
00:00:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, this is now a familiar scenario of using two components, the mixture",00:00:28,10,So familiar scenario using two components mixture
00:00:39,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,model to try to fact out the background words from one topic or word distribution.,00:00:33,10,model try fact background words one topic word distribution
00:00:39,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Yeah.,00:00:39,10,Yeah
00:00:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, we're interested in computing",00:00:41,10,So interested computing
00:00:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,this estimate and we're going to try to adjust these,00:00:45,10,estimate going try adjust
00:00:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,probability values to maximize the probability of the observed documents.,00:00:50,10,probability values maximize probability observed documents
00:00:58,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And know that we're assumed all the other parameters are known.,00:00:55,10,And know assumed parameters known
00:01:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, the only thing unknown is these water properties, this given by zero something.",00:00:58,10,So thing unknown water properties given zero something
00:01:10,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And in this lecture, we're going to look into how to compute this maximum like or",00:01:04,10,And lecture going look compute maximum like
00:01:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,estimate.,00:01:10,10,estimate
00:01:15,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Now this started with the idea of,00:01:12,10,Now started idea
00:01:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,separating the words in the text data into two groups.,00:01:15,10,separating words text data two groups
00:01:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,One group will be explained by the background model.,00:01:19,10,One group explained background model
00:01:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,The other group will be explained by the unknown topical order.,00:01:23,10,The group explained unknown topical order
00:01:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,After all this is the basic idea of the mixture model.,00:01:28,10,After basic idea mixture model
00:01:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"But, suppose we actually know which word is from which distribution.",00:01:32,10,But suppose actually know word distribution
00:01:41,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So that would mean, for example, these words, the, is, and",00:01:36,10,So would mean example words
00:01:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"we, are known to be from this background origin, distribution.",00:01:41,10,known background origin distribution
00:01:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"On the other hand, the other words, text mining,",00:01:45,10,On hand words text mining
00:01:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"clustering, etcetera are known to be from the topic word, distribution.",00:01:48,10,clustering etcetera known topic word distribution
00:01:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"If you can see the color, that these are showing blue.",00:01:54,10,If see color showing blue
00:02:02,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"These blue words are, they are assumed to be from the topic word, distribution.",00:01:57,10,These blue words assumed topic word distribution
00:02:07,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,If we already know how to separate these words.,00:02:03,10,If already know separate words
00:02:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Then the problem of estimating the word distribution,00:02:07,10,Then problem estimating word distribution
00:02:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"would be extremely simple, right?",00:02:09,10,would extremely simple right
00:02:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"If you think about this for a moment, you'll realize that, well,",00:02:11,10,If think moment realize well
00:02:21,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,we can simply take all these words that are known to be from,00:02:16,10,simply take words known
00:02:24,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"this word distribution, see that's a d and normalize them.",00:02:21,10,word distribution see normalize
00:02:29,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So indeed this problem would be very easy to solve if we had known,00:02:24,10,So indeed problem would easy solve known
00:02:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,which words are from which it is written precisely.,00:02:29,10,words written precisely
00:02:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And this is in fact,",00:02:33,10,And fact
00:02:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,making this model no longer a mystery model because we can already observe which,00:02:36,10,making model longer mystery model already observe
00:02:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,of these distribution has been used to generate which part of the data.,00:02:40,10,distribution used generate part data
00:02:51,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So we, actually go back to the single order distribution problem.",00:02:44,10,So actually go back single order distribution problem
00:02:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And in this case, let's call these words",00:02:51,10,And case let call words
00:03:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"that are known to be from theta d, a pseudo document of d prime.",00:02:55,10,known theta pseudo document prime
00:03:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And now all we have to do is just normalize these word,00:03:01,10,And normalize word
00:03:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"accounts for each word, w sub i.",00:03:06,10,accounts word w sub
00:03:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And that's fairly straightforward,",00:03:09,10,And fairly straightforward
00:03:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,and it's just dictated by the maximum estimator.,00:03:12,10,dictated maximum estimator
00:03:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now, this idea, however, doesn't work because we in practice,",00:03:17,10,Now idea however work practice
00:03:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,don't really know which word is from which distribution.,00:03:23,10,really know word distribution
00:03:29,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,But this gives us an idea of perhaps,00:03:26,10,But gives us idea perhaps
00:03:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,we can guess which word is from which distribution.,00:03:29,10,guess word distribution
00:03:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Specifically, given all the parameters,",00:03:34,10,Specifically given parameters
00:03:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,can we infer the distribution a word is from?,00:03:37,10,infer distribution word
00:03:47,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So let's assume that we actually know tentative probabilities for,00:03:41,10,So let assume actually know tentative probabilities
00:03:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,these words in theta sub d.,00:03:47,10,words theta sub
00:03:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So now all the parameters are known for this mystery model.,00:03:50,10,So parameters known mystery model
00:03:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now let's consider word, like a text.",00:03:55,10,Now let consider word like text
00:04:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So the question is, do you think text is more likely,",00:03:59,10,So question think text likely
00:04:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,having been generated from theta sub d or from theta sub b?,00:04:03,10,generated theta sub theta sub b
00:04:09,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, in other words,",00:04:08,10,So words
00:04:13,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,we are to infer which distribution has been used to generate this text.,00:04:09,10,infer distribution used generate text
00:04:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now, this inference process is a typical of basing an inference situation,",00:04:15,10,Now inference process typical basing inference situation
00:04:24,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,where we have some prior about these two distributions.,00:04:19,10,prior two distributions
00:04:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So can you see what is our prior here?,00:04:24,10,So see prior
00:04:33,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Well, the prior here is the probability of each distribution, right.",00:04:27,10,Well prior probability distribution right
00:04:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So the prior is given by these two probabilities.,00:04:33,10,So prior given two probabilities
00:04:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"In this case, the prior is saying that each model is equally likely.",00:04:38,10,In case prior saying model equally likely
00:04:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,But we can imagine perhaps a different apply is possible.,00:04:44,10,But imagine perhaps different apply possible
00:04:52,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So this is called a pry because this is our guess,00:04:48,10,So called pry guess
00:04:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,of which distribution has been used to generate the word.,00:04:52,10,distribution used generate word
00:04:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Before we even observed the word.,00:04:55,10,Before even observed word
00:05:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So that's why we call it a pry.,00:04:57,10,So call pry
00:05:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,If we don't observe the word we don't know what word has been observed.,00:05:01,10,If observe word know word observed
00:05:10,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Our best guess is to say, well, they're equally likely.",00:05:05,10,Our best guess say well equally likely
00:05:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So it's just like flipping a coin.,00:05:10,10,So like flipping coin
00:05:14,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now in basic inference,",00:05:13,10,Now basic inference
00:05:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,we typical them with our belief after we have observed the evidence.,00:05:14,10,typical belief observed evidence
00:05:20,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So what is the evidence here?,00:05:18,10,So evidence
00:05:23,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Well, the evidence here is the word text.",00:05:20,10,Well evidence word text
00:05:29,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Now that we know we're interested in the word text.,00:05:25,10,Now know interested word text
00:05:31,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So text can be regarded as evidence.,00:05:29,10,So text regarded evidence
00:05:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And if we use base,00:05:31,10,And use base
00:05:41,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"rule to combine the prior and the theta likelihood,",00:05:36,10,rule combine prior theta likelihood
00:05:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,what we will end up with is to combine the prior,00:05:41,10,end combine prior
00:05:52,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,with the likelihood that you see here.,00:05:46,10,likelihood see
00:05:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Which is basically the probability of the word text from each distribution.,00:05:52,10,Which basically probability word text distribution
00:06:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And we see that in both cases text is possible.,00:05:57,10,And see cases text possible
00:06:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Note that even in the background it is still possible,",00:06:00,10,Note even background still possible
00:06:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,it just has a very small probability.,00:06:03,10,small probability
00:06:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So intuitively what would be your guess seeing this case?,00:06:07,10,So intuitively would guess seeing case
00:06:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now if you're like many others, you would guess text is probably",00:06:13,10,Now like many others would guess text probably
00:06:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"from c.subd it's more likely from c.subd, why?",00:06:17,10,c subd likely c subd
00:06:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And you will probably see that it's because text has,00:06:22,10,And probably see text
00:06:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,a much higher probability here by the C now sub D than,00:06:27,10,much higher probability C sub D
00:06:39,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,by the background model which has a very small probability.,00:06:32,10,background model small probability
00:06:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And by this we're going to say well, text is more likely from theta sub d.",00:06:39,10,And going say well text likely theta sub
00:06:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So you see our guess of which distributing has been used with,00:06:44,10,So see guess distributing used
00:06:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"the generated text would depend on how high the probability of the data,",00:06:49,10,generated text would depend high probability data
00:06:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"the text, is in each word distribution.",00:06:55,10,text word distribution
00:07:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,We can do tentative guess that distribution that gives is a word,00:06:59,10,We tentative guess distribution gives word
00:07:04,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,higher probability.,00:07:03,10,higher probability
00:07:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And this is likely to maximize the likelihood.,00:07:04,10,And likely maximize likelihood
00:07:15,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"All right, so we are going to choose a word that has a higher likelihood.",00:07:08,10,All right going choose word higher likelihood
00:07:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, in other words we are going to compare these two probabilities",00:07:15,10,So words going compare two probabilities
00:07:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,of the word given by each of these distributions.,00:07:21,10,word given distributions
00:07:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,But our guess must also be affected by the prior.,00:07:25,10,But guess must also affected prior
00:07:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So we also need to compare these two priors.,00:07:30,10,So also need compare two priors
00:07:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Why?,00:07:34,10,Why
00:07:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Because imagine if we adjust these probabilities.,00:07:34,10,Because imagine adjust probabilities
00:07:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"We're going to say,",00:07:38,10,We going say
00:07:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,the probability of choosing a background model is almost 100%.,00:07:40,10,probability choosing background model almost 100
00:07:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Now if we have that kind of strong prior, then that would affect your gas.",00:07:44,10,Now kind strong prior would affect gas
00:07:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"You might think,",00:07:49,10,You might think
00:07:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"well, wait a moment, maybe texter could have been from the background as well.",00:07:50,10,well wait moment maybe texter could background well
00:07:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Although the probability is very small here the prior is very high.,00:07:55,10,Although probability small prior high
00:08:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So in the end, we have to combine the two.",00:08:01,10,So end combine two
00:08:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And the base formula provides us a solid and,00:08:03,10,And base formula provides us solid
00:08:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,principle way of making this kind of guess to quantify that.,00:08:08,10,principle way making kind guess quantify
00:08:18,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So more specifically, let's think about the probability that this word text",00:08:13,10,So specifically let think probability word text
00:08:21,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,has been generated in fact from theta sub d.,00:08:18,10,generated fact theta sub
00:08:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Well, in order for text to be generated from theta sub d, two things must happen.",00:08:21,10,Well order text generated theta sub two things must happen
00:08:31,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"First, the theta sub d must have been selected.",00:08:27,10,First theta sub must selected
00:08:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, we have the selection probability here.",00:08:31,10,So selection probability
00:08:41,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And secondly we also have to actually have observed the text from the distribution.,00:08:34,10,And secondly also actually observed text distribution
00:08:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So, when we multiply the two together, we get the probability",00:08:41,10,So multiply two together get probability
00:08:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,that text has in fact been generated from zero sub d.,00:08:45,10,text fact generated zero sub
00:08:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Similarly, for the background model and",00:08:50,10,Similarly background model
00:09:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,the probability of generating text is another product of similar form.,00:08:54,10,probability generating text another product similar form
00:09:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,Now we also introduced late in the variable z here to denote,00:09:00,10,Now also introduced late variable z denote
00:09:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,whether the word is from the background or the topic.,00:09:05,10,whether word background topic
00:09:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"When z is 0, it means it's from the topic, theta sub d.",00:09:11,10,When z 0 means topic theta sub
00:09:21,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"When it's 1, it means it's from the background, theta sub B.",00:09:17,10,When 1 means background theta sub B
00:09:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"So now we have the probability that text is generated from each,",00:09:21,10,So probability text generated
00:09:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,then we can simply normalize them to have estimate,00:09:26,10,simply normalize estimate
00:09:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,of the probability that the word text is from,00:09:32,10,probability word text
00:09:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,theta sub d or from theta sub B.,00:09:36,10,theta sub theta sub B
00:09:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And equivalently the probability that Z is equal to zero,",00:09:42,10,And equivalently probability Z equal zero
00:09:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,given that the observed evidence is text.,00:09:46,10,given observed evidence text
00:09:54,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,So this is application of base rule.,00:09:51,10,So application base rule
00:10:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,But this step is very crucial for understanding the EM hours.,00:09:56,10,But step crucial understanding EM hours
00:10:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Because if we can do this, then we would be able to first,",00:10:01,10,Because would able first
00:10:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,initialize the parameter values somewhat randomly.,00:10:06,10,initialize parameter values somewhat randomly
00:10:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"And then, we're going to take a guess of these Z values and",00:10:12,10,And going take guess Z values
00:10:20,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"all, which distributing has been used to generate which word.",00:10:17,10,distributing used generate word
00:10:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And the initialize the parameter values would allow us to have a complete,00:10:21,10,And initialize parameter values would allow us complete
00:10:31,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"specification of the mixture model, which allows us to apply Bayes'",00:10:26,10,specification mixture model allows us apply Bayes
00:10:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,rule to infer which distribution is more likely to generate each word.,00:10:31,10,rule infer distribution likely generate word
00:10:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,And this prediction essentially helped us,00:10:36,10,And prediction essentially helped us
00:10:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,to separate words from the two distributions.,00:10:40,10,separate words two distributions
00:10:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,"Although we can't separate them for sure,",00:10:44,10,Although separate sure
00:10:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1,2.10,but we can separate then probabilistically as shown here.,00:10:48,10,separate probabilistically shown
00:00:05,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,[SOUND] This lecture is about the syntagmatic,00:00:00,11,SOUND This lecture syntagmatic
00:00:12,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,relation discovery and conditional entropy.,00:00:05,11,relation discovery conditional entropy
00:00:12,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"In this lecture,",00:00:12,11,In lecture
00:00:16,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we're going to continue the discussion of word association mining and analysis.,00:00:12,11,going continue discussion word association mining analysis
00:00:22,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"We're going to talk about the conditional entropy, which is useful for",00:00:18,11,We going talk conditional entropy useful
00:00:25,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,discovering syntagmatic relations.,00:00:22,11,discovering syntagmatic relations
00:00:29,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Earlier, we talked about using entropy to capture",00:00:25,11,Earlier talked using entropy capture
00:00:33,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,how easy it is to predict the presence or absence of a word.,00:00:29,11,easy predict presence absence word
00:00:37,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, we'll address a different scenario where",00:00:34,11,Now address different scenario
00:00:41,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we assume that we know something about the text segment.,00:00:37,11,assume know something text segment
00:00:48,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So now the question is, suppose we know that eats occurred in the segment.",00:00:41,11,So question suppose know eats occurred segment
00:00:51,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,How would that help us predict the presence or,00:00:48,11,How would help us predict presence
00:00:53,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"absence of water, like in meat?",00:00:51,11,absence water like meat
00:00:58,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And in particular, we want to know whether the presence of eats",00:00:53,11,And particular want know whether presence eats
00:01:00,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,has helped us predict the presence of meat.,00:00:58,11,helped us predict presence meat
00:01:05,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And if we frame this using entrophy,",00:01:02,11,And frame using entrophy
00:01:10,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,that would mean we are interested in knowing whether knowing,00:01:05,11,would mean interested knowing whether knowing
00:01:15,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the presence of eats could reduce uncertainty about the meats.,00:01:10,11,presence eats could reduce uncertainty meats
00:01:18,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Or, reduce the entrophy of the random variable",00:01:15,11,Or reduce entrophy random variable
00:01:23,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,corresponding to the presence or absence of meat.,00:01:18,11,corresponding presence absence meat
00:01:27,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"We can also ask as a question, what if we know of the absents of eats?",00:01:23,11,We also ask question know absents eats
00:01:33,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Would that also help us predict the presence or absence of meat?,00:01:28,11,Would also help us predict presence absence meat
00:01:39,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,These questions can be addressed by using another,00:01:34,11,These questions addressed using another
00:01:43,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,concept called a conditioning entropy.,00:01:39,11,concept called conditioning entropy
00:01:48,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So to explain this concept, let's first look at the scenario we had before,",00:01:43,11,So explain concept let first look scenario
00:01:51,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,when we know nothing about the segment.,00:01:48,11,know nothing segment
00:01:56,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So we have these probabilities indicating whether a word like meat occurs,",00:01:51,11,So probabilities indicating whether word like meat occurs
00:01:58,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,or it doesn't occur in the segment.,00:01:56,11,occur segment
00:02:02,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And we have an entropy function that looks like what you see on the slide.,00:01:58,11,And entropy function looks like see slide
00:02:07,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now suppose we know eats is present, so",00:02:03,11,Now suppose know eats present
00:02:11,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,now we know the value of another random variable that denotes eats.,00:02:07,11,know value another random variable denotes eats
00:02:15,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, that would change all these probabilities to",00:02:12,11,Now would change probabilities
00:02:17,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,conditional probabilities.,00:02:15,11,conditional probabilities
00:02:20,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Where we look at the presence or absence of meat,",00:02:17,11,Where look presence absence meat
00:02:25,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,given that we know eats occurred in the context.,00:02:21,11,given know eats occurred context
00:02:27,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So as a result,",00:02:25,11,So result
00:02:31,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,if we replace these probabilities with their corresponding conditional,00:02:27,11,replace probabilities corresponding conditional
00:02:36,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"probabilities in the entropy function, we'll get the conditional entropy.",00:02:31,11,probabilities entropy function get conditional entropy
00:02:42,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So this equation now here would be,00:02:37,11,So equation would
00:02:46,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the conditional entropy.,00:02:42,11,conditional entropy
00:02:49,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Conditional on the presence of eats.,00:02:46,11,Conditional presence eats
00:02:57,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So, you can see this is essentially the same entropy function as you have",00:02:52,11,So see essentially entropy function
00:03:01,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"seen before, except that all the probabilities now have a condition.",00:02:57,11,seen except probabilities condition
00:03:09,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And this then tells us the entropy of meat,",00:03:04,11,And tells us entropy meat
00:03:13,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,after we have known eats occurring in the segment.,00:03:09,11,known eats occurring segment
00:03:17,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And of course, we can also define this conditional entropy for",00:03:14,11,And course also define conditional entropy
00:03:20,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the scenario where we don't see eats.,00:03:17,11,scenario see eats
00:03:25,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So if we know it did not occur in the segment, then this entry condition of",00:03:20,11,So know occur segment entry condition
00:03:30,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,entropy would capture the instances of meat in that condition.,00:03:25,11,entropy would capture instances meat condition
00:03:34,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So now, putting different scenarios together,",00:03:30,11,So putting different scenarios together
00:03:37,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we have the completed definition of conditional entropy as follows.,00:03:34,11,completed definition conditional entropy follows
00:03:48,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Basically, we're going to consider both scenarios of the value of eats zero, one,",00:03:39,11,Basically going consider scenarios value eats zero one
00:03:54,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,and this gives us a probability that eats is equal to zero or one.,00:03:48,11,gives us probability eats equal zero one
00:03:58,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Basically, whether eats is present or absent.",00:03:54,11,Basically whether eats present absent
00:03:59,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And this of course,",00:03:58,11,And course
00:04:04,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,is the conditional entropy of meat in that particular scenario.,00:03:59,11,conditional entropy meat particular scenario
00:04:10,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So if you expanded this entropy,",00:04:05,11,So expanded entropy
00:04:14,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,then you have the following equation.,00:04:10,11,following equation
00:04:19,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Where you see the involvement of those conditional probabilities.,00:04:15,11,Where see involvement conditional probabilities
00:04:26,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now in general, for any discrete random variables x and y, we have",00:04:21,11,Now general discrete random variables x
00:04:35,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the conditional entropy is no larger than the entropy of the variable x.,00:04:27,11,conditional entropy larger entropy variable x
00:04:41,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So basically, this is upper bound for the conditional entropy.",00:04:35,11,So basically upper bound conditional entropy
00:04:46,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"That means by knowing more information about the segment,",00:04:41,11,That means knowing information segment
00:04:49,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we want to be able to increase uncertainty.,00:04:46,11,want able increase uncertainty
00:04:51,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,We can only reduce uncertainty.,00:04:49,11,We reduce uncertainty
00:04:56,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And that intuitively makes sense because as we know more information,",00:04:51,11,And intuitively makes sense know information
00:05:00,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,it should always help us make the prediction.,00:04:56,11,always help us make prediction
00:05:04,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And cannot hurt the prediction in any case.,00:05:00,11,And cannot hurt prediction case
00:05:08,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, what's interesting here is also to think about what's the minimum possible",00:05:05,11,Now interesting also think minimum possible
00:05:11,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,value of this conditional entropy?,00:05:08,11,value conditional entropy
00:05:16,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, we know that the maximum value is the entropy of X.",00:05:11,11,Now know maximum value entropy X
00:05:20,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"But what about the minimum, so what do you think?",00:05:17,11,But minimum think
00:05:28,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"I hope you can reach the conclusion that the minimum possible value, would be zero.",00:05:22,11,I hope reach conclusion minimum possible value would zero
00:05:33,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And it will be interesting to think about under what situation will achieve this.,00:05:28,11,And interesting think situation achieve
00:05:37,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So, let's see how we can use conditional entropy to capture syntagmatic relation.",00:05:34,11,So let see use conditional entropy capture syntagmatic relation
00:05:44,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now of course, this conditional entropy gives us directly",00:05:39,11,Now course conditional entropy gives us directly
00:05:48,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,one way to measure the association of two words.,00:05:44,11,one way measure association two words
00:05:53,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Because it tells us to what extent, we can predict the one",00:05:48,11,Because tells us extent predict one
00:05:58,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,word given that we know the presence or absence of another word.,00:05:53,11,word given know presence absence another word
00:06:03,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Now before we look at the intuition of conditional entropy in capturing,00:05:58,11,Now look intuition conditional entropy capturing
00:06:09,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"syntagmatic relations, it's useful to think of a very special case, listed here.",00:06:03,11,syntagmatic relations useful think special case listed
00:06:17,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"That is, the conditional entropy of the word given itself.",00:06:09,11,That conditional entropy word given
00:06:22,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So here,",00:06:19,11,So
00:06:28,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we listed this conditional entropy in the middle.,00:06:22,11,listed conditional entropy middle
00:06:31,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So, it's here.",00:06:28,11,So
00:06:35,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So, what is the value of this?",00:06:33,11,So value
00:06:43,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, this means we know where the meat occurs in the sentence.",00:06:36,11,Now means know meat occurs sentence
00:06:47,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And we hope to predict whether the meat occurs in the sentence.,00:06:43,11,And hope predict whether meat occurs sentence
00:06:52,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And of course, this is 0 because there's no incident anymore.",00:06:47,11,And course 0 incident anymore
00:06:55,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Once we know whether the word occurs in the segment,",00:06:52,11,Once know whether word occurs segment
00:06:59,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,we'll already know the answer of the prediction.,00:06:55,11,already know answer prediction
00:07:00,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So this is zero.,00:06:59,11,So zero
00:07:03,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And that's also when this conditional entropy reaches the minimum.,00:07:00,11,And also conditional entropy reaches minimum
00:07:08,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So now, let's look at some other cases.",00:07:06,11,So let look cases
00:07:15,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So this is a case of knowing the and trying to predict the meat.,00:07:09,11,So case knowing trying predict meat
00:07:20,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And this is a case of knowing eats and trying to predict the meat.,00:07:15,11,And case knowing eats trying predict meat
00:07:22,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Which one do you think is smaller?,00:07:20,11,Which one think smaller
00:07:27,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,No doubt smaller entropy means easier for prediction.,00:07:22,11,No doubt smaller entropy means easier prediction
00:07:33,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Which one do you think is higher?,00:07:31,11,Which one think higher
00:07:34,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Which one is not smaller?,00:07:33,11,Which one smaller
00:07:41,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Well, if you at the uncertainty, then in the first case,",00:07:36,11,Well uncertainty first case
00:07:45,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the doesn't really tell us much about the meat.,00:07:41,11,really tell us much meat
00:07:51,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So knowing the occurrence of the doesn't really help us reduce entropy that much.,00:07:45,11,So knowing occurrence really help us reduce entropy much
00:07:56,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So it stays fairly close to the original entropy of meat.,00:07:51,11,So stays fairly close original entropy meat
00:08:01,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Whereas in the case of eats, eats is related to meat.",00:07:56,11,Whereas case eats eats related meat
00:08:04,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So knowing presence of eats or absence of eats,",00:08:01,11,So knowing presence eats absence eats
00:08:07,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,would help us predict whether meat occurs.,00:08:04,11,would help us predict whether meat occurs
00:08:14,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So it can help us reduce entropy of meat.,00:08:07,11,So help us reduce entropy meat
00:08:20,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So we should expect the sigma term, namely this one, to have a smaller entropy.",00:08:14,11,So expect sigma term namely one smaller entropy
00:08:25,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And that means there is a stronger association between meat and eats.,00:08:21,11,And means stronger association meat eats
00:08:36,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So we now also know when this w is the same as this,00:08:29,11,So also know w
00:08:41,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"meat, then the conditional entropy would reach its minimum, which is 0.",00:08:36,11,meat conditional entropy would reach minimum 0
00:08:45,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And for what kind of words would either reach its maximum?,00:08:41,11,And kind words would either reach maximum
00:08:49,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Well, that's when this stuff is not really related to meat.",00:08:45,11,Well stuff really related meat
00:08:55,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And like the for example, it would be very close to the maximum,",00:08:49,11,And like example would close maximum
00:08:58,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,which is the entropy of meat itself.,00:08:55,11,entropy meat
00:09:03,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So this suggests that when you use conditional entropy for,00:08:59,11,So suggests use conditional entropy
00:09:07,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"mining syntagmatic relations, the hours would look as follows.",00:09:03,11,mining syntagmatic relations hours would look follows
00:09:14,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"For each word W1, we're going to enumerate the overall other words W2.",00:09:10,11,For word W1 going enumerate overall words W2
00:09:21,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And then, we can compute the conditional entropy of W1 given W2.",00:09:14,11,And compute conditional entropy W1 given W2
00:09:26,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,We thought all the candidate was in ascending order of the conditional entropy,00:09:22,11,We thought candidate ascending order conditional entropy
00:09:30,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"because we're out of favor, a world that has a small entropy.",00:09:26,11,favor world small entropy
00:09:34,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Meaning that it helps us predict the time of the word W1.,00:09:30,11,Meaning helps us predict time word W1
00:09:38,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And then, we're going to take the top ring of the candidate words as words that have",00:09:34,11,And going take top ring candidate words words
00:09:40,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,potential syntagmatic relations with W1.,00:09:38,11,potential syntagmatic relations W1
00:09:47,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Note that we need to use a threshold to find these words.,00:09:41,11,Note need use threshold find words
00:09:51,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"The stresser can be the number of top candidates take, or",00:09:47,11,The stresser number top candidates take
00:09:54,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,absolute value for the conditional entropy.,00:09:51,11,absolute value conditional entropy
00:10:00,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Now, this would allow us to mine the most",00:09:55,11,Now would allow us mine
00:10:03,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"strongly correlated words with a particular word, W1 here.",00:10:00,11,strongly correlated words particular word W1
00:10:10,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"But, this algorithm does not help us mine the strongest",00:10:06,11,But algorithm help us mine strongest
00:10:14,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,that K syntagmatical relations from an entire collection.,00:10:10,11,K syntagmatical relations entire collection
00:10:19,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Because in order to do that, we have to ensure that these conditional entropies",00:10:14,11,Because order ensure conditional entropies
00:10:24,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,are comparable across different words.,00:10:19,11,comparable across different words
00:10:28,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,In this case of discovering the mathematical relations for,00:10:24,11,In case discovering mathematical relations
00:10:33,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"a targeted word like W1, we only need to compare the conditional entropies",00:10:28,11,targeted word like W1 need compare conditional entropies
00:10:38,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"for W1, given different words.",00:10:34,11,W1 given different words
00:10:40,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"And in this case, they are comparable.",00:10:38,11,And case comparable
00:10:43,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,All right.,00:10:41,11,All right
00:10:48,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"So, the conditional entropy of W1, given W2, and the conditional entropy of W1,",00:10:43,11,So conditional entropy W1 given W2 conditional entropy W1
00:10:49,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,given W3 are comparable.,00:10:48,11,given W3 comparable
00:10:55,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,They all measure how hard it is to predict the W1.,00:10:51,11,They measure hard predict W1
00:11:00,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"But, if we think about the two pairs,",00:10:55,11,But think two pairs
00:11:06,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"where we share W2 in the same condition, and we try to predict the W1 and W3.",00:11:00,11,share W2 condition try predict W1 W3
00:11:11,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Then, the conditional entropies are actually not comparable.",00:11:06,11,Then conditional entropies actually comparable
00:11:15,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,You can think of about this question.,00:11:11,11,You think question
00:11:17,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Why?,00:11:15,11,Why
00:11:19,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So why are they not comfortable?,00:11:17,11,So comfortable
00:11:23,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Well, that was because they have a different outer bounds.",00:11:19,11,Well different outer bounds
00:11:25,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,Right? So those outer bounds are precisely,00:11:23,11,Right So outer bounds precisely
00:11:29,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,the entropy of W1 and the entropy of W3.,00:11:25,11,entropy W1 entropy W3
00:11:31,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,And they have different upper bounds.,00:11:29,11,And different upper bounds
00:11:35,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So we cannot really compare them in this way.,00:11:31,11,So cannot really compare way
00:11:36,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,So how do we address this problem?,00:11:35,11,So address problem
00:11:45,2,Syntagmatic Relation Discovery- Conditional Entropy,1.11,"Well later, we'll discuss, we can use mutual information to solve this problem.",00:11:38,11,Well later discuss use mutual information solve problem
00:00:06,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,[SOUND] >> This,00:00:00,1,SOUND This
00:00:11,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,lecture is about topic mining and analysis.,00:00:06,1,lecture topic mining analysis
00:00:14,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,We're going to talk about its motivation and task definition.,00:00:11,1,We going talk motivation task definition
00:00:22,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,In this lecture we're going to talk about different kind of mining task.,00:00:17,1,In lecture going talk different kind mining task
00:00:28,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"As you see on this road map, we have just covered",00:00:23,1,As see road map covered
00:00:33,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"mining knowledge about language, namely discovery of",00:00:28,1,mining knowledge language namely discovery
00:00:37,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,word associations such as paradigmatic and relations and syntagmatic relations.,00:00:33,1,word associations paradigmatic relations syntagmatic relations
00:00:43,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Now, starting from this lecture, we're going to talk about mining another kind of",00:00:39,1,Now starting lecture going talk mining another kind
00:00:47,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"knowledge, which is content mining, and",00:00:43,1,knowledge content mining
00:00:55,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,trying to discover knowledge about the main topics in the text.,00:00:47,1,trying discover knowledge main topics text
00:00:58,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And we call that topic mining and analysis.,00:00:56,1,And call topic mining analysis
00:01:04,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"In this lecture, we're going to talk about its motivation and the task definition.",00:00:59,1,In lecture going talk motivation task definition
00:01:08,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So first of all, let's look at the concept of topic.",00:01:04,1,So first let look concept topic
00:01:12,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So topic is something that we all understand, I think, but",00:01:08,1,So topic something understand I think
00:01:15,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,it's actually not that easy to formally define.,00:01:12,1,actually easy formally define
00:01:20,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Roughly speaking, topic is the main idea discussed in text data.",00:01:15,1,Roughly speaking topic main idea discussed text data
00:01:25,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And you can think of this as a theme or subject of a discussion or conversation.,00:01:20,1,And think theme subject discussion conversation
00:01:28,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,It can also have different granularities.,00:01:25,1,It also different granularities
00:01:31,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"For example, we can talk about the topic of a sentence.",00:01:28,1,For example talk topic sentence
00:01:34,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"A topic of article, aa topic of paragraph or",00:01:31,1,A topic article aa topic paragraph
00:01:40,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"the topic of all the research articles in the research library, right,",00:01:34,1,topic research articles research library right
00:01:45,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,so different grand narratives of topics obviously have different applications.,00:01:40,1,different grand narratives topics obviously different applications
00:01:51,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Indeed, there are many applications that require discovery of topics in text, and",00:01:46,1,Indeed many applications require discovery topics text
00:01:52,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,they're analyzed then.,00:01:51,1,analyzed
00:01:54,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Here are some examples.,00:01:52,1,Here examples
00:01:58,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"For example, we might be interested in knowing about what are Twitter",00:01:54,1,For example might interested knowing Twitter
00:02:00,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,users are talking about today?,00:01:58,1,users talking today
00:02:03,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Are they talking about NBA sports, or",00:02:00,1,Are talking NBA sports
00:02:08,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"are they talking about some international events, etc.?",00:02:03,1,talking international events etc
00:02:12,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Or we are interested in knowing about research topics.,00:02:08,1,Or interested knowing research topics
00:02:17,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"For example, one might be interested in knowing what are the current research",00:02:12,1,For example one might interested knowing current research
00:02:21,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"topics in data mining, and how are they different from those five years ago?",00:02:17,1,topics data mining different five years ago
00:02:26,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Now this involves discovery of topics in data mining literatures and,00:02:21,1,Now involves discovery topics data mining literatures
00:02:32,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,also we want to discover topics in today's literature and those in the past.,00:02:26,1,also want discover topics today literature past
00:02:34,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And then we can make a comparison.,00:02:32,1,And make comparison
00:02:38,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,We might also be also interested in knowing what do people like about,00:02:34,1,We might also also interested knowing people like
00:02:43,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"some products like the iPhone 6, and what do they dislike?",00:02:38,1,products like iPhone 6 dislike
00:02:48,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And this involves discovering topics in positive opinions about,00:02:43,1,And involves discovering topics positive opinions
00:02:52,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,iPhone 6 and also negative reviews about it.,00:02:48,1,iPhone 6 also negative reviews
00:02:56,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Or perhaps we're interested in knowing what are the major topics debated in 2012,00:02:52,1,Or perhaps interested knowing major topics debated 2012
00:02:58,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,presidential election?,00:02:56,1,presidential election
00:03:04,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"And all these have to do with discovering topics in text and analyzing them,",00:02:59,1,And discovering topics text analyzing
00:03:08,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,and we're going to talk about a lot of techniques for doing this.,00:03:04,1,going talk lot techniques
00:03:12,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,In general we can view a topic as some knowledge about the world.,00:03:08,1,In general view topic knowledge world
00:03:17,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So from text data we expect to discover a number of topics, and",00:03:12,1,So text data expect discover number topics
00:03:22,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,then these topics generally provide a description about the world.,00:03:17,1,topics generally provide description world
00:03:25,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And it tells us something about the world.,00:03:22,1,And tells us something world
00:03:28,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"About a product, about a person etc.",00:03:25,1,About product person etc
00:03:32,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Now when we have some non-text data,",00:03:29,1,Now non text data
00:03:36,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,then we can have more context for analyzing the topics.,00:03:32,1,context analyzing topics
00:03:41,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"For example, we might know the time associated with the text data, or",00:03:36,1,For example might know time associated text data
00:03:47,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"locations where the text data were produced,",00:03:41,1,locations text data produced
00:03:52,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"or the authors of the text, or the sources of the text, etc.",00:03:47,1,authors text sources text etc
00:03:54,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"All such meta data, or",00:03:52,1,All meta data
00:03:59,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"context variables can be associated with the topics that we discover, and",00:03:54,1,context variables associated topics discover
00:04:05,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,then we can use these context variables help us analyze patterns of topics.,00:03:59,1,use context variables help us analyze patterns topics
00:04:09,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"For example, looking at topics over time, we would be able to discover",00:04:05,1,For example looking topics time would able discover
00:04:14,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"whether there's a trending topic, or some topics might be fading away.",00:04:09,1,whether trending topic topics might fading away
00:04:18,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Soon you are looking at topics in different locations.,00:04:15,1,Soon looking topics different locations
00:04:24,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,We might know some insights about people's opinions in different locations.,00:04:18,1,We might know insights people opinions different locations
00:04:29,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,So that's why mining topics is very important.,00:04:26,1,So mining topics important
00:04:34,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Now, let's look at the tasks of topic mining and analysis.",00:04:29,1,Now let look tasks topic mining analysis
00:04:39,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"In general, it would involve first discovering a lot of topics, in this case,",00:04:34,1,In general would involve first discovering lot topics case
00:04:40,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,k topics.,00:04:39,1,k topics
00:04:45,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"And then we also would like to know, which topics are covered in which documents,",00:04:40,1,And also would like know topics covered documents
00:04:46,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,to what extent.,00:04:45,1,extent
00:04:52,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So for example, in document one, we might see that Topic 1 is covered a lot,",00:04:46,1,So example document one might see Topic 1 covered lot
00:04:57,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Topic 2 and Topic k are covered with a small portion.,00:04:52,1,Topic 2 Topic k covered small portion
00:05:00,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"And other topics, perhaps, are not covered.",00:04:58,1,And topics perhaps covered
00:05:06,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Document two, on the other hand, covered Topic 2 very well,",00:05:00,1,Document two hand covered Topic 2 well
00:05:10,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"but it did not cover Topic 1 at all, and",00:05:06,1,cover Topic 1
00:05:15,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"it also covers Topic k to some extent, etc., right?",00:05:10,1,also covers Topic k extent etc right
00:05:19,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So now you can see there are generally two different tasks, or",00:05:15,1,So see generally two different tasks
00:05:25,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"sub-tasks, the first is to discover k topics from a collection of text laid out.",00:05:19,1,sub tasks first discover k topics collection text laid
00:05:27,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,What are these k topics?,00:05:25,1,What k topics
00:05:28,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Okay, major topics in the text they are.",00:05:27,1,Okay major topics text
00:05:33,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,The second task is to figure out which documents cover which topics,00:05:28,1,The second task figure documents cover topics
00:05:34,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,to what extent.,00:05:33,1,extent
00:05:37,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So more formally, we can define the problem as follows.",00:05:34,1,So formally define problem follows
00:05:42,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"First, we have, as input, a collection of N text documents.",00:05:37,1,First input collection N text documents
00:05:47,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Here we can denote the text collection as C, and",00:05:42,1,Here denote text collection C
00:05:51,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,denote text article as d i.,00:05:47,1,denote text article
00:05:56,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"And, we generally also need to have as input the number of topics, k.",00:05:51,1,And generally also need input number topics k
00:06:01,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,But there may be techniques that can automatically suggest a number of topics.,00:05:56,1,But may techniques automatically suggest number topics
00:06:06,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"But in the techniques that we will discuss, which are also the most useful",00:06:01,1,But techniques discuss also useful
00:06:12,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"techniques, we often need to specify a number of topics.",00:06:06,1,techniques often need specify number topics
00:06:19,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"Now the output would then be the k topics that we would like to discover,",00:06:14,1,Now output would k topics would like discover
00:06:23,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,in order as theta sub one through theta sub k.,00:06:19,1,order theta sub one theta sub k
00:06:29,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Also we want to generate the coverage of topics in each document of d sub i And,00:06:24,1,Also want generate coverage topics document sub And
00:06:32,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,this is denoted by pi sub i j.,00:06:29,1,denoted pi sub j
00:06:38,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And pi sub ij is the probability of document d sub i,00:06:33,1,And pi sub ij probability document sub
00:06:41,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,covering topic theta sub j.,00:06:38,1,covering topic theta sub j
00:06:45,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So obviously for each document, we have a set of such values to indicate to",00:06:41,1,So obviously document set values indicate
00:06:47,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"what extent the document covers, each topic.",00:06:45,1,extent document covers topic
00:06:53,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,And we can assume that these probabilities sum to one.,00:06:48,1,And assume probabilities sum one
00:06:57,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Because a document won't be able to cover,00:06:53,1,Because document able cover
00:07:02,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"other topics outside of the topics that we discussed, that we discovered.",00:06:57,1,topics outside topics discussed discovered
00:07:08,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So now, the question is, how do we define theta sub i, how do we define the topic?",00:07:02,1,So question define theta sub define topic
00:07:11,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,Now this problem has not been completely defined,00:07:08,1,Now problem completely defined
00:07:15,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,until we define what is exactly theta.,00:07:11,1,define exactly theta
00:07:19,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,"So in the next few lectures,",00:07:16,1,So next lectures
00:07:24,3,Topic Mining and Analysis- Motivation and Task Definition,2.1,we're going to talk about different ways to define theta.,00:07:19,1,going talk different ways define theta
00:00:12,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,This lecture is about Probabilistic Topic Models for topic mining and analysis.,00:00:06,3,This lecture Probabilistic Topic Models topic mining analysis
00:00:14,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"In this lecture,",00:00:13,3,In lecture
00:00:16,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,we're going to continue talking about the topic mining and analysis.,00:00:14,3,going continue talking topic mining analysis
00:00:20,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We're going to introduce probabilistic topic models.,00:00:18,3,We going introduce probabilistic topic models
00:00:26,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So this is a slide that you have seen earlier,",00:00:22,3,So slide seen earlier
00:00:30,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,where we discussed the problems with using a term as a topic.,00:00:26,3,discussed problems using term topic
00:00:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So, to solve these problems intuitively we need to use",00:00:30,3,So solve problems intuitively need use
00:00:37,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,more words to describe the topic.,00:00:35,3,words describe topic
00:00:43,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And this will address the problem of lack of expressive power.,00:00:37,3,And address problem lack expressive power
00:00:45,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"When we have more words that we can use to describe the topic,",00:00:43,3,When words use describe topic
00:00:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,that we can describe complicated topics.,00:00:45,3,describe complicated topics
00:00:54,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,To address the second problem we need to introduce weights on words.,00:00:49,3,To address second problem need introduce weights words
00:00:59,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"This is what allows you to distinguish subtle differences in topics, and",00:00:54,3,This allows distinguish subtle differences topics
00:01:04,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,to introduce semantically related words in a fuzzy manner.,00:00:59,3,introduce semantically related words fuzzy manner
00:01:09,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Finally, to solve the problem of word ambiguity, we need to split",00:01:04,3,Finally solve problem word ambiguity need split
00:01:14,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"ambiguous word, so that we can disambiguate its topic.",00:01:09,3,ambiguous word disambiguate topic
00:01:21,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,It turns out that all these can be done by using a probabilistic topic model.,00:01:15,3,It turns done using probabilistic topic model
00:01:25,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And that's why we're going to spend a lot of lectures to talk about this topic.,00:01:21,3,And going spend lot lectures talk topic
00:01:28,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So the basic idea here is that,",00:01:25,3,So basic idea
00:01:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,improve the replantation of topic as one distribution.,00:01:28,3,improve replantation topic one distribution
00:01:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So what you see now is the older replantation.,00:01:32,3,So see older replantation
00:01:40,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Where we replanted each topic, it was just one word, or one term, or one phrase.",00:01:35,3,Where replanted topic one word one term one phrase
00:01:45,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,But now we're going to use a word distribution to describe the topic.,00:01:40,3,But going use word distribution describe topic
00:01:47,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So here you see that for sports.,00:01:45,3,So see sports
00:01:50,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We're going to use the word distribution over,00:01:47,3,We going use word distribution
00:01:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,theoretical speaking all the words in our vocabulary.,00:01:50,3,theoretical speaking words vocabulary
00:01:59,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So for example, the high probability words here are sports,",00:01:54,3,So example high probability words sports
00:02:03,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"game, basketball, football, play, star, etc.",00:01:59,3,game basketball football play star etc
00:02:06,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,These are sports related terms.,00:02:03,3,These sports related terms
00:02:10,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And of course it would also give a non-zero probability to some other word,00:02:06,3,And course would also give non zero probability word
00:02:15,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"like Trouble which might be related to sports in general,",00:02:10,3,like Trouble might related sports general
00:02:17,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,not so much related to topic.,00:02:15,3,much related topic
00:02:23,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In general we can imagine a non zero probability for all the words.,00:02:18,3,In general imagine non zero probability words
00:02:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And some words that are not read and would have very, very small probabilities.",00:02:23,3,And words read would small probabilities
00:02:29,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And these probabilities will sum to one.,00:02:27,3,And probabilities sum one
00:02:34,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So that it forms a distribution of all the words.,00:02:31,3,So forms distribution words
00:02:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Now intuitively, this distribution represents a topic in that if we assemble",00:02:36,3,Now intuitively distribution represents topic assemble
00:02:46,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"words from the distribution, we tended to see words that are ready to dispose.",00:02:41,3,words distribution tended see words ready dispose
00:02:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"You can also see, as a very special case, if the probability of the mass",00:02:48,3,You also see special case probability mass
00:02:57,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"is concentrated in entirely on just one word, it's sports.",00:02:53,3,concentrated entirely one word sports
00:03:01,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And this basically degenerates to the symbol foundation,00:02:57,3,And basically degenerates symbol foundation
00:03:03,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,of a topic was just one word.,00:03:01,3,topic one word
00:03:10,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"But as a distribution, this topic of representation can,",00:03:04,3,But distribution topic representation
00:03:13,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"in general, involve many words to describe a topic and",00:03:10,3,general involve many words describe topic
00:03:17,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,can model several differences in semantics of a topic.,00:03:13,3,model several differences semantics topic
00:03:24,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Similarly we can model Travel and Science with their respective distributions.,00:03:17,3,Similarly model Travel Science respective distributions
00:03:30,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"In the distribution for Travel we see top words like attraction, trip, flight etc.",00:03:24,3,In distribution Travel see top words like attraction trip flight etc
00:03:36,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Whereas in Science we see scientist, spaceship, telescope, or",00:03:31,3,Whereas Science see scientist spaceship telescope
00:03:39,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"genomics, and, you know, science related terms.",00:03:36,3,genomics know science related terms
00:03:43,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now that doesn't mean sports related terms,00:03:39,3,Now mean sports related terms
00:03:46,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,will necessarily have zero probabilities for science.,00:03:43,3,necessarily zero probabilities science
00:03:51,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In general we can imagine all of these words we have now zero probabilities.,00:03:46,3,In general imagine words zero probabilities
00:03:55,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"It's just that for a particular topic in some words we have very,",00:03:51,3,It particular topic words
00:03:56,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,very small probabilities.,00:03:55,3,small probabilities
00:04:02,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now you can also see there are some words that are shared by these topics.,00:03:58,3,Now also see words shared topics
00:04:07,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"When I say shared it just means even with some probability threshold,",00:04:02,3,When I say shared means even probability threshold
00:04:10,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,you can still see one word occurring much more topics.,00:04:07,3,still see one word occurring much topics
00:04:13,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In this case I mark them in black.,00:04:10,3,In case I mark black
00:04:17,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So you can see travel, for example, occurred in all the three topics here, but",00:04:13,3,So see travel example occurred three topics
00:04:19,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,with different probabilities.,00:04:17,3,different probabilities
00:04:23,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"It has the highest probability for the Travel topic, 0.05.",00:04:19,3,It highest probability Travel topic 0 05
00:04:29,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"But with much smaller probabilities for Sports and Science, which makes sense.",00:04:23,3,But much smaller probabilities Sports Science makes sense
00:04:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And similarly, you can see a Star also occurred in Sports and",00:04:29,3,And similarly see Star also occurred Sports
00:04:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Science with reasonably high probabilities.,00:04:32,3,Science reasonably high probabilities
00:04:39,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Because they might be actually related to the two topics.,00:04:35,3,Because might actually related two topics
00:04:43,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So with this replantation it addresses the three problems that I mentioned earlier.,00:04:39,3,So replantation addresses three problems I mentioned earlier
00:04:46,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"First, it now uses multiple words to describe a topic.",00:04:43,3,First uses multiple words describe topic
00:04:50,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So it allows us to describe a fairly complicated topics.,00:04:46,3,So allows us describe fairly complicated topics
00:04:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Second, it assigns weights to terms.",00:04:50,3,Second assigns weights terms
00:04:57,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So now we can model several differences of semantics.,00:04:53,3,So model several differences semantics
00:05:02,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And you can bring in related words together to model a topic.,00:04:57,3,And bring related words together model topic
00:05:07,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Third, because we have probabilities for the same word in different topics,",00:05:02,3,Third probabilities word different topics
00:05:12,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,we can disintegrate the sense of word.,00:05:07,3,disintegrate sense word
00:05:16,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"In the text to decode it's underlying topic,",00:05:12,3,In text decode underlying topic
00:05:22,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,to address all these three problems with this new way of representing a topic.,00:05:16,3,address three problems new way representing topic
00:05:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So now of course our problem definition has been refined just slightly.,00:05:22,3,So course problem definition refined slightly
00:05:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,The slight is very similar to what you've seen before except we have,00:05:27,3,The slight similar seen except
00:05:34,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,added refinement for what our topic is.,00:05:32,3,added refinement topic
00:05:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Now each topic is word distribution, and for each word distribution we know",00:05:34,3,Now topic word distribution word distribution know
00:05:45,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,that all the probabilities should sum to one with all the words in the vocabulary.,00:05:41,3,probabilities sum one words vocabulary
00:05:47,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So you see a constraint here.,00:05:45,3,So see constraint
00:05:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And we still have another constraint on the topic coverage, namely pis.",00:05:47,3,And still another constraint topic coverage namely pis
00:05:58,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So all the Pi sub ij's must sum to one for the same document.,00:05:53,3,So Pi sub ij must sum one document
00:06:01,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So how do we solve this problem?,00:05:59,3,So solve problem
00:06:05,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Well, let's look at this problem as a computation problem.",00:06:01,3,Well let look problem computation problem
00:06:07,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So we clearly specify it's input and,00:06:05,3,So clearly specify input
00:06:11,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,output and illustrate it here on this side.,00:06:07,3,output illustrate side
00:06:12,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Input of course is our text data.,00:06:11,3,Input course text data
00:06:18,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"C is our collection but we also generally assume we know the number of topics, k.",00:06:12,3,C collection also generally assume know number topics k
00:06:22,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Or we hypothesize a number and then try to bind k topics,",00:06:18,3,Or hypothesize number try bind k topics
00:06:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,even though we don't know the exact topics that exist in the collection.,00:06:22,3,even though know exact topics exist collection
00:06:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And V is the vocabulary that has a set of words that determines what,00:06:27,3,And V vocabulary set words determines
00:06:38,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,units would be treated as the basic units for analysis.,00:06:32,3,units would treated basic units analysis
00:06:44,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In most cases we'll use words as the basis for analysis.,00:06:38,3,In cases use words basis analysis
00:06:46,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And that means each word is a unique.,00:06:44,3,And means word unique
00:06:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now the output would consist of as first a set of topics represented by theta I's.,00:06:47,3,Now output would consist first set topics represented theta I
00:06:55,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Each theta I is a word distribution.,00:06:53,3,Each theta I word distribution
00:07:02,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we also want to know the coverage of topics in each document.,00:06:56,3,And also want know coverage topics document
00:07:03,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So that's.,00:07:02,3,So
00:07:06,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,That the same pi ijs that we have seen before.,00:07:03,3,That pi ijs seen
00:07:13,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So given a set of text data we would like compute all these distributions and,00:07:07,3,So given set text data would like compute distributions
00:07:16,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,all these coverages as you have seen on this slide.,00:07:13,3,coverages seen slide
00:07:21,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now of course there may be many different ways of solving this problem.,00:07:18,3,Now course may many different ways solving problem
00:07:24,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"In theory, you can write the [INAUDIBLE] program to solve this problem,",00:07:21,3,In theory write INAUDIBLE program solve problem
00:07:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,but here we're going to introduce,00:07:24,3,going introduce
00:07:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,a general way of solving this problem called a generative model.,00:07:27,3,general way solving problem called generative model
00:07:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And this is, in fact, a very general idea and",00:07:32,3,And fact general idea
00:07:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,it's a principle way of using statistical modeling to solve text mining problems.,00:07:35,3,principle way using statistical modeling solve text mining problems
00:07:46,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And here I dimmed the picture that you have seen before,00:07:41,3,And I dimmed picture seen
00:07:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,in order to show the generation process.,00:07:46,3,order show generation process
00:07:55,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So the idea of this approach is actually to first design a model for our data.,00:07:49,3,So idea approach actually first design model data
00:08:02,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So we design a probabilistic model to model how the data are generated.,00:07:55,3,So design probabilistic model model data generated
00:08:04,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Of course, this is based on our assumption.",00:08:02,3,Of course based assumption
00:08:08,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,The actual data aren't necessarily generating this way.,00:08:04,3,The actual data necessarily generating way
00:08:11,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So that gave us a probability distribution of the data,00:08:08,3,So gave us probability distribution data
00:08:13,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,that you are seeing on this slide.,00:08:11,3,seeing slide
00:08:18,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Given a particular model and parameters that are denoted by lambda.,00:08:13,3,Given particular model parameters denoted lambda
00:08:22,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So this template of actually consists of,00:08:18,3,So template actually consists
00:08:24,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,all the parameters that we're interested in.,00:08:22,3,parameters interested
00:08:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And these parameters in general will control the behavior of,00:08:24,3,And parameters general control behavior
00:08:29,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,the probability risk model.,00:08:27,3,probability risk model
00:08:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Meaning that if you set these parameters with different values and,00:08:29,3,Meaning set parameters different values
00:08:36,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,it will give some data points higher probabilities than others.,00:08:32,3,give data points higher probabilities others
00:08:39,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"Now in this case of course, for our text mining problem or",00:08:36,3,Now case course text mining problem
00:08:44,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,more precisely topic mining problem we have the following plans.,00:08:39,3,precisely topic mining problem following plans
00:08:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,First of all we have theta i's which is a word distribution snd then we have,00:08:44,3,First theta word distribution snd
00:08:52,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,a set of pis for each document.,00:08:49,3,set pis document
00:08:58,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And since we have n documents, so we have n sets of pis, and each set the pi up.",00:08:52,3,And since n documents n sets pis set pi
00:09:01,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,The pi values will sum to one.,00:08:58,3,The pi values sum one
00:09:06,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So this is to say that we first would pretend we already,00:09:01,3,So say first would pretend already
00:09:10,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,have these word distributions and the coverage numbers.,00:09:06,3,word distributions coverage numbers
00:09:18,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And then we can see how we can generate data by using such distributions.,00:09:10,3,And see generate data using distributions
00:09:21,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So how do we model the data in this way?,00:09:18,3,So model data way
00:09:25,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we assume that the data are actual symbols,00:09:21,3,And assume data actual symbols
00:09:29,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,drawn from such a model that depends on these parameters.,00:09:25,3,drawn model depends parameters
00:09:31,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now one interesting question here is to,00:09:29,3,Now one interesting question
00:09:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,think about how many parameters are there in total?,00:09:32,3,think many parameters total
00:09:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now obviously we can already see n multiplied by K parameters.,00:09:35,3,Now obviously already see n multiplied K parameters
00:09:42,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,For pi's.,00:09:41,3,For pi
00:09:44,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We also see k theta i's.,00:09:42,3,We also see k theta
00:09:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"But each theta i is actually a set of probability values, right?",00:09:44,3,But theta actually set probability values right
00:09:51,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,It's a distribution of words.,00:09:49,3,It distribution words
00:09:54,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So I leave this as an exercise for,00:09:51,3,So I leave exercise
00:09:59,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,you to figure out exactly how many parameters there are here.,00:09:54,3,figure exactly many parameters
00:10:04,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Now once we set up the model then we can fit the model to our data.,00:09:59,3,Now set model fit model data
00:10:07,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Meaning that we can estimate the parameters or,00:10:04,3,Meaning estimate parameters
00:10:11,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,infer the parameters based on the data.,00:10:07,3,infer parameters based data
00:10:14,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In other words we would like to adjust these parameter values.,00:10:11,3,In words would like adjust parameter values
00:10:20,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Until we give our data set the maximum probability.,00:10:14,3,Until give data set maximum probability
00:10:22,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"I just said, depending on the parameter values,",00:10:20,3,I said depending parameter values
00:10:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,some data points will have higher probabilities than others.,00:10:22,3,data points higher probabilities others
00:10:28,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"What we're interested in, here,",00:10:27,3,What interested
00:10:33,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,is what parameter values will give our data set the highest probability?,00:10:28,3,parameter values give data set highest probability
00:10:37,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So I also illustrate the problem with a picture that you see here.,00:10:33,3,So I also illustrate problem picture see
00:10:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"On the X axis I just illustrate lambda, the parameters,",00:10:37,3,On X axis I illustrate lambda parameters
00:10:44,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,as a one dimensional variable.,00:10:41,3,one dimensional variable
00:10:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"It's oversimplification, obviously, but it suffices to show the idea.",00:10:44,3,It oversimplification obviously suffices show idea
00:10:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And the Y axis shows the probability of the data, observe.",00:10:49,3,And Y axis shows probability data observe
00:10:57,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,This probability obviously depends on this setting of lambda.,00:10:53,3,This probability obviously depends setting lambda
00:11:01,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So that's why it varies as you change the value of lambda.,00:10:57,3,So varies change value lambda
00:11:04,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,What we're interested here is to find the lambda star.,00:11:01,3,What interested find lambda star
00:11:09,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,That would maximize the probability of the observed data.,00:11:05,3,That would maximize probability observed data
00:11:15,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So this would be, then, our estimate of the parameters.",00:11:10,3,So would estimate parameters
00:11:17,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And these parameters,",00:11:15,3,And parameters
00:11:21,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,note that are precisely what we hoped to discover from text data.,00:11:17,3,note precisely hoped discover text data
00:11:25,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So we'd treat these parameters as actually the outcome or,00:11:21,3,So treat parameters actually outcome
00:11:28,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,the output of the data mining algorithm.,00:11:25,3,output data mining algorithm
00:11:32,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So this is the general idea of using,00:11:28,3,So general idea using
00:11:38,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,a generative model for text mining.,00:11:32,3,generative model text mining
00:11:42,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"First, we design a model with some parameter values to fit",00:11:38,3,First design model parameter values fit
00:11:44,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,the data as well as we can.,00:11:42,3,data well
00:11:47,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"After we have fit the data, we will recover some parameter value.",00:11:44,3,After fit data recover parameter value
00:11:48,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We will use the specific parameter value And,00:11:47,3,We use specific parameter value And
00:11:50,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,those would be the output of the algorithm.,00:11:48,3,would output algorithm
00:11:55,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we'll treat those as actually the discovered knowledge from text data.,00:11:50,3,And treat actually discovered knowledge text data
00:11:59,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,By varying the model of course we can discover different knowledge.,00:11:55,3,By varying model course discover different knowledge
00:12:03,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So to summarize, we introduced a new way of representing topic,",00:11:59,3,So summarize introduced new way representing topic
00:12:09,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,namely representing as word distribution and this has the advantage of using,00:12:03,3,namely representing word distribution advantage using
00:12:14,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,multiple words to describe a complicated topic.It also allow us to assign,00:12:09,3,multiple words describe complicated topic It also allow us assign
00:12:19,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,weights on words so we have more than several variations of semantics.,00:12:14,3,weights words several variations semantics
00:12:23,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"We talked about the task of topic mining, and answers.",00:12:19,3,We talked task topic mining answers
00:12:26,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,When we define a topic as distribution.,00:12:23,3,When define topic distribution
00:12:30,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,So the importer is a clashing of text articles and a number of topics and,00:12:26,3,So importer clashing text articles number topics
00:12:33,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,a vocabulary set and the output is a set of topics.,00:12:30,3,vocabulary set output set topics
00:12:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,Each is a word distribution and,00:12:33,3,Each word distribution
00:12:38,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,also the coverage of all the topics in each document.,00:12:35,3,also coverage topics document
00:12:43,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And these are formally represented by theta i's and pi i's.,00:12:38,3,And formally represented theta pi
00:12:48,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we have two constraints here for these parameters.,00:12:43,3,And two constraints parameters
00:12:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,The first is the constraints on the worded distributions.,00:12:48,3,The first constraints worded distributions
00:12:56,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,In each worded distribution the probability of all the words,00:12:53,3,In worded distribution probability words
00:12:59,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"must sum to 1, all the words in the vocabulary.",00:12:56,3,must sum 1 words vocabulary
00:13:03,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,The second constraint is on the topic coverage in each document.,00:12:59,3,The second constraint topic coverage document
00:13:08,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,A document is not allowed to recover a topic outside of the set of topics that,00:13:03,3,A document allowed recover topic outside set topics
00:13:10,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,we are discovering.,00:13:08,3,discovering
00:13:17,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"So, the coverage of each of these k topics would sum to one for a document.",00:13:10,3,So coverage k topics would sum one document
00:13:21,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We also introduce a general idea of using a generative model for text mining.,00:13:17,3,We also introduce general idea using generative model text mining
00:13:27,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And the idea here is, first we're design a model to model the generation of data.",00:13:21,3,And idea first design model model generation data
00:13:30,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,We simply assume that they are generative in this way.,00:13:27,3,We simply assume generative way
00:13:34,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And inside the model we embed some parameters that we're interested in,00:13:30,3,And inside model embed parameters interested
00:13:35,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,denoted by lambda.,00:13:34,3,denoted lambda
00:13:40,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,"And then we can infer the most likely parameter values lambda star,",00:13:36,3,And infer likely parameter values lambda star
00:13:41,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,given a particular data set.,00:13:40,3,given particular data set
00:13:48,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we can then take the lambda star as knowledge discovered from the text for,00:13:43,3,And take lambda star knowledge discovered text
00:13:49,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,our problem.,00:13:48,3,problem
00:13:53,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,And we can adjust the design of the model and,00:13:50,3,And adjust design model
00:13:58,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,the parameters to discover various kinds of knowledge from text.,00:13:53,3,parameters discover various kinds knowledge text
00:14:04,3,Topic Mining and Analysis- Probabilistic Topic Models,2.3,As you will see later in the other lectures.,00:13:58,3,As see later lectures
00:00:03,3,Latent Dirichlet Allocation (LDA)- Part 1,2.15,[MUSIC],00:00:00,15,MUSIC
00:00:04,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,[SOUND],00:00:00,13,SOUND
00:00:09,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"In general, we can use the empirical count",00:00:06,13,In general use empirical count
00:00:15,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,of events in the observed data to estimate the probabilities.,00:00:09,13,events observed data estimate probabilities
00:00:19,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And a commonly used technique is called a maximum likelihood estimate,",00:00:15,13,And commonly used technique called maximum likelihood estimate
00:00:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,where we simply normalize the observe accounts.,00:00:19,13,simply normalize observe accounts
00:00:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So if we do that, we can see, we can compute these probabilities as follows.",00:00:22,13,So see compute probabilities follows
00:00:36,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"For estimating the probability that we see a water current in a segment,",00:00:30,13,For estimating probability see water current segment
00:00:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,we simply normalize the count of segments that contain this word.,00:00:36,13,simply normalize count segments contain word
00:00:47,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,So let's first take a look at the data here.,00:00:42,13,So let first take look data
00:00:52,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"On the right side, you see a list of some, hypothesizes the data.",00:00:47,13,On right side see list hypothesizes data
00:00:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,These are segments.,00:00:52,13,These segments
00:00:59,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And in some segments you see both words occur, they are indicated as ones for",00:00:55,13,And segments see words occur indicated ones
00:01:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,both columns.,00:00:59,13,columns
00:01:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"In some other cases only one will occur, so only that column has one and",00:01:01,13,In cases one occur column one
00:01:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,the other column has zero.,00:01:05,13,column zero
00:01:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And in all, of course, in some other cases none of the words occur,",00:01:07,13,And course cases none words occur
00:01:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,so they are both zeros.,00:01:11,13,zeros
00:01:19,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And for estimating these probabilities, we simply need to collect the three counts.",00:01:13,13,And estimating probabilities simply need collect three counts
00:01:23,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So the three counts are first, the count of W1.",00:01:20,13,So three counts first count W1
00:01:27,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And that's the total number of segments that contain word W1.,00:01:23,13,And total number segments contain word W1
00:01:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,It's just as the ones in the column of W1.,00:01:27,13,It ones column W1
00:01:34,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,We can count how many ones we have seen there.,00:01:30,13,We count many ones seen
00:01:40,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"The segment count is for word 2, and we just count the ones in the second column.",00:01:34,13,The segment count word 2 count ones second column
00:01:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And these will give us the total number of segments that contain W2.,00:01:40,13,And give us total number segments contain W2
00:01:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,The third count is when both words occur.,00:01:45,13,The third count words occur
00:01:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So this time, we're going to count the sentence where both columns have ones.",00:01:49,13,So time going count sentence columns ones
00:02:00,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And then, so this would give us the total number of segments",00:01:56,13,And would give us total number segments
00:02:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,where we have seen both W1 and W2.,00:02:00,13,seen W1 W2
00:02:08,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Once we have these counts, we can just normalize these counts by N,",00:02:03,13,Once counts normalize counts N
00:02:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"which is the total number of segments, and",00:02:08,13,total number segments
00:02:16,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,this will give us the probabilities that we need to compute original information.,00:02:11,13,give us probabilities need compute original information
00:02:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Now, there is a small problem, when we have zero counts sometimes.",00:02:16,13,Now small problem zero counts sometimes
00:02:27,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And in this case, we don't want a zero probability because our data may be",00:02:22,13,And case want zero probability data may
00:02:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"a small sample and in general, we would believe that it's potentially possible for",00:02:27,13,small sample general would believe potentially possible
00:02:35,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,a [INAUDIBLE] to avoid any context.,00:02:33,13,INAUDIBLE avoid context
00:02:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So, to address this problem, we can use a technique called smoothing.",00:02:35,13,So address problem use technique called smoothing
00:02:43,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And that's basically to add some small constant to these counts,",00:02:39,13,And basically add small constant counts
00:02:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,and so that we don't get the zero probability in any case.,00:02:43,13,get zero probability case
00:02:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Now, the best way to understand smoothing is imagine that we actually observed more",00:02:48,13,Now best way understand smoothing imagine actually observed
00:03:00,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"data than we actually have, because we'll pretend we observed some pseudo-segments.",00:02:54,13,data actually pretend observed pseudo segments
00:03:04,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"I illustrated on the top, on the right side on the slide.",00:03:00,13,I illustrated top right side slide
00:03:10,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And these pseudo-segments would contribute additional counts,00:03:04,13,And pseudo segments would contribute additional counts
00:03:15,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,of these words so that no event will have zero probability.,00:03:10,13,words event zero probability
00:03:18,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Now, in particular we introduce the four pseudo-segments.",00:03:15,13,Now particular introduce four pseudo segments
00:03:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,Each is weighted at one quarter.,00:03:18,13,Each weighted one quarter
00:03:25,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And these represent the four different combinations of occurrences of this word.,00:03:20,13,And represent four different combinations occurrences word
00:03:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So now each event, each combination will have",00:03:25,13,So event combination
00:03:35,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,at least one count or at least a non-zero count from this pseudo-segment.,00:03:30,13,least one count least non zero count pseudo segment
00:03:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So, in the actual segments that we'll observe,",00:03:35,13,So actual segments observe
00:03:44,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,it's okay if we haven't observed all of the combinations.,00:03:39,13,okay observed combinations
00:03:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So more specifically, you can see the 0.5 here after it comes from the two",00:03:44,13,So specifically see 0 5 comes two
00:03:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"ones in the two pseudo-segments, because each is weighted at one quarter.",00:03:49,13,ones two pseudo segments weighted one quarter
00:03:59,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"We add them up, we get 0.5.",00:03:55,13,We add get 0 5
00:04:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And similar to this, 0.05 comes from one single",00:03:59,13,And similar 0 05 comes one single
00:04:08,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,pseudo-segment that indicates the two words occur together.,00:04:03,13,pseudo segment indicates two words occur together
00:04:14,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And of course in the denominator we add the total number of pseudo-segments that,00:04:09,13,And course denominator add total number pseudo segments
00:04:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"we add, in this case, we added a four pseudo-segments.",00:04:14,13,add case added four pseudo segments
00:04:21,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Each is weighed at one quarter so the total of the sum is, after the one.",00:04:17,13,Each weighed one quarter total sum one
00:04:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So, that's why in the denominator you'll see a one there.",00:04:21,13,So denominator see one
00:04:31,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So, this basically concludes the discussion of how to compute a these",00:04:25,13,So basically concludes discussion compute
00:04:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,four syntagmatic relation discoveries.,00:04:31,13,four syntagmatic relation discoveries
00:04:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Now, so to summarize, syntagmatic relation can generally",00:04:36,13,Now summarize syntagmatic relation generally
00:04:46,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,be discovered by measuring correlations between occurrences of two words.,00:04:42,13,discovered measuring correlations occurrences two words
00:04:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,We've introduced the three concepts from information theory.,00:04:46,13,We introduced three concepts information theory
00:04:53,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Entropy, which measures the uncertainty of a random variable X.",00:04:49,13,Entropy measures uncertainty random variable X
00:04:59,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Conditional entropy, which measures the entropy of X given we know Y.",00:04:53,13,Conditional entropy measures entropy X given know Y
00:05:04,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And mutual information of X and Y, which matches the entropy reduction of X",00:04:59,13,And mutual information X Y matches entropy reduction X
00:05:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"due to knowing Y, or entropy reduction of Y due to knowing X.",00:05:04,13,due knowing Y entropy reduction Y due knowing X
00:05:12,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,They are the same.,00:05:11,13,They
00:05:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,So these three concepts are actually very useful for other applications as well.,00:05:12,13,So three concepts actually useful applications well
00:05:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,That's why we spent some time to explain this in detail.,00:05:17,13,That spent time explain detail
00:05:23,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"But in particular, they are also very useful for",00:05:20,13,But particular also useful
00:05:25,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,discovering syntagmatic relations.,00:05:23,13,discovering syntagmatic relations
00:05:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"In particular, mutual information is a principal way for",00:05:25,13,In particular mutual information principal way
00:05:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,discovering such a relation.,00:05:30,13,discovering relation
00:05:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,It allows us to have values computed on different pairs of,00:05:32,13,It allows us values computed different pairs
00:05:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,words that are comparable and so we can rank these pairs and,00:05:37,13,words comparable rank pairs
00:05:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,discover the strongest syntagmatic from a collection of documents.,00:05:42,13,discover strongest syntagmatic collection documents
00:05:53,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"Now, note that there is some relation between syntagmatic relation discovery and",00:05:48,13,Now note relation syntagmatic relation discovery
00:05:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,[INAUDIBLE] relation discovery.,00:05:53,13,INAUDIBLE relation discovery
00:06:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,So we already discussed the possibility of using BM25 to achieve waiting for,00:05:55,13,So already discussed possibility using BM25 achieve waiting
00:06:06,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,terms in the context to potentially also suggest the candidates,00:06:01,13,terms context potentially also suggest candidates
00:06:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,that have syntagmatic relations with the candidate word.,00:06:06,13,syntagmatic relations candidate word
00:06:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"But here, once we use mutual information to discover syntagmatic relations,",00:06:11,13,But use mutual information discover syntagmatic relations
00:06:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,we can also represent the context with this mutual information as weights.,00:06:17,13,also represent context mutual information weights
00:06:29,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,So this would give us another way to represent,00:06:24,13,So would give us another way represent
00:06:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"the context of a word, like a cat.",00:06:29,13,context word like cat
00:06:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And if we do the same for all the words, then we can cluster these words or",00:06:33,13,And words cluster words
00:06:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,compare the similarity between these words based on their context similarity.,00:06:37,13,compare similarity words based context similarity
00:06:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,So this provides yet another way to do term weighting for,00:06:42,13,So provides yet another way term weighting
00:06:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,paradigmatic relation discovery.,00:06:45,13,paradigmatic relation discovery
00:06:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And so to summarize this whole part about word association mining.,00:06:48,13,And summarize whole part word association mining
00:06:59,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"We introduce two basic associations, called a paradigmatic and",00:06:55,13,We introduce two basic associations called paradigmatic
00:07:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,a syntagmatic relations.,00:06:59,13,syntagmatic relations
00:07:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"These are fairly general, they apply to any items in any language, so",00:07:01,13,These fairly general apply items language
00:07:10,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"the units don't have to be words, they can be phrases or entities.",00:07:05,13,units words phrases entities
00:07:16,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"We introduced multiple statistical approaches for discovering them,",00:07:11,13,We introduced multiple statistical approaches discovering
00:07:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"mainly showing that pure statistical approaches are visible,",00:07:16,13,mainly showing pure statistical approaches visible
00:07:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,are variable for discovering both kind of relations.,00:07:20,13,variable discovering kind relations
00:07:28,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And they can be combined to perform joint analysis, as well.",00:07:24,13,And combined perform joint analysis well
00:07:35,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"These approaches can be applied to any text with no human effort,",00:07:28,13,These approaches applied text human effort
00:07:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"mostly because they are based on counting of words, yet",00:07:35,13,mostly based counting words yet
00:07:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,they can actually discover interesting relations of words.,00:07:39,13,actually discover interesting relations words
00:07:47,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"We can also use different ways with defining context and segment, and",00:07:44,13,We also use different ways defining context segment
00:07:51,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,this would lead us to some interesting variations of applications.,00:07:47,13,would lead us interesting variations applications
00:07:56,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"For example, the context can be very narrow like a few words, around a word, or",00:07:51,13,For example context narrow like words around word
00:08:00,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"a sentence, or maybe paragraphs, as using differing contexts would",00:07:56,13,sentence maybe paragraphs using differing contexts would
00:08:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,allows to discover different flavors of paradigmatical relations.,00:08:00,13,allows discover different flavors paradigmatical relations
00:08:09,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"And similarly, counting co-occurrences using let's say,",00:08:05,13,And similarly counting co occurrences using let say
00:08:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,visual information to discover syntagmatical relations.,00:08:09,13,visual information discover syntagmatical relations
00:08:19,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"We also have to define the segment, and the segment can be defined as a narrow",00:08:13,13,We also define segment segment defined narrow
00:08:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,text window or a longer text article.,00:08:19,13,text window longer text article
00:08:26,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And this would give us different kinds of associations.,00:08:22,13,And would give us different kinds associations
00:08:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"These discovery associations can support many other applications,",00:08:26,13,These discovery associations support many applications
00:08:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,in both information retrieval and text and data mining.,00:08:32,13,information retrieval text data mining
00:08:44,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"So here are some recommended readings, if you want to know more about the topic.",00:08:37,13,So recommended readings want know topic
00:08:46,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"The first is a book with a chapter on collocations,",00:08:44,13,The first book chapter collocations
00:08:50,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,which is quite relevant to the topic of these lectures.,00:08:46,13,quite relevant topic lectures
00:08:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,The second is an article about using various,00:08:50,13,The second article using various
00:08:58,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,statistical measures to discover lexical atoms.,00:08:55,13,statistical measures discover lexical atoms
00:09:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,Those are phrases that are non-compositional.,00:08:58,13,Those phrases non compositional
00:09:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"For example, hot dog is not really a dog that's hot,",00:09:03,13,For example hot dog really dog hot
00:09:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,blue chip is not a chip that's blue.,00:09:08,13,blue chip chip blue
00:09:16,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,And the paper has a discussion about some techniques for discovering such phrases.,00:09:11,13,And paper discussion techniques discovering phrases
00:09:23,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,The third one is a new paper on a unified way to discover both paradigmatical,00:09:17,13,The third one new paper unified way discover paradigmatical
00:09:29,2,Syntagmatic Relation Discovery- Mutual Information- Part 2,1.13,"relations and a syntagmatical relations, using random works on word graphs.",00:09:23,13,relations syntagmatical relations using random works word graphs
00:00:05,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,[SOUND] This lecture is about,00:00:00,9,SOUND This lecture
00:00:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,how to mine text data with,00:00:05,9,mine text data
00:00:15,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,social network as context.,00:00:09,9,social network context
00:00:18,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,In this lecture we're going to continue discussing contextual text mining.,00:00:15,9,In lecture going continue discussing contextual text mining
00:00:25,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"In particular, we're going to look at the social network of others as context.",00:00:18,9,In particular going look social network others context
00:00:31,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So first, what's our motivation for using network context for analysis of text?",00:00:26,9,So first motivation using network context analysis text
00:00:36,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The context of a text article can form a network.,00:00:32,9,The context text article form network
00:00:40,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,For example the authors of research articles,00:00:37,9,For example authors research articles
00:00:42,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,might form collaboration networks.,00:00:40,9,might form collaboration networks
00:00:48,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But authors of social media content might form social networks.,00:00:44,9,But authors social media content might form social networks
00:00:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"For example, in Twitter people might follow each other.",00:00:48,9,For example Twitter people might follow
00:01:00,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Or in Facebook as people might claim friends of others, etc.",00:00:51,9,Or Facebook people might claim friends others etc
00:01:07,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So such context connects the content of the others.,00:01:00,9,So context connects content others
00:01:12,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Similarly, locations associated with text can also be connected to form",00:01:07,9,Similarly locations associated text also connected form
00:01:13,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,geographical network.,00:01:12,9,geographical network
00:01:18,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But in general you can can imagine the metadata of the text data,00:01:13,9,But general imagine metadata text data
00:01:22,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,can form some kind of network if they have some relations.,00:01:18,9,form kind network relations
00:01:29,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Now there is some benefit in jointly analyzing text and,00:01:24,9,Now benefit jointly analyzing text
00:01:34,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,its social network context or network context in general.,00:01:29,9,social network context network context general
00:01:40,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that's because we can use network to impose some constraints on topics of text.,00:01:34,9,And use network impose constraints topics text
00:01:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So for example it's reasonable to assume that authors,00:01:41,9,So example reasonable assume authors
00:01:50,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,connected in collaboration networks tend to write about the similar topics.,00:01:45,9,connected collaboration networks tend write similar topics
00:01:57,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So such heuristics can be used to guide us in analyzing topics.,00:01:53,9,So heuristics used guide us analyzing topics
00:02:06,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Text also can help characterize the content associated with each subnetwork.,00:01:57,9,Text also help characterize content associated subnetwork
00:02:10,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And this is to say that both,00:02:06,9,And say
00:02:15,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"kinds of data, the network and text, can help each other.",00:02:11,9,kinds data network text help
00:02:21,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So for example the difference in opinions expressed that are in,00:02:16,9,So example difference opinions expressed
00:02:27,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,two subnetworks can be reviewed by doing this type of joint analysis.,00:02:21,9,two subnetworks reviewed type joint analysis
00:02:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So here briefly you could use a model called a network supervised topic model.,00:02:30,9,So briefly could use model called network supervised topic model
00:02:43,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,In this slide we're going to give some general ideas.,00:02:40,9,In slide going give general ideas
00:02:46,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And then in the next slide we're going to give some more details.,00:02:43,9,And next slide going give details
00:02:53,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But in general in this part of the course we don't have enough time to cover,00:02:48,9,But general part course enough time cover
00:02:56,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,these frontier topics in detail.,00:02:53,9,frontier topics detail
00:03:01,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But we provide references that would allow you to,00:02:56,9,But provide references would allow
00:03:04,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,read more about the topic to know the details.,00:03:01,9,read topic know details
00:03:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But it should still be useful to know the general ideas.,00:03:05,9,But still useful know general ideas
00:03:16,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And to know what they can do to know when you might be able to use them.,00:03:09,9,And know know might able use
00:03:22,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So the general idea of network supervised topic model is the following.,00:03:16,9,So general idea network supervised topic model following
00:03:28,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Let's start with viewing the regular topic models.,00:03:22,9,Let start viewing regular topic models
00:03:33,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Like if you had an LDA as sorting optimization problem.,00:03:28,9,Like LDA sorting optimization problem
00:03:34,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Of course, in this case,",00:03:33,9,Of course case
00:03:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the optimization objective function is a likelihood function.,00:03:34,9,optimization objective function likelihood function
00:03:42,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So we often use maximum likelihood estimator to obtain the parameters.,00:03:38,9,So often use maximum likelihood estimator obtain parameters
00:03:47,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And these parameters will give us useful information that we want to,00:03:42,9,And parameters give us useful information want
00:03:50,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,obtain from text data.,00:03:47,9,obtain text data
00:03:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"For example, topics.",00:03:50,9,For example topics
00:03:56,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So we want to maximize the probability of tests that are given the parameters,00:03:51,9,So want maximize probability tests given parameters
00:04:01,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,generally denoted by number.,00:03:56,9,generally denoted number
00:04:06,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The main idea of incorporating network is,00:04:01,9,The main idea incorporating network
00:04:12,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,to think about the constraints that can be imposed based on the network.,00:04:06,9,think constraints imposed based network
00:04:16,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"In general, the idea is to use the network to",00:04:12,9,In general idea use network
00:04:20,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"impose some constraints on the model parameters, lambda here.",00:04:16,9,impose constraints model parameters lambda
00:04:21,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"For example,",00:04:20,9,For example
00:04:27,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the text at adjacent nodes of the network can be similar to cover similar topics.,00:04:21,9,text adjacent nodes network similar cover similar topics
00:04:31,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Indeed, in many cases, they tend to cover similar topics.",00:04:27,9,Indeed many cases tend cover similar topics
00:04:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So we may be able to smooth the topic distributions,00:04:34,9,So may able smooth topic distributions
00:04:43,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,on the graph on the network so that adjacent nodes will have,00:04:39,9,graph network adjacent nodes
00:04:48,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,very similar topic distributions.,00:04:43,9,similar topic distributions
00:04:53,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So they will share a common distribution on the topics.,00:04:48,9,So share common distribution topics
00:05:00,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Or have just a slight variations of the topic of distributions, of the coverage.",00:04:53,9,Or slight variations topic distributions coverage
00:05:07,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So, technically, what we can do is simply to add a network and",00:05:02,9,So technically simply add network
00:05:11,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,use the regularizers to the likelihood of objective function as shown here.,00:05:07,9,use regularizers likelihood objective function shown
00:05:14,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So instead of just optimize the probability of test,00:05:11,9,So instead optimize probability test
00:05:18,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"data given parameters lambda, we're going to optimize another function F.",00:05:14,9,data given parameters lambda going optimize another function F
00:05:26,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,This function combines the likelihood with a regularizer function called R here.,00:05:19,9,This function combines likelihood regularizer function called R
00:05:31,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And the regularizer defines the the parameters lambda and the Network.,00:05:26,9,And regularizer defines parameters lambda Network
00:05:34,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,It tells us basically,00:05:31,9,It tells us basically
00:05:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,what kind of parameters are preferred from a network constraint perspective.,00:05:34,9,kind parameters preferred network constraint perspective
00:05:41,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So you can easily see this is in effect,00:05:38,9,So easily see effect
00:05:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,implementing the idea of imposing some prior on the model parameters.,00:05:41,9,implementing idea imposing prior model parameters
00:05:50,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Only that we're not necessary having a probabilistic model, but",00:05:45,9,Only necessary probabilistic model
00:05:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the idea is the same.,00:05:50,9,idea
00:05:55,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,We're going to combine the two in one single objective function.,00:05:51,9,We going combine two one single objective function
00:06:02,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So, the advantage of this idea is that it's quite general.",00:05:57,9,So advantage idea quite general
00:06:06,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Here the top model can be any generative model for text.,00:06:02,9,Here top model generative model text
00:06:11,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"It doesn't have to be PLSA or LEA, or the current topic models.",00:06:07,9,It PLSA LEA current topic models
00:06:17,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And similarly, the network can be also in a network.",00:06:12,9,And similarly network also network
00:06:19,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Any graph that connects these text objects.,00:06:17,9,Any graph connects text objects
00:06:26,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,This regularizer can also be any regularizer.,00:06:22,9,This regularizer also regularizer
00:06:31,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,We can be flexible in capturing different heuristics that we want to capture.,00:06:26,9,We flexible capturing different heuristics want capture
00:06:36,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And finally, the function F can also vary, so",00:06:32,9,And finally function F also vary
00:06:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,there can be many different ways to combine them.,00:06:36,9,many different ways combine
00:06:42,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So, this general idea is actually quite, quite powerful.",00:06:38,9,So general idea actually quite quite powerful
00:06:47,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,It offers a general approach to combining these different,00:06:42,9,It offers general approach combining different
00:06:52,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,types of data in single optimization framework.,00:06:47,9,types data single optimization framework
00:06:54,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And this general idea can really be applied for any problem.,00:06:52,9,And general idea really applied problem
00:06:59,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"But here in this paper reference here,",00:06:56,9,But paper reference
00:07:05,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,a particular instantiation called a NetPLSA was started.,00:06:59,9,particular instantiation called NetPLSA started
00:07:06,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"In this case, it's just for",00:07:05,9,In case
00:07:11,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,instantiating of PLSA to incorporate this simple constraint imposed by network.,00:07:06,9,instantiating PLSA incorporate simple constraint imposed network
00:07:15,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And the prior here is the neighbors on,00:07:11,9,And prior neighbors
00:07:18,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the network must have similar topic distribution.,00:07:15,9,network must similar topic distribution
00:07:20,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,They must cover similar topics in similar ways.,00:07:18,9,They must cover similar topics similar ways
00:07:22,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that's basically what it says in English.,00:07:20,9,And basically says English
00:07:27,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So technically we just have a modified objective function here.,00:07:24,9,So technically modified objective function
00:07:32,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Let's define both the texts you can actually see in the network graph G here.,00:07:27,9,Let define texts actually see network graph G
00:07:36,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And if you look at this formula,",00:07:34,9,And look formula
00:07:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,you can actually recognize some part fairly familiarly.,00:07:36,9,actually recognize part fairly familiarly
00:07:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Because they are, they should be fairly familiar to you by now.",00:07:40,9,Because fairly familiar
00:07:49,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So can you recognize which part is the likelihood for,00:07:45,9,So recognize part likelihood
00:07:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the test given the topic model?,00:07:49,9,test given topic model
00:07:58,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Well if you look at it, you will see this part is precisely the PLSA log-likelihood",00:07:52,9,Well look see part precisely PLSA log likelihood
00:08:04,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,that we want to maximize when we estimate parameters for PLSA alone.,00:07:58,9,want maximize estimate parameters PLSA alone
00:08:10,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But the second equation shows some additional constraints on the parameters.,00:08:04,9,But second equation shows additional constraints parameters
00:08:15,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And in particular, we'll see here it's to measure",00:08:10,9,And particular see measure
00:08:21,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the difference between the topic coverage at node u and node v.,00:08:15,9,difference topic coverage node u node v
00:08:25,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The two adjacent nodes on the network.,00:08:21,9,The two adjacent nodes network
00:08:27,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,We want their distributions to be similar.,00:08:25,9,We want distributions similar
00:08:31,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So here we are computing the square of their differences and,00:08:27,9,So computing square differences
00:08:34,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,we want to minimize this difference.,00:08:31,9,want minimize difference
00:08:40,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And note that there's a negative sign in front of this sum, this whole sum here.",00:08:34,9,And note negative sign front sum whole sum
00:08:46,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So this makes it possible to find,00:08:40,9,So makes possible find
00:08:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the parameters that are both to,00:08:46,9,parameters
00:08:57,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,maximize the PLSA log-likelihood.,00:08:51,9,maximize PLSA log likelihood
00:09:01,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"That means the parameters will fit the data well and,",00:08:57,9,That means parameters fit data well
00:09:05,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,also to respect that this constraint from the network.,00:09:01,9,also respect constraint network
00:09:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And this is the negative sign that I just mentioned.,00:09:06,9,And negative sign I mentioned
00:09:12,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Because this is an negative sign, when we maximize this",00:09:09,9,Because negative sign maximize
00:09:16,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,object in function we'll actually minimize this statement term here.,00:09:12,9,object function actually minimize statement term
00:09:23,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So if we look further in this picture we'll see,00:09:19,9,So look picture see
00:09:29,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the results will weight of edge between u and v here.,00:09:23,9,results weight edge u v
00:09:32,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that space from out network.,00:09:29,9,And space network
00:09:34,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"If you have a weight that says well,",00:09:32,9,If weight says well
00:09:38,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,these two nodes are strong collaborators of researchers.,00:09:34,9,two nodes strong collaborators researchers
00:09:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,These two are strong connections between two people in a social network.,00:09:38,9,These two strong connections two people social network
00:09:46,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And they would have weight.,00:09:45,9,And would weight
00:09:52,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Then that means it would be more important that they're topic coverages are similar.,00:09:46,9,Then means would important topic coverages similar
00:09:54,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that's basically what it says here.,00:09:52,9,And basically says
00:09:57,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And finally you see a parameter lambda here.,00:09:55,9,And finally see parameter lambda
00:10:02,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,This is a new parameter to control the influence of network constraint.,00:09:57,9,This new parameter control influence network constraint
00:10:07,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"We can see easily, if lambda is set to 0, we just go back to the standard PLSA.",00:10:02,9,We see easily lambda set 0 go back standard PLSA
00:10:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"But when lambda is set to a larger value,",00:10:07,9,But lambda set larger value
00:10:14,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,then we will let the network influence the estimated models more.,00:10:09,9,let network influence estimated models
00:10:19,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So as you can see, the effect here is that we're going to do basically PLSA.",00:10:14,9,So see effect going basically PLSA
00:10:24,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But we're going to also try to make the topic coverages,00:10:19,9,But going also try make topic coverages
00:10:28,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,on the two nodes that are strongly connected to be similar.,00:10:24,9,two nodes strongly connected similar
00:10:30,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And we ensure their coverages are similar.,00:10:28,9,And ensure coverages similar
00:10:37,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"So here are some of the several results, from that paper.",00:10:33,9,So several results paper
00:10:41,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,This is slide shows the record results of using PLSA.,00:10:37,9,This slide shows record results using PLSA
00:10:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And the data here is DBLP data, bibliographic data,",00:10:41,9,And data DBLP data bibliographic data
00:10:48,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,about research articles.,00:10:45,9,research articles
00:10:55,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And the experiments have to do with using four communities of applications.,00:10:48,9,And experiments using four communities applications
00:10:56,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,IR information retrieval.,00:10:55,9,IR information retrieval
00:10:59,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,DM stands for data mining.,00:10:56,9,DM stands data mining
00:11:00,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,ML for machinery and web.,00:10:59,9,ML machinery web
00:11:05,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"There are four communities of articles, and we were hoping",00:11:00,9,There four communities articles hoping
00:11:14,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,to see that the topic mining can help us uncover these four communities.,00:11:06,9,see topic mining help us uncover four communities
00:11:19,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But from these assembled topics that you have seen here that are generated by PLSA.,00:11:14,9,But assembled topics seen generated PLSA
00:11:24,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And PLSA is unable to generate the four communities that,00:11:19,9,And PLSA unable generate four communities
00:11:26,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,correspond to our intuition.,00:11:24,9,correspond intuition
00:11:30,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The reason was because they are all mixed together and,00:11:26,9,The reason mixed together
00:11:33,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,there are many words that are shared by these communities.,00:11:30,9,many words shared communities
00:11:37,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So it's not that easy to use four topics to separate them.,00:11:33,9,So easy use four topics separate
00:11:41,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"If we use more topics, perhaps we will have more coherent topics.",00:11:37,9,If use topics perhaps coherent topics
00:11:48,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"But what's interesting is that if we use the NetPLSA where the network,",00:11:42,9,But interesting use NetPLSA network
00:11:54,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,the collaboration network in this case of authors is used to impose constraints.,00:11:48,9,collaboration network case authors used impose constraints
00:11:57,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And in this case we also use four topics.,00:11:54,9,And case also use four topics
00:12:01,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,But Ned Pierre said we gave much more meaningful topics.,00:11:57,9,But Ned Pierre said gave much meaningful topics
00:12:07,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So here we'll see that these topics correspond well to the four communities.,00:12:01,9,So see topics correspond well four communities
00:12:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The first is information retrieval.,00:12:07,9,The first information retrieval
00:12:11,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,The second is data mining.,00:12:09,9,The second data mining
00:12:12,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Third is machine learning.,00:12:11,9,Third machine learning
00:12:13,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And the fourth is web.,00:12:12,9,And fourth web
00:12:18,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So that separation was mostly because of the influence of network,00:12:13,9,So separation mostly influence network
00:12:24,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,where with leverage is a collaboration network information.,00:12:18,9,leverage collaboration network information
00:12:28,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Essentially the people that form a collaborating network,00:12:24,9,Essentially people form collaborating network
00:12:32,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,would then be kind of assumed to write about similar topics.,00:12:28,9,would kind assumed write similar topics
00:12:35,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that's why we're going to have more coherent topics.,00:12:32,9,And going coherent topics
00:12:39,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And if you just listen to text data alone based on the occurrences,",00:12:35,9,And listen text data alone based occurrences
00:12:42,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,you won't get such coherent topics.,00:12:39,9,get coherent topics
00:12:45,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Even though a topic model, like PLSA or",00:12:42,9,Even though topic model like PLSA
00:12:50,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,LDA also should be able to pick up co-occurring words.,00:12:45,9,LDA also able pick co occurring words
00:12:55,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So in general the topics that they generate represent,00:12:50,9,So general topics generate represent
00:12:58,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,words that co-occur each other.,00:12:55,9,words co occur
00:13:03,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"But still they cannot generate such a coherent results as NetPLSA,",00:12:58,9,But still cannot generate coherent results NetPLSA
00:13:07,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,showing that the network contest is very useful here.,00:13:03,9,showing network contest useful
00:13:13,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,Now a similar model could have been also useful to to characterize the content,00:13:08,9,Now similar model could also useful characterize content
00:13:16,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,associated with each subnetwork of collaborations.,00:13:13,9,associated subnetwork collaborations
00:13:24,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So a more general view of text mining in context of network is you,00:13:19,9,So general view text mining context network
00:13:29,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,treat text as living in a rich information network environment.,00:13:24,9,treat text living rich information network environment
00:13:35,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And that means we can connect all the related data together as a big network.,00:13:29,9,And means connect related data together big network
00:13:41,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And text data can be associated with a lot of structures in the network.,00:13:35,9,And text data associated lot structures network
00:13:46,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"For example, text data can be associated with the nodes of the network, and",00:13:41,9,For example text data associated nodes network
00:13:51,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,that's basically what we just discussed in the NetPLSA.,00:13:46,9,basically discussed NetPLSA
00:13:56,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"But text data can be associated with age as well, or paths or even subnetworks.",00:13:51,9,But text data associated age well paths even subnetworks
00:14:01,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And such a way to represent texts that are in the big environment of,00:13:56,9,And way represent texts big environment
00:14:04,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,all the context information is very powerful.,00:14:01,9,context information powerful
00:14:09,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"Because it allows to analyze all the data, all the information together.",00:14:04,9,Because allows analyze data information together
00:14:16,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,"And so in general, analysis of text should be using the entire network",00:14:09,9,And general analysis text using entire network
00:14:21,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,information that's related to the text data.,00:14:16,9,information related text data
00:14:23,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,So here's one suggested reading.,00:14:21,9,So one suggested reading
00:14:27,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,And this is the paper about NetPLSA where you can find more details about the model,00:14:23,9,And paper NetPLSA find details model
00:14:29,5,Contextual Text Mining- Mining Topics with Social Network Context,4.9,and how to make such a model.,00:14:27,9,make model
00:00:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,[SOUND] Now lets look at another behaviour of the Mixed Model and,00:00:00,9,SOUND Now lets look another behaviour Mixed Model
00:00:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,in this case lets look at the response to data frequencies.,00:00:07,9,case lets look response data frequencies
00:00:19,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So what you are seeing now is basically the likelihood of function for,00:00:14,9,So seeing basically likelihood function
00:00:24,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,the two word document and we now in this case the solution is text.,00:00:19,9,two word document case solution text
00:00:28,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,A probability of 0.9 and the a probability of 0.1.,00:00:24,9,A probability 0 9 probability 0 1
00:00:31,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Now it's interesting to,00:00:28,9,Now interesting
00:00:35,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,think about a scenario where we start adding more words to the document.,00:00:31,9,think scenario start adding words document
00:00:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So what would happen if we add many the's to the document?,00:00:35,9,So would happen add many document
00:00:44,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Now this would change the game, right?",00:00:41,9,Now would change game right
00:00:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So, how?",00:00:44,9,So
00:00:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Well, picture, what would the likelihood function look like now?",00:00:45,9,Well picture would likelihood function look like
00:00:54,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Well, it start with the likelihood function for the two words, right?",00:00:50,9,Well start likelihood function two words right
00:00:56,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"As we add more words, we know that.",00:00:54,9,As add words know
00:00:59,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,But we have to just multiply the likelihood function by,00:00:56,9,But multiply likelihood function
00:01:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,additional terms to account for the additional.,00:00:59,9,additional terms account additional
00:01:04,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,occurrences of that.,00:01:02,9,occurrences
00:01:05,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Since in this case,",00:01:04,9,Since case
00:01:09,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"all the additional terms are the, we're going to just multiply by this term.",00:01:05,9,additional terms going multiply term
00:01:11,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Right? For the probability of the.,00:01:09,9,Right For probability
00:01:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"And if we have another occurrence of the, we'd multiply again by the same term,",00:01:12,9,And another occurrence multiply term
00:01:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,and so on and forth.,00:01:17,9,forth
00:01:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Add as many terms as the number of the's that we add to the document, d'.",00:01:20,9,Add many terms number add document
00:01:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Now this obviously changes the likelihood function.,00:01:25,9,Now obviously changes likelihood function
00:01:36,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So what's interesting is now to think about how would that change our solution?,00:01:30,9,So interesting think would change solution
00:01:37,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So what's the optimal solution now?,00:01:36,9,So optimal solution
00:01:42,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Now, intuitively you'd know the original solution,",00:01:38,9,Now intuitively know original solution
00:01:46,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"pulling the 9 versus pulling the ,will no longer be optimal for this new function.",00:01:42,9,pulling 9 versus pulling longer optimal new function
00:01:47,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Right?,00:01:46,9,Right
00:01:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"But, the question is how should we change it.",00:01:48,9,But question change
00:01:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,What general is to sum to one.,00:01:50,9,What general sum one
00:01:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So he know we must take away some probability the mass from one word and,00:01:53,9,So know must take away probability mass one word
00:02:00,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,add the probability mass to the other word.,00:01:57,9,add probability mass word
00:02:04,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,The question is which word to have reduce the probability and,00:02:00,9,The question word reduce probability
00:02:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,which word to have a larger probability.,00:02:04,9,word larger probability
00:02:10,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"And in particular, let's think about the probability of the.",00:02:07,9,And particular let think probability
00:02:12,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Should it be increased to be more than 0.1?,00:02:10,9,Should increased 0 1
00:02:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Or should we decrease it to less than 0.1?,00:02:12,9,Or decrease less 0 1
00:02:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,What do you think?,00:02:16,9,What think
00:02:23,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Now you might want to pause the video a moment to think more about.,00:02:19,9,Now might want pause video moment think
00:02:24,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,This question.,00:02:23,9,This question
00:02:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Because this has to do with understanding of important behavior of a mixture model.,00:02:24,9,Because understanding important behavior mixture model
00:02:35,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"And indeed, other maximum likelihood estimator.",00:02:30,9,And indeed maximum likelihood estimator
00:02:40,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Now if you look at the formula for a moment, then you will see it seems like",00:02:35,9,Now look formula moment see seems like
00:02:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,another object Function is more influenced by the than text.,00:02:40,9,another object Function influenced text
00:02:48,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Before, each computer.",00:02:45,9,Before computer
00:02:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So now as you can imagine, it would make sense to actually",00:02:48,9,So imagine would make sense actually
00:02:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,assign a smaller probability for text and lock it.,00:02:53,9,assign smaller probability text lock
00:03:01,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,To make room for a larger probability for the.,00:02:57,9,To make room larger probability
00:03:04,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Why? Because the is repeated many times.,00:03:01,9,Why Because repeated many times
00:03:08,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"If we increase it a little bit, it will have more positive impact.",00:03:04,9,If increase little bit positive impact
00:03:13,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Whereas a slight decrease of text will have relatively small impact,00:03:08,9,Whereas slight decrease text relatively small impact
00:03:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"because it occurred just one, right?",00:03:13,9,occurred one right
00:03:23,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So this means there is another behavior that we observe here.,00:03:17,9,So means another behavior observe
00:03:29,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,That is high frequency words generated with high probabilities,00:03:23,9,That high frequency words generated high probabilities
00:03:31,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,from all the distributions.,00:03:29,9,distributions
00:03:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"And, this is no surprise at all,",00:03:31,9,And surprise
00:03:37,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"because after all, we are maximizing the likelihood of the data.",00:03:33,9,maximizing likelihood data
00:03:44,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So the more a word occurs, then it makes more sense to give such a word",00:03:37,9,So word occurs makes sense give word
00:03:48,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,a higher probability because the impact would be more on the likelihood function.,00:03:44,9,higher probability impact would likelihood function
00:03:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,This is in fact a very general phenomenon of all the maximum likelihood estimator.,00:03:48,9,This fact general phenomenon maximum likelihood estimator
00:03:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"But in this case, we can see as we see more occurrences of a term,",00:03:53,9,But case see see occurrences term
00:04:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,it also encourages the unknown distribution theta sub d,00:03:57,9,also encourages unknown distribution theta sub
00:04:05,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,to assign a somewhat higher probability to this word.,00:04:02,9,assign somewhat higher probability word
00:04:11,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Now it's also interesting to think about the impact of probability of Theta sub B.,00:04:07,9,Now also interesting think impact probability Theta sub B
00:04:16,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,The probability of choosing one of the two component models.,00:04:11,9,The probability choosing one two component models
00:04:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Now we've been so far assuming that each model is equally likely.,00:04:16,9,Now far assuming model equally likely
00:04:21,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And that gives us 0.5.,00:04:20,9,And gives us 0 5
00:04:26,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,But you can again look at this likelihood function and try to picture what would,00:04:21,9,But look likelihood function try picture would
00:04:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,happen if we increase the probability of choosing a background model.,00:04:26,9,happen increase probability choosing background model
00:04:34,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Now you will see these terms for the,",00:04:30,9,Now see terms
00:04:38,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,we have a different form where the probability that would be,00:04:34,9,different form probability would
00:04:45,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,even larger because the background has a high probability for the word and,00:04:40,9,even larger background high probability word
00:04:51,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,the coefficient in front of 0.9 which is now 0.5 would be even larger.,00:04:45,9,coefficient front 0 9 0 5 would even larger
00:04:54,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"When this is larger, the overall result would be larger.",00:04:51,9,When larger overall result would larger
00:04:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And that also makes this the less important for,00:04:54,9,And also makes less important
00:05:01,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,theta sub d to increase the probability before the.,00:04:57,9,theta sub increase probability
00:05:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,Because it's already very large.,00:05:01,9,Because already large
00:05:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So the impact here of increasing the probability of the is somewhat,00:05:03,9,So impact increasing probability somewhat
00:05:10,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"regulated by this coefficient, the point of i.",00:05:07,9,regulated coefficient point
00:05:13,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"If it's larger on the background,",00:05:10,9,If larger background
00:05:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,then it becomes less important to increase the value.,00:05:13,9,becomes less important increase value
00:05:20,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So this means the behavior here,",00:05:17,9,So means behavior
00:05:25,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"which is high frequency words tend to get the high probabilities, are effected or",00:05:20,9,high frequency words tend get high probabilities effected
00:05:30,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,regularized somewhat by the probability of choosing each component.,00:05:25,9,regularized somewhat probability choosing component
00:05:33,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,The more likely a component is being chosen.,00:05:30,9,The likely component chosen
00:05:37,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,It's more important that to have higher values for these frequent words.,00:05:33,9,It important higher values frequent words
00:05:44,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"If you have a various small probability of being chosen, then the incentive is less.",00:05:37,9,If various small probability chosen incentive less
00:05:50,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So to summarize, we have just discussed the mixture model.",00:05:44,9,So summarize discussed mixture model
00:05:55,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And we discussed that the estimation problem of the mixture model and,00:05:50,9,And discussed estimation problem mixture model
00:06:01,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,particular with this discussed some general behavior of the estimator and,00:05:55,9,particular discussed general behavior estimator
00:06:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,that means we can expect our estimator to capture these infusions.,00:06:01,9,means expect estimator capture infusions
00:06:10,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,First every component model,00:06:07,9,First every component model
00:06:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,attempts to assign high probabilities to high frequent their words in the data.,00:06:10,9,attempts assign high probabilities high frequent words data
00:06:18,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And this is to collaboratively maximize likelihood.,00:06:14,9,And collaboratively maximize likelihood
00:06:23,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Second, different component models tend to bet high probabilities on different words.",00:06:18,9,Second different component models tend bet high probabilities different words
00:06:28,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And this is to avoid a competition or waste of probability.,00:06:23,9,And avoid competition waste probability
00:06:31,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And this would allow them to collaborate more efficiently to maximize,00:06:28,9,And would allow collaborate efficiently maximize
00:06:32,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,the likelihood.,00:06:31,9,likelihood
00:06:39,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"So, the probability of choosing each component regulates the collaboration and",00:06:33,9,So probability choosing component regulates collaboration
00:06:42,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,the competition between component models.,00:06:39,9,competition component models
00:06:47,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"It would allow some component models to respond more to the change,",00:06:42,9,It would allow component models respond change
00:06:51,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"for example, of frequency of the theta point in the data.",00:06:47,9,example frequency theta point data
00:06:56,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,We also talked about the special case of fixing one component to a background,00:06:53,9,We also talked special case fixing one component background
00:06:57,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"word distribution, right?",00:06:56,9,word distribution right
00:07:02,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"And this distribution can be estimated by using a collection of documents,",00:06:57,9,And distribution estimated using collection documents
00:07:07,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"a large collection of English documents, by using just one distribution and",00:07:02,9,large collection English documents using one distribution
00:07:12,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,then we'll just have normalized frequencies of terms to,00:07:07,9,normalized frequencies terms
00:07:14,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,give us the probabilities of all these words.,00:07:12,9,give us probabilities words
00:07:17,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"Now when we use such a specialized mixture model,",00:07:14,9,Now use specialized mixture model
00:07:22,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,we show that we can effectively get rid of that one word in the other component.,00:07:17,9,show effectively get rid one word component
00:07:26,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,And that would make this cover topic more discriminative.,00:07:23,9,And would make cover topic discriminative
00:07:32,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,This is also an example of imposing a prior on the model parameter and,00:07:27,9,This also example imposing prior model parameter
00:07:37,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,the prior here basically means one model must be exactly the same as the background,00:07:32,9,prior basically means one model must exactly background
00:07:42,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"language model and if you recall what we talked about in Bayesian estimation, and",00:07:37,9,language model recall talked Bayesian estimation
00:07:48,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,this prior will allow us to favor a model that is consistent with our prior.,00:07:42,9,prior allow us favor model consistent prior
00:07:53,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,"In fact, if it's not consistent we're going to say the model is impossible.",00:07:48,9,In fact consistent going say model impossible
00:07:56,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,So it has a zero prior probability.,00:07:53,9,So zero prior probability
00:07:59,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,That effectively excludes such a scenario.,00:07:56,9,That effectively excludes scenario
00:08:03,3,Probabilistic Topic Models- Mixture Model Estimation- Part 2,2.9,This is also issue that we'll talk more later.,00:07:59,9,This also issue talk later
00:00:07,2,Paradigmatic Relation Discovery Part 1,1.8,[SOUND] This,00:00:00,8,SOUND This
00:00:14,2,Paradigmatic Relation Discovery Part 1,1.8,lecture is about the Paradigmatics Relation Discovery.,00:00:07,8,lecture Paradigmatics Relation Discovery
00:00:19,2,Paradigmatic Relation Discovery Part 1,1.8,In this lecture we are going to talk about how to discover a particular kind of word,00:00:14,8,In lecture going talk discover particular kind word
00:00:22,2,Paradigmatic Relation Discovery Part 1,1.8,association called a paradigmatical relation.,00:00:19,8,association called paradigmatical relation
00:00:30,2,Paradigmatic Relation Discovery Part 1,1.8,"By definition, two words are paradigmatically",00:00:25,8,By definition two words paradigmatically
00:00:34,2,Paradigmatic Relation Discovery Part 1,1.8,related if they share a similar context.,00:00:30,8,related share similar context
00:00:39,2,Paradigmatic Relation Discovery Part 1,1.8,"Namely, they occur in similar positions in text.",00:00:34,8,Namely occur similar positions text
00:00:44,2,Paradigmatic Relation Discovery Part 1,1.8,So naturally our idea of discovering such a relation is to look at the context,00:00:39,8,So naturally idea discovering relation look context
00:00:49,2,Paradigmatic Relation Discovery Part 1,1.8,of each word and then try to compute the similarity of those contexts.,00:00:44,8,word try compute similarity contexts
00:00:54,2,Paradigmatic Relation Discovery Part 1,1.8,"So here is an example of context of a word, cat.",00:00:50,8,So example context word cat
00:01:01,2,Paradigmatic Relation Discovery Part 1,1.8,Here I have taken the word cat out of the context and,00:00:55,8,Here I taken word cat context
00:01:08,2,Paradigmatic Relation Discovery Part 1,1.8,you can see we are seeing some remaining words in the sentences that contain cat.,00:01:01,8,see seeing remaining words sentences contain cat
00:01:12,2,Paradigmatic Relation Discovery Part 1,1.8,"Now, we can do the same thing for another word like dog.",00:01:09,8,Now thing another word like dog
00:01:18,2,Paradigmatic Relation Discovery Part 1,1.8,So in general we would like to capture such a context and then try to assess,00:01:13,8,So general would like capture context try assess
00:01:23,2,Paradigmatic Relation Discovery Part 1,1.8,the similarity of the context of cat and the context of a word like dog.,00:01:18,8,similarity context cat context word like dog
00:01:29,2,Paradigmatic Relation Discovery Part 1,1.8,So now the question is how can we formally represent the context and,00:01:24,8,So question formally represent context
00:01:31,2,Paradigmatic Relation Discovery Part 1,1.8,then define the similarity function.,00:01:29,8,define similarity function
00:01:38,2,Paradigmatic Relation Discovery Part 1,1.8,"So first, we note that the context actually contains a lot of words.",00:01:33,8,So first note context actually contains lot words
00:01:43,2,Paradigmatic Relation Discovery Part 1,1.8,"So, they can be regarded as a pseudo document, a imagine",00:01:38,8,So regarded pseudo document imagine
00:01:49,2,Paradigmatic Relation Discovery Part 1,1.8,"document, but there are also different ways of looking at the context.",00:01:43,8,document also different ways looking context
00:01:57,2,Paradigmatic Relation Discovery Part 1,1.8,"For example, we can look at the word that occurs before the word cat.",00:01:49,8,For example look word occurs word cat
00:02:00,2,Paradigmatic Relation Discovery Part 1,1.8,We can call this context Left1 context.,00:01:57,8,We call context Left1 context
00:02:04,2,Paradigmatic Relation Discovery Part 1,1.8,"All right, so in this case you will see words like my, his, or",00:02:00,8,All right case see words like
00:02:07,2,Paradigmatic Relation Discovery Part 1,1.8,"big, a, the, et cetera.",00:02:04,8,big et cetera
00:02:12,2,Paradigmatic Relation Discovery Part 1,1.8,These are the words that can occur to left of the word cat.,00:02:07,8,These words occur left word cat
00:02:19,2,Paradigmatic Relation Discovery Part 1,1.8,"So we say my cat, his cat, big cat, a cat, et cetera.",00:02:12,8,So say cat cat big cat cat et cetera
00:02:24,2,Paradigmatic Relation Discovery Part 1,1.8,"Similarly, we can also collect the words that occur right after the word cat.",00:02:19,8,Similarly also collect words occur right word cat
00:02:28,2,Paradigmatic Relation Discovery Part 1,1.8,"We can call this context Right1, and",00:02:24,8,We call context Right1
00:02:34,2,Paradigmatic Relation Discovery Part 1,1.8,"here we see words like eats, ate, is, has, et cetera.",00:02:28,8,see words like eats ate et cetera
00:02:35,2,Paradigmatic Relation Discovery Part 1,1.8,"Or, more generally,",00:02:34,8,Or generally
00:02:41,2,Paradigmatic Relation Discovery Part 1,1.8,we can look at all the words in the window of text around the word cat.,00:02:35,8,look words window text around word cat
00:02:46,2,Paradigmatic Relation Discovery Part 1,1.8,"Here, let's say we can take a window of 8 words around the word cat.",00:02:41,8,Here let say take window 8 words around word cat
00:02:48,2,Paradigmatic Relation Discovery Part 1,1.8,We call this context Window8.,00:02:46,8,We call context Window8
00:02:54,2,Paradigmatic Relation Discovery Part 1,1.8,"Now, of course, you can see all the words from left or from right, and",00:02:49,8,Now course see words left right
00:02:58,2,Paradigmatic Relation Discovery Part 1,1.8,so we'll have a bag of words in general to represent the context.,00:02:54,8,bag words general represent context
00:03:06,2,Paradigmatic Relation Discovery Part 1,1.8,"Now, such a word based representation would actually give us",00:03:01,8,Now word based representation would actually give us
00:03:12,2,Paradigmatic Relation Discovery Part 1,1.8,an interesting way to define the perspective of measuring the similarity.,00:03:06,8,interesting way define perspective measuring similarity
00:03:15,2,Paradigmatic Relation Discovery Part 1,1.8,"Because if you look at just the similarity of Left1,",00:03:12,8,Because look similarity Left1
00:03:21,2,Paradigmatic Relation Discovery Part 1,1.8,"then we'll see words that share just the words in the left context,",00:03:15,8,see words share words left context
00:03:27,2,Paradigmatic Relation Discovery Part 1,1.8,and we kind of ignored the other words that are also in the general context.,00:03:21,8,kind ignored words also general context
00:03:32,2,Paradigmatic Relation Discovery Part 1,1.8,"So that gives us one perspective to measure the similarity, and similarly,",00:03:27,8,So gives us one perspective measure similarity similarly
00:03:34,2,Paradigmatic Relation Discovery Part 1,1.8,"if we only use the Right1 context,",00:03:32,8,use Right1 context
00:03:38,2,Paradigmatic Relation Discovery Part 1,1.8,we will capture this narrative from another perspective.,00:03:34,8,capture narrative another perspective
00:03:43,2,Paradigmatic Relation Discovery Part 1,1.8,Using both the Left1 and Right1 of course would allow us to capture,00:03:38,8,Using Left1 Right1 course would allow us capture
00:03:47,2,Paradigmatic Relation Discovery Part 1,1.8,the similarity with even more strict criteria.,00:03:43,8,similarity even strict criteria
00:03:54,2,Paradigmatic Relation Discovery Part 1,1.8,"So in general, context may contain adjacent words, like eats and",00:03:49,8,So general context may contain adjacent words like eats
00:03:59,2,Paradigmatic Relation Discovery Part 1,1.8,"my, that you see here, or non-adjacent words, like Saturday,",00:03:54,8,see non adjacent words like Saturday
00:04:02,2,Paradigmatic Relation Discovery Part 1,1.8,"Tuesday, or some other words in the context.",00:03:59,8,Tuesday words context
00:04:10,2,Paradigmatic Relation Discovery Part 1,1.8,And this flexibility also allows us to match the similarity in somewhat,00:04:05,8,And flexibility also allows us match similarity somewhat
00:04:11,2,Paradigmatic Relation Discovery Part 1,1.8,different ways.,00:04:10,8,different ways
00:04:13,2,Paradigmatic Relation Discovery Part 1,1.8,"Sometimes this is useful,",00:04:11,8,Sometimes useful
00:04:19,2,Paradigmatic Relation Discovery Part 1,1.8,as we might want to capture similarity base on general content.,00:04:13,8,might want capture similarity base general content
00:04:25,2,Paradigmatic Relation Discovery Part 1,1.8,That would give us loosely related paradigmatical relations.,00:04:19,8,That would give us loosely related paradigmatical relations
00:04:29,2,Paradigmatic Relation Discovery Part 1,1.8,Whereas if you use only the words immediately to the left and,00:04:25,8,Whereas use words immediately left
00:04:35,2,Paradigmatic Relation Discovery Part 1,1.8,"to the right of the word, then you likely will capture words that are very",00:04:29,8,right word likely capture words
00:04:39,2,Paradigmatic Relation Discovery Part 1,1.8,much related by their syntactical categories and semantics.,00:04:35,8,much related syntactical categories semantics
00:04:46,2,Paradigmatic Relation Discovery Part 1,1.8,So the general idea of discovering paradigmatical relations,00:04:41,8,So general idea discovering paradigmatical relations
00:04:50,2,Paradigmatic Relation Discovery Part 1,1.8,is to compute the similarity of context of two words.,00:04:46,8,compute similarity context two words
00:04:55,2,Paradigmatic Relation Discovery Part 1,1.8,"So here, for example, we can measure the similarity of cat and",00:04:50,8,So example measure similarity cat
00:04:59,2,Paradigmatic Relation Discovery Part 1,1.8,dog based on the similarity of their context.,00:04:55,8,dog based similarity context
00:05:02,2,Paradigmatic Relation Discovery Part 1,1.8,"In general, we can combine all kinds of views of the context.",00:04:59,8,In general combine kinds views context
00:05:06,2,Paradigmatic Relation Discovery Part 1,1.8,"And so the similarity function is, in general,",00:05:02,8,And similarity function general
00:05:10,2,Paradigmatic Relation Discovery Part 1,1.8,a combination of similarities on different context.,00:05:06,8,combination similarities different context
00:05:14,2,Paradigmatic Relation Discovery Part 1,1.8,"And of course, we can also assign weights to these different",00:05:10,8,And course also assign weights different
00:05:20,2,Paradigmatic Relation Discovery Part 1,1.8,similarities to allow us to focus more on a particular kind of context.,00:05:14,8,similarities allow us focus particular kind context
00:05:24,2,Paradigmatic Relation Discovery Part 1,1.8,"And this would be naturally application specific, but again,",00:05:20,8,And would naturally application specific
00:05:28,2,Paradigmatic Relation Discovery Part 1,1.8,here the main idea for discovering pardigmatically related words is,00:05:24,8,main idea discovering pardigmatically related words
00:05:32,2,Paradigmatic Relation Discovery Part 1,1.8,to computer the similarity of their context.,00:05:28,8,computer similarity context
00:05:37,2,Paradigmatic Relation Discovery Part 1,1.8,So next let's see how we exactly compute these similarity functions.,00:05:32,8,So next let see exactly compute similarity functions
00:05:42,2,Paradigmatic Relation Discovery Part 1,1.8,"Now to answer this question, it is useful to think of bag of words",00:05:37,8,Now answer question useful think bag words
00:05:46,2,Paradigmatic Relation Discovery Part 1,1.8,representation as vectors in a vector space model.,00:05:42,8,representation vectors vector space model
00:05:53,2,Paradigmatic Relation Discovery Part 1,1.8,Now those of you who have been familiar with information retrieval or,00:05:48,8,Now familiar information retrieval
00:05:57,2,Paradigmatic Relation Discovery Part 1,1.8,textual retrieval techniques would realize that vector space model has,00:05:53,8,textual retrieval techniques would realize vector space model
00:06:02,2,Paradigmatic Relation Discovery Part 1,1.8,been used frequently for modeling documents and queries for search.,00:05:57,8,used frequently modeling documents queries search
00:06:08,2,Paradigmatic Relation Discovery Part 1,1.8,But here we also find it convenient to model the context of a word for,00:06:02,8,But also find convenient model context word
00:06:11,2,Paradigmatic Relation Discovery Part 1,1.8,paradigmatic relation discovery.,00:06:08,8,paradigmatic relation discovery
00:06:15,2,Paradigmatic Relation Discovery Part 1,1.8,So the idea of this approach is to view each,00:06:11,8,So idea approach view
00:06:20,2,Paradigmatic Relation Discovery Part 1,1.8,word in our vocabulary as defining one dimension in a high dimensional space.,00:06:15,8,word vocabulary defining one dimension high dimensional space
00:06:23,2,Paradigmatic Relation Discovery Part 1,1.8,"So we have N words in total in the vocabulary,",00:06:20,8,So N words total vocabulary
00:06:27,2,Paradigmatic Relation Discovery Part 1,1.8,"then we have N dimensions, as illustrated here.",00:06:23,8,N dimensions illustrated
00:06:34,2,Paradigmatic Relation Discovery Part 1,1.8,"And on the bottom, you can see a frequency vector representing a context,",00:06:27,8,And bottom see frequency vector representing context
00:06:39,2,Paradigmatic Relation Discovery Part 1,1.8,"and here we see where eats occurred 5 times in this context,",00:06:34,8,see eats occurred 5 times context
00:06:43,2,Paradigmatic Relation Discovery Part 1,1.8,"ate occurred 3 times, et cetera.",00:06:39,8,ate occurred 3 times et cetera
00:06:48,2,Paradigmatic Relation Discovery Part 1,1.8,So this vector can then be placed in this vector space model.,00:06:43,8,So vector placed vector space model
00:06:53,2,Paradigmatic Relation Discovery Part 1,1.8,"So in general, we can represent a pseudo document or",00:06:48,8,So general represent pseudo document
00:06:58,2,Paradigmatic Relation Discovery Part 1,1.8,"context of cat as one vector, d1, and another word,",00:06:53,8,context cat one vector d1 another word
00:07:04,2,Paradigmatic Relation Discovery Part 1,1.8,"dog, might give us a different context, so d2.",00:06:58,8,dog might give us different context d2
00:07:07,2,Paradigmatic Relation Discovery Part 1,1.8,And then we can measure the similarity of these two vectors.,00:07:04,8,And measure similarity two vectors
00:07:10,2,Paradigmatic Relation Discovery Part 1,1.8,"So by viewing context in the vector space model,",00:07:07,8,So viewing context vector space model
00:07:15,2,Paradigmatic Relation Discovery Part 1,1.8,we convert the problem of paradigmatical relation discovery,00:07:10,8,convert problem paradigmatical relation discovery
00:07:18,2,Paradigmatic Relation Discovery Part 1,1.8,into the problem of computing the vectors and their similarity.,00:07:15,8,problem computing vectors similarity
00:07:24,2,Paradigmatic Relation Discovery Part 1,1.8,"So the two questions that we have to address are first,",00:07:20,8,So two questions address first
00:07:28,2,Paradigmatic Relation Discovery Part 1,1.8,"how to compute each vector, and that is how to compute xi or yi.",00:07:24,8,compute vector compute xi yi
00:07:33,2,Paradigmatic Relation Discovery Part 1,1.8,And the other question is how do you compute the similarity.,00:07:31,8,And question compute similarity
00:07:40,2,Paradigmatic Relation Discovery Part 1,1.8,"Now in general, there are many approaches that can be used to solve the problem, and",00:07:35,8,Now general many approaches used solve problem
00:07:43,2,Paradigmatic Relation Discovery Part 1,1.8,most of them are developed for information retrieval.,00:07:40,8,developed information retrieval
00:07:47,2,Paradigmatic Relation Discovery Part 1,1.8,And they have been shown to work well for,00:07:43,8,And shown work well
00:07:52,2,Paradigmatic Relation Discovery Part 1,1.8,matching a query vector and a document vector.,00:07:47,8,matching query vector document vector
00:07:57,2,Paradigmatic Relation Discovery Part 1,1.8,But we can adapt many of the ideas to compute a similarity,00:07:52,8,But adapt many ideas compute similarity
00:08:01,2,Paradigmatic Relation Discovery Part 1,1.8,of context documents for our purpose here.,00:07:57,8,context documents purpose
00:08:05,2,Paradigmatic Relation Discovery Part 1,1.8,"So let's first look at the one plausible approach,",00:08:01,8,So let first look one plausible approach
00:08:10,2,Paradigmatic Relation Discovery Part 1,1.8,where we try to match the similarity of context based on,00:08:05,8,try match similarity context based
00:08:15,2,Paradigmatic Relation Discovery Part 1,1.8,"the expected overlap of words, and we call this EOWC.",00:08:10,8,expected overlap words call EOWC
00:08:22,2,Paradigmatic Relation Discovery Part 1,1.8,So the idea here is to represent a context by a word vector,00:08:17,8,So idea represent context word vector
00:08:28,2,Paradigmatic Relation Discovery Part 1,1.8,where each word has a weight that's equal to the probability,00:08:22,8,word weight equal probability
00:08:35,2,Paradigmatic Relation Discovery Part 1,1.8,"that a randomly picked word from this document vector, is this word.",00:08:28,8,randomly picked word document vector word
00:08:39,2,Paradigmatic Relation Discovery Part 1,1.8,"So in other words, xi is defined as the normalized",00:08:35,8,So words xi defined normalized
00:08:43,2,Paradigmatic Relation Discovery Part 1,1.8,"account of word wi in the context, and",00:08:39,8,account word wi context
00:08:48,2,Paradigmatic Relation Discovery Part 1,1.8,this can be interpreted as the probability that you would,00:08:43,8,interpreted probability would
00:08:54,2,Paradigmatic Relation Discovery Part 1,1.8,actually pick this word from d1 if you randomly picked a word.,00:08:48,8,actually pick word d1 randomly picked word
00:09:01,2,Paradigmatic Relation Discovery Part 1,1.8,"Now, of course these xi's would sum to one because they are normalized frequencies,",00:08:56,8,Now course xi would sum one normalized frequencies
00:09:05,2,Paradigmatic Relation Discovery Part 1,1.8,and this means the vector is,00:09:02,8,means vector
00:09:08,2,Paradigmatic Relation Discovery Part 1,1.8,actually probability of the distribution over words.,00:09:05,8,actually probability distribution words
00:09:15,2,Paradigmatic Relation Discovery Part 1,1.8,"So, the vector d2 can be also computed in the same way, and",00:09:10,8,So vector d2 also computed way
00:09:23,2,Paradigmatic Relation Discovery Part 1,1.8,this would give us then two probability distributions representing two contexts.,00:09:15,8,would give us two probability distributions representing two contexts
00:09:28,2,Paradigmatic Relation Discovery Part 1,1.8,"So, that addresses the problem how to compute the vectors, and",00:09:24,8,So addresses problem compute vectors
00:09:31,2,Paradigmatic Relation Discovery Part 1,1.8,next let's see how we can define similarity in this approach.,00:09:28,8,next let see define similarity approach
00:09:35,2,Paradigmatic Relation Discovery Part 1,1.8,"Well, here, we simply define the similarity as a dot product of two",00:09:31,8,Well simply define similarity dot product two
00:09:39,2,Paradigmatic Relation Discovery Part 1,1.8,"vectors, and this is defined as a sum of the products",00:09:35,8,vectors defined sum products
00:09:43,2,Paradigmatic Relation Discovery Part 1,1.8,of the corresponding elements of the two vectors.,00:09:41,8,corresponding elements two vectors
00:09:51,2,Paradigmatic Relation Discovery Part 1,1.8,"Now, it's interesting to see that this similarity function",00:09:46,8,Now interesting see similarity function
00:09:57,2,Paradigmatic Relation Discovery Part 1,1.8,"actually has a nice interpretation, and that is this.",00:09:51,8,actually nice interpretation
00:10:02,2,Paradigmatic Relation Discovery Part 1,1.8,"Dot product, in fact that gives us the probability that two",00:09:57,8,Dot product fact gives us probability two
00:10:08,2,Paradigmatic Relation Discovery Part 1,1.8,randomly picked words from the two contexts are identical.,00:10:02,8,randomly picked words two contexts identical
00:10:12,2,Paradigmatic Relation Discovery Part 1,1.8,That means if we try to pick a word from one context and try to pick another,00:10:08,8,That means try pick word one context try pick another
00:10:17,2,Paradigmatic Relation Discovery Part 1,1.8,"word from another context, we can then ask the question, are they identical?",00:10:12,8,word another context ask question identical
00:10:22,2,Paradigmatic Relation Discovery Part 1,1.8,"If the two contexts are very similar, then we should expect we frequently will",00:10:17,8,If two contexts similar expect frequently
00:10:27,2,Paradigmatic Relation Discovery Part 1,1.8,see the two words picked from the two contexts are identical.,00:10:22,8,see two words picked two contexts identical
00:10:30,2,Paradigmatic Relation Discovery Part 1,1.8,"If they are very different, then the chance of seeing",00:10:27,8,If different chance seeing
00:10:34,2,Paradigmatic Relation Discovery Part 1,1.8,identical words being picked from the two contexts would be small.,00:10:30,8,identical words picked two contexts would small
00:10:39,2,Paradigmatic Relation Discovery Part 1,1.8,"So this intuitively makes sense, right, for measuring similarity of contexts.",00:10:34,8,So intuitively makes sense right measuring similarity contexts
00:10:46,2,Paradigmatic Relation Discovery Part 1,1.8,Now you might want to also take a look at the exact formulas and,00:10:41,8,Now might want also take look exact formulas
00:10:51,2,Paradigmatic Relation Discovery Part 1,1.8,see why this can be interpreted as the probability that,00:10:46,8,see interpreted probability
00:10:55,2,Paradigmatic Relation Discovery Part 1,1.8,two randomly picked words are identical.,00:10:51,8,two randomly picked words identical
00:11:04,2,Paradigmatic Relation Discovery Part 1,1.8,"So if you just stare at the formula to check what's inside this sum,",00:10:57,8,So stare formula check inside sum
00:11:12,2,Paradigmatic Relation Discovery Part 1,1.8,then you will see basically in each case it gives us the probability that,00:11:04,8,see basically case gives us probability
00:11:17,2,Paradigmatic Relation Discovery Part 1,1.8,"we will see an overlap on a particular word, wi.",00:11:12,8,see overlap particular word wi
00:11:23,2,Paradigmatic Relation Discovery Part 1,1.8,"And where xi gives us a probability that we will pick this particular word from d1,",00:11:17,8,And xi gives us probability pick particular word d1
00:11:28,2,Paradigmatic Relation Discovery Part 1,1.8,and yi gives us the probability of picking this word from d2.,00:11:23,8,yi gives us probability picking word d2
00:11:32,2,Paradigmatic Relation Discovery Part 1,1.8,"And when we pick the same word from the two contexts,",00:11:28,8,And pick word two contexts
00:11:34,2,Paradigmatic Relation Discovery Part 1,1.8,"then we have an identical pick, right so.",00:11:32,8,identical pick right
00:11:42,2,Paradigmatic Relation Discovery Part 1,1.8,"That's one possible approach, EOWC, extracted overlap of words in context.",00:11:34,8,That one possible approach EOWC extracted overlap words context
00:11:49,2,Paradigmatic Relation Discovery Part 1,1.8,"Now as always, we would like to assess whether this approach it would work well.",00:11:42,8,Now always would like assess whether approach would work well
00:11:52,2,Paradigmatic Relation Discovery Part 1,1.8,"Now of course, ultimately we have to test the approach with real data and",00:11:49,8,Now course ultimately test approach real data
00:11:56,2,Paradigmatic Relation Discovery Part 1,1.8,see if it gives us really semantically related words.,00:11:52,8,see gives us really semantically related words
00:12:01,2,Paradigmatic Relation Discovery Part 1,1.8,"Really give us paradigmatical relations, but",00:11:57,8,Really give us paradigmatical relations
00:12:05,2,Paradigmatic Relation Discovery Part 1,1.8,analytically we can also analyze this formula a little bit.,00:12:01,8,analytically also analyze formula little bit
00:12:11,2,Paradigmatic Relation Discovery Part 1,1.8,"So first, as I said, it does make sense, right, because this",00:12:05,8,So first I said make sense right
00:12:15,2,Paradigmatic Relation Discovery Part 1,1.8,formula will give a higher score if there is more overlap between the two contexts.,00:12:11,8,formula give higher score overlap two contexts
00:12:17,2,Paradigmatic Relation Discovery Part 1,1.8,So that's exactly what we want.,00:12:15,8,So exactly want
00:12:21,2,Paradigmatic Relation Discovery Part 1,1.8,"But if you analyze the formula more carefully,",00:12:17,8,But analyze formula carefully
00:12:24,2,Paradigmatic Relation Discovery Part 1,1.8,"then you also see there might be some potential problems,",00:12:21,8,also see might potential problems
00:12:27,2,Paradigmatic Relation Discovery Part 1,1.8,and specifically there are two potential problems.,00:12:24,8,specifically two potential problems
00:12:33,2,Paradigmatic Relation Discovery Part 1,1.8,"First, it might favor matching one frequent term very well,",00:12:27,8,First might favor matching one frequent term well
00:12:35,2,Paradigmatic Relation Discovery Part 1,1.8,over matching more distinct terms.,00:12:33,8,matching distinct terms
00:12:44,2,Paradigmatic Relation Discovery Part 1,1.8,"And that is because in the dot product, if one element has a high value and this",00:12:36,8,And dot product one element high value
00:12:50,2,Paradigmatic Relation Discovery Part 1,1.8,"element is shared by both contexts and it contributes a lot to the overall sum,",00:12:44,8,element shared contexts contributes lot overall sum
00:12:55,2,Paradigmatic Relation Discovery Part 1,1.8,"it might indeed make the score higher than in another case,",00:12:51,8,might indeed make score higher another case
00:13:01,2,Paradigmatic Relation Discovery Part 1,1.8,where the two vectors actually have a lot of overlap in different terms.,00:12:55,8,two vectors actually lot overlap different terms
00:13:06,2,Paradigmatic Relation Discovery Part 1,1.8,"But each term has a relatively low frequency, so this may not be desirable.",00:13:01,8,But term relatively low frequency may desirable
00:13:09,2,Paradigmatic Relation Discovery Part 1,1.8,"Of course, this might be desirable in some other cases.",00:13:06,8,Of course might desirable cases
00:13:14,2,Paradigmatic Relation Discovery Part 1,1.8,"But in our case, we should intuitively prefer a case where we match",00:13:09,8,But case intuitively prefer case match
00:13:19,2,Paradigmatic Relation Discovery Part 1,1.8,"more different terms in the context, so that we have more confidence",00:13:14,8,different terms context confidence
00:13:24,2,Paradigmatic Relation Discovery Part 1,1.8,in saying that the two words indeed occur in similar context.,00:13:19,8,saying two words indeed occur similar context
00:13:27,2,Paradigmatic Relation Discovery Part 1,1.8,If you only rely on one term and,00:13:24,8,If rely one term
00:13:32,2,Paradigmatic Relation Discovery Part 1,1.8,"that's a little bit questionable, it may not be robust.",00:13:27,8,little bit questionable may robust
00:13:38,2,Paradigmatic Relation Discovery Part 1,1.8,"Now the second problem is that it treats every word equally, right.",00:13:34,8,Now second problem treats every word equally right
00:13:42,2,Paradigmatic Relation Discovery Part 1,1.8,So if you match a word like the and,00:13:38,8,So match word like
00:13:47,2,Paradigmatic Relation Discovery Part 1,1.8,"it will be the same as matching a word like eats, but",00:13:42,8,matching word like eats
00:13:52,2,Paradigmatic Relation Discovery Part 1,1.8,intuitively we know matching the isn't really,00:13:47,8,intuitively know matching really
00:13:57,2,Paradigmatic Relation Discovery Part 1,1.8,surprising because the occurs everywhere.,00:13:52,8,surprising occurs everywhere
00:14:02,2,Paradigmatic Relation Discovery Part 1,1.8,So matching the is not as such strong evidence as matching what,00:13:57,8,So matching strong evidence matching
00:14:07,2,Paradigmatic Relation Discovery Part 1,1.8,"a word like eats, which doesn't occur frequently.",00:14:02,8,word like eats occur frequently
00:14:11,2,Paradigmatic Relation Discovery Part 1,1.8,So this is another problem of this approach.,00:14:07,8,So another problem approach
00:14:19,2,Paradigmatic Relation Discovery Part 1,1.8,In the next chapter we are going to talk about how to address these problems.,00:14:13,8,In next chapter going talk address problems
00:00:08,2,Text Representation- Part 2,1.6,[SOUND].,00:00:00,6,SOUND
00:00:11,2,Text Representation- Part 2,1.6,"So, as we explained the different text",00:00:08,6,So explained different text
00:00:15,2,Text Representation- Part 2,1.6,representation tends to enable different analysis.,00:00:11,6,representation tends enable different analysis
00:00:19,2,Text Representation- Part 2,1.6,"In particular, we can gradually add more and",00:00:16,6,In particular gradually add
00:00:24,2,Text Representation- Part 2,1.6,more deeper analysis results to represent text data.,00:00:19,6,deeper analysis results represent text data
00:00:27,2,Text Representation- Part 2,1.6,And that would open up a more interesting representation,00:00:24,6,And would open interesting representation
00:00:33,2,Text Representation- Part 2,1.6,opportunities and also analysis capacities.,00:00:29,6,opportunities also analysis capacities
00:00:37,2,Text Representation- Part 2,1.6,"So, this table summarizes what we have just seen.",00:00:33,6,So table summarizes seen
00:00:39,2,Text Representation- Part 2,1.6,So the first column shows the text representation.,00:00:37,6,So first column shows text representation
00:00:44,2,Text Representation- Part 2,1.6,The second visualizes the generality of such a representation.,00:00:39,6,The second visualizes generality representation
00:00:48,2,Text Representation- Part 2,1.6,Meaning whether we can do this kind of representation accurately for,00:00:44,6,Meaning whether kind representation accurately
00:00:51,2,Text Representation- Part 2,1.6,all the text data or only some of them.,00:00:48,6,text data
00:00:54,2,Text Representation- Part 2,1.6,And the third column shows the enabled analysis techniques.,00:00:51,6,And third column shows enabled analysis techniques
00:01:00,2,Text Representation- Part 2,1.6,And the final column shows some examples of application that,00:00:56,6,And final column shows examples application
00:01:04,2,Text Representation- Part 2,1.6,can be achieved through this level of representation.,00:01:00,6,achieved level representation
00:01:06,2,Text Representation- Part 2,1.6,So let's take a look at them.,00:01:04,6,So let take look
00:01:12,2,Text Representation- Part 2,1.6,So as a stream text can only be processed by stream processing algorithms.,00:01:06,6,So stream text processed stream processing algorithms
00:01:14,2,Text Representation- Part 2,1.6,"It's very robust, it's general.",00:01:12,6,It robust general
00:01:17,2,Text Representation- Part 2,1.6,And there was still some interesting applications that can be down,00:01:15,6,And still interesting applications
00:01:18,2,Text Representation- Part 2,1.6,at this level.,00:01:17,6,level
00:01:20,2,Text Representation- Part 2,1.6,"For example, compression of text.",00:01:18,6,For example compression text
00:01:24,2,Text Representation- Part 2,1.6,Doesn't necessarily need to know the word boundaries.,00:01:20,6,Doesn necessarily need know word boundaries
00:01:27,2,Text Representation- Part 2,1.6,Although knowing word boundaries might actually also help.,00:01:24,6,Although knowing word boundaries might actually also help
00:01:32,2,Text Representation- Part 2,1.6,Word base repetition is a very important level of representation.,00:01:28,6,Word base repetition important level representation
00:01:34,2,Text Representation- Part 2,1.6,It's quite general and,00:01:32,6,It quite general
00:01:39,2,Text Representation- Part 2,1.6,"relatively robust, indicating they were a lot of analysis techniques.",00:01:34,6,relatively robust indicating lot analysis techniques
00:01:44,2,Text Representation- Part 2,1.6,"Such as word relation analysis, topic analysis and sentiment analysis.",00:01:39,6,Such word relation analysis topic analysis sentiment analysis
00:01:48,2,Text Representation- Part 2,1.6,And there are many applications that can be enabled by this kind of analysis.,00:01:44,6,And many applications enabled kind analysis
00:01:54,2,Text Representation- Part 2,1.6,"For example, thesaurus discovery has to do with discovering related words.",00:01:48,6,For example thesaurus discovery discovering related words
00:02:00,2,Text Representation- Part 2,1.6,And topic and opinion related applications are abounded.,00:01:54,6,And topic opinion related applications abounded
00:02:03,2,Text Representation- Part 2,1.6,"And there are, for example, people",00:02:00,6,And example people
00:02:08,2,Text Representation- Part 2,1.6,might be interesting in knowing the major topics covered in the collection of texts.,00:02:03,6,might interesting knowing major topics covered collection texts
00:02:12,2,Text Representation- Part 2,1.6,And this can be the case in research literature.,00:02:08,6,And case research literature
00:02:18,2,Text Representation- Part 2,1.6,And scientists want to know what are the most important research topics today.,00:02:12,6,And scientists want know important research topics today
00:02:22,2,Text Representation- Part 2,1.6,Or customer service people might want to know all our major complaints from their,00:02:18,6,Or customer service people might want know major complaints
00:02:28,2,Text Representation- Part 2,1.6,customers by mining their e-mail messages.,00:02:22,6,customers mining e mail messages
00:02:33,2,Text Representation- Part 2,1.6,And business intelligence people might be interested in,00:02:28,6,And business intelligence people might interested
00:02:38,2,Text Representation- Part 2,1.6,understanding consumers' opinions about their products and the competitors',00:02:33,6,understanding consumers opinions products competitors
00:02:42,2,Text Representation- Part 2,1.6,products to figure out what are the winning features of their products.,00:02:38,6,products figure winning features products
00:02:47,2,Text Representation- Part 2,1.6,"And, in general, there are many",00:02:43,6,And general many
00:02:51,2,Text Representation- Part 2,1.6,applications that can be enabled by the representation at this level.,00:02:47,6,applications enabled representation level
00:02:58,2,Text Representation- Part 2,1.6,"Now, moving down, we'll see we can gradually add additional representations.",00:02:53,6,Now moving see gradually add additional representations
00:03:01,2,Text Representation- Part 2,1.6,"By adding syntactical structures, we can enable, of course,",00:02:58,6,By adding syntactical structures enable course
00:03:03,2,Text Representation- Part 2,1.6,syntactical graph analysis.,00:03:01,6,syntactical graph analysis
00:03:09,2,Text Representation- Part 2,1.6,We can use graph mining algorithms to analyze syntactic graphs.,00:03:03,6,We use graph mining algorithms analyze syntactic graphs
00:03:13,2,Text Representation- Part 2,1.6,And some applications are related to this kind of representation.,00:03:09,6,And applications related kind representation
00:03:14,2,Text Representation- Part 2,1.6,"For example,",00:03:13,6,For example
00:03:18,2,Text Representation- Part 2,1.6,stylistic analysis generally requires syntactical structure representation.,00:03:14,6,stylistic analysis generally requires syntactical structure representation
00:03:26,2,Text Representation- Part 2,1.6,We can also generate the structure based features.,00:03:22,6,We also generate structure based features
00:03:32,2,Text Representation- Part 2,1.6,And those are features that might help us classify the text objects into different,00:03:26,6,And features might help us classify text objects different
00:03:37,2,Text Representation- Part 2,1.6,categories by looking at the structures sometimes in the classification.,00:03:32,6,categories looking structures sometimes classification
00:03:39,2,Text Representation- Part 2,1.6,It can be more accurate.,00:03:37,6,It accurate
00:03:43,2,Text Representation- Part 2,1.6,"For example, if you want to classify articles into",00:03:39,6,For example want classify articles
00:03:49,2,Text Representation- Part 2,1.6,different categories corresponding to different authors.,00:03:45,6,different categories corresponding different authors
00:03:56,2,Text Representation- Part 2,1.6,You want to figure out which of the k authors has actually written,00:03:49,6,You want figure k authors actually written
00:04:01,2,Text Representation- Part 2,1.6,"this article, then you generally need to look at the syntactic structures.",00:03:56,6,article generally need look syntactic structures
00:04:05,2,Text Representation- Part 2,1.6,"When we add entities and relations,",00:04:03,6,When add entities relations
00:04:09,2,Text Representation- Part 2,1.6,then we can enable other techniques such as knowledge graph and,00:04:05,6,enable techniques knowledge graph
00:04:13,2,Text Representation- Part 2,1.6,"answers, or information network and answers in general.",00:04:09,6,answers information network answers general
00:04:20,2,Text Representation- Part 2,1.6,And this analysis enable applications about entities.,00:04:13,6,And analysis enable applications entities
00:04:22,2,Text Representation- Part 2,1.6,"For example,",00:04:22,6,For example
00:04:27,2,Text Representation- Part 2,1.6,discovery of all the knowledge and opinions about real world entities.,00:04:22,6,discovery knowledge opinions real world entities
00:04:31,2,Text Representation- Part 2,1.6,You can also use this level representation,00:04:28,6,You also use level representation
00:04:35,2,Text Representation- Part 2,1.6,to integrate everything about anything from scaled resources.,00:04:31,6,integrate everything anything scaled resources
00:04:40,2,Text Representation- Part 2,1.6,"Finally, when we add logical predicates,",00:04:37,6,Finally add logical predicates
00:04:44,2,Text Representation- Part 2,1.6,"that would enable large inference, of course.",00:04:40,6,would enable large inference course
00:04:46,2,Text Representation- Part 2,1.6,And this can be very useful for,00:04:44,6,And useful
00:04:48,2,Text Representation- Part 2,1.6,integrating analysis of scattered knowledge.,00:04:46,6,integrating analysis scattered knowledge
00:04:53,2,Text Representation- Part 2,1.6,"For example, we can also add ontology on top of the,",00:04:50,6,For example also add ontology top
00:04:58,2,Text Representation- Part 2,1.6,"extracted the information from text, to make inferences.",00:04:54,6,extracted information text make inferences
00:05:04,2,Text Representation- Part 2,1.6,"A good of example of application in this enabled by this level of representation,",00:04:59,6,A good example application enabled level representation
00:05:07,2,Text Representation- Part 2,1.6,is a knowledge assistant for biologists.,00:05:04,6,knowledge assistant biologists
00:05:14,2,Text Representation- Part 2,1.6,And this program that can help a biologist manage all the relevant knowledge from,00:05:07,6,And program help biologist manage relevant knowledge
00:05:21,2,Text Representation- Part 2,1.6,literature about a research problem such as understanding functions of genes.,00:05:14,6,literature research problem understanding functions genes
00:05:27,2,Text Representation- Part 2,1.6,And the computer can make inferences,00:05:22,6,And computer make inferences
00:05:32,2,Text Representation- Part 2,1.6,about some of the hypothesis that the biologist might be interesting.,00:05:27,6,hypothesis biologist might interesting
00:05:36,2,Text Representation- Part 2,1.6,"For example, whether a gene has a certain function, and",00:05:32,6,For example whether gene certain function
00:05:42,2,Text Representation- Part 2,1.6,"then the intelligent program can read the literature to extract the relevant facts,",00:05:36,6,intelligent program read literature extract relevant facts
00:05:45,2,Text Representation- Part 2,1.6,doing compiling and information extracting.,00:05:42,6,compiling information extracting
00:05:50,2,Text Representation- Part 2,1.6,And then using a logic system to actually track that's the answers,00:05:45,6,And using logic system actually track answers
00:05:56,2,Text Representation- Part 2,1.6,to researchers questioning about what genes are related to what functions.,00:05:50,6,researchers questioning genes related functions
00:06:01,2,Text Representation- Part 2,1.6,So in order to support this level of application,00:05:57,6,So order support level application
00:06:04,2,Text Representation- Part 2,1.6,we need to go as far as logical representation.,00:06:01,6,need go far logical representation
00:06:10,2,Text Representation- Part 2,1.6,"Now, this course is covering techniques mainly based on word based representation.",00:06:04,6,Now course covering techniques mainly based word based representation
00:06:14,2,Text Representation- Part 2,1.6,And these techniques are general and,00:06:12,6,And techniques general
00:06:19,2,Text Representation- Part 2,1.6,robust and that's more widely used in various applications.,00:06:14,6,robust widely used various applications
00:06:26,2,Text Representation- Part 2,1.6,"In fact, in virtually all the text mining applications you need this level of",00:06:21,6,In fact virtually text mining applications need level
00:06:32,2,Text Representation- Part 2,1.6,representation and then techniques that support analysis of text in this level.,00:06:26,6,representation techniques support analysis text level
00:06:39,2,Text Representation- Part 2,1.6,But obviously all these other levels can be combined and,00:06:35,6,But obviously levels combined
00:06:45,2,Text Representation- Part 2,1.6,should be combined in order to support the sophisticated applications.,00:06:39,6,combined order support sophisticated applications
00:06:48,2,Text Representation- Part 2,1.6,"So to summarize, here are the major takeaway points.",00:06:45,6,So summarize major takeaway points
00:06:53,2,Text Representation- Part 2,1.6,Text representation determines what kind of mining algorithms can be applied.,00:06:48,6,Text representation determines kind mining algorithms applied
00:06:57,2,Text Representation- Part 2,1.6,"And there are multiple ways to represent the text, strings, words,",00:06:53,6,And multiple ways represent text strings words
00:07:03,2,Text Representation- Part 2,1.6,"syntactic structures, entity-relation graphs, knowledge predicates, etc.",00:06:57,6,syntactic structures entity relation graphs knowledge predicates etc
00:07:08,2,Text Representation- Part 2,1.6,And these different representations should in general,00:07:03,6,And different representations general
00:07:13,2,Text Representation- Part 2,1.6,be combined in real applications to the extent we can.,00:07:08,6,combined real applications extent
00:07:20,2,Text Representation- Part 2,1.6,"For example, even if we cannot do accurate representations",00:07:13,6,For example even cannot accurate representations
00:07:25,2,Text Representation- Part 2,1.6,"of syntactic structures, we can state that partial structures strictly.",00:07:20,6,syntactic structures state partial structures strictly
00:07:29,2,Text Representation- Part 2,1.6,"And if we can recognize some entities, that would be great.",00:07:25,6,And recognize entities would great
00:07:32,2,Text Representation- Part 2,1.6,So in general we want to do as much as we can.,00:07:29,6,So general want much
00:07:37,2,Text Representation- Part 2,1.6,"And when different levels are combined together,",00:07:34,6,And different levels combined together
00:07:41,2,Text Representation- Part 2,1.6,"we can enable a richer analysis, more powerful analysis.",00:07:37,6,enable richer analysis powerful analysis
00:07:46,2,Text Representation- Part 2,1.6,This course however focuses on word-based representation.,00:07:42,6,This course however focuses word based representation
00:07:52,2,Text Representation- Part 2,1.6,"Such techniques have also several advantage, first of they are general and",00:07:46,6,Such techniques also several advantage first general
00:07:55,2,Text Representation- Part 2,1.6,"robust, so they are applicable to any natural language.",00:07:52,6,robust applicable natural language
00:07:59,2,Text Representation- Part 2,1.6,That's a big advantage over other approaches that rely on,00:07:55,6,That big advantage approaches rely
00:08:03,2,Text Representation- Part 2,1.6,more fragile natural language processing techniques.,00:07:59,6,fragile natural language processing techniques
00:08:07,2,Text Representation- Part 2,1.6,"Secondly, it does not require much manual effort, or",00:08:03,6,Secondly require much manual effort
00:08:11,2,Text Representation- Part 2,1.6,"sometimes, it does not require any manual effort.",00:08:07,6,sometimes require manual effort
00:08:14,2,Text Representation- Part 2,1.6,"So that's, again, an important benefit,",00:08:11,6,So important benefit
00:08:17,2,Text Representation- Part 2,1.6,because that means that you can apply it directly to any application.,00:08:14,6,means apply directly application
00:08:25,2,Text Representation- Part 2,1.6,"Third, these techniques are actually surprisingly powerful and",00:08:20,6,Third techniques actually surprisingly powerful
00:08:27,2,Text Representation- Part 2,1.6,effective form in implications.,00:08:25,6,effective form implications
00:08:32,2,Text Representation- Part 2,1.6,Although not all of course as I just explained.,00:08:29,6,Although course I explained
00:08:38,2,Text Representation- Part 2,1.6,Now they are very effective partly because the words,00:08:34,6,Now effective partly words
00:08:44,2,Text Representation- Part 2,1.6,are invented by humans as basically units for communications.,00:08:38,6,invented humans basically units communications
00:08:51,2,Text Representation- Part 2,1.6,So they are actually quite sufficient for representing all kinds of semantics.,00:08:45,6,So actually quite sufficient representing kinds semantics
00:09:00,2,Text Representation- Part 2,1.6,So that makes this kind of word-based representation all so powerful.,00:08:53,6,So makes kind word based representation powerful
00:09:05,2,Text Representation- Part 2,1.6,"And finally, such a word-based representation and the techniques enable",00:09:00,6,And finally word based representation techniques enable
00:09:11,2,Text Representation- Part 2,1.6,by such a representation can be combined with many other sophisticated approaches.,00:09:05,6,representation combined many sophisticated approaches
00:09:15,2,Text Representation- Part 2,1.6,So they're not competing with each other.,00:09:14,6,So competing
00:00:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, I just showed you that empirically the likelihood will converge,",00:00:07,12,So I showed empirically likelihood converge
00:00:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,but theoretically it can also be proved that EM algorithm will,00:00:12,12,theoretically also proved EM algorithm
00:00:19,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,converge to a local maximum.,00:00:17,12,converge local maximum
00:00:24,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So here's just an illustration of what happened and a detailed explanation.,00:00:19,12,So illustration happened detailed explanation
00:00:29,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"This required more knowledge about that,",00:00:24,12,This required knowledge
00:00:36,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"some of that inequalities, that we haven't really covered yet.",00:00:29,12,inequalities really covered yet
00:00:45,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So here what you see is on the X dimension, we have a c0 value.",00:00:39,12,So see X dimension c0 value
00:00:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,This is a parameter that we have.,00:00:45,12,This parameter
00:00:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,On the y axis we see the likelihood function.,00:00:46,12,On axis see likelihood function
00:00:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So this curve is the original likelihood function,",00:00:49,12,So curve original likelihood function
00:01:04,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,and this is the one that we hope to maximize.,00:00:57,12,one hope maximize
00:01:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And we hope to find a c0 value at this point to maximize this.,00:01:04,12,And hope find c0 value point maximize
00:01:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,But in the case of Mitsumoto we can not easily find an analytic solution,00:01:06,12,But case Mitsumoto easily find analytic solution
00:01:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,to the problem.,00:01:11,12,problem
00:01:14,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, we have to resolve the numerical errors, and",00:01:12,12,So resolve numerical errors
00:01:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,the EM algorithm is such an algorithm.,00:01:14,12,EM algorithm algorithm
00:01:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,It's a Hill-Climb algorithm.,00:01:16,12,It Hill Climb algorithm
00:01:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,That would mean you start with some random guess.,00:01:17,12,That would mean start random guess
00:01:26,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"Let's say you start from here, that's your starting point.",00:01:22,12,Let say start starting point
00:01:32,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And then you try to improve this by moving this to,00:01:26,12,And try improve moving
00:01:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,another point where you can have a higher likelihood.,00:01:32,12,another point higher likelihood
00:01:37,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So that's the ideal hill climbing.,00:01:35,12,So ideal hill climbing
00:01:43,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And in the EM algorithm, the way we achieve this is to do two things.",00:01:37,12,And EM algorithm way achieve two things
00:01:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"First, we'll fix a lower bound of likelihood function.",00:01:43,12,First fix lower bound likelihood function
00:01:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So this is the lower bound.,00:01:46,12,So lower bound
00:01:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,See here.,00:01:48,12,See
00:01:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And once we fit the lower bound, we can then maximize the lower bound.",00:01:51,12,And fit lower bound maximize lower bound
00:01:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And of course, the reason why this works,",00:01:57,12,And course reason works
00:02:02,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,is because the lower bound is much easier to optimize.,00:01:59,12,lower bound much easier optimize
00:02:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So we know our current guess is here.,00:02:02,12,So know current guess
00:02:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And by maximizing the lower bound, we'll move this point to the top.",00:02:05,12,And maximizing lower bound move point top
00:02:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,To here.,00:02:11,12,To
00:02:14,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,Right?,00:02:13,12,Right
00:02:20,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And we can then map to the original likelihood function, we find this point.",00:02:14,12,And map original likelihood function find point
00:02:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"Because it's a lower bound, we are guaranteed to improve this guess, right?",00:02:20,12,Because lower bound guaranteed improve guess right
00:02:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,Because we improve our lower bound and then the original likelihood,00:02:25,12,Because improve lower bound original likelihood
00:02:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,curve which is above this lower bound will definitely be improved as well.,00:02:30,12,curve lower bound definitely improved well
00:02:39,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So we already know it's improving the lower bound.,00:02:36,12,So already know improving lower bound
00:02:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So we definitely improve this original likelihood function,",00:02:39,12,So definitely improve original likelihood function
00:02:47,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,which is above this lower bound.,00:02:42,12,lower bound
00:02:49,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, in our example,",00:02:47,12,So example
00:02:53,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,the current guess is parameter value given by the current generation.,00:02:49,12,current guess parameter value given current generation
00:02:57,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And then the next guess is the re-estimated parameter values.,00:02:53,12,And next guess estimated parameter values
00:03:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,From this illustration you can see the next guess,00:02:57,12,From illustration see next guess
00:03:03,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,is always better than the current guess.,00:03:01,12,always better current guess
00:03:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"Unless it has reached the maximum, where it will be stuck there.",00:03:03,12,Unless reached maximum stuck
00:03:08,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So the two would be equal.,00:03:06,12,So two would equal
00:03:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, the E-step is basically",00:03:08,12,So E step basically
00:03:17,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,to compute this lower bound.,00:03:12,12,compute lower bound
00:03:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,We don't directly just compute this likelihood function but,00:03:17,12,We directly compute likelihood function
00:03:25,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,we compute the length of the variable values and,00:03:22,12,compute length variable values
00:03:28,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,these are basically a part of this lower bound.,00:03:25,12,basically part lower bound
00:03:31,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,This helps determine the lower bound.,00:03:28,12,This helps determine lower bound
00:03:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,The M-step on the other hand is to maximize the lower bound.,00:03:31,12,The M step hand maximize lower bound
00:03:37,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,It allows us to move parameters to a new point.,00:03:34,12,It allows us move parameters new point
00:03:41,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And that's why EM algorithm is guaranteed to converge to a local maximum.,00:03:37,12,And EM algorithm guaranteed converge local maximum
00:03:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"Now, as you can imagine, when we have many local maxima,",00:03:42,12,Now imagine many local maxima
00:03:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,we also have to repeat the EM algorithm multiple times.,00:03:46,12,also repeat EM algorithm multiple times
00:03:54,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,In order to figure out which one is the actual global maximum.,00:03:50,12,In order figure one actual global maximum
00:03:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And this actually in general is a difficult problem in numeral optimization.,00:03:54,12,And actually general difficult problem numeral optimization
00:04:02,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So here for example had we started from here,",00:03:59,12,So example started
00:04:06,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,then we gradually just climb up to this top.,00:04:02,12,gradually climb top
00:04:11,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, that's not optimal, and we'd like to climb up all the way to here,",00:04:06,12,So optimal like climb way
00:04:16,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,so the only way to climb up to this gear is to start from somewhere here or here.,00:04:11,12,way climb gear start somewhere
00:04:22,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"So, in the EM algorithm, we generally would have to start from different points",00:04:16,12,So EM algorithm generally would start different points
00:04:27,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,or have some other way to determine a good initial starting point.,00:04:22,12,way determine good initial starting point
00:04:34,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,To summarize in this lecture we introduced the EM algorithm.,00:04:29,12,To summarize lecture introduced EM algorithm
00:04:38,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,This is a general algorithm for computing maximum maximum likelihood estimate of all,00:04:34,12,This general algorithm computing maximum maximum likelihood estimate
00:04:42,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"kinds of models, so not just for our simple model.",00:04:38,12,kinds models simple model
00:04:46,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"And it's a hill-climbing algorithm, so it can only converge to a local maximum and",00:04:42,12,And hill climbing algorithm converge local maximum
00:04:48,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,it will depend on initial points.,00:04:46,12,depend initial points
00:04:55,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,The general idea is that we will have two steps to improve the estimate of.,00:04:49,12,The general idea two steps improve estimate
00:05:00,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,In the E-step we roughly [INAUDIBLE] how many there are by predicting values,00:04:55,12,In E step roughly INAUDIBLE many predicting values
00:05:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,of useful hidden variables that we would use to simplify the estimation.,00:05:00,12,useful hidden variables would use simplify estimation
00:05:10,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"In our case, this is the distribution that has been used to generate the word.",00:05:05,12,In case distribution used generate word
00:05:15,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,In the M-step then we would exploit such augmented data which would make,00:05:10,12,In M step would exploit augmented data would make
00:05:20,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"it easier to estimate the distribution, to improve the estimate of parameters.",00:05:15,12,easier estimate distribution improve estimate parameters
00:05:24,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,Here improve is guaranteed in terms of the likelihood function.,00:05:20,12,Here improve guaranteed terms likelihood function
00:05:30,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,Note that it's not necessary that we will have a stable convergence of,00:05:24,12,Note necessary stable convergence
00:05:35,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,parameter value even though the likelihood function is ensured to increase.,00:05:30,12,parameter value even though likelihood function ensured increase
00:05:40,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,There are some properties that have to be satisfied in order for the parameters,00:05:35,12,There properties satisfied order parameters
00:05:44,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,also to convert into some stable value.,00:05:40,12,also convert stable value
00:05:50,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,Now here data augmentation is done probabilistically.,00:05:47,12,Now data augmentation done probabilistically
00:05:51,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,"That means,",00:05:50,12,That means
00:05:54,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,we're not going to just say exactly what's the value of a hidden variable.,00:05:51,12,going say exactly value hidden variable
00:05:59,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,But we're going to have a probability distribution over the possible values of,00:05:54,12,But going probability distribution possible values
00:06:01,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,these hidden variables.,00:05:59,12,hidden variables
00:06:05,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,So this causes a split of counts of events probabilistically.,00:06:01,12,So causes split counts events probabilistically
00:06:12,3,Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3,2.12,And in our case we'll split the word counts between the two distributions.,00:06:07,12,And case split word counts two distributions
00:00:07,1,Course Prerequisites & Completion,,[SOUND] This,00:00:00,2,SOUND This
00:00:10,1,Course Prerequisites & Completion,,lecture is a brief introduction to the course.,00:00:07,2,lecture brief introduction course
00:00:17,1,Course Prerequisites & Completion,,"We're going to cover the objectives of the course, the prerequisites and",00:00:12,2,We going cover objectives course prerequisites
00:00:22,1,Course Prerequisites & Completion,,"course formats, reference books and how to complete the course.",00:00:17,2,course formats reference books complete course
00:00:25,1,Course Prerequisites & Completion,,The objectives of the course are the following.,00:00:22,2,The objectives course following
00:00:29,1,Course Prerequisites & Completion,,"First, we would like to cover the basic context and",00:00:25,2,First would like cover basic context
00:00:32,1,Course Prerequisites & Completion,,practical techniques of text data mining.,00:00:29,2,practical techniques text data mining
00:00:40,1,Course Prerequisites & Completion,,"So this means we will not be able to cover some advanced techniques in detail,",00:00:32,2,So means able cover advanced techniques detail
00:00:44,1,Course Prerequisites & Completion,,but whether we choose the practical use for,00:00:40,2,whether choose practical use
00:00:48,1,Course Prerequisites & Completion,,techniques and then treat them in order.,00:00:44,2,techniques treat order
00:00:53,1,Course Prerequisites & Completion,,We're going to also cover the basic concepts that are very useful for,00:00:48,2,We going also cover basic concepts useful
00:00:54,1,Course Prerequisites & Completion,,many applications.,00:00:53,2,many applications
00:00:59,1,Course Prerequisites & Completion,,The second objective is to cover more general techniques for,00:00:54,2,The second objective cover general techniques
00:01:05,1,Course Prerequisites & Completion,,"text or data mining, so we emphasize the coverage of general",00:00:59,2,text data mining emphasize coverage general
00:01:12,1,Course Prerequisites & Completion,,techniques that can be applicable to any text in any natural language.,00:01:05,2,techniques applicable text natural language
00:01:16,1,Course Prerequisites & Completion,,We also hope that these techniques to either,00:01:12,2,We also hope techniques either
00:01:22,1,Course Prerequisites & Completion,,automatically work on problems without any human effort or,00:01:16,2,automatically work problems without human effort
00:01:26,1,Course Prerequisites & Completion,,only requiring minimum human effort.,00:01:22,2,requiring minimum human effort
00:01:30,1,Course Prerequisites & Completion,,So these criteria have helped others to choose,00:01:26,2,So criteria helped others choose
00:01:36,1,Course Prerequisites & Completion,,techniques that can be applied to many applications.,00:01:30,2,techniques applied many applications
00:01:42,1,Course Prerequisites & Completion,,"This is in contrast to some more detailed analysis of text data,",00:01:36,2,This contrast detailed analysis text data
00:01:46,1,Course Prerequisites & Completion,,particularly using natural language processing techniques.,00:01:42,2,particularly using natural language processing techniques
00:01:49,1,Course Prerequisites & Completion,,Now such techniques are also very important.,00:01:46,2,Now techniques also important
00:01:54,1,Course Prerequisites & Completion,,"And they are indeed, necessary for some of the applications,",00:01:49,2,And indeed necessary applications
00:02:01,1,Course Prerequisites & Completion,,"where we would like to go in-depth to understand text, they are in more detail.",00:01:54,2,would like go depth understand text detail
00:02:05,1,Course Prerequisites & Completion,,"Such detail in understanding techniques, however,",00:02:01,2,Such detail understanding techniques however
00:02:11,1,Course Prerequisites & Completion,,are generally not scalable and they tend to require a lot of human effort.,00:02:05,2,generally scalable tend require lot human effort
00:02:14,1,Course Prerequisites & Completion,,So they cannot be easy to apply to any domain.,00:02:11,2,So cannot easy apply domain
00:02:18,1,Course Prerequisites & Completion,,"So as you can imagine in practice,",00:02:15,2,So imagine practice
00:02:23,1,Course Prerequisites & Completion,,it would be beneficial to combine both kinds of techniques using,00:02:18,2,would beneficial combine kinds techniques using
00:02:29,1,Course Prerequisites & Completion,,the general techniques that we'll be covering in this course as a basis and,00:02:23,2,general techniques covering course basis
00:02:35,1,Course Prerequisites & Completion,,improve these techniques by using more human effort whenever it's appropriate.,00:02:29,2,improve techniques using human effort whenever appropriate
00:02:42,1,Course Prerequisites & Completion,,We also would like to provide a hands-on experience to you in multiple aspects.,00:02:35,2,We also would like provide hands experience multiple aspects
00:02:48,1,Course Prerequisites & Completion,,"First, you'll do some experiments using a text mining toolkit and",00:02:42,2,First experiments using text mining toolkit
00:02:51,1,Course Prerequisites & Completion,,implementing text mining algorithms.,00:02:48,2,implementing text mining algorithms
00:02:57,1,Course Prerequisites & Completion,,"Second, you will have opportunity to experiment with some algorithms for",00:02:51,2,Second opportunity experiment algorithms
00:03:01,1,Course Prerequisites & Completion,,text mining and analytics to try them on some datasets and,00:02:57,2,text mining analytics try datasets
00:03:04,1,Course Prerequisites & Completion,,to understand how to do experiments.,00:03:01,2,understand experiments
00:03:10,1,Course Prerequisites & Completion,,"And finally, you have opportunity to participate in a competition",00:03:05,2,And finally opportunity participate competition
00:03:12,1,Course Prerequisites & Completion,,of text-based prediction task.,00:03:10,2,text based prediction task
00:03:15,1,Course Prerequisites & Completion,,You're expected to know the basic concepts of computer science.,00:03:12,2,You expected know basic concepts computer science
00:03:19,1,Course Prerequisites & Completion,,"For example, the data structures and",00:03:15,2,For example data structures
00:03:25,1,Course Prerequisites & Completion,,some other really basic concepts in computer science.,00:03:19,2,really basic concepts computer science
00:03:29,1,Course Prerequisites & Completion,,You are also expected to be familiar with programming and,00:03:25,2,You also expected familiar programming
00:03:33,1,Course Prerequisites & Completion,,"comfortable with programming, particularly with C++.",00:03:29,2,comfortable programming particularly C
00:03:36,1,Course Prerequisites & Completion,,"This course, however is not about programming.",00:03:33,2,This course however programming
00:03:39,1,Course Prerequisites & Completion,,"So you are not expected to do a lot of coding, but",00:03:36,2,So expected lot coding
00:03:44,1,Course Prerequisites & Completion,,we're going to give you C++ toolkit that's fairly sophisticated.,00:03:39,2,going give C toolkit fairly sophisticated
00:03:49,1,Course Prerequisites & Completion,,So you have to be comfortable with handling such a toolkit and,00:03:44,2,So comfortable handling toolkit
00:03:53,1,Course Prerequisites & Completion,,you may be asked to write a small amount of code.,00:03:49,2,may asked write small amount code
00:04:00,1,Course Prerequisites & Completion,,It's also useful if you know some concepts and,00:03:56,2,It also useful know concepts
00:04:07,1,Course Prerequisites & Completion,,"techniques in probability and statistics, but it's not necessary.",00:04:00,2,techniques probability statistics necessary
00:04:13,1,Course Prerequisites & Completion,,Knowing such knowledge would help you understand some of the algorithm in,00:04:08,2,Knowing knowledge would help understand algorithm
00:04:14,1,Course Prerequisites & Completion,,more depth.,00:04:13,2,depth
00:04:23,1,Course Prerequisites & Completion,,The format of the course is lectures plus quizzes that will be given to you,00:04:16,2,The format course lectures plus quizzes given
00:04:31,1,Course Prerequisites & Completion,,in the regular basis and there is also optional programming assignment.,00:04:25,2,regular basis also optional programming assignment
00:04:36,1,Course Prerequisites & Completion,,"Now, we've made programming assignments optional.",00:04:33,2,Now made programming assignments optional
00:04:40,1,Course Prerequisites & Completion,,"Not because it's not important, but",00:04:36,2,Not important
00:04:46,1,Course Prerequisites & Completion,,because we suspect that the not all of you will have the need for,00:04:40,2,suspect need
00:04:51,1,Course Prerequisites & Completion,,computing resources to do the program assignment.,00:04:46,2,computing resources program assignment
00:04:55,1,Course Prerequisites & Completion,,"So naturally, we would encourage all of you to try to do",00:04:51,2,So naturally would encourage try
00:05:01,1,Course Prerequisites & Completion,,"the program assignments, if possible as that will be a great way",00:04:55,2,program assignments possible great way
00:05:06,1,Course Prerequisites & Completion,,to learn about the knowledge that we teach in this course.,00:05:01,2,learn knowledge teach course
00:05:10,1,Course Prerequisites & Completion,,"There's no required reading for this course,",00:05:06,2,There required reading course
00:05:15,1,Course Prerequisites & Completion,,but I was list some of the useful reference books here.,00:05:10,2,I list useful reference books
00:05:22,1,Course Prerequisites & Completion,,So we expect you to be able to understand all the essential materials by just,00:05:18,2,So expect able understand essential materials
00:05:27,1,Course Prerequisites & Completion,,watching the actual videos and you should be able to answer all the quiz,00:05:22,2,watching actual videos able answer quiz
00:05:32,1,Course Prerequisites & Completion,,questions by just watching the videos.,00:05:27,2,questions watching videos
00:05:38,1,Course Prerequisites & Completion,,"But it's always good to read additional books in the larger scope of knowledge,",00:05:33,2,But always good read additional books larger scope knowledge
00:05:43,1,Course Prerequisites & Completion,,so here is this the four books.,00:05:39,2,four books
00:05:47,1,Course Prerequisites & Completion,,The first is a textbook about statistical language processing.,00:05:43,2,The first textbook statistical language processing
00:05:52,1,Course Prerequisites & Completion,,Some of the chapters [INAUDIBLE] are especially relevant to this course.,00:05:47,2,Some chapters INAUDIBLE especially relevant course
00:05:57,1,Course Prerequisites & Completion,,"The second one is a textbook about information retrieval,",00:05:52,2,The second one textbook information retrieval
00:06:01,1,Course Prerequisites & Completion,,but it has broadly covered a number of techniques that,00:05:57,2,broadly covered number techniques
00:06:05,1,Course Prerequisites & Completion,,are really in the category of text mining techniques.,00:06:01,2,really category text mining techniques
00:06:10,1,Course Prerequisites & Completion,,"So it's also useful, because of that.",00:06:05,2,So also useful
00:06:16,1,Course Prerequisites & Completion,,The third book is actually a collection of silly articles and,00:06:10,2,The third book actually collection silly articles
00:06:20,1,Course Prerequisites & Completion,,it has broadly covered all the aspects of mining text data.,00:06:16,2,broadly covered aspects mining text data
00:06:26,1,Course Prerequisites & Completion,,The mostly relevant chapters are also listed here.,00:06:21,2,The mostly relevant chapters also listed
00:06:31,1,Course Prerequisites & Completion,,"In these chapters, you can find some in depth discussion of cutting",00:06:26,2,In chapters find depth discussion cutting
00:06:36,1,Course Prerequisites & Completion,,edge research on the topics that we discussed in this course.,00:06:31,2,edge research topics discussed course
00:06:40,1,Course Prerequisites & Completion,,And the last one is actually a book that Sean Massung and,00:06:36,2,And last one actually book Sean Massung
00:06:45,1,Course Prerequisites & Completion,,I are currently writing and we're going to make the rough,00:06:40,2,I currently writing going make rough
00:06:51,1,Course Prerequisites & Completion,,draft chapters available at this URL listed right here.,00:06:45,2,draft chapters available URL listed right
00:06:54,1,Course Prerequisites & Completion,,You can also find additional reference books and,00:06:51,2,You also find additional reference books
00:06:58,1,Course Prerequisites & Completion,,other readings at the URL listed at the bottom.,00:06:54,2,readings URL listed bottom
00:07:02,1,Course Prerequisites & Completion,,"So finally, some information about how",00:06:59,2,So finally information
00:07:07,1,Course Prerequisites & Completion,,to complete the course this information is also on the web.,00:07:02,2,complete course information also web
00:07:09,1,Course Prerequisites & Completion,,So I just briefly go over it and,00:07:07,2,So I briefly go
00:07:15,1,Course Prerequisites & Completion,,you can complete the course by earning one of the following badges.,00:07:09,2,complete course earning one following badges
00:07:18,1,Course Prerequisites & Completion,,One is Course Achievement Badge.,00:07:15,2,One Course Achievement Badge
00:07:23,1,Course Prerequisites & Completion,,"To earn that, you have to have at least a 70%",00:07:18,2,To earn least 70
00:07:28,1,Course Prerequisites & Completion,,average score on all the quizzes combined.,00:07:23,2,average score quizzes combined
00:07:35,1,Course Prerequisites & Completion,,It does mean every quiz has to be 70% or better.,00:07:28,2,It mean every quiz 70 better
00:07:39,1,Course Prerequisites & Completion,,"The second batch here, this is a Course Mastery Badge and",00:07:35,2,The second batch Course Mastery Badge
00:07:44,1,Course Prerequisites & Completion,,"this just requires a higher score, 90% average score for the quizzes.",00:07:39,2,requires higher score 90 average score quizzes
00:07:52,1,Course Prerequisites & Completion,,There are also three optional programming badges.,00:07:48,2,There also three optional programming badges
00:07:56,1,Course Prerequisites & Completion,,"I said earlier that we encourage you to do programming assignments, but",00:07:52,2,I said earlier encourage programming assignments
00:07:59,1,Course Prerequisites & Completion,,"they're not necessary, they're not required.",00:07:56,2,necessary required
00:08:02,1,Course Prerequisites & Completion,,The first is Programming Achievement Badge.,00:07:59,2,The first Programming Achievement Badge
00:08:05,1,Course Prerequisites & Completion,,This is similar to the call switching from the badge.,00:08:02,2,This similar call switching badge
00:08:12,1,Course Prerequisites & Completion,,Here would require you to get at least 70% average score on programming assignments.,00:08:05,2,Here would require get least 70 average score programming assignments
00:08:17,1,Course Prerequisites & Completion,,"And similarly, the mastery badge",00:08:12,2,And similarly mastery badge
00:08:22,1,Course Prerequisites & Completion,,is given to those who can score,00:08:17,2,given score
00:08:26,1,Course Prerequisites & Completion,,90% average score or better.,00:08:22,2,90 average score better
00:08:33,1,Course Prerequisites & Completion,,The last badge is a Text Mining Competition Leader Badge and,00:08:26,2,The last badge Text Mining Competition Leader Badge
00:08:39,1,Course Prerequisites & Completion,,this is given to those of you who do well in the competition task.,00:08:33,2,given well competition task
00:08:44,1,Course Prerequisites & Completion,,"And specifically, we're planning to give",00:08:39,2,And specifically planning give
00:08:49,1,Course Prerequisites & Completion,,the badge to the top 30% in the leaderboard.,00:08:44,2,badge top 30 leaderboard
00:00:07,5,Contextual Text Mining- Motivation,4.7,[SOUND] This,00:00:00,7,SOUND This
00:00:09,5,Contextual Text Mining- Motivation,4.7,lecture is about the contextual text mining.,00:00:07,7,lecture contextual text mining
00:00:14,5,Contextual Text Mining- Motivation,4.7,Contextual text mining is related to multiple,00:00:11,7,Contextual text mining related multiple
00:00:18,5,Contextual Text Mining- Motivation,4.7,"kinds of knowledge that we mine from text data, as I'm showing here.",00:00:14,7,kinds knowledge mine text data I showing
00:00:23,5,Contextual Text Mining- Motivation,4.7,"It's related to topic mining because you can make topics associated with context,",00:00:18,7,It related topic mining make topics associated context
00:00:25,5,Contextual Text Mining- Motivation,4.7,like time or location.,00:00:23,7,like time location
00:00:29,5,Contextual Text Mining- Motivation,4.7,"And similarly, we can make opinion mining more contextualized,",00:00:25,7,And similarly make opinion mining contextualized
00:00:32,5,Contextual Text Mining- Motivation,4.7,making opinions connected to context.,00:00:29,7,making opinions connected context
00:00:38,5,Contextual Text Mining- Motivation,4.7,It's related to text based prediction because it allows us to combine non-text,00:00:34,7,It related text based prediction allows us combine non text
00:00:43,5,Contextual Text Mining- Motivation,4.7,data with text data to derive sophisticated predictors for,00:00:38,7,data text data derive sophisticated predictors
00:00:45,5,Contextual Text Mining- Motivation,4.7,the prediction problem.,00:00:43,7,prediction problem
00:00:49,5,Contextual Text Mining- Motivation,4.7,"So more specifically, why are we interested in contextual text mining?",00:00:45,7,So specifically interested contextual text mining
00:00:54,5,Contextual Text Mining- Motivation,4.7,"Well, that's first because text often has rich context information.",00:00:49,7,Well first text often rich context information
00:01:01,5,Contextual Text Mining- Motivation,4.7,"And this can include direct context such as meta-data, and also indirect context.",00:00:54,7,And include direct context meta data also indirect context
00:01:06,5,Contextual Text Mining- Motivation,4.7,"So, the direct context can grow the meta-data such as time,",00:01:01,7,So direct context grow meta data time
00:01:10,5,Contextual Text Mining- Motivation,4.7,"location, authors, and source of the text data.",00:01:06,7,location authors source text data
00:01:12,5,Contextual Text Mining- Motivation,4.7,And they're almost always available to us.,00:01:10,7,And almost always available us
00:01:19,5,Contextual Text Mining- Motivation,4.7,Indirect context refers to additional data related to the meta-data.,00:01:14,7,Indirect context refers additional data related meta data
00:01:24,5,Contextual Text Mining- Motivation,4.7,"So for example, from office, we can further obtain additional",00:01:19,7,So example office obtain additional
00:01:29,5,Contextual Text Mining- Motivation,4.7,"context such as social network of the author, or the author's age.",00:01:24,7,context social network author author age
00:01:34,5,Contextual Text Mining- Motivation,4.7,"Such information is not in general directly related to the text, yet",00:01:30,7,Such information general directly related text yet
00:01:37,5,Contextual Text Mining- Motivation,4.7,"through the process, we can connect them.",00:01:34,7,process connect
00:01:41,5,Contextual Text Mining- Motivation,4.7,"There could be other text data from the same source,",00:01:37,7,There could text data source
00:01:46,5,Contextual Text Mining- Motivation,4.7,as this one through the other text can be connected with this text as well.,00:01:41,7,one text connected text well
00:01:51,5,Contextual Text Mining- Motivation,4.7,"So in general, any related data can be regarded as context.",00:01:46,7,So general related data regarded context
00:01:53,5,Contextual Text Mining- Motivation,4.7,So there could be removed or rated for context.,00:01:51,7,So could removed rated context
00:01:56,5,Contextual Text Mining- Motivation,4.7,And so what's the use?,00:01:55,7,And use
00:01:59,5,Contextual Text Mining- Motivation,4.7,What is text context used for?,00:01:56,7,What text context used
00:02:05,5,Contextual Text Mining- Motivation,4.7,"Well, context can be used to partition text data in many interesting ways.",00:01:59,7,Well context used partition text data many interesting ways
00:02:11,5,Contextual Text Mining- Motivation,4.7,It can almost allow us to partition text data in other ways as we need.,00:02:05,7,It almost allow us partition text data ways need
00:02:14,5,Contextual Text Mining- Motivation,4.7,And this is very important because this allows,00:02:11,7,And important allows
00:02:18,5,Contextual Text Mining- Motivation,4.7,us to do interesting comparative analyses.,00:02:14,7,us interesting comparative analyses
00:02:21,5,Contextual Text Mining- Motivation,4.7,"It also in general, provides meaning to the discovered topics,",00:02:18,7,It also general provides meaning discovered topics
00:02:23,5,Contextual Text Mining- Motivation,4.7,if we associate the text with context.,00:02:21,7,associate text context
00:02:30,5,Contextual Text Mining- Motivation,4.7,So here's illustration of how context,00:02:25,7,So illustration context
00:02:35,5,Contextual Text Mining- Motivation,4.7,can be regarded as interesting ways of partitioning of text data.,00:02:30,7,regarded interesting ways partitioning text data
00:02:39,5,Contextual Text Mining- Motivation,4.7,So here I just showed some research papers published in different years.,00:02:35,7,So I showed research papers published different years
00:02:43,5,Contextual Text Mining- Motivation,4.7,"On different venues,",00:02:41,7,On different venues
00:02:48,5,Contextual Text Mining- Motivation,4.7,"different conference names here listed on the bottom like the SIGIR or ACL, etc.",00:02:43,7,different conference names listed bottom like SIGIR ACL etc
00:02:53,5,Contextual Text Mining- Motivation,4.7,Now such text data can be partitioned,00:02:49,7,Now text data partitioned
00:02:55,5,Contextual Text Mining- Motivation,4.7,in many interesting ways because we have context.,00:02:53,7,many interesting ways context
00:03:01,5,Contextual Text Mining- Motivation,4.7,So the context here just includes time and the conference venues.,00:02:56,7,So context includes time conference venues
00:03:04,5,Contextual Text Mining- Motivation,4.7,But perhaps we can include some other variables as well.,00:03:01,7,But perhaps include variables well
00:03:08,5,Contextual Text Mining- Motivation,4.7,But let's see how we can partition this interesting of ways.,00:03:06,7,But let see partition interesting ways
00:03:12,5,Contextual Text Mining- Motivation,4.7,"First, we can treat each paper as a separate unit.",00:03:08,7,First treat paper separate unit
00:03:17,5,Contextual Text Mining- Motivation,4.7,"So in this case, a paper ID and the, each paper has its own context.",00:03:12,7,So case paper ID paper context
00:03:22,5,Contextual Text Mining- Motivation,4.7,It's independent.,00:03:17,7,It independent
00:03:27,5,Contextual Text Mining- Motivation,4.7,But we can also treat all the papers within 1998 as one group and,00:03:22,7,But also treat papers within 1998 one group
00:03:32,5,Contextual Text Mining- Motivation,4.7,this is only possible because of the availability of time.,00:03:27,7,possible availability time
00:03:34,5,Contextual Text Mining- Motivation,4.7,And we can partition data in this way.,00:03:32,7,And partition data way
00:03:38,5,Contextual Text Mining- Motivation,4.7,"This would allow us to compare topics for example, in different years.",00:03:34,7,This would allow us compare topics example different years
00:03:42,5,Contextual Text Mining- Motivation,4.7,"Similarly, we can partition the data based on the menus.",00:03:39,7,Similarly partition data based menus
00:03:47,5,Contextual Text Mining- Motivation,4.7,We can get all the SIGIR papers and compare those papers with the rest.,00:03:42,7,We get SIGIR papers compare papers rest
00:03:51,5,Contextual Text Mining- Motivation,4.7,"Or compare SIGIR papers with KDD papers, with ACL papers.",00:03:47,7,Or compare SIGIR papers KDD papers ACL papers
00:03:58,5,Contextual Text Mining- Motivation,4.7,"We can also partition the data to obtain the papers written by authors in the U.S.,",00:03:52,7,We also partition data obtain papers written authors U S
00:04:03,5,Contextual Text Mining- Motivation,4.7,"and that of course, uses additional context of the authors.",00:03:58,7,course uses additional context authors
00:04:08,5,Contextual Text Mining- Motivation,4.7,And this would allow us to then compare such a subset with,00:04:03,7,And would allow us compare subset
00:04:12,5,Contextual Text Mining- Motivation,4.7,another set of papers written by also seen in other countries.,00:04:08,7,another set papers written also seen countries
00:04:17,5,Contextual Text Mining- Motivation,4.7,"Or we can obtain a set of papers about text mining, and",00:04:13,7,Or obtain set papers text mining
00:04:21,5,Contextual Text Mining- Motivation,4.7,this can be compared with papers about another topic.,00:04:17,7,compared papers another topic
00:04:25,5,Contextual Text Mining- Motivation,4.7,And note that these partitionings can be also,00:04:21,7,And note partitionings also
00:04:28,5,Contextual Text Mining- Motivation,4.7,intersected with each other to generate even more complicated partitions.,00:04:25,7,intersected generate even complicated partitions
00:04:34,5,Contextual Text Mining- Motivation,4.7,"And so in general, this enables discovery of knowledge associated with",00:04:29,7,And general enables discovery knowledge associated
00:04:35,5,Contextual Text Mining- Motivation,4.7,different context as needed.,00:04:34,7,different context needed
00:04:40,5,Contextual Text Mining- Motivation,4.7,"And in particular, we can compare different contexts.",00:04:37,7,And particular compare different contexts
00:04:43,5,Contextual Text Mining- Motivation,4.7,And this often gives us a lot of useful knowledge.,00:04:40,7,And often gives us lot useful knowledge
00:04:49,5,Contextual Text Mining- Motivation,4.7,"For example, comparing topics over time, we can see trends of topics.",00:04:43,7,For example comparing topics time see trends topics
00:04:53,5,Contextual Text Mining- Motivation,4.7,Comparing topics in different contexts can also reveal differences,00:04:49,7,Comparing topics different contexts also reveal differences
00:04:55,5,Contextual Text Mining- Motivation,4.7,about the two contexts.,00:04:53,7,two contexts
00:04:59,5,Contextual Text Mining- Motivation,4.7,So there are many interesting questions that require contextual text mining.,00:04:55,7,So many interesting questions require contextual text mining
00:05:01,5,Contextual Text Mining- Motivation,4.7,Here I list some very specific ones.,00:04:59,7,Here I list specific ones
00:05:05,5,Contextual Text Mining- Motivation,4.7,"For example, what topics have been getting increasing attention",00:05:01,7,For example topics getting increasing attention
00:05:07,5,Contextual Text Mining- Motivation,4.7,recently in data mining research?,00:05:05,7,recently data mining research
00:05:08,5,Contextual Text Mining- Motivation,4.7,"Now to answer this question,",00:05:07,7,Now answer question
00:05:11,5,Contextual Text Mining- Motivation,4.7,obviously we need to analyze text in the context of time.,00:05:08,7,obviously need analyze text context time
00:05:17,5,Contextual Text Mining- Motivation,4.7,So time is context in this case.,00:05:13,7,So time context case
00:05:20,5,Contextual Text Mining- Motivation,4.7,Is there any difference in the responses of people in different regions,00:05:17,7,Is difference responses people different regions
00:05:22,5,Contextual Text Mining- Motivation,4.7,"to the event, to any event?",00:05:20,7,event event
00:05:25,5,Contextual Text Mining- Motivation,4.7,So this is a very broad an answer to this question.,00:05:22,7,So broad answer question
00:05:28,5,Contextual Text Mining- Motivation,4.7,"In this case of course, location is the context.",00:05:25,7,In case course location context
00:05:31,5,Contextual Text Mining- Motivation,4.7,What are the common research interests of two researchers?,00:05:28,7,What common research interests two researchers
00:05:34,5,Contextual Text Mining- Motivation,4.7,"In this case, authors can be the context.",00:05:31,7,In case authors context
00:05:38,5,Contextual Text Mining- Motivation,4.7,Is there any difference in the research topics published by authors in the USA and,00:05:34,7,Is difference research topics published authors USA
00:05:39,5,Contextual Text Mining- Motivation,4.7,those outside?,00:05:38,7,outside
00:05:43,5,Contextual Text Mining- Motivation,4.7,"Now in this case, the context would include the authors and",00:05:39,7,Now case context would include authors
00:05:46,5,Contextual Text Mining- Motivation,4.7,their affiliation and location.,00:05:43,7,affiliation location
00:05:51,5,Contextual Text Mining- Motivation,4.7,So this goes beyond just the author himself or herself.,00:05:47,7,So goes beyond author
00:05:55,5,Contextual Text Mining- Motivation,4.7,We need to look at the additional information connected to the author.,00:05:51,7,We need look additional information connected author
00:05:58,5,Contextual Text Mining- Motivation,4.7,Is there any difference in the opinions of all the topics expressed on,00:05:55,7,Is difference opinions topics expressed
00:06:00,5,Contextual Text Mining- Motivation,4.7,one social network and another?,00:05:58,7,one social network another
00:06:04,5,Contextual Text Mining- Motivation,4.7,"In this case, the social network of authors and the topic can be a context.",00:06:00,7,In case social network authors topic context
00:06:10,5,Contextual Text Mining- Motivation,4.7,Other topics in news data that are correlated with sudden changes in,00:06:06,7,Other topics news data correlated sudden changes
00:06:11,5,Contextual Text Mining- Motivation,4.7,stock prices.,00:06:10,7,stock prices
00:06:16,5,Contextual Text Mining- Motivation,4.7,"In this case, we can use a time series such as stock prices as context.",00:06:11,7,In case use time series stock prices context
00:06:20,5,Contextual Text Mining- Motivation,4.7,"What issues mattered in the 2012 presidential campaign, or",00:06:17,7,What issues mattered 2012 presidential campaign
00:06:21,5,Contextual Text Mining- Motivation,4.7,presidential election?,00:06:20,7,presidential election
00:06:26,5,Contextual Text Mining- Motivation,4.7,"Now in this case, time serves again as context.",00:06:21,7,Now case time serves context
00:06:29,5,Contextual Text Mining- Motivation,4.7,"So, as you can see, the list can go on and on.",00:06:26,7,So see list go
00:06:34,5,Contextual Text Mining- Motivation,4.7,"Basically, contextual text mining can have many applications.",00:06:29,7,Basically contextual text mining many applications
00:00:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,[SOUND].,00:00:00,12,SOUND
00:00:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,This lecture is about the syntagmatic relation discovery and mutual information.,00:00:07,12,This lecture syntagmatic relation discovery mutual information
00:00:18,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,In this lecture we are going to continue discussing syntagmatic relation discovery.,00:00:13,12,In lecture going continue discussing syntagmatic relation discovery
00:00:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In particular, we are going to talk about another",00:00:18,12,In particular going talk another
00:00:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the concept in the information series, we called it mutual information and",00:00:20,12,concept information series called mutual information
00:00:28,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,how it can be used to discover syntagmatic relations.,00:00:24,12,used discover syntagmatic relations
00:00:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Before we talked about the problem of conditional entropy and,00:00:28,12,Before talked problem conditional entropy
00:00:38,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,that is the conditional entropy computed different pairs of words.,00:00:32,12,conditional entropy computed different pairs words
00:00:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"It is not really comparable, so that makes it harder with this cover,",00:00:38,12,It really comparable makes harder cover
00:00:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,strong synagmatic relations globally from corpus.,00:00:42,12,strong synagmatic relations globally corpus
00:00:53,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So now we are going to introduce mutual information, which is another concept",00:00:48,12,So going introduce mutual information another concept
00:00:57,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"in the information series that allows us to, sometimes,",00:00:53,12,information series allows us sometimes
00:01:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,normalize the conditional entropy to make it more comparable across different pairs.,00:00:57,12,normalize conditional entropy make comparable across different pairs
00:01:10,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In particular, mutual information in order to find I(X:Y),",00:01:04,12,In particular mutual information order find I X Y
00:01:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,matches the entropy reduction of X obtained from knowing Y.,00:01:10,12,matches entropy reduction X obtained knowing Y
00:01:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,More specifically the question we are interested in here is how much,00:01:17,12,More specifically question interested much
00:01:25,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,of an entropy of X can we obtain by knowing Y.,00:01:22,12,entropy X obtain knowing Y
00:01:31,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So mathematically it can be defined as the difference between,00:01:27,12,So mathematically defined difference
00:01:36,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the original entropy of X, and the condition of Y of X given Y.",00:01:31,12,original entropy X condition Y X given Y
00:01:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And you might see, as you can see here it can also be defined",00:01:37,12,And might see see also defined
00:01:47,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,as reduction of entropy of Y because of knowing X.,00:01:42,12,reduction entropy Y knowing X
00:01:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Now normally the two conditional interface H of X given Y and,00:01:48,12,Now normally two conditional interface H X given Y
00:01:58,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the entropy of Y given X are not equal, but interestingly,",00:01:54,12,entropy Y given X equal interestingly
00:02:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the reduction of entropy by knowing one of them, is actually equal.",00:01:58,12,reduction entropy knowing one actually equal
00:02:12,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, this quantity is called a Mutual Information in order to buy I here.",00:02:05,12,So quantity called Mutual Information order buy I
00:02:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And this function has some interesting properties, first it is also non-negative.",00:02:12,12,And function interesting properties first also non negative
00:02:21,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,This is easy to understand because the original entropy is always,00:02:17,12,This easy understand original entropy always
00:02:29,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,not going to be lower than the possibility reduced conditional entropy.,00:02:22,12,going lower possibility reduced conditional entropy
00:02:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In other words, the conditional entropy will never exceed the original entropy.",00:02:29,12,In words conditional entropy never exceed original entropy
00:02:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Knowing some information can always help us potentially, but",00:02:33,12,Knowing information always help us potentially
00:02:40,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,will not hurt us in predicting x.,00:02:37,12,hurt us predicting x
00:02:46,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,The signal property is that it is symmetric like additional,00:02:41,12,The signal property symmetric like additional
00:02:51,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"entropy is not symmetrical, mutual information is, and",00:02:46,12,entropy symmetrical mutual information
00:02:56,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the third property is that It reaches its minimum, zero, if and",00:02:51,12,third property It reaches minimum zero
00:03:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,only if the two random variables are completely independent.,00:02:56,12,two random variables completely independent
00:03:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,That means knowing one of them does not tell us anything about the other and,00:03:01,12,That means knowing one tell us anything
00:03:14,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,this last property can be verified by simply looking at the equation above and,00:03:07,12,last property verified simply looking equation
00:03:19,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,it reaches 0 if and only the conditional entropy of X,00:03:14,12,reaches 0 conditional entropy X
00:03:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,[INAUDIBLE] Y is exactly the same as original entropy of X.,00:03:19,12,INAUDIBLE Y exactly original entropy X
00:03:28,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So that means knowing why it did not help at all and that is when X and,00:03:24,12,So means knowing help X
00:03:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,a Y are completely independent.,00:03:28,12,Y completely independent
00:03:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Now when we fix X to rank different Ys using conditional entropy,00:03:32,12,Now fix X rank different Ys using conditional entropy
00:03:44,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,would give the same order as ranking based on mutual information,00:03:37,12,would give order ranking based mutual information
00:03:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"because in the function here, H(X) is fixed because X is fixed.",00:03:44,12,function H X fixed X fixed
00:03:53,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So ranking based on mutual entropy is exactly the same as ranking based on,00:03:49,12,So ranking based mutual entropy exactly ranking based
00:03:57,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the conditional entropy of X given Y, but",00:03:53,12,conditional entropy X given Y
00:04:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the mutual information allows us to compare different pairs of x and y.,00:03:57,12,mutual information allows us compare different pairs x
00:04:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, that is why mutual information is more general and in general, more useful.",00:04:03,12,So mutual information general general useful
00:04:14,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, let us examine the intuition of using mutual information for",00:04:10,12,So let us examine intuition using mutual information
00:04:15,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Syntagmatical Relation Mining.,00:04:14,12,Syntagmatical Relation Mining
00:04:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Now, the question we ask forcing that relation mining is,",00:04:17,12,Now question ask forcing relation mining
00:04:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"whenever ""eats"" occurs, what other words also tend to occur?",00:04:20,12,whenever eats occurs words also tend occur
00:04:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So this question can be framed as a mutual information question, that is,",00:04:25,12,So question framed mutual information question
00:04:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"which words have high mutual information was eats,",00:04:30,12,words high mutual information eats
00:04:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,so computer the missing information between eats and other words.,00:04:33,12,computer missing information eats words
00:04:44,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And if we do that, and it is basically a base on the same as conditional",00:04:39,12,And basically base conditional
00:04:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"we will see that words that are strongly associated with eats,",00:04:44,12,see words strongly associated eats
00:04:50,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,will have a high point.,00:04:48,12,high point
00:04:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Whereas words that are not related will have lower mutual information.,00:04:50,12,Whereas words related lower mutual information
00:04:58,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"For this, I will give some example here.",00:04:55,12,For I give example
00:05:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"The mutual information between ""eats"" and ""meats"",",00:04:58,12,The mutual information eats meats
00:05:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"which is the same as between ""meats"" and ""eats,"" because the information is",00:05:01,12,meats eats information
00:05:10,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,symmetrical is expected to be higher than the mutual information between eats and,00:05:05,12,symmetrical expected higher mutual information eats
00:05:14,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the, because knowing the does not really help us as a predictor.",00:05:10,12,knowing really help us predictor
00:05:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"It is similar, and knowing eats does not help us predicting,",00:05:14,12,It similar knowing eats help us predicting
00:05:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the as well.,00:05:17,12,well
00:05:26,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,And you also can easily see that the mutual,00:05:22,12,And also easily see mutual
00:05:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"information between a word and itself is the largest,",00:05:26,12,information word largest
00:05:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,which is equal to the entropy of this word and,00:05:32,12,equal entropy word
00:05:42,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"so, because in this case the reduction is",00:05:37,12,case reduction
00:05:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,maximum because knowing one allows us to predict the other completely.,00:05:42,12,maximum knowing one allows us predict completely
00:05:50,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So the conditional entropy is zero,",00:05:48,12,So conditional entropy zero
00:05:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,therefore the mutual information reaches its maximum.,00:05:50,12,therefore mutual information reaches maximum
00:06:02,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"It is going to be larger, then are equal to the machine volume eats in other words.",00:05:54,12,It going larger equal machine volume eats words
00:06:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,In other words picking any other word and,00:06:02,12,In words picking word
00:06:08,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the computer picking between eats and that word.,00:06:05,12,computer picking eats word
00:06:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,You will not get any information larger the computation from eats and itself.,00:06:08,12,You get information larger computation eats
00:06:21,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So now let us look at how to compute the mute information.,00:06:16,12,So let us look compute mute information
00:06:23,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Now in order to do that, we often",00:06:21,12,Now order often
00:06:29,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"use a different form of mutual information, and we can mathematically",00:06:25,12,use different form mutual information mathematically
00:06:34,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,rewrite the mutual information into the form shown on this slide.,00:06:29,12,rewrite mutual information form shown slide
00:06:38,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Where we essentially see a formula that computes what is,00:06:34,12,Where essentially see formula computes
00:06:43,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,called a KL-divergence or divergence.,00:06:38,12,called KL divergence divergence
00:06:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,This is another term in information theory.,00:06:43,12,This another term information theory
00:06:48,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,It measures the divergence between two distributions.,00:06:45,12,It measures divergence two distributions
00:06:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Now, if you look at the formula, it is also sum over many combinations of",00:06:50,12,Now look formula also sum many combinations
00:06:58,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"different values of the two random variables but inside the sum,",00:06:54,12,different values two random variables inside sum
00:07:04,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,mainly we are doing a comparison between two joint distributions.,00:06:58,12,mainly comparison two joint distributions
00:07:06,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"The numerator has the joint,",00:07:04,12,The numerator joint
00:07:11,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,actual observed the joint distribution of the two random variables.,00:07:06,12,actual observed joint distribution two random variables
00:07:15,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,The bottom part or the denominator can be,00:07:12,12,The bottom part denominator
00:07:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"interpreted as the expected joint distribution of the two random variables,",00:07:15,12,interpreted expected joint distribution two random variables
00:07:26,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"if they were independent because when two random variables are independent,",00:07:20,12,independent two random variables independent
00:07:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,they are joined distribution is equal to the product of the two probabilities.,00:07:26,12,joined distribution equal product two probabilities
00:07:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So this comparison will tell us whether the two variables are indeed independent.,00:07:35,12,So comparison tell us whether two variables indeed independent
00:07:43,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"If they are indeed independent then we would expect that the two are the same,",00:07:39,12,If indeed independent would expect two
00:07:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"but if the numerator is different from the denominator, that would mean",00:07:44,12,numerator different denominator would mean
00:07:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the two variables are not independent and that helps measure the association.,00:07:49,12,two variables independent helps measure association
00:08:00,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,The sum is simply to take into consideration of all of the combinations,00:07:56,12,The sum simply take consideration combinations
00:08:04,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,of the values of these two random variables.,00:08:00,12,values two random variables
00:08:08,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In our case, each random variable can choose one of the two values,",00:08:04,12,In case random variable choose one two values
00:08:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"zero or one, so we have four combinations here.",00:08:08,12,zero one four combinations
00:08:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"If we look at this form of mutual information, it shows that the mutual",00:08:13,12,If look form mutual information shows mutual
00:08:21,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,information matches the divergence of the actual joint distribution,00:08:17,12,information matches divergence actual joint distribution
00:08:25,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,from the expected distribution under the independence assumption.,00:08:21,12,expected distribution independence assumption
00:08:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"The larger this divergence is, the higher the mutual information would be.",00:08:25,12,The larger divergence higher mutual information would
00:08:37,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So now let us further look at what are exactly the probabilities,",00:08:33,12,So let us look exactly probabilities
00:08:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,involved in this formula of mutual information.,00:08:37,12,involved formula mutual information
00:08:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And here, this is all the probabilities involve, and it is easy for",00:08:41,12,And probabilities involve easy
00:08:46,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,you to verify that.,00:08:45,12,verify
00:08:51,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Basically, we have first to [INAUDIBLE] probabilities",00:08:46,12,Basically first INAUDIBLE probabilities
00:08:56,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,corresponding to the presence or absence of each word.,00:08:51,12,corresponding presence absence word
00:08:59,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, for w1, we have two probabilities shown here.",00:08:56,12,So w1 two probabilities shown
00:09:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"They should sum to one, because a word can either be present or absent.",00:09:02,12,They sum one word either present absent
00:09:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In the segment, and similarly for",00:09:07,12,In segment similarly
00:09:18,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the second word, we also have two probabilities representing presence or",00:09:13,12,second word also two probabilities representing presence
00:09:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"absences of this word, and there is some to y as well.",00:09:18,12,absences word well
00:09:26,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And finally, we have a lot of joined probabilities that represent",00:09:21,12,And finally lot joined probabilities represent
00:09:31,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the scenarios of co-occurrences of the two words, and they are shown here.",00:09:26,12,scenarios co occurrences two words shown
00:09:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,And they sum to one because the two words can only have these four,00:09:34,12,And sum one two words four
00:09:41,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,possible scenarios.,00:09:39,12,possible scenarios
00:09:43,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Either they both occur, so",00:09:41,12,Either occur
00:09:49,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"in that case both variables will have a value of one, or one of them occurs.",00:09:43,12,case variables value one one occurs
00:09:50,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,There are two scenarios.,00:09:49,12,There two scenarios
00:09:55,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,In these two cases one of the random variables will be equal to one and,00:09:51,12,In two cases one random variables equal one
00:10:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the other will be zero and finally we have the scenario when none of them occurs.,00:09:55,12,zero finally scenario none occurs
00:10:06,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,This is when the two variables taking a value of zero.,00:10:03,12,This two variables taking value zero
00:10:12,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So these are the probabilities involved in the calculation of mutual information,",00:10:07,12,So probabilities involved calculation mutual information
00:10:13,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,over here.,00:10:12,12,
00:10:18,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"Once we know how to calculate these probabilities,",00:10:16,12,Once know calculate probabilities
00:10:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,we can easily calculate the new gene formation.,00:10:18,12,easily calculate new gene formation
00:10:28,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,It is also interesting to know that there are actually some relations or,00:10:24,12,It also interesting know actually relations
00:10:32,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"constraint among these probabilities, and we already saw two of them, right?",00:10:28,12,constraint among probabilities already saw two right
00:10:36,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So in the previous slide,",00:10:32,12,So previous slide
00:10:41,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,that you have seen that the marginal probabilities of these,00:10:36,12,seen marginal probabilities
00:10:46,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"words sum to one and we also have seen this constraint,",00:10:41,12,words sum one also seen constraint
00:10:53,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"that says the two words have these four scenarios of co-occurrency,",00:10:46,12,says two words four scenarios co occurrency
00:10:57,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,but we also have some additional constraints listed in the bottom.,00:10:53,12,also additional constraints listed bottom
00:11:03,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"For example, this one means if we add up",00:10:58,12,For example one means add
00:11:07,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the probabilities that we observe the two words occur together and,00:11:03,12,probabilities observe two words occur together
00:11:12,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the probabilities when the first word occurs and the second word does not occur.,00:11:07,12,probabilities first word occurs second word occur
00:11:16,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,We get exactly the probability that the first word is observed.,00:11:12,12,We get exactly probability first word observed
00:11:20,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"In other words, when the word is observed.",00:11:16,12,In words word observed
00:11:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"When the first word is observed, and",00:11:20,12,When first word observed
00:11:27,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"there are only two scenarios, depending on whether the second word is also observed.",00:11:22,12,two scenarios depending whether second word also observed
00:11:31,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, this probability captures the first scenario when the second word",00:11:27,12,So probability captures first scenario second word
00:11:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"actually is also observed, and",00:11:31,12,actually also observed
00:11:38,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,this captures the second scenario when the second word is not observed.,00:11:33,12,captures second scenario second word observed
00:11:40,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So, we only see the first word, and",00:11:38,12,So see first word
00:11:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,it is easy to see the other equations also follow the same reasoning.,00:11:40,12,easy see equations also follow reasoning
00:11:50,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Now these equations allow us to compute some probabilities based on,00:11:46,12,Now equations allow us compute probabilities based
00:11:54,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"other probabilities, and this can simplify the computation.",00:11:50,12,probabilities simplify computation
00:12:01,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"So more specifically, if we know the probability that",00:11:55,12,So specifically know probability
00:12:06,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"a word is present, like in this case, so if we know this, and",00:12:01,12,word present like case know
00:12:12,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"if we know the probability of the presence of the second word,",00:12:06,12,know probability presence second word
00:12:17,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"then we can easily compute the absence probability, right?",00:12:12,12,easily compute absence probability right
00:12:22,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"It is very easy to use this equation to do that, and so",00:12:17,12,It easy use equation
00:12:27,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,we take care of the computation of these probabilities of presence and,00:12:22,12,take care computation probabilities presence
00:12:29,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,absence of each word.,00:12:27,12,absence word
00:12:33,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Now let's look at the [INAUDIBLE] distribution.,00:12:29,12,Now let look INAUDIBLE distribution
00:12:36,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Let us assume that we also have available,00:12:33,12,Let us assume also available
00:12:39,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,the probability that they occurred together.,00:12:36,12,probability occurred together
00:12:44,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Now it is easy to see that we can actually compute all the rest of these,00:12:39,12,Now easy see actually compute rest
00:12:45,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,probabilities based on these.,00:12:44,12,probabilities based
00:12:51,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Specifically for example using this equation we can compute,00:12:46,12,Specifically example using equation compute
00:12:56,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"the probability that the first word occurred and the second word did not,",00:12:51,12,probability first word occurred second word
00:13:02,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"because we know these probabilities in the boxes, and similarly using this",00:12:56,12,know probabilities boxes similarly using
00:13:05,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,equation we can compute the probability that we observe only the second word.,00:13:02,12,equation compute probability observe second word
00:13:06,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,Word.,00:13:05,12,Word
00:13:10,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"And then finally, this probability can be calculated",00:13:06,12,And finally probability calculated
00:13:14,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"by using this equation because now this is known, and",00:13:10,12,using equation known
00:13:19,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"this is also known, and this is already known, right.",00:13:14,12,also known already known right
00:13:23,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So this can be easier to calculate.,00:13:19,12,So easier calculate
00:13:24,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So now this can be calculated.,00:13:23,12,So calculated
00:13:30,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,So this slide shows that we only need to know how to compute,00:13:26,12,So slide shows need know compute
00:13:35,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"these three probabilities that are shown in the boxes,",00:13:30,12,three probabilities shown boxes
00:13:43,2,Syntagmatic Relation Discovery- Mutual Information- Part 1,1.12,"naming the presence of each word and the co-occurence of both words, in a segment.",00:13:35,12,naming presence word co occurence words segment
